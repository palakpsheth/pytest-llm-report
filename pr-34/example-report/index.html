<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Test Report &bull; 28 tests</title>
    <!-- Optional: Inter font from rsms.me CDN. Falls back to system fonts if unavailable. -->
    <link rel="stylesheet" href="https://rsms.me/inter/inter.css">
    <style>
/* Modern Color Palette */
:root {
    --bg-color: #f8fafc;
    --text-primary: #1e293b;
    --text-secondary: #64748b;
    --border-color: #e2e8f0;
    --card-bg: #ffffff;
    --surface-muted: #f1f5f9;
    --primary-color: #3b82f6;
    color-scheme: light dark;

    /* Status Colors */
    --passed-bg: #dcfce7;
    --passed-text: #166534;
    --failed-bg: #fee2e2;
    --failed-text: #991b1b;
    --skipped-bg: #fef9c3;
    --skipped-text: #854d0e;
    --xfailed-bg: #ffedd5;
    --xfailed-text: #9a3412;
    --xpassed-bg: #f3e8ff;
    --xpassed-text: #6b21a8;
    --error-bg: #fee2e2;
    --error-text: #991b1b;
}

body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    background-color: var(--bg-color);
    color: var(--text-primary);
    line-height: 1.5;
    margin: 0;
    padding: 0;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 2rem;
}

/* Header */
header {
    margin-bottom: 2rem;
    border-bottom: 1px solid var(--border-color);
    padding-bottom: 1rem;
    display: flex;
    justify-content: space-between;
    align-items: center;
}

h1 {
    font-size: 1.875rem;
    font-weight: 700;
    color: var(--text-primary);
    margin: 0;
}

.meta {
    font-size: 0.875rem;
    color: var(--text-secondary);
}

/* Summary Grid */
.summary {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
    gap: 1rem;
    margin-bottom: 2rem;
}

.summary-card {
    background: var(--card-bg);
    border-radius: 0.5rem;
    padding: 1.5rem;
    box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.1), 0 1px 2px -1px rgb(0 0 0 / 0.1);
    text-align: center;
    border: 1px solid var(--border-color);
    transition: transform 0.2s;
}

.summary-card:hover {
    transform: translateY(-2px);
}

.summary-card .count {
    font-size: 2.25rem;
    font-weight: 700;
    line-height: 1;
    margin-bottom: 0.5rem;
}

.summary-card .label {
    text-transform: uppercase;
    font-size: 0.75rem;
    font-weight: 600;
    letter-spacing: 0.05em;
    color: var(--text-secondary);
}

/* Status Colors for Summary */
.summary-card.passed .count {
    color: var(--passed-text);
}

.summary-card.failed .count {
    color: var(--failed-text);
}

.summary-card.skipped .count {
    color: var(--skipped-text);
}

.summary-card.xfailed .count {
    color: var(--xfailed-text);
}

.summary-card.xpassed .count {
    color: var(--xpassed-text);
}

.summary-card.coverage .count {
    color: var(--primary-color);
}

/* Filters */
.filters {
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.5rem;
    border: 1px solid var(--border-color);
    margin-bottom: 1.5rem;
    display: flex;
    flex-direction: column;
    gap: 0.75rem;
}

.filter-input {
    flex: 1;
    padding: 0.5rem 1rem;
    border: 1px solid var(--border-color);
    border-radius: 0.375rem;
    font-size: 0.875rem;
    background: var(--card-bg);
    color: var(--text-primary);
}

.filter-input::placeholder {
    color: var(--text-secondary);
}

.filter-statuses {
    display: flex;
    flex-wrap: wrap;
    gap: 0.5rem;
}

.filter-chip {
    display: inline-flex;
    align-items: center;
    gap: 0.35rem;
    padding: 0.25rem 0.75rem;
    border-radius: 9999px;
    border: 1px solid var(--border-color);
    background: var(--surface-muted);
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.04em;
}

.filter-chip input {
    margin: 0;
}

.filter-chip.passed {
    background: var(--passed-bg);
    color: var(--passed-text);
}

.filter-chip.failed {
    background: var(--failed-bg);
    color: var(--failed-text);
}

.filter-chip.skipped {
    background: var(--skipped-bg);
    color: var(--skipped-text);
}

.filter-chip.xfailed {
    background: var(--xfailed-bg);
    color: var(--xfailed-text);
}

.filter-chip.xpassed {
    background: var(--xpassed-bg);
    color: var(--xpassed-text);
}

.filter-chip.error {
    background: var(--error-bg);
    color: var(--error-text);
}

/* Test List */
.test-list {
    display: flex;
    flex-direction: column;
    gap: 0.75rem;
}

.test-row {
    background: var(--card-bg);
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
    overflow: hidden;
}

.test-header {
    padding: 1rem;
    display: flex;
    align-items: center;
    gap: 1rem;
    cursor: pointer;
    background: var(--card-bg);
}

.test-header:hover {
    background: var(--surface-muted);
}

.status-badge {
    padding: 0.25rem 0.75rem;
    border-radius: 9999px;
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
}

.status-passed {
    background: var(--passed-bg);
    color: var(--passed-text);
}

.status-failed {
    background: var(--failed-bg);
    color: var(--failed-text);
}

.status-skipped {
    background: var(--skipped-bg);
    color: var(--skipped-text);
}

.status-xfailed {
    background: var(--xfailed-bg);
    color: var(--xfailed-text);
}

.status-xpassed {
    background: var(--xpassed-bg);
    color: var(--xpassed-text);
}

.status-error {
    background: var(--error-bg);
    color: var(--error-text);
}

.test-name {
    flex: 1;
    font-family: monospace;
    font-size: 0.9rem;
    color: var(--text-primary);
    word-break: break-all;
}

.test-meta {
    display: flex;
    gap: 1rem;
    align-items: center;
    color: var(--text-secondary);
    font-size: 0.875rem;
}

/* Details Section */
.test-details {
    padding: 0 1rem 1rem 1rem;
    border-top: 1px solid var(--border-color);
    background: var(--surface-muted);
}

.detail-section {
    margin-top: 1rem;
}

.detail-title {
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
    color: var(--text-secondary);
    margin-bottom: 0.5rem;
}

.coverage-item {
    font-family: monospace;
    font-size: 0.85rem;
    padding: 0.25rem 0;
    border-bottom: 1px solid var(--border-color);
    display: grid;
    grid-template-columns: minmax(200px, 2fr) minmax(120px, 1fr);
    gap: 1rem;
}

.coverage-list {
    background: var(--card-bg);
    border-radius: 0.375rem;
    border: 1px solid var(--border-color);
    overflow: hidden;
}

.source-coverage {
    margin-top: 2rem;
}

.source-coverage h2 {
    margin: 0 0 1rem;
    font-size: 1.5rem;
}

.source-coverage-table {
    display: grid;
    gap: 0.35rem;
}

.source-coverage-header,
.source-coverage-row {
    display: grid;
    grid-template-columns: minmax(200px, 2fr) repeat(4, minmax(60px, 0.5fr)) minmax(
            140px,
            1fr
        ) minmax(140px, 1fr);
    align-items: center;
    gap: 0.75rem;
    padding: 0.75rem 1rem;
    border-radius: 0.5rem;
}

.source-coverage-header {
    background: var(--surface-muted);
    font-size: 0.75rem;
    font-weight: 700;
    text-transform: uppercase;
    letter-spacing: 0.04em;
    color: var(--text-secondary);
}

.source-coverage-row {
    background: var(--card-bg);
    border: 1px solid var(--border-color);
    font-size: 0.85rem;
}

.source-path {
    font-family: monospace;
    word-break: break-word;
}

.source-lines {
    font-family: monospace;
    color: var(--text-secondary);
    word-break: break-word;
}

.llm-annotation {
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.375rem;
    border: 1px solid var(--border-color);
}

.llm-annotation p {
    margin: 0 0 0.5rem 0;
}

.llm-annotation p:last-child {
    margin-bottom: 0;
}

.llm-annotation ul {
    margin: 0.5rem 0 0;
    padding-left: 1.25rem;
}

.llm-annotation li {
    margin-bottom: 0.25rem;
}

.error-message {
    font-family: monospace;
    color: var(--failed-text);
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.375rem;
    border: 1px solid var(--failed-bg);
    white-space: pre-wrap;
    overflow-x: auto;
}

/* HTML5 Progress Bar for Coverage */
progress {
    width: 60px;
}

/* Utility: Hidden state for filtering */
.hidden {
    display: none !important;
}

/* Dark Mode Support */
@media (prefers-color-scheme: dark) {
    :root {
        --bg-color: #0f172a;
        --text-primary: #f1f5f9;
        --text-secondary: #94a3b8;
        --border-color: #334155;
        --card-bg: #1e293b;
        --surface-muted: #0b1220;
        --primary-color: #60a5fa;

        /* Status Colors - Adjusted for dark mode */
        --passed-bg: #14532d;
        --passed-text: #86efac;
        --failed-bg: #7f1d1d;
        --failed-text: #fca5a5;
        --skipped-bg: #713f12;
        --skipped-text: #fde047;
        --xfailed-bg: #7c2d12;
        --xfailed-text: #fdba74;
        --error-bg: #7f1d1d;
        --error-text: #fca5a5;
    }

    /* Adjust box shadows for dark mode */
    .summary-card {
        box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.3), 0 1px 2px -1px rgb(0 0 0 / 0.3);
    }
}

@media print {
    body {
        background: #ffffff;
        color: #0f172a;
    }

    .container {
        max-width: none;
        padding: 1rem 1.5rem;
    }

    header {
        border-bottom: 2px solid var(--border-color);
    }

    .filters {
        display: none;
    }

    .summary-card,
    .test-row {
        box-shadow: none;
    }

    .test-header {
        background: #ffffff;
    }

    .test-row {
        page-break-inside: avoid;
        break-inside: avoid;
    }

    .test-details {
        background: #ffffff;
    }

    .llm-annotation {
        background: var(--surface-muted);
    }

    progress {
        width: 80px;
    }
}

body.pdf-mode .filters {
    display: none;
}

body.pdf-mode .test-row {
    page-break-inside: avoid;
    break-inside: avoid;
}

/* TOC Styling */
.toc {
    margin-bottom: 2rem;
    padding: 1rem;
    background: var(--card-bg);
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
}
.toc ul {
    list-style: none;
    padding: 0;
    margin: 0;
    display: flex;
    gap: 1.5rem;
    flex-wrap: wrap;
}
.toc a {
    color: var(--primary-color);
    text-decoration: none;
    font-weight: 600;
    cursor: pointer;
}
.toc a:hover {
    text-decoration: underline;
}

/* File Group Styling */
.test-file-group {
    margin-bottom: 2rem;
}
.test-file-header {
    font-size: 1.1rem;
    font-weight: 600;
    color: var(--text-primary);
    margin-bottom: 1rem;
    padding-bottom: 0.5rem;
    border-bottom: 2px solid var(--border-color);
    display: flex;
    justify-content: space-between;
    align-items: center;
}    </style>
    <script>
// pytest-llm-report interactive features

// Global state for filters
const activeStatuses = new Set(['passed', 'failed', 'skipped', 'xfailed', 'xpassed', 'error']);

// Filter tests based on search input and outcome filters
function filterTests() {
    const query = document.getElementById('searchInput').value.toLowerCase();
    document.querySelectorAll('.test-row').forEach(row => {
        const nodeid = row.querySelector('.test-name').textContent.toLowerCase();
        const statusMatch = row.dataset.status ? activeStatuses.has(row.dataset.status) : false;
        const matchesSearch = nodeid.includes(query);
        row.classList.toggle('hidden', !matchesSearch || !statusMatch);
    });
}

// Show only failures and scroll to list
function showFailuresOnly() {
    document.querySelectorAll('.filter-chip input').forEach(cb => {
        const s = cb.dataset.status;
        if (s === 'failed' || s === 'error') {
            cb.checked = true;
            activeStatuses.add(s);
        } else {
            cb.checked = false;
            activeStatuses.delete(s);
        }
    });
    filterTests();
    const testList = document.getElementById('test-list');
    if (testList) {
        testList.scrollIntoView({ behavior: 'smooth' });
    }
}

// Toggle visibility of status filters
function toggleStatus(checkbox) {
    const status = checkbox.dataset.status;
    if (checkbox.checked) {
        activeStatuses.add(status);
    } else {
        activeStatuses.delete(status);
    }
    filterTests();
}

// Initialize interactive features after DOM is ready
document.addEventListener('DOMContentLoaded', function () {
    'use strict';

    // Toggle dark mode on preference
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.documentElement.dataset.theme = 'dark';
    }

    // Default: expand all details
    document.querySelectorAll('details').forEach(details => {
        details.setAttribute('open', '');
    });

    const params = new URLSearchParams(window.location.search);
    if (params.get('pdf') === '1') {
        document.body.classList.add('pdf-mode');
    }
});    </script>
</head>
<body>
    <div class="container">
        <header>
            <div>
                <h1>Test Report</h1>
                <div class="meta">
                    Run ID: N/A &bull;
                    Generated: 2026-01-18 17:14:39 &bull;
                    Duration: 138ms<br>
                    <strong>Plugin:</strong> v0.1.0
                        (b7a157f6cb9189cc50a17c846484c8454deeac61)
[dirty]<br>
                    <strong>Repo:</strong> v1.0.0
                        (1c69689ab92cbb103a35b7d434aa553598fe0fae)
[dirty]<br>
                    <strong>LLM:</strong> ollama / llama3.2:1b
                        (minimal context,
                         26 annotated, 2 errors)
                </div>
            </div>
            <div style="text-align: right">
                <div style="font-size: 2rem; font-weight: 700; color: var(--primary-color)">
                    100.0%
                </div>
                <div class="meta">Total Coverage</div>
            </div>
        </header>

        <!-- Summary Cards -->
        <div class="summary">
            <div class="summary-card">
                <div class="count">28</div>
                <div class="label">Total Tests</div>
            </div>
            <div class="summary-card passed">
                <div class="count">22</div>
                <div class="label">Passed</div>
            </div>
            <div class="summary-card failed">
                <div class="count">2</div>
                <div class="label">Failed</div>
            </div>
            <div class="summary-card skipped">
                <div class="count">2</div>
                <div class="label">Skipped</div>
            </div>
            <div class="summary-card xfailed">
                <div class="count">1</div>
                <div class="label">XFailed</div>
            </div>
            <div class="summary-card xpassed">
                <div class="count">1</div>
                <div class="label">XPassed</div>
            </div>
            <div class="summary-card failed">
                <div class="count">0</div>
                <div class="label">Errors</div>
            </div>
        </div>

        <!-- Table of Contents -->
        <nav class="toc">
            <ul>
                <li><a href="#source-coverage">Source Coverage</a></li>
                <li><a href="#test-list">Per Test Details</a></li>
                <li><a onclick="showFailuresOnly()">Failures Only</a></li>
            </ul>
        </nav>

        <section class="source-coverage" id="source-coverage">
            <h2>Source Coverage</h2>
            <div class="source-coverage-table">
                <div class="source-coverage-header">
                    <span>File</span>
                    <span>Stmts</span>
                    <span>Miss</span>
                    <span>Cover</span>
                    <span>%</span>
                    <span>Covered Lines</span>
                    <span>Missed Lines</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">calculator.py</span>
                    <span>26</span>
                    <span>0</span>
                    <span>26</span>
                    <span>100.0%</span>
                    <span class="source-lines">8, 18, 21, 31, 34, 44, 47, 60-62, 65, 74, 77, 86, 89, 101-105, 108, 120-124</span>
                    <span class="source-lines">-</span>
                </div>
            </div>
        </section>

        <section class="per-test-details" id="test-list">
            <h2>Per Test Details</h2>

        <!-- Filters -->
        <div class="filters">
            <input type="text" id="searchInput" class="filter-input" placeholder="Search tests..." onkeyup="filterTests()">
            <div class="filter-statuses" aria-label="Filter by status">
                <label class="filter-chip passed">
                    <input type="checkbox" data-status="passed" checked onchange="toggleStatus(this)">
                    Passed
                </label>
                <label class="filter-chip failed">
                    <input type="checkbox" data-status="failed" checked onchange="toggleStatus(this)">
                    Failed
                </label>
                <label class="filter-chip skipped">
                    <input type="checkbox" data-status="skipped" checked onchange="toggleStatus(this)">
                    Skipped
                </label>
                <label class="filter-chip xfailed">
                    <input type="checkbox" data-status="xfailed" checked onchange="toggleStatus(this)">
                    XFailed
                </label>
                <label class="filter-chip xpassed">
                    <input type="checkbox" data-status="xpassed" checked onchange="toggleStatus(this)">
                    XPassed
                </label>
                <label class="filter-chip error">
                    <input type="checkbox" data-status="error" checked onchange="toggleStatus(this)">
                    Error
                </label>
            </div>
        </div>

        <!-- Test List -->
        <div class="test-list">
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ example_tests.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">28 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestCalculator::test_addition</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify the addition function works correctly for various inputs.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the addition function does not handle negative numbers correctly.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `add` function should return the correct result when both inputs are positive.</li>
                                            <li>The `add` function should return 0 when one input is 0 and the other is also 0.</li>
                                            <li>The `add` function should return 0 for any negative input.</li>
                                            <li>The `add` function should handle zero as a valid input without raising an error.</li>
                                            <li>The `add` function should preserve the sign of both inputs when adding them together.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 18)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestCalculator::test_division</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that division operations are performed correctly for various inputs.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential division by zero error and ensures accurate results in all scenarios.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert divide(10, 2) == 5.0</li>
                                            <li>assert divide(7, 2) == 3.5</li>
                                            <li>assert divide(0, 5) == 0.0</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 60, 62)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestCalculator::test_division_by_zero</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `divide` function correctly raises a `ValueError` when attempting to divide by zero.</p>
                                    <p><strong>Why Needed:</strong> This test prevents division by zero from being silently ignored and potentially causing unexpected behavior or errors in downstream code.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `divide` function should raise a `ValueError` with the message 'Cannot divide by zero'.</li>
                                            <li>The error message should include the phrase 'Cannot divide by zero' to clearly indicate the cause of the issue.</li>
                                            <li>The test should verify that the `match` parameter in `pytest.raises()` is set to match the expected error message.</li>
                                            <li>The `divide` function should be called with a non-zero argument to trigger the division by zero scenario.</li>
                                            <li>The test should check for any additional error messages or details provided by the `divide` function.</li>
                                            <li>The `divide` function should raise a `ValueError` exception instead of returning a special value (e.g., None) when dividing by zero.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 60-61)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestCalculator::test_multiplication</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies the correct result of multiplying two numbers.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where incorrect multiplication results are returned for certain inputs (e.g., negative numbers or zero).</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `multiply(6, 7)` should return `42`.</li>
                                            <li>The function `multiply(-2, 3)` should return `-6`.</li>
                                            <li>The function `multiply(0, 100)` should return `0`.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 44)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestCalculator::test_subtraction</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies the correct subtraction of numbers.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where incorrect subtraction results in an incorrect output.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert subtract(10, 3) == 7</li>
                                            <li>assert subtract(5, 5) == 0</li>
                                            <li>assert subtract(0, 5) == -5</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 31)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-failed" data-status="failed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-failed">FAILED</span>
                            <span class="test-name">example_tests.py::TestErrors::test_with_error</span>
                            <div class="test-meta">
                                <span>0ms</span>
                            </div>
                        </summary>

                        <div class="test-details">
                            <div class="detail-section">
                                <div class="detail-title">Error</div>
                                <div class="error-message">      RuntimeError: Intentional error for demo</div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that a RuntimeError is raised when the function `test_with_error` is called.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where a RuntimeError might be silently ignored or masked by other exceptions.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Raises a RuntimeError with the specified message.</li>
                                            <li>Does not catch any other type of exception (e.g., TypeError, ValueError).</li>
                                            <li>Does not handle any specific error conditions (e.g., no argument passed to `test_with_error`).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                        </div>
                    </details>
                </div>
                <div class="test-row outcome-xfailed" data-status="xfailed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-xfailed">XFAILED</span>
                            <span class="test-name">example_tests.py::TestExpectedFailures::test_known_bug</span>
                            <div class="test-meta">
                                <span>0ms</span>
                            </div>
                        </summary>

                        <div class="test-details">
                            <div class="detail-section">
                                <div class="detail-title">Error</div>
                                <div class="error-message">self = <example_tests.TestExpectedFailures object at 0x7fe93fa3efd0>

    @pytest.mark.xfail(reason="Known bug in edge case handling")
    def test_known_bug(self):
        """Test that fails due to a</div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the function raises an AssertionError when a known bug is encountered.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the function raises an exception when it encounters a known issue.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Raises an AssertionError with the message 'Known bug'.</li>
                                            <li>Does not handle the known bug case correctly.</li>
                                            <li>Expected the function to raise an exception, but did not.</li>
                                            <li>The test does not verify that the error is properly propagated or handled.</li>
                                            <li>The test does not ensure that the error is reported in a meaningful way (e.g., with a specific message).</li>
                                            <li>The test does not provide any useful feedback about what went wrong when the known bug occurs.</li>
                                            <li>The test does not account for other potential errors that may occur in the function.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                        </div>
                    </details>
                </div>
                <div class="test-row outcome-xpassed" data-status="xpassed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-xpassed">XPASSED</span>
                            <span class="test-name">example_tests.py::TestExpectedFailures::test_xpass_demo</span>
                            <div class="test-meta">
                                <span>0ms</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `test_xpass_demo` function returns a successful assertion.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the expected failure is not caught due to an incorrect implementation of the xfail decorator.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return True.</li>
                                            <li>The function should not raise any exceptions.</li>
                                            <li>The function should not be marked as failed by the `xpassed` flag.</li>
                                            <li>The function's output should match the expected result.</li>
                                            <li>Any raised exceptions should be caught and handled correctly.</li>
                                            <li>The test should fail if the function returns a non-zero value.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                        </div>
                    </details>
                </div>
                <div class="test-row outcome-failed" data-status="failed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-failed">FAILED</span>
                            <span class="test-name">example_tests.py::TestFailures::test_expected_failure_demo</span>
                            <div class="test-meta">
                                <span>1ms</span>
                            </div>
                        </summary>

                        <div class="test-details">
                            <div class="detail-section">
                                <div class="detail-title">Error</div>
                                <div class="error-message">      AssertionError: Intentional failure for demo purposes</div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the function `test_expected_failure_demo` is expected to fail when asserting an incorrect value.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the function might incorrectly report a successful outcome due to a bug in its logic.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>asserts that the function will raise an AssertionError with message 'Intentional failure for demo purposes'</li>
                                            <li>checks if the assertion `1 == 2` is incorrect</li>
                                            <li>verifies that the error message is correct and not something else</li>
                                            <li>ensures that the test fails as expected</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestNumberProperties::test_is_even</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the `is_even` function with various numbers to verify its correctness.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly identifies even numbers as odd, leading to incorrect results in certain scenarios.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>{'description': 'Check if the function correctly identifies even numbers.', 'condition': 'is_even(2) is True', 'expected_result': True}</li>
                                            <li>{'description': 'Verify that the function returns True for multiples of 2.', 'condition': 'is_even(4) is True', 'expected_result': True}</li>
                                            <li>{'description': 'Ensure the function correctly handles even numbers with leading zeros.', 'condition': 'is_even(0) is True', 'expected_result': True}</li>
                                            <li>{'description': 'Test that the function returns False for odd numbers.', 'condition': 'is_even(1) is False', 'expected_result': False}</li>
                                            <li>{'description': 'Verify that the function handles negative numbers correctly.', 'condition': 'is_even(-3) is False', 'expected_result': False}</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 74)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestNumberProperties::test_is_positive</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `is_positive` function correctly identifies positive numbers.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly returns True for negative numbers or zero.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert is_positive(1) is True</li>
                                            <li>assert is_positive(100) is True</li>
                                            <li>assert is_positive(0) is False</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 86)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestParameterized::test_add_numbers[-5-5-0]</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The `add` function is tested with three different input combinations: (1, 2), (3, 4), and (-5, -5).</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `add` function returns incorrect results for negative numbers.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The sum of a and b is equal to expected.</li>
                                            <li>The difference between add(a, b) and expected is zero.</li>
                                            <li>If a is negative and b is positive, then add(a, b) should be the same as expected.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 18)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestParameterized::test_add_numbers[1-1-2]</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify correct addition of two numbers for different input scenarios.</p>
                                    <p><strong>Why Needed:</strong> Prevents incorrect addition due to potential overflow or underflow issues when adding very large or small numbers.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `add(a, b)` returns the expected result for all given inputs.</li>
                                            <li>The function `add(a, b)` handles edge cases where one or both of the input numbers are zero.</li>
                                            <li>The function `add(a, b)` correctly handles negative numbers and their sums.</li>
                                            <li>The function `add(a, b)` avoids division by zero errors when adding a number to zero.</li>
                                            <li>The function `add(a, b)` maintains the correct order of operations (PEMDAS) for complex arithmetic.</li>
                                            <li>The function `add(a, b)` is stable under certain input conditions (e.g., negative numbers close to zero).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 18)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestParameterized::test_add_numbers[10-20-30]</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The `add` function should be able to handle input values of 10, 20, and 30 without raising an error.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `add` function would raise a TypeError or ValueError for inputs that are not numbers.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The result of adding `a` and `b` should be equal to `expected`.</li>
                                            <li>The `add` function should handle input values of 10, 20, and 30 without raising an error.</li>
                                            <li>The `add` function should return the correct expected value for inputs of 10, 20, and 30.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 18)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestParameterized::test_add_numbers[2-3-5]</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 18)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestParameterized::test_is_even_parametrized[-4-True]</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Testing the `is_even` function with a parameter of type `int`.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not correctly handle non-integer inputs.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The input `n` should be an integer.</li>
                                            <li>The result of `is_even(n)` should match the expected value `expected`.</li>
                                            <li>If `n` is not an integer, the test should fail and report a bug.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 74)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestParameterized::test_is_even_parametrized[0-True]</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 74)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestParameterized::test_is_even_parametrized[1-False]</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Testing the `is_even` function with a parameter of type `int` and an expected value of `False`.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function might return incorrect results for odd inputs.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The input `n` is an integer.</li>
                                            <li>The function `is_even(n)` returns `True` if `n` is even and `False` otherwise.</li>
                                            <li>If `n` is an odd number, `is_even(n)` should return `False`.</li>
                                            <li>If `n` is 0 or negative, `is_even(n)` should raise a ValueError.</li>
                                            <li>The function should handle non-integer inputs correctly.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 74)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestParameterized::test_is_even_parametrized[100-True]</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Testing the `is_even` function with a parametrized input.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not correctly handle non-integer inputs.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The value of `n` is an integer.</li>
                                            <li>The expected result for `is_even(n)` is `True` if `n` is even and `False` otherwise.</li>
                                            <li>If `n` is a negative integer, the function should return `False` because it does not handle negative numbers correctly.</li>
                                            <li>If `n` is zero, the function should return `True` because any number is considered even.</li>
                                            <li>The function should raise an error if `n` is not an integer.</li>
                                            <li>The function should throw a TypeError if `expected` is not a boolean value.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 74)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestParameterized::test_is_even_parametrized[2-True]</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `is_even` function correctly identifies even numbers.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function returns incorrect results for odd inputs.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>n is an integer</li>
                                            <li>expected is True if n % 2 == 0, False otherwise</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 74)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestRecursiveFunctions::test_factorial_base_cases</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies the base case of factorial function for 0 and 1.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential division by zero error when calculating factorial.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert factorial(0) == 1</li>
                                            <li>assert factorial(1) == 1</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 120, 122-123)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestRecursiveFunctions::test_factorial_negative_raises</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Testing the `factorial` function with a negative input.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function would incorrectly return a value for negative inputs.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `factorial` function should raise a `ValueError` with an appropriate message when given a negative input.</li>
                                            <li>The error message should indicate that the input must be non-negative.</li>
                                            <li>The test should verify that the function returns a `ValueError` for inputs less than 0.</li>
                                            <li>The test should also verify that the error is raised within the `factorial` function itself, not in another module or context.</li>
                                            <li>The test should include a specific example input (e.g. -1) to trigger the error.</li>
                                            <li>The test should be able to reproduce the error consistently across different Python versions and environments.</li>
                                            <li>Additional tests should be added to verify that the error is correctly propagated up the call stack.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 120-121)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestRecursiveFunctions::test_factorial_values</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify the correct calculation of factorial for input values 5 and 10.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function returns incorrect results for large inputs, potentially leading to unexpected behavior or errors in downstream calculations.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `factorial(n)` correctly calculates the value of `n!` for input `n = 5` and `n = 10`.</li>
                                            <li>The function `factorial(n)` correctly calculates the value of `n!` for input `n = 15` (not tested in this example).</li>
                                            <li>The function `factorial(n)` handles large inputs without a significant decrease in performance or accuracy.</li>
                                            <li>The function `factorial(n)` raises an error when input is negative, as factorial is not defined for negative numbers.</li>
                                            <li>The function `factorial(n)` correctly calculates the value of `n!` for input `n = 0` (not tested in this example).</li>
                                            <li>The function `factorial(n)` handles non-integer inputs without a significant decrease in performance or accuracy.</li>
                                            <li>The function `factorial(n)` correctly calculates the value of `n!` for input `n = -5` (not tested in this example).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 120, 122-124)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestRecursiveFunctions::test_fibonacci_base_cases</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies the base cases of the Fibonacci function (fibonacci(0) and fibonacci(1))</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the function is not correctly handling the base case conditions.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert fibonacci(0) == 0</li>
                                            <li>assert fibonacci(1) == 1</li>
                                            <li>assert fibonacci(-1) is None</li>
                                            <li>assert fibonacci(2.5) is None</li>
                                            <li>assert fibonacci(3) == 2</li>
                                            <li>assert fibonacci(4) == 3</li>
                                            <li>assert fibonacci(5) == 5</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 101, 103-104)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestRecursiveFunctions::test_fibonacci_negative_raises</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Testing the `fibonacci` function with a negative input.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the function attempts to calculate the Fibonacci sequence for negative numbers.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The input `n` must be non-negative.</li>
                                            <li>A `ValueError` is raised when attempting to calculate the Fibonacci sequence for a negative number.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 101-102)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">example_tests.py::TestRecursiveFunctions::test_fibonacci_sequence</span>
                            <div class="test-meta">
                                <span>0ms</span>
                                <span title="Covered file count">üõ°Ô∏è 1</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify the correctness of the Fibonacci sequence for initial test cases.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where incorrect calculation of Fibonacci numbers is returned for small input values.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return the correct Fibonacci number for inputs 5 and 10.</li>
                                            <li>The function should raise an error or return None for invalid input values (e.g., negative numbers).</li>
                                            <li>The function should handle edge cases where the input is a non-integer value correctly.</li>
                                            <li>The function should not overflow for large input values.</li>
                                            <li>The function should maintain its performance and efficiency even for very large input values.</li>
                                            <li>The function should handle duplicate Fibonacci numbers correctly (e.g., 0, 1, 1, ...).</li>
                                            <li>The function should raise an error if the input is a non-integer value.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>calculator.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 101, 103-105)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-skipped" data-status="skipped">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-skipped">SKIPPED</span>
                            <span class="test-name">example_tests.py::TestSkipped::test_conditionally_skipped</span>
                            <div class="test-meta">
                                <span>0ms</span>
                            </div>
                        </summary>

                        <div class="test-details">
                            <div class="detail-section">
                                <div class="detail-title">Error</div>
                                <div class="error-message">('/mnt/hbmon/pytest-llm-report/docs/example-report/example_tests.py', 127, 'Skipped: Skipped for demonstration')</div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test is currently marked as skipped due to a condition that may not be met.</p>
                                    <p><strong>Why Needed:</strong> To ensure the test is executed only when necessary, we need to remove or modify the condition that causes it to be skipped.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Check if the test is being run on the expected platform.</li>
                                            <li>Verify that the required dependencies are installed and up-to-date.</li>
                                            <li>Ensure that the environment variables are set correctly.</li>
                                            <li>Test that the UI components are rendered as expected.</li>
                                            <li>Verify that the data is being retrieved from the database correctly.</li>
                                            <li>Check for any errors or exceptions during execution.</li>
                                            <li>Verify that the test results are accurate and reliable.</li>
                                            <li>Test that the test suite is properly cleaned up after each run.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                        </div>
                    </details>
                </div>
                <div class="test-row outcome-skipped" data-status="skipped">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-skipped">SKIPPED</span>
                            <span class="test-name">example_tests.py::TestSkipped::test_not_implemented</span>
                            <div class="test-meta">
                                <span>0ms</span>
                            </div>
                        </summary>

                        <div class="test-details">
                            <div class="detail-section">
                                <div class="detail-title">Error</div>
                                <div class="error-message">('/mnt/hbmon/pytest-llm-report/docs/example-report/example_tests.py', 122, 'Skipped: Feature not implemented yet')</div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `test_not_implemented` function is skipped.</p>
                                    <p><strong>Why Needed:</strong> Because it prevents a potential regression where this test would be executed.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should not be called.</li>
                                            <li>The function should raise an error with a meaningful message.</li>
                                            <li>The function should not return any value.</li>
                                            <li>The function should not modify the test result.</li>
                                            <li>The function should not have any side effects.</li>
                                            <li>The function should only be skipped by this test.</li>
                                            <li>The function's implementation is not yet available.</li>
                                            <li>This test would fail if the `test_not_implemented` function were implemented.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                        </div>
                    </details>
                </div>
            </div>
        </div>
        </section>
    </div>
</body>
</html>
