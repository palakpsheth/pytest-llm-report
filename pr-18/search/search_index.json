{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"pytest-llm-report Documentation","text":"<p>Welcome to pytest-llm-report, a pytest plugin for generating human-friendly test reports with optional LLM annotations.</p> <p>Live Test Report Available</p> <p>View the latest test report for this project:</p> <p>\ud83e\uddea Interactive Test Report \ud83d\udcc4 Test Report Docs</p> <p>Looking for more context? See the Test Report page for a live embed and a static example report.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Install the plugin: <pre><code>pip install pytest-llm-report\n</code></pre></p> <p>Run tests with a report: <pre><code>pytest --cov=my_pkg --cov-context=test --llm-report=report.html\n</code></pre></p>"},{"location":"#features","title":"Features","text":"<ul> <li>HTML and JSON reports with per-test coverage</li> <li>LLM annotations describing what each test verifies</li> <li>Local or cloud LLM support (Ollama, OpenAI, Anthropic, Gemini)</li> <li>Privacy-first - LLM is disabled by default</li> <li>CI/CD ready - artifacts for pipelines</li> <li>Aggregation - Combine reports from multiple runs (see Aggregation)</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started</li> <li>Configuration</li> <li>LLM Providers</li> <li>Coverage Setup</li> <li>Test Report</li> <li>Report Format</li> <li>Privacy</li> <li>Security</li> <li>Troubleshooting</li> <li>Contributing</li> </ul>"},{"location":"aggregation/","title":"Report Aggregation","text":"<p>pytest-llm-report allows you to aggregate multiple test reports into a single, unified report. This is particularly useful for:</p> <ul> <li>Parallel Test Execution: Combining results from multiple CI jobs or sharded runs.</li> <li>Multi-Environment Testing: Merging results from different Python versions or platforms.</li> <li>Flaky Test Analysis: Combining multiple runs to spot inconsistent failures.</li> </ul>"},{"location":"aggregation/#usage","title":"Usage","text":"<p>To aggregate reports, you use the plugin in a special aggregation mode using <code>--llm-aggregate-dir</code>:</p> <pre><code>pytest --collect-only \\\n  --llm-aggregate-dir=path/to/reports \\\n  --llm-report=aggregated.html\n</code></pre> <p>[!NOTE] We use <code>--collect-only</code> so that pytest performs collection and runs its terminal summary hooks without executing any tests. The plugin performs aggregation in the <code>pytest_terminal_summary</code> hook when <code>--llm-aggregate-dir</code> is set, which lets aggregation run once after collection without introducing a separate pytest command.</p>"},{"location":"aggregation/#configuration-options","title":"Configuration Options","text":"Option Description Default <code>--llm-aggregate-dir DIR</code> Directory containing JSON reports (<code>.json</code>) to aggregate None <code>--llm-aggregate-policy POLICY</code> Aggregation policy (<code>latest</code>, <code>merge</code>, <code>all</code>) <code>latest</code> <code>--llm-aggregate-run-id ID</code> Unique ID for the new aggregated run Auto-generated"},{"location":"aggregation/#aggregation-policies","title":"Aggregation Policies","text":"<ul> <li>latest (default): Groups results by test ID and keeps only the one with the latest start time. Useful for combining sharded runs or retries.</li> <li>merge: Groups results by test ID but keeps strictly distinct outcomes (not fully implemented yet, falls back to latest).</li> <li>all: Keeps all results as separate entries. Useful for analyzing history.</li> </ul>"},{"location":"aggregation/#ci-example-github-actions","title":"CI Example (GitHub Actions)","text":"<p>Here's how to aggregate reports from a matrix strategy in GitHub Actions:</p> <pre><code>jobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python: [\"3.11\", \"3.12\"]\n    steps:\n      - uses: actions/checkout@v4\n      - name: Run tests\n        run: |\n          pytest --llm-report-json=report.json \\\n                 --llm-aggregate-run-id=${{ github.run_id }}-${{ matrix.python }}\n      - uses: actions/upload-artifact@v4\n        with:\n          name: report-${{ matrix.python }}\n          path: report.json\n\n  report:\n    needs: test\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/download-artifact@v4\n        with:\n          pattern: report-*\n          path: all-reports\n          merge-multiple: true\n\n      - name: Aggregate Reports\n        run: |\n          pytest --collect-only \\\n            --llm-aggregate-dir=all-reports \\\n            --llm-report=aggregated.html\n</code></pre>"},{"location":"configuration/","title":"Configuration Reference","text":"<p>Complete configuration reference for pytest-llm-report.</p>"},{"location":"configuration/#cli-options","title":"CLI Options","text":"Option Description Default <code>--llm-report PATH</code> HTML report output path None <code>--llm-report-json PATH</code> JSON report output path None <code>--llm-pdf PATH</code> PDF report output (requires Playwright) None <code>--llm-evidence-bundle PATH</code> Evidence bundle zip output None <code>--llm-aggregate-dir DIR</code> Directory with reports to aggregate None <code>--llm-aggregate-policy POLICY</code> Aggregation policy: latest, merge, all latest <code>--llm-aggregate-run-id ID</code> Unique run ID Auto-generated <code>--llm-aggregate-group-id ID</code> Group ID for related runs None <p>Tip: Run <code>pytest --help</code> to see usage examples for each option.</p>"},{"location":"configuration/#pyprojecttoml-options","title":"pyproject.toml Options","text":"<pre><code>[tool.pytest.ini_options]\n# Output defaults\nllm_report_html = \"reports/test-report.html\"\nllm_report_json = \"reports/test-report.json\"\n\n# LLM provider (none, ollama, litellm, gemini)\nllm_report_provider = \"none\"\nllm_report_model = \"llama3.2\"\nllm_report_context_mode = \"minimal\"\n</code></pre>"},{"location":"configuration/#llm-provider-settings","title":"LLM Provider Settings","text":""},{"location":"configuration/#provider-none-default","title":"Provider: none (default)","text":"<p>No LLM calls. Reports are generated without annotations.</p>"},{"location":"configuration/#provider-ollama","title":"Provider: ollama","text":"<pre><code>llm_report_provider = \"ollama\"\nllm_report_model = \"llama3.2\"\n</code></pre> <p>Environment variables: - <code>OLLAMA_HOST</code>: Ollama server URL (default: http://127.0.0.1:11434)</p>"},{"location":"configuration/#provider-litellm","title":"Provider: litellm","text":"<pre><code>llm_report_provider = \"litellm\"\nllm_report_model = \"gpt-4o-mini\"\n</code></pre> <p>Environment variables depend on the model (e.g., <code>OPENAI_API_KEY</code>).</p>"},{"location":"configuration/#provider-gemini","title":"Provider: gemini","text":"<pre><code>llm_report_provider = \"gemini\"\nllm_report_model = \"gemini-1.5-flash-latest\"\n</code></pre> <p>Environment variables: - <code>GEMINI_API_TOKEN</code>: Gemini API key</p>"},{"location":"configuration/#pytest-markers","title":"pytest Markers","text":"<pre><code>import pytest\n\n# Opt out of LLM annotation\n@pytest.mark.llm_opt_out\ndef test_skip_llm():\n    pass\n\n# Override context mode\n@pytest.mark.llm_context(\"complete\")\ndef test_with_full_context():\n    pass\n\n# Associate with requirements\n@pytest.mark.requirement(\"REQ-001\", \"REQ-002\")\ndef test_requirement():\n    pass\n</code></pre>"},{"location":"configuration/#context-modes","title":"Context Modes","text":"Mode Description <code>minimal</code> Test code only, no additional context <code>balanced</code> Test code + covered files (limited) <code>complete</code> Test code + all covered files up to limits"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"Variable Description <code>OLLAMA_HOST</code> Ollama server URL <code>OPENAI_API_KEY</code> OpenAI API key (for litellm) <code>ANTHROPIC_API_KEY</code> Anthropic API key (for litellm) <code>GEMINI_API_TOKEN</code> Gemini API key"},{"location":"coverage/","title":"Coverage Configuration","text":"<p>This guide explains how to configure pytest-cov for use with pytest-llm-report.</p>"},{"location":"coverage/#what-is-cov-contexttest","title":"What is <code>--cov-context=test</code>?","text":"<p>The <code>--cov-context=test</code> flag tells pytest-cov to record which test covered each line of code. This is called \"coverage contexts.\"</p> <p>Without contexts, coverage data only tells you which lines were executed. With contexts, you know exactly which test exercised each line.</p>"},{"location":"coverage/#why-is-it-required","title":"Why is it Required?","text":"<p>pytest-llm-report uses coverage contexts to: - Show which source files and lines each test covers - Generate per-test coverage reports - Help LLM understand what code the test exercises</p>"},{"location":"coverage/#configuring-your-repository","title":"Configuring Your Repository","text":""},{"location":"coverage/#option-1-pyprojecttoml-recommended","title":"Option 1: pyproject.toml (Recommended)","text":"<p>Add to your <code>pyproject.toml</code>:</p> <pre><code>[tool.pytest.ini_options]\naddopts = [\n    \"--cov=your_package\",       # Replace with your package name\n    \"--cov-context=test\",       # Required for per-test coverage mapping\n    \"--cov-report=\",            # Suppress default coverage report (optional)\n]\n</code></pre>"},{"location":"coverage/#option-2-pytestini","title":"Option 2: pytest.ini","text":"<pre><code>[pytest]\naddopts =\n    --cov=your_package\n    --cov-context=test\n    --cov-report=\n</code></pre>"},{"location":"coverage/#option-3-setupcfg","title":"Option 3: setup.cfg","text":"<pre><code>[tool:pytest]\naddopts =\n    --cov=your_package\n    --cov-context=test\n    --cov-report=\n</code></pre>"},{"location":"coverage/#option-4-command-line","title":"Option 4: Command Line","text":"<pre><code>pytest --cov=your_package --cov-context=test --llm-report=report.html\n</code></pre>"},{"location":"coverage/#configuration-options","title":"Configuration Options","text":"Option Description <code>--cov=PKG</code> Package/directory to measure coverage for <code>--cov-context=test</code> Record context for each test <code>--cov-report=</code> Suppress default report (set empty) <code>--cov-append</code> Append to existing coverage data"},{"location":"coverage/#example-multi-package-project","title":"Example: Multi-Package Project","text":"<pre><code>[tool.pytest.ini_options]\naddopts = [\n    \"--cov=src/package_a\",\n    \"--cov=src/package_b\",\n    \"--cov-context=test\",\n    \"--llm-report=reports/test-report.html\",\n    \"--llm-report-json=reports/test-report.json\",\n]\n</code></pre>"},{"location":"coverage/#paralleldistributed-testing","title":"Parallel/Distributed Testing","text":"<p>For <code>pytest-xdist</code> (parallel testing), coverage data is automatically combined:</p> <pre><code>pytest -n auto --cov=your_package --cov-context=test --llm-report=report.html\n</code></pre> <p>The plugin reads both <code>.coverage</code> and <code>.coverage.*</code> files.</p>"},{"location":"coverage/#ci-integration","title":"CI Integration","text":"<p>Example GitHub Actions workflow:</p> <pre><code>- name: Run tests\n  run: |\n    pytest \\\n      --cov=your_package \\\n      --cov-context=test \\\n      --cov-report=xml \\\n      --llm-report=report.html \\\n      --llm-report-json=report.json\n\n- name: Upload report\n  uses: actions/upload-artifact@v4\n  with:\n    name: test-report\n    path: |\n      report.html\n      report.json\n</code></pre>"},{"location":"coverage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"coverage/#no-coverage-contexts-found-warning","title":"\"No coverage contexts found\" warning","text":"<p>This means <code>--cov-context=test</code> was not set. Add it to your pytest configuration.</p>"},{"location":"coverage/#no-coverage-file-found-warning","title":"\"No .coverage file found\" warning","text":"<p>Ensure you're running with <code>--cov=your_package</code>. The <code>.coverage</code> file must exist after the test run.</p>"},{"location":"coverage/#coverage-data-seems-stale","title":"Coverage data seems stale","text":"<p>Run <code>coverage erase</code> before your test run to clear old data:</p> <pre><code>coverage erase &amp;&amp; pytest --cov=your_package --cov-context=test\n</code></pre>"},{"location":"deployment/","title":"Deployment Instructions","text":"<p>This guide covers publishing pytest-llm-report to PyPI and setting up CI/CD.</p>"},{"location":"deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>uv (recommended) or pip</li> <li>PyPI account with API token</li> </ul>"},{"location":"deployment/#local-development-setup","title":"Local Development Setup","text":"<pre><code># Clone the repository\ngit clone https://github.com/palakpsheth/pytest-llm-report.git\ncd pytest-llm-report\n\n# Create virtual environment and install\nuv sync\n\n# Run tests\nuv run pytest\n\n# Run with coverage\nuv run pytest --cov=pytest_llm_report --cov-report=term-missing\n</code></pre>"},{"location":"deployment/#building","title":"Building","text":"<pre><code># Build sdist and wheel\nuv build\n\n# Check the build\nls dist/\n# pytest_llm_report-0.1.0.tar.gz\n# pytest_llm_report-0.1.0-py3-none-any.whl\n</code></pre>"},{"location":"deployment/#publishing-to-pypi","title":"Publishing to PyPI","text":""},{"location":"deployment/#option-1-using-uv-recommended","title":"Option 1: Using uv (Recommended)","text":"<pre><code># Set PyPI token\nexport UV_PUBLISH_TOKEN=\"pypi-...\"\n\n# Publish\nuv publish\n</code></pre>"},{"location":"deployment/#option-2-using-twine","title":"Option 2: Using twine","text":"<pre><code># Install twine\npip install twine\n\n# Upload\ntwine upload dist/*\n</code></pre>"},{"location":"deployment/#option-3-github-actions-automated","title":"Option 3: GitHub Actions (Automated)","text":"<p>The CI workflow automatically publishes on tagged releases.</p> <pre><code># Tag a release\ngit tag v0.1.0\ngit push origin v0.1.0\n</code></pre>"},{"location":"deployment/#github-actions-cicd","title":"GitHub Actions CI/CD","text":"<p>Create <code>.github/workflows/ci.yml</code>:</p> <pre><code>name: CI\n\non:\n  push:\n    branches: [main]\n  pull_request:\n  release:\n    types: [published]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.11\", \"3.12\", \"3.13\"]\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@v4\n\n      - name: Set up Python ${{ matrix.python-version }}\n        run: uv python install ${{ matrix.python-version }}\n\n      - name: Install dependencies\n        run: uv sync --all-extras\n\n      - name: Run tests\n        run: |\n          uv run pytest \\\n            --cov=pytest_llm_report \\\n            --cov-context=test \\\n            --cov-report=xml \\\n            --cov-fail-under=90\n\n      - name: Upload coverage\n        uses: codecov/codecov-action@v4\n\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: astral-sh/setup-uv@v4\n      - run: uv sync\n      - run: uv run ruff check .\n      - run: uv run ruff format --check .\n\n  publish:\n    needs: [test, lint]\n    runs-on: ubuntu-latest\n    if: github.event_name == 'release'\n\n    steps:\n      - uses: actions/checkout@v4\n      - uses: astral-sh/setup-uv@v4\n\n      - name: Build\n        run: uv build\n\n      - name: Publish to PyPI\n        run: uv publish\n        env:\n          UV_PUBLISH_TOKEN: ${{ secrets.PYPI_API_TOKEN }}\n</code></pre>"},{"location":"deployment/#version-management","title":"Version Management","text":"<p>Update version in <code>src/pytest_llm_report/__about__.py</code>:</p> <pre><code>__version__ = \"0.1.0\"\n</code></pre>"},{"location":"deployment/#release-checklist","title":"Release Checklist","text":"<ol> <li>[ ] Update version in <code>__about__.py</code></li> <li>[ ] Update CHANGELOG.md</li> <li>[ ] Run full test suite: <code>uv run pytest</code></li> <li>[ ] Check lint: <code>uv run ruff check .</code></li> <li>[ ] Build: <code>uv build</code></li> <li>[ ] Test install: <code>pip install dist/*.whl</code></li> <li>[ ] Create git tag: <code>git tag v0.1.0</code></li> <li>[ ] Push tag: <code>git push origin v0.1.0</code></li> <li>[ ] Create GitHub release</li> </ol>"},{"location":"deployment/#verification","title":"Verification","text":"<p>After publishing:</p> <pre><code># Install from PyPI\npip install pytest-llm-report\n\n# Verify installation\npython -c \"import pytest_llm_report; print(pytest_llm_report.__version__)\"\n\n# Test in a project\npytest --llm-report=report.html\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code># Basic\npip install pytest-llm-report\n\n# With uv\nuv add pytest-llm-report\n</code></pre> <p>For LLM support: <pre><code># Ollama (local)\npip install pytest-llm-report[ollama]\n\n# LiteLLM (cloud)\npip install pytest-llm-report[litellm]\n\n# Gemini (cloud)\npip install pytest-llm-report[gemini]\n</code></pre></p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>You need <code>pytest-cov</code> with context tracking:</p> <pre><code>pip install pytest-cov\n</code></pre>"},{"location":"getting-started/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/#1-configure-coverage","title":"1. Configure coverage","text":"<p>Add to your <code>pyproject.toml</code>:</p> <pre><code>[tool.pytest.ini_options]\naddopts = [\n    \"--cov=your_package\",\n    \"--cov-context=test\",\n    \"--cov-report=\",\n]\n</code></pre>"},{"location":"getting-started/#2-generate-a-report","title":"2. Generate a report","text":"<pre><code>pytest --llm-report=report.html\n</code></pre>"},{"location":"getting-started/#3-view-the-report","title":"3. View the report","text":"<p>Open <code>report.html</code> in a browser to see: - Test summary (passed/failed/skipped) - Per-test coverage (files and lines) - Filter and search functionality - Dark mode (automatic based on system preferences)</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - All options</li> <li>Coverage Setup - Coverage contexts explained</li> <li>LLM Providers - Add AI annotations</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"privacy/","title":"Privacy","text":"<p>pytest-llm-report is designed with privacy as a priority.</p>"},{"location":"privacy/#default-behavior","title":"Default Behavior","text":"<p>By default, no data leaves your machine: - LLM provider is <code>\"none\"</code> - All processing is local - Reports are local files only</p>"},{"location":"privacy/#when-llm-is-enabled","title":"When LLM is Enabled","text":"<p>If you enable an LLM provider:</p> Provider Data Destination <code>none</code> Nowhere (default) <code>ollama</code> Your local Ollama server <code>litellm</code> External cloud provider <code>gemini</code> External cloud provider"},{"location":"privacy/#whats-sent-to-llm","title":"What's Sent to LLM","text":"<p>Only when explicitly enabled: - Test function source code - Covered file contents (based on context mode) - Test nodeid</p>"},{"location":"privacy/#whats-never-sent","title":"What's Never Sent","text":"<ul> <li>Environment variables</li> <li>Credentials or secrets</li> <li>System information</li> <li>Test output (stdout/stderr)</li> </ul>"},{"location":"privacy/#protecting-sensitive-code","title":"Protecting Sensitive Code","text":""},{"location":"privacy/#opt-out-specific-tests","title":"Opt-out specific tests","text":"<pre><code>@pytest.mark.llm_opt_out\ndef test_sensitive():\n    pass\n</code></pre>"},{"location":"privacy/#exclude-files-from-context","title":"Exclude files from context","text":"<pre><code>[tool.pytest_llm_report]\nllm_context_exclude_globs = [\n    \"*secret*\",\n    \"*proprietary*\",\n    \"internal/*\",\n]\n</code></pre>"},{"location":"privacy/#use-minimal-context","title":"Use minimal context","text":"<pre><code>[tool.pytest_llm_report]\nllm_context_mode = \"minimal\"  # Only test code, no covered files\n</code></pre>"},{"location":"privacy/#recommendations-by-environment","title":"Recommendations by Environment","text":"Environment Recommended Provider Public CI <code>none</code> Private CI <code>ollama</code> or <code>none</code> Local dev <code>ollama</code> Open source <code>none</code> <p>See also: policies/privacy.md</p>"},{"location":"providers/","title":"LLM Providers","text":"<p>pytest-llm-report supports multiple LLM providers for generating test annotations.</p>"},{"location":"providers/#provider-none-default","title":"Provider: none (default)","text":"<p>No LLM calls. Reports contain test results and coverage only.</p> <pre><code>[tool.pytest_llm_report]\nprovider = \"none\"\n</code></pre>"},{"location":"providers/#provider-ollama","title":"Provider: ollama","text":"<p>Local LLM using Ollama.</p>"},{"location":"providers/#setup","title":"Setup","text":"<ol> <li>Install Ollama</li> <li>Pull a model: <code>ollama pull llama3.2</code></li> <li>Configure:</li> </ol> <pre><code>[tool.pytest_llm_report]\nprovider = \"ollama\"\nmodel = \"llama3.2\"\nollama_host = \"http://127.0.0.1:11434\"\n</code></pre>"},{"location":"providers/#recommended-models","title":"Recommended models","text":"Model Size Speed Quality <code>llama3.2</code> 2GB Fast Good <code>qwen2.5-coder:7b</code> 4GB Medium Better <code>qwen2.5-coder:14b</code> 8GB Slow Best"},{"location":"providers/#provider-litellm","title":"Provider: litellm","text":"<p>Cloud LLMs via LiteLLM.</p>"},{"location":"providers/#setup_1","title":"Setup","text":"<ol> <li> <p>Set API key:    <pre><code>export OPENAI_API_KEY=\"sk-...\"\n# or\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre></p> </li> <li> <p>Configure:    <pre><code>[tool.pytest_llm_report]\nprovider = \"litellm\"\nmodel = \"gpt-4o-mini\"\n</code></pre></p> </li> </ol>"},{"location":"providers/#supported-models","title":"Supported models","text":"<ul> <li>OpenAI: <code>gpt-4o-mini</code>, <code>gpt-4o</code></li> <li>Anthropic: <code>claude-3-haiku-20240307</code>, <code>claude-3-5-sonnet-20241022</code></li> <li>Many more via LiteLLM</li> </ul>"},{"location":"providers/#provider-gemini","title":"Provider: gemini","text":"<p>Cloud LLMs via the Gemini API.</p>"},{"location":"providers/#setup_2","title":"Setup","text":"<ol> <li> <p>Set API key:    <pre><code>export GEMINI_API_TOKEN=\"...\"\n</code></pre></p> </li> <li> <p>Configure:    <pre><code>[tool.pytest_llm_report]\nprovider = \"gemini\"\nmodel = \"gemini-1.5-flash-latest\"\n</code></pre></p> </li> </ol>"},{"location":"providers/#supported-models_1","title":"Supported models","text":"<ul> <li><code>gemini-2.5-flash</code></li> <li><code>gemini-2.5-pro</code></li> <li><code>gemini-2.0-flash-exp</code></li> <li><code>gemini-2.0-flash</code></li> <li><code>gemini-2.0-flash-001</code></li> <li><code>gemini-2.0-flash-exp-image-generation</code></li> <li><code>gemini-2.0-flash-lite-001</code></li> <li><code>gemini-2.0-flash-lite</code></li> <li><code>gemini-2.0-flash-lite-preview-02-05</code></li> <li><code>gemini-2.0-flash-lite-preview</code></li> <li><code>gemini-exp-1206</code></li> <li><code>gemini-2.5-flash-preview-tts</code></li> <li><code>gemini-2.5-pro-preview-tts</code></li> <li><code>gemini-flash-latest</code></li> <li><code>gemini-flash-lite-latest</code></li> <li><code>gemini-pro-latest</code></li> <li><code>gemini-2.5-flash-lite</code></li> <li><code>gemini-2.5-flash-image-preview</code></li> <li><code>gemini-2.5-flash-image</code></li> <li><code>gemini-2.5-flash-preview-09-2025</code></li> <li><code>gemini-2.5-flash-lite-preview-09-2025</code></li> <li><code>gemini-3-pro-preview</code></li> <li><code>gemini-3-flash-preview</code></li> <li><code>gemini-3-pro-image-preview</code></li> <li><code>nano-banana-pro-preview</code></li> <li><code>gemini-robotics-er-1.5-preview</code></li> <li><code>gemini-2.5-computer-use-preview-10-2025</code></li> <li><code>deep-research-pro-preview-12-2025</code></li> <li><code>gemma-3-1b-it</code></li> <li><code>gemma-3-4b-it</code></li> <li><code>gemma-3-12b-it</code></li> <li><code>gemma-3-27b-it</code></li> <li><code>gemma-3n-e4b-it</code></li> <li><code>gemma-3n-e2b-it</code></li> </ul>"},{"location":"providers/#caching","title":"Caching","text":"<p>LLM responses are cached to reduce API calls:</p> <pre><code>[tool.pytest_llm_report]\ncache_dir = \".pytest_llm_cache\"\nllm_cache_ttl_seconds = 86400  # 24 hours\n</code></pre> <p>Clear cache: <pre><code>rm -rf .pytest_llm_cache\n</code></pre></p>"},{"location":"report-format/","title":"Report Format","text":"<p>This document describes the JSON report format produced by pytest-llm-report.</p>"},{"location":"report-format/#schema-version","title":"Schema Version","text":"<p>The current schema version is <code>1.0.0</code>. Check the <code>schema_version</code> field in any report output.</p> <p>A JSON schema file is available at <code>schemas/report.schema.json</code>.</p>"},{"location":"report-format/#top-level-fields","title":"Top-Level Fields","text":"Field Type Required Description <code>schema_version</code> string yes Schema version (semver) <code>run_meta</code> object yes Run metadata <code>summary</code> object yes Summary statistics <code>tests</code> array yes Test results (sorted by nodeid) <code>collection_errors</code> array no Collection errors <code>warnings</code> array no Warnings during generation <code>artifacts</code> array no Generated artifact files <code>source_coverage</code> array no Per-file coverage summary <code>custom_metadata</code> object no User-provided metadata <code>sha256</code> string no SHA256 hash of this report <code>hmac_signature</code> string no Optional HMAC signature"},{"location":"report-format/#run-metadata-run_meta","title":"Run Metadata (<code>run_meta</code>)","text":"Field Type Description <code>start_time</code> string UTC start time (ISO 8601) <code>end_time</code> string UTC end time (ISO 8601) <code>duration</code> number Duration in seconds <code>pytest_version</code> string pytest version <code>plugin_version</code> string pytest-llm-report version <code>python_version</code> string Python version <code>platform</code> string OS platform <code>git_sha</code> string Git commit SHA (if available) <code>git_dirty</code> boolean Uncommitted changes flag <code>exit_code</code> integer pytest exit code <code>interrupted</code> boolean Whether run was interrupted <code>collect_only</code> boolean Collect-only run flag <code>collected_count</code> integer Tests collected <code>selected_count</code> integer Tests selected to run <code>deselected_count</code> integer Tests deselected <code>rerun_count</code> integer Total reruns <code>pytest_invocation</code> array Sanitized command line args <code>pytest_config_summary</code> object Sanitized ini options"},{"location":"report-format/#aggregation-fields","title":"Aggregation Fields","text":"<p>When aggregation is enabled (<code>--llm-aggregate-dir</code>):</p> Field Type Description <code>run_id</code> string Unique identifier for this run <code>run_group_id</code> string Group ID for related runs <code>is_aggregated</code> boolean Whether this is aggregated <code>aggregation_policy</code> string Policy: latest, merge, or all <code>run_count</code> integer Number of runs aggregated <code>source_reports</code> array Source report paths and hashes"},{"location":"report-format/#summary-statistics-summary","title":"Summary Statistics (<code>summary</code>)","text":"Field Type Description <code>total</code> integer Total number of tests <code>passed</code> integer Number of passed tests <code>failed</code> integer Number of failed tests <code>skipped</code> integer Number of skipped tests <code>xfailed</code> integer Number of xfailed tests <code>xpassed</code> integer Number of xpassed tests <code>error</code> integer Number of tests with errors <code>total_duration</code> number Total duration in seconds <code>coverage_total_percent</code> number Total coverage percentage (if available)"},{"location":"report-format/#test-result-fields","title":"Test Result Fields","text":"<p>Each test in the <code>tests</code> array has:</p> Field Type Required Description <code>nodeid</code> string yes Full pytest nodeid <code>outcome</code> string yes passed/failed/skipped/xfailed/xpassed/error <code>duration</code> number yes Duration in seconds <code>phase</code> string yes setup/call/teardown <code>error_message</code> string no Error message if failed <code>param_id</code> string no Parameter id for parameterized tests <code>coverage</code> array no Coverage entries <code>llm_annotation</code> object no LLM annotation <code>llm_opt_out</code> boolean no LLM annotation opt-out <code>requirements</code> array no Requirement IDs"},{"location":"report-format/#coverage-entry","title":"Coverage Entry","text":"Field Type Description <code>file_path</code> string Repo-relative file path <code>line_ranges</code> string Compact ranges (e.g., \"1-3, 5\") <code>line_count</code> integer Total lines covered"},{"location":"report-format/#source-coverage-entry","title":"Source Coverage Entry","text":"Field Type Description <code>file_path</code> string Repo-relative file path <code>statements</code> integer Total statements <code>missed</code> integer Missed statements <code>covered</code> integer Covered statements <code>coverage_percent</code> number Coverage percentage for the file <code>covered_ranges</code> string Covered line ranges <code>missed_ranges</code> string Missed line ranges"},{"location":"report-format/#llm-annotation","title":"LLM Annotation","text":"Field Type Description <code>scenario</code> string What the test verifies <code>why_needed</code> string What bug it prevents <code>key_assertions</code> array Critical checks (3-8 bullets) <code>confidence</code> number Confidence score (0-1) <code>error</code> string Error if LLM call failed <code>context_summary</code> object Context used for annotation"},{"location":"security/","title":"Security","text":"<p>Security considerations for pytest-llm-report.</p>"},{"location":"security/#reporting-vulnerabilities","title":"Reporting Vulnerabilities","text":"<p>Please report security issues via: 1. GitHub Private Vulnerability Reporting 2. Email: security@example.com</p> <p>Do not open public issues for security vulnerabilities.</p>"},{"location":"security/#secure-defaults","title":"Secure Defaults","text":"Setting Default Why <code>provider</code> <code>\"none\"</code> No external data transmission Exclude globs secrets patterns Prevents accidental exposure Redact patterns API keys, tokens Sanitizes CLI output"},{"location":"security/#report-integrity","title":"Report Integrity","text":"<p>Reports include: - SHA256 content hash - Git SHA for source correlation - Optional HMAC signature</p> <p>Verify report integrity: <pre><code>sha256sum report.json\n</code></pre></p>"},{"location":"security/#llm-security","title":"LLM Security","text":"<p>When using LLM providers:</p>"},{"location":"security/#local-ollama","title":"Local (Ollama)","text":"<ul> <li>Data stays on your machine</li> <li>No network exposure</li> <li>Full control over model</li> </ul>"},{"location":"security/#cloud-litellm-gemini","title":"Cloud (LiteLLM, Gemini)","text":"<ul> <li>Data sent to external provider</li> <li>Review provider's security policies</li> <li>Consider data residency requirements</li> </ul>"},{"location":"security/#best-practices","title":"Best Practices","text":"<ol> <li>Use <code>provider = \"none\"</code> in public repos</li> <li>Use local Ollama for sensitive code</li> <li>Review excluded patterns for your needs</li> <li>Store reports in access-controlled locations</li> <li>Enable HMAC for compliance requirements</li> </ol> <p>See also: - policies/threat-model.md - policies/security-baseline.md</p>"},{"location":"test-report/","title":"Test Report","text":"<p>Direct Links</p> <ul> <li>Open the latest CI-generated report</li> <li>Open the static example report</li> </ul> <p>The embedded report above is the latest CI-generated run. The static example showcases a mix of passed, failed, skipped, xfailed, xpassed, and error results.</p>"},{"location":"test-report/#about-the-test-report","title":"About the Test Report","text":"<p>This report is automatically generated during CI/CD runs and includes:</p> <ul> <li>Test Results: Pass/fail status for all tests</li> <li>Coverage Data: Per-test coverage information showing which files and lines each test exercises</li> <li>Execution Time: Duration for each test</li> <li>Error Messages: Full error details for failed tests</li> <li>Source Coverage: Per-file covered/missed/percentage summary (similar to pytest-cov)</li> </ul> <p>The report uses the same pytest-llm-report plugin that this documentation describes.</p>"},{"location":"test-report/#report-features","title":"Report Features","text":""},{"location":"test-report/#interactive-filtering","title":"Interactive Filtering","text":"<ul> <li>Search tests by name</li> <li>Filter by test status (passed, failed, skipped, etc.)</li> <li>Toggle visibility of passed tests</li> </ul>"},{"location":"test-report/#coverage-details","title":"Coverage Details","text":"<p>For each test, you can expand to see:</p> <ul> <li>Which source files were executed during the test</li> <li>Specific line ranges covered in each file</li> <li>Total line count per file</li> </ul>"},{"location":"test-report/#report-metadata","title":"Report Metadata","text":"<p>The report header shows:</p> <ul> <li>Total test count and breakdown by status</li> <li>Overall test suite duration</li> <li>Total coverage percentage (when available)</li> <li>Run metadata (timestamp, Python version, etc.)</li> </ul>"},{"location":"test-report/#source-coverage-summary","title":"Source Coverage Summary","text":"<p>At the bottom of the report you'll find a table of source files with:</p> <ul> <li>Statements, missed, and covered counts</li> <li>Coverage percentage per file</li> <li>Covered and missed line ranges for quick inspection</li> </ul> <p>\u2190 Back to Documentation</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and solutions for pytest-llm-report.</p>"},{"location":"troubleshooting/#no-coverage-contexts-found","title":"\"No coverage contexts found\"","text":"<p>Problem: Report shows warnings about missing coverage contexts.</p> <p>Solution: Run with <code>--cov-context=test</code>: <pre><code>pytest --cov=my_pkg --cov-context=test --llm-report=report.html\n</code></pre></p>"},{"location":"troubleshooting/#no-coverage-file-found","title":"\"No .coverage file found\"","text":"<p>Problem: Plugin can't find coverage data.</p> <p>Solutions: 1. Ensure pytest-cov is installed: <code>pip install pytest-cov</code> 2. Add <code>--cov=your_package</code> to pytest options 3. Check you're not running with <code>--no-cov</code></p>"},{"location":"troubleshooting/#empty-coverage-in-reports","title":"Empty coverage in reports","text":"<p>Problem: Tests show 0 covered files.</p> <p>Solutions: 1. Verify <code>--cov-context=test</code> is set 2. Check source path: <code>--cov=correct/path</code> 3. Clear old data: <code>coverage erase</code></p>"},{"location":"troubleshooting/#llm-annotations-not-appearing","title":"LLM annotations not appearing","text":"<p>Problem: Tests don't have LLM annotations.</p> <p>Solutions: 1. Check provider is set: <code>provider = \"ollama\"</code>, <code>\"litellm\"</code>, or <code>\"gemini\"</code> 2. Verify Ollama is running: <code>ollama serve</code> 3. Check API key is set for cloud providers (LiteLLM or Gemini) 4. Confirm the provider dependency is installed (e.g., <code>pytest-llm-report[litellm]</code>) 5. Look for errors in pytest output or the report's \"LLM error\" field</p>"},{"location":"troubleshooting/#ollama-connection-refused","title":"Ollama connection refused","text":"<p>Problem: Can't connect to Ollama.</p> <p>Solutions: 1. Start Ollama: <code>ollama serve</code> 2. Check host: <code>ollama_host = \"http://127.0.0.1:11434\"</code> 3. Verify model is pulled: <code>ollama list</code></p>"},{"location":"troubleshooting/#report-not-generated","title":"Report not generated","text":"<p>Problem: No HTML/JSON output.</p> <p>Solutions: 1. Check output path: <code>--llm-report=/path/to/report.html</code> 2. Ensure directory exists 3. Check for errors at end of pytest output</p>"},{"location":"troubleshooting/#dark-mode-not-working","title":"Dark mode not working","text":"<p>Problem: Report doesn't switch to dark theme.</p> <p>Solutions: 1. Check your OS/browser dark mode setting is enabled 2. Ensure you're using a modern browser (Chrome 76+, Firefox 67+, Safari 12.1+) 3. Some browsers require <code>about:config</code> flags for <code>prefers-color-scheme</code></p>"},{"location":"troubleshooting/#still-stuck","title":"Still stuck?","text":"<p>Open an issue on GitHub with: - pytest-llm-report version - Full pytest output (with <code>-v</code>) - Your pyproject.toml configuration</p>"},{"location":"adr/0001-coverage-contexts/","title":"ADR 0001: Coverage Contexts","text":""},{"location":"adr/0001-coverage-contexts/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/0001-coverage-contexts/#context","title":"Context","text":"<p>pytest-llm-report needs to know which test covered which lines of code. This enables per-test coverage reporting and provides context for LLM annotation.</p>"},{"location":"adr/0001-coverage-contexts/#decision","title":"Decision","text":"<p>We require users to run with <code>--cov-context=test</code> which instructs coverage.py to record the test nodeid for each covered line.</p>"},{"location":"adr/0001-coverage-contexts/#consequences","title":"Consequences","text":""},{"location":"adr/0001-coverage-contexts/#positive","title":"Positive","text":"<ul> <li>Precise per-test coverage data</li> <li>Works with pytest-cov out of the box</li> <li>No custom instrumentation needed</li> </ul>"},{"location":"adr/0001-coverage-contexts/#negative","title":"Negative","text":"<ul> <li>Requires user to add <code>--cov-context=test</code> flag</li> <li>Slightly increases coverage data size</li> <li>Some older coverage.py versions may not support it fully</li> </ul>"},{"location":"adr/0001-coverage-contexts/#alternatives-considered","title":"Alternatives Considered","text":"<ol> <li>Custom pytest hook - Would require reimplementing coverage</li> <li>Post-processing coverage data - Less accurate, timing issues</li> <li>AST analysis - Complex, doesn't capture runtime behavior</li> </ol>"},{"location":"adr/0002-llm-providers/","title":"ADR 0002: LLM Providers","text":""},{"location":"adr/0002-llm-providers/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/0002-llm-providers/#context","title":"Context","text":"<p>The plugin needs to support multiple LLM providers for test annotation while maintaining privacy and flexibility.</p>"},{"location":"adr/0002-llm-providers/#decision","title":"Decision","text":"<p>Implement a provider abstraction with four providers: 1. <code>none</code> - No LLM (default) 2. <code>ollama</code> - Local LLM via Ollama 3. <code>litellm</code> - Cloud LLMs via LiteLLM library 4. <code>gemini</code> - Cloud LLMs via Gemini API</p>"},{"location":"adr/0002-llm-providers/#consequences","title":"Consequences","text":""},{"location":"adr/0002-llm-providers/#positive","title":"Positive","text":"<ul> <li>Privacy by default (provider=none)</li> <li>Local option for sensitive code (Ollama)</li> <li>Broad cloud support via LiteLLM (100+ models)</li> <li>Dedicated Gemini integration for teams standardizing on Google AI</li> <li>Clean abstraction for adding new providers</li> </ul>"},{"location":"adr/0002-llm-providers/#negative","title":"Negative","text":"<ul> <li>LiteLLM is a heavy dependency</li> <li>Ollama requires local setup</li> <li>Different providers have different quality/speed</li> </ul>"},{"location":"adr/0002-llm-providers/#alternatives-considered","title":"Alternatives Considered","text":"<ol> <li>Direct API integration - More work, less flexibility</li> <li>Single provider - Limits user choice</li> <li>LangChain - Too heavy, unnecessary complexity</li> </ol>"},{"location":"adr/0003-report-formats/","title":"ADR 0003: Report Formats","text":""},{"location":"adr/0003-report-formats/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"adr/0003-report-formats/#context","title":"Context","text":"<p>Reports need to be useful for both humans and CI systems.</p>"},{"location":"adr/0003-report-formats/#decision","title":"Decision","text":"<p>Support three output formats: 1. HTML - Human-readable, interactive 2. JSON - Machine-readable, CI integration 3. PDF (optional) - Portable, archival</p>"},{"location":"adr/0003-report-formats/#consequences","title":"Consequences","text":""},{"location":"adr/0003-report-formats/#positive","title":"Positive","text":"<ul> <li>HTML for daily use (filtering, search)</li> <li>JSON for CI/CD pipelines and tooling</li> <li>PDF for compliance and sharing</li> <li>Deterministic output (sorted by nodeid)</li> </ul>"},{"location":"adr/0003-report-formats/#negative","title":"Negative","text":"<ul> <li>PDF requires Playwright (heavy dependency)</li> <li>Three formats to maintain</li> <li>HTML templates add complexity</li> </ul>"},{"location":"adr/0003-report-formats/#format-details","title":"Format Details","text":""},{"location":"adr/0003-report-formats/#html","title":"HTML","text":"<ul> <li>Jinja2 templates</li> <li>Embedded CSS/JS (single file)</li> <li>Dark mode support</li> <li>Client-side filtering</li> </ul>"},{"location":"adr/0003-report-formats/#json","title":"JSON","text":"<ul> <li>Schema-validated</li> <li>SHA256 hash for integrity</li> <li>Optional HMAC signature</li> <li>Aggregation-ready</li> </ul>"},{"location":"adr/0003-report-formats/#pdf","title":"PDF","text":"<ul> <li>HTML-to-PDF via Playwright</li> <li>Matches HTML layout</li> <li>Optional (extra dependency)</li> </ul>"}]}