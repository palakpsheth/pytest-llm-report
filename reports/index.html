<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Test Report &bull; 387 tests</title>
    <!-- Optional: Inter font from rsms.me CDN. Falls back to system fonts if unavailable. -->
    <link rel="stylesheet" href="https://rsms.me/inter/inter.css">
    <style>
/* Modern Color Palette */
:root {
    --bg-color: #f8fafc;
    --text-primary: #1e293b;
    --text-secondary: #64748b;
    --border-color: #e2e8f0;
    --card-bg: #ffffff;
    --surface-muted: #f1f5f9;
    --primary-color: #3b82f6;
    color-scheme: light dark;

    /* Status Colors */
    --passed-bg: #dcfce7;
    --passed-text: #166534;
    --failed-bg: #fee2e2;
    --failed-text: #991b1b;
    --skipped-bg: #fef9c3;
    --skipped-text: #854d0e;
    --xfailed-bg: #ffedd5;
    --xfailed-text: #9a3412;
    --xpassed-bg: #f3e8ff;
    --xpassed-text: #6b21a8;
    --error-bg: #fee2e2;
    --error-text: #991b1b;
}

body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    background-color: var(--bg-color);
    color: var(--text-primary);
    line-height: 1.5;
    margin: 0;
    padding: 0;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 2rem;
}

/* Header */
header {
    margin-bottom: 2rem;
    border-bottom: 1px solid var(--border-color);
    padding-bottom: 1rem;
    display: flex;
    justify-content: space-between;
    align-items: center;
}

h1 {
    font-size: 1.875rem;
    font-weight: 700;
    color: var(--text-primary);
    margin: 0;
}

.meta {
    font-size: 0.875rem;
    color: var(--text-secondary);
}

/* Summary Grid */
.summary {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
    gap: 1rem;
    margin-bottom: 2rem;
}

.summary-card {
    background: var(--card-bg);
    border-radius: 0.5rem;
    padding: 1.5rem;
    box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.1), 0 1px 2px -1px rgb(0 0 0 / 0.1);
    text-align: center;
    border: 1px solid var(--border-color);
    transition: transform 0.2s;
}

.summary-card:hover {
    transform: translateY(-2px);
}

.summary-card .count {
    font-size: 2.25rem;
    font-weight: 700;
    line-height: 1;
    margin-bottom: 0.5rem;
}

.summary-card .label {
    text-transform: uppercase;
    font-size: 0.75rem;
    font-weight: 600;
    letter-spacing: 0.05em;
    color: var(--text-secondary);
}

/* Status Colors for Summary */
.summary-card.passed .count {
    color: var(--passed-text);
}

.summary-card.failed .count {
    color: var(--failed-text);
}

.summary-card.skipped .count {
    color: var(--skipped-text);
}

.summary-card.xfailed .count {
    color: var(--xfailed-text);
}

.summary-card.xpassed .count {
    color: var(--xpassed-text);
}

.summary-card.coverage .count {
    color: var(--primary-color);
}

/* Filters */
.filters {
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.5rem;
    border: 1px solid var(--border-color);
    margin-bottom: 1.5rem;
    display: flex;
    flex-direction: column;
    gap: 0.75rem;
}

.filter-input {
    flex: 1;
    padding: 0.5rem 1rem;
    border: 1px solid var(--border-color);
    border-radius: 0.375rem;
    font-size: 0.875rem;
    background: var(--card-bg);
    color: var(--text-primary);
}

.filter-input::placeholder {
    color: var(--text-secondary);
}

.filter-statuses {
    display: flex;
    flex-wrap: wrap;
    gap: 0.5rem;
}

.filter-chip {
    display: inline-flex;
    align-items: center;
    gap: 0.35rem;
    padding: 0.25rem 0.75rem;
    border-radius: 9999px;
    border: 1px solid var(--border-color);
    background: var(--surface-muted);
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.04em;
}

.filter-chip input {
    margin: 0;
}

.filter-chip.passed {
    background: var(--passed-bg);
    color: var(--passed-text);
}

.filter-chip.failed {
    background: var(--failed-bg);
    color: var(--failed-text);
}

.filter-chip.skipped {
    background: var(--skipped-bg);
    color: var(--skipped-text);
}

.filter-chip.xfailed {
    background: var(--xfailed-bg);
    color: var(--xfailed-text);
}

.filter-chip.xpassed {
    background: var(--xpassed-bg);
    color: var(--xpassed-text);
}

.filter-chip.error {
    background: var(--error-bg);
    color: var(--error-text);
}

/* Test List */
.test-list {
    display: flex;
    flex-direction: column;
    gap: 0.75rem;
}

.test-row {
    background: var(--card-bg);
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
    overflow: hidden;
}

.test-header {
    padding: 1rem;
    display: flex;
    align-items: center;
    gap: 1rem;
    cursor: pointer;
    background: var(--card-bg);
}

.test-header:hover {
    background: var(--surface-muted);
}

.status-badge {
    padding: 0.25rem 0.75rem;
    border-radius: 9999px;
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
}

.status-passed {
    background: var(--passed-bg);
    color: var(--passed-text);
}

.status-failed {
    background: var(--failed-bg);
    color: var(--failed-text);
}

.status-skipped {
    background: var(--skipped-bg);
    color: var(--skipped-text);
}

.status-xfailed {
    background: var(--xfailed-bg);
    color: var(--xfailed-text);
}

.status-xpassed {
    background: var(--xpassed-bg);
    color: var(--xpassed-text);
}

.status-error {
    background: var(--error-bg);
    color: var(--error-text);
}

.test-name {
    flex: 1;
    font-family: monospace;
    font-size: 0.9rem;
    color: var(--text-primary);
    word-break: break-all;
}

.test-meta {
    display: flex;
    gap: 1rem;
    align-items: center;
    color: var(--text-secondary);
    font-size: 0.875rem;
}

/* Details Section */
.test-details {
    padding: 0 1rem 1rem 1rem;
    border-top: 1px solid var(--border-color);
    background: var(--surface-muted);
}

.detail-section {
    margin-top: 1rem;
}

.detail-title {
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
    color: var(--text-secondary);
    margin-bottom: 0.5rem;
}

.coverage-item {
    font-family: monospace;
    font-size: 0.85rem;
    padding: 0.25rem 0;
    border-bottom: 1px solid var(--border-color);
    display: grid;
    grid-template-columns: minmax(200px, 2fr) minmax(120px, 1fr);
    gap: 1rem;
}

.coverage-list {
    background: var(--card-bg);
    border-radius: 0.375rem;
    border: 1px solid var(--border-color);
    overflow: hidden;
}

.source-coverage {
    margin-top: 2rem;
}

.source-coverage h2 {
    margin: 0 0 1rem;
    font-size: 1.5rem;
}

.source-coverage-table {
    display: grid;
    gap: 0.35rem;
}

.source-coverage-header,
.source-coverage-row {
    display: grid;
    grid-template-columns: minmax(200px, 2fr) repeat(4, minmax(60px, 0.5fr)) minmax(
            140px,
            1fr
        ) minmax(140px, 1fr);
    align-items: center;
    gap: 0.75rem;
    padding: 0.75rem 1rem;
    border-radius: 0.5rem;
}

.source-coverage-header {
    background: var(--surface-muted);
    font-size: 0.75rem;
    font-weight: 700;
    text-transform: uppercase;
    letter-spacing: 0.04em;
    color: var(--text-secondary);
}

.source-coverage-row {
    background: var(--card-bg);
    border: 1px solid var(--border-color);
    font-size: 0.85rem;
}

.source-path {
    font-family: monospace;
    word-break: break-word;
}

.source-lines {
    font-family: monospace;
    color: var(--text-secondary);
    word-break: break-word;
}

.llm-annotation {
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.375rem;
    border: 1px solid var(--border-color);
}

.llm-annotation p {
    margin: 0 0 0.5rem 0;
}

.llm-annotation p:last-child {
    margin-bottom: 0;
}

.llm-annotation ul {
    margin: 0.5rem 0 0;
    padding-left: 1.25rem;
}

.llm-annotation li {
    margin-bottom: 0.25rem;
}

.error-message {
    font-family: monospace;
    color: var(--failed-text);
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.375rem;
    border: 1px solid var(--failed-bg);
    white-space: pre-wrap;
    overflow-x: auto;
}

/* HTML5 Progress Bar for Coverage */
progress {
    width: 60px;
}

/* Utility: Hidden state for filtering */
.hidden {
    display: none !important;
}

/* Dark Mode Support */
@media (prefers-color-scheme: dark) {
    :root {
        --bg-color: #0f172a;
        --text-primary: #f1f5f9;
        --text-secondary: #94a3b8;
        --border-color: #334155;
        --card-bg: #1e293b;
        --surface-muted: #0b1220;
        --primary-color: #60a5fa;

        /* Status Colors - Adjusted for dark mode */
        --passed-bg: #14532d;
        --passed-text: #86efac;
        --failed-bg: #7f1d1d;
        --failed-text: #fca5a5;
        --skipped-bg: #713f12;
        --skipped-text: #fde047;
        --xfailed-bg: #7c2d12;
        --xfailed-text: #fdba74;
        --error-bg: #7f1d1d;
        --error-text: #fca5a5;
    }

    /* Adjust box shadows for dark mode */
    .summary-card {
        box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.3), 0 1px 2px -1px rgb(0 0 0 / 0.3);
    }
}

@media print {
    body {
        background: #ffffff;
        color: #0f172a;
    }

    .container {
        max-width: none;
        padding: 1rem 1.5rem;
    }

    header {
        border-bottom: 2px solid var(--border-color);
    }

    .filters {
        display: none;
    }

    .summary-card,
    .test-row {
        box-shadow: none;
    }

    .test-header {
        background: #ffffff;
    }

    .test-row {
        page-break-inside: avoid;
        break-inside: avoid;
    }

    .test-details {
        background: #ffffff;
    }

    .llm-annotation {
        background: var(--surface-muted);
    }

    progress {
        width: 80px;
    }
}

body.pdf-mode .filters {
    display: none;
}

body.pdf-mode .test-row {
    page-break-inside: avoid;
    break-inside: avoid;
}    </style>
    <script>
// pytest-llm-report interactive features

// Global state for filters
const activeStatuses = new Set(['passed', 'failed', 'skipped', 'xfailed', 'xpassed', 'error']);

// Filter tests based on search input and outcome filters
function filterTests() {
    const query = document.getElementById('searchInput').value.toLowerCase();
    document.querySelectorAll('.test-row').forEach(row => {
        const nodeid = row.querySelector('.test-name').textContent.toLowerCase();
        const statusMatch = row.dataset.status ? activeStatuses.has(row.dataset.status) : false;
        const matchesSearch = nodeid.includes(query);
        row.classList.toggle('hidden', !matchesSearch || !statusMatch);
    });
}

// Toggle visibility of status filters
function toggleStatus(checkbox) {
    const status = checkbox.dataset.status;
    if (checkbox.checked) {
        activeStatuses.add(status);
    } else {
        activeStatuses.delete(status);
    }
    filterTests();
}

// Initialize interactive features after DOM is ready
document.addEventListener('DOMContentLoaded', function () {
    'use strict';

    // Toggle dark mode on preference
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.documentElement.dataset.theme = 'dark';
    }

    // Default: expand all details
    document.querySelectorAll('details').forEach(details => {
        details.setAttribute('open', '');
    });

    const params = new URLSearchParams(window.location.search);
    if (params.get('pdf') === '1') {
        document.body.classList.add('pdf-mode');
    }
});    </script>
</head>
<body>
    <div class="container">
        <header>
            <div>
                <h1>Test Report</h1>
                <div class="meta">
                    Run ID: 20947644922-py3.12 &bull;
                    Generated: 2026-01-13 06:57:28 &bull;
                    Duration: 34.37s<br>
                    <strong>Plugin:</strong> v0.1.0
                        (2f498263985a34902252c53c11fb820445bd8f21)
[dirty]<br>
                    <strong>Repo:</strong> v0.1.0
                        (b9e3e95cf545147fb1baae5e547e056f45911b0a)
<br>
                    <strong>LLM:</strong> ollama / llama3.2:1b
                        (minimal context,
                         383 annotated, 3 errors)
                </div>
            </div>
            <div style="text-align: right">
                <div style="font-size: 2rem; font-weight: 700; color: var(--primary-color)">
                    92.91%
                </div>
                <div class="meta">Total Coverage</div>
            </div>
        </header>

        <!-- Summary Cards -->
        <div class="summary">
            <div class="summary-card">
                <div class="count">387</div>
                <div class="label">Total Tests</div>
            </div>
            <div class="summary-card passed">
                <div class="count">387</div>
                <div class="label">Passed</div>
            </div>
            <div class="summary-card failed">
                <div class="count">0</div>
                <div class="label">Failed</div>
            </div>
            <div class="summary-card skipped">
                <div class="count">0</div>
                <div class="label">Skipped</div>
            </div>
            <div class="summary-card xfailed">
                <div class="count">0</div>
                <div class="label">XFailed</div>
            </div>
            <div class="summary-card xpassed">
                <div class="count">0</div>
                <div class="label">XPassed</div>
            </div>
            <div class="summary-card failed">
                <div class="count">0</div>
                <div class="label">Errors</div>
            </div>
        </div>

        <!-- Filters -->
        <div class="filters">
            <input type="text" id="searchInput" class="filter-input" placeholder="Search tests..." onkeyup="filterTests()">
            <div class="filter-statuses" aria-label="Filter by status">
                <label class="filter-chip passed">
                    <input type="checkbox" data-status="passed" checked onchange="toggleStatus(this)">
                    Passed
                </label>
                <label class="filter-chip failed">
                    <input type="checkbox" data-status="failed" checked onchange="toggleStatus(this)">
                    Failed
                </label>
                <label class="filter-chip skipped">
                    <input type="checkbox" data-status="skipped" checked onchange="toggleStatus(this)">
                    Skipped
                </label>
                <label class="filter-chip xfailed">
                    <input type="checkbox" data-status="xfailed" checked onchange="toggleStatus(this)">
                    XFailed
                </label>
                <label class="filter-chip xpassed">
                    <input type="checkbox" data-status="xpassed" checked onchange="toggleStatus(this)">
                    XPassed
                </label>
                <label class="filter-chip error">
                    <input type="checkbox" data-status="error" checked onchange="toggleStatus(this)">
                    Error
                </label>
            </div>
        </div>

        <!-- Test List -->
        <div class="test-list">
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_all_policy</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the aggregate function correctly handles all policy when aggregating multiple reports.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where an aggregation of multiple reports with different policies would not retain all tests due to incorrect filtering.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The aggregated report should have exactly two retained tests, regardless of the policy used.</li>
                                        <li>Each retained test should be from one of the original reports.</li>
                                        <li>The aggregate function should correctly filter out tests based on the aggregation policy.</li>
                                        <li>The aggregate function should not retain any tests that are explicitly excluded by the policy.</li>
                                        <li>The aggregate function should handle multiple policies correctly, allowing for different filtering rules to be applied.</li>
                                        <li>The test case should pass even if the policy is set to 'none' or 'partial'.</li>
                                        <li>The test case should fail if the policy is set to 'all' and one of the reports has a retained test that does not match the policy.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 52, 55-56, 59, 61-63, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134, 141, 146, 148-153, 155, 157-159, 170, 217, 219-223, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_dir_not_exists</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the aggregate function does not attempt to aggregate a non-existent directory.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential error where an empty or nonexistent directory is attempted to be aggregated.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregate` method of the `aggregator` instance should return None when the provided directory does not exist.</li>
                                        <li>A `PathError` exception should not be raised if the directory does not exist.</li>
                                        <li>The aggregate function should not attempt to create or modify any files or directories within the non-existent directory.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 52, 55-57, 109-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_latest_policy</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `aggregate_latest_policy` function correctly selects the latest report when there are multiple reports of the same test with different times.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the `aggregate` function would incorrectly select the first report it encounters as the latest, instead of the most recent one.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `outcome` of the aggregated report is set to 'passed' if there are multiple reports of the same test with different times.</li>
                                        <li>The number of tests in the aggregated report is 1.</li>
                                        <li>The outcome of the first test in the aggregated report is 'passed'.</li>
                                        <li>The `run_meta.run_count` attribute is equal to 2.</li>
                                        <li>The `summary.passed` attribute is equal to 1.</li>
                                        <li>The `summary.failed` attribute is equal to 0.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">77 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134, 141, 146, 148-153, 155, 157-159, 170, 182, 184-188, 190-191, 194, 217, 219-223, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_no_dir_configured</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The aggregator function should not throw an error when no directory configuration is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the aggregator function throws an exception when no directory configuration is specified.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>agg.aggregate() should return None</li>
                                        <li>agg.aggregate() should not raise an exception</li>
                                        <li>mock_config.aggregate_dir should be set to None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 44, 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_no_reports</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that aggregate function returns None when no reports exist and no files are found.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the aggregate function fails to return an empty list of reports when there are no files or reports available.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The aggregate function should return None for this scenario.</li>
                                        <li>The aggregate function should not attempt to access any files or reports.</li>
                                        <li>There should be no error raised if the file system is empty.</li>
                                        <li>The aggregate function should handle a case where there are no reports available without raising an exception.</li>
                                        <li>The aggregate function should preserve the original behavior when called with a non-empty list of reports.</li>
                                        <li>The aggregate function should not attempt to access any files or reports in this scenario.</li>
                                        <li>There should be no assertion error raised if the file system is empty.</li>
                                        <li>The aggregate function should handle a case where there are no files found without raising an exception.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52, 55-57, 109-110, 113-114, 170)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_with_coverage_and_llm_annotations</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that coverage and LLM annotations are properly deserialized and can be re-serialized.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in core functionality by ensuring accurate coverage and LLM annotation deserialization.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>coverage: Ensure coverage is correctly deserialized with the expected file paths and line ranges.</li>
                                        <li>LLM Annotation: Verify correct deserialization of LLM annotation with scenario, why-need, and key assertions.</li>
                                        <li>Re-serialization: Confirm that the aggregated report can be re-serialized with accurate coverage and LLM annotation.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">81 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134-137, 141-144, 146, 148-153, 155, 157-159, 170, 182, 184-188, 194, 217, 219-223, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">32 lines (ranges: 40-43, 104-107, 109-111, 113, 115, 161-165, 167, 169, 171, 173, 176-180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_with_source_coverage</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `aggregate` method correctly aggregates source coverage data from a temporary report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the aggregation of source coverage data fails to produce accurate results due to incomplete or missing reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `source_coverage` attribute of the aggregated result is an instance of `SourceCoverageEntry`.</li>
                                        <li>The `file_path` attribute of the first `SourceCoverageEntry` in the aggregated result matches the expected file path.</li>
                                        <li>All statements in the source code are covered by at least 83.33% of the total coverage.</li>
                                        <li>At least 2 out of 10 statements were missed in the source code.</li>
                                        <li>The coverage percentage is between 1 and 5, inclusive for the first range and exclusive for the second range.</li>
                                        <li>The missing ranges are '6' and '12'.</li>
                                        <li>All covered statements are within the specified ranges.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">66 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 148-155, 157-159, 170, 182, 184-186, 194, 217, 219-220, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_load_coverage_from_source</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loading coverage from configured source file when option is not set.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an unconfigured source file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that the `_load_coverage_from_source` method returns `None` when `llm_coverage_source` is `None`.</li>
                                        <li>Verify that the `_load_coverage_from_source` method raises a UserWarning with the message 'Coverage source not found' when `llm_coverage_source` is '/nonexistent/coverage'.</li>
                                        <li>Verify that the `_load_coverage_from_source` method returns `None` when `llm_coverage_source` does not exist.</li>
                                        <li>Verify that the mock coverage object is created and returned by the mock cov_cls before calling it.</li>
                                        <li>Verify that the mock mapper object is created and returned by the mock mapper_cls before calling it.</li>
                                        <li>Verify that the `map_source_coverage` method of the mock mapper returns a list with one entry.</li>
                                        <li>Verify that the `load` method of the mock cov_cls is called once.</li>
                                        <li>Verify that the `map_source_coverage` method of the mock mapper has been called with the mock cov object.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 245-246, 248-249, 251, 253-257, 259, 262-263, 265-266, 269-271, 273)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_recalculate_summary</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the _recalculate_summary method preserves the total duration of tests and maintains correct coverage percentages.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the total duration is not preserved or the coverage percentage does not match the expected values after recalculating the summary.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total number of tests passed, failed, skipped, xfailed, xpassed, and error should be equal to the original counts.</li>
                                        <li>The total duration of all tests should remain unchanged.</li>
                                        <li>The coverage percentage should match the expected value for each test type (total, passed, failed, skipped, xfailed, xpassed, and error).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 217, 219-233, 235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_skips_invalid_json</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case: Skipping an invalid JSON aggregation report</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where a test fails due to an unexpected error when aggregating reports with non-JSON content.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregate` function should skip the 'invalid.json' file because it does not contain valid JSON data.</li>
                                        <li>The `aggregate` function should only count the valid report ('valid.json') in its run meta.</li>
                                        <li>When an invalid JSON file is encountered, a UserWarning is raised with a message indicating that the aggregation skipped the file.</li>
                                        <li>The test verifies that the `aggregate` function correctly handles missing fields in the aggregated report.</li>
                                        <li>The test ensures that only the valid report is included in the run meta of the aggregated result.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">71 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119-120, 125, 127-128, 148-153, 155, 157-159, 162, 164-166, 168, 170, 182, 184-186, 194, 217, 219-220, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation_maximal.py::TestAggregationMaximal::test_recalculate_summary_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the aggregator recalculates the summary correctly when given a set of tests with varying coverage totals.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the aggregator's behavior when handling different coverage scenarios, ensuring accurate summary calculations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>summary.total == 2</li>
                                        <li>summary.passed == 1</li>
                                        <li>summary.failed == 1</li>
                                        <li>summary.coverage_total_percent == 88.5</li>
                                        <li>summary.total_duration == 3.0</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 44, 217, 219-225, 235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_cached_tests_are_skipped</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> This test verifies that cached tests are skipped by the annotator.</p>
                                <p><strong>Why Needed:</strong> To prevent regression in the annotator's caching behavior and ensure it only skips cached tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `mock_provider` is not called with any arguments.</li>
                                        <li>The `mock_cache` is not called with any arguments.</li>
                                        <li>The `mock_assembler` is not called with any arguments.</li>
                                        <li>The `test_cached_tests_are_skipped` method does not call the `cached_test` function.</li>
                                        <li>The `cached_test` function is not called by the `mock_provider` or `mock_cache`.</li>
                                        <li>The `mock_assembler` does not call the `annotator` instance before calling its methods.</li>
                                        <li>The `mock_provider` and `mock_cache` do not have any side effects that would cause them to be called with arguments.</li>
                                        <li>The `test_cached_tests_are_skipped` method is not called by other test functions in the same scope.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-81, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The annotator function is called concurrently without proper synchronization.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential concurrency issue where multiple annotations are performed simultaneously, potentially leading to incorrect results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_provider.call_args.assert_called_once_with(self, 'annotation', mock_assembler)</li>
                                        <li>mock_cache.call_args.assert_called_once_with(self, 'annotation', mock_assembler)</li>
                                        <li>mock_assembler.call_args.assert_called_once_with(self, 'annotation')</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">64 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104-112, 129-135, 137, 139, 229-232, 234, 236-237, 239, 245-246, 248-253, 255, 261, 266)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation_handles_failures</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that concurrent annotation handles failures by verifying the annotator's behavior.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the annotator fails to handle failures in a concurrent environment.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotator should not fail when encountering an error while processing annotations concurrently.</li>
                                        <li>The annotator should log the error and continue processing other annotations without interruption.</li>
                                        <li>The annotator's cache should be updated correctly even if it encounters errors during annotation.</li>
                                        <li>The annotator's progress bar should update correctly even if it encounters errors during annotation.</li>
                                        <li>The annotator's output should not be affected by concurrent failures.</li>
                                        <li>The annotator's error messages should contain relevant information about the failure.</li>
                                        <li>The annotator's logging behavior should be consistent across different environments.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104-112, 129-135, 137-139, 229-232, 234, 236-237, 239, 245-246, 248-253, 255, 261-264, 266)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_progress_reporting</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the progress reporting is correctly implemented in the annotate function.</p>
                                <p><strong>Why Needed:</strong> This test prevents regressions where the progress reporting might not be accurately tracked or reported.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `progress` attribute of the annotated object should increase as the annotation progresses.</li>
                                        <li>The `total_progress` attribute of the annotator should correctly track the total number of annotations.</li>
                                        <li>The `annotation_id` attribute of each annotation should increment in a predictable manner.</li>
                                        <li>The `annotated_object` attribute of each annotation should be updated correctly after progress reporting.</li>
                                        <li>The `progress` and `total_progress` attributes of the annotator should be reset to 0 when no new annotations are added.</li>
                                        <li>The `annotation_id` attribute of the last annotated object should match its original value before the test started.</li>
                                        <li>The `annotated_object` attribute of the last annotated object should contain a valid annotation data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation</span>
                        <div class="test-meta">
                            <span>12.00s</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies the sequential annotation functionality of the annotator.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in sequential annotation by ensuring that the annotator properly handles multiple annotations in a row.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `mock_provider` is called with a valid `annotator` instance.</li>
                                        <li>The `mock_cache` is not called during the execution of the test.</li>
                                        <li>No exceptions are raised when calling `mock_assembler` on the `annotator` instance.</li>
                                        <li>The `annotator` instance is properly updated after each annotation.</li>
                                        <li>No unexpected behavior occurs when annotating multiple items in a row.</li>
                                        <li>The `mock_provider` and `mock_cache` instances are not reused across tests.</li>
                                        <li>The `mock_assembler` instance is only called once during the execution of the test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_skips_if_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the annotator skips tests when LLM is disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where LLM is disabled and annotators are still executed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `annotate_tests` should not be called with an empty list of tests.</li>
                                        <li>The function `annotate_tests` should not call the `LLM` provider.</li>
                                        <li>The `LLM` provider should be set to 'none' before calling `annotate_tests`.</li>
                                        <li>If LLM is disabled, the annotator should do nothing and return without making any changes to the test results.</li>
                                        <li>The function `annotate_tests` should not modify the test results or report any changes.</li>
                                        <li>The function `annotate_tests` should only be called with a valid configuration object.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 45-46)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_skips_if_provider_unavailable</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The annotator skips tests when the provider is unavailable.</p>
                                <p><strong>Why Needed:</strong> To prevent skipping of critical tests due to a provider's unavailability.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocked Provider Mock</li>
                                        <li>Provider Unavailable Error</li>
                                        <li>Test Skipping Due to Provider Unavailability</li>
                                        <li>Annotator Behavior When Provider Unavailable</li>
                                        <li>Error Handling for Provider Unavailability</li>
                                        <li>Test Case Prioritization Based on Provider Availability</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_concurrent_with_progress_and_errors</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests annotator concurrent with progress and errors</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the annotator fails to report progress or first error when annotating concurrently.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should correctly report progress and first error in concurrent mode.</li>
                                        <li>The function should append 'Processing 2 test(s)' to the progress messages list.</li>
                                        <li>The function should include 'LLM annotation' in the progress messages list.</li>
                                        <li>The function should not fail to report any errors when annotating concurrently.</li>
                                        <li>The function should correctly handle scenarios where multiple tasks are annotated concurrently.</li>
                                        <li>The function should update the cache with the correct number of annotations.</li>
                                        <li>The function should return 2 annotations as expected when there are 2 concurrent tasks.</li>
                                        <li>The function should not raise an exception when annotating concurrently.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 229-232, 234, 236-237, 239-242, 245-246, 248-253, 255-258, 261-264, 266)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_sequential_rate_limit_wait</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Should wait if rate limit interval has not elapsed.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in sequential annotation tasks where the rate limit interval has not yet elapsed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The time.sleep() function was called.</li>
                                        <li>The monotonic() function returned a value greater than 1.0s.</li>
                                        <li>The time.sleep() function was called again.</li>
                                        <li>The monotonic() function returned a value less than or equal to 1.0s.</li>
                                        <li>The time.sleep() function did not call itself.</li>
                                        <li>The monotonic() function returned the correct interval (1.0s) after the first sleep call.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">23 lines (ranges: 165-168, 170-171, 173-174, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_tests_cached_progress</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Should report progress for cached tests.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the annotator does not report progress for cached tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `progress_msgs` list should contain any messages that indicate a cache hit (e.g. `(cache): test_cached`).</li>
                                        <li>The `progress_msgs` list should not be empty after running the test.</li>
                                        <li>Any message in the `progress_msgs` list should start with '(cache): ' to identify it as related to caching.</li>
                                        <li>The `progress_msgs` list should contain messages for all cached tests (not just one).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">37 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-84, 97-98, 100, 127, 129-135, 137, 139)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_tests_provider_unavailable</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the annotator fails to annotate tests when the provider is not available.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the annotator would incorrectly report tests as successful even though they are unavailable.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mocks.get_provider().is_available() returns False</li>
                                        <li>annotate_tests(tests, config) will print 'not available. Skipping annotations'</li>
                                        <li>tests[0].outcome is not 'passed' after the annotation fails</li>
                                        <li>mock_provider.is_available() was called with a return value of False</li>
                                        <li>mock_provider.return_value is set to mock_provider</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_coverage_v2.py::test_base_parse_response_malformed_json_after_extract</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `test_base_parse_response_malformed_json_after_extract` function will fail when a malformed JSON is passed.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function `test_base_parse_response_malformed_json_after_extract` fails due to an invalid JSON in the response, causing it to raise a `JSONDecodeError`.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `_parse_response(response)` will return `None` when the input is malformed JSON.</li>
                                        <li>The error message returned by `annotation.error` should be 'Failed to parse LLM response as JSON'.</li>
                                        <li>The function `provider._parse_response(response)` will raise a `JSONDecodeError` exception with the specified error message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 186-187, 190-191, 194-195, 220-221)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_coverage_v2.py::test_base_parse_response_non_string_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests that the `test_base_parse_response_non_string_fields` function handles non-string fields in the response data correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly assumes all fields are strings when they may not be.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return an annotation with the correct scenario value.</li>
                                        <li>The function should return an annotation with the correct why_needed list value.</li>
                                        <li>The function should return an annotation with the correct key_assertion list value.</li>
                                        <li>The `scenario` attribute of the annotation should be set to '123'.</li>
                                        <li>The `why_needed` attribute of the annotation should be set to ['list'].</li>
                                        <li>The `key_assertions` attribute of the annotation should contain the string 'a'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203-207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_gemini_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `get_gemini_provider` function returns a `GeminiProvider` instance.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails if the `gemini` provider is not properly configured or if there's an issue with the Gemini API.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The returned `provider` object should be an instance of `GeminiProvider`.</li>
                                        <li>The `provider` attribute of the returned `provider` object should contain a valid `GeminiProvider` instance.</li>
                                        <li>The `provider` attribute of the returned `provider` object should have a value that is not `None` or `undefined`.</li>
                                        <li>The `provider` attribute of the returned `provider` object should be an instance of `GeminiProvider` with the correct class name.</li>
                                        <li>The `provider` attribute of the returned `provider` object should contain the correct attributes (e.g., `name`, `api_key`, etc.).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 245, 247, 249, 252, 257, 262-263, 265)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_invalid_provider</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that a ValueError is raised when an unknown LLM provider is specified.</p>
                                <p><strong>Why Needed:</strong> This test prevents the introduction of an Unknown LLM provider error in future code changes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider` raises a ValueError with the message 'Unknown LLM provider: invalid'.</li>
                                        <li>The exception type is `ValueError`.</li>
                                        <li>The exception message contains the string 'Unknown LLM provider: invalid'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 245, 247, 249, 252, 257, 262, 267)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_litellm_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `get_litellm_provider` function returns a LitELLMProvider instance.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `get_litellm_provider` function does not return an instance of LiteLLMProvider if the provider is not set to 'litellm'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider(config)` returns an instance of `LiteLLMProvider`.</li>
                                        <li>The `provider` attribute of the returned `LiteLLMProvider` instance is set to `'litellm'`.</li>
                                        <li>An exception is not raised if the provider is not set to 'litellm'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52-53, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_noop_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `get_noop_provider` function returns a `NoopProvider` instance when no provider is specified.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case a new provider is added without updating the test.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The returned value should be an instance of `NoopProvider`.</li>
                                        <li>The `provider` attribute of the returned value should not be `None`.</li>
                                        <li>The type of the returned value should match the expected type, which is `NoopProvider`.</li>
                                        <li>The `get_provider` function should return a valid provider instance when no provider is specified.</li>
                                        <li>A new provider should not break the test if it's added without updating the test.</li>
                                        <li>The test should pass even after adding a new provider to the system.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_ollama_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `get_ollama_provider` function returns an instance of OllamaProvider.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the `get_ollama_provider` function fails to return an instance of OllamaProvider due to incorrect configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider(config)` is called with the correct provider name 'ollama'.</li>
                                        <li>The returned value `provider` is an instance of `OllamaProvider`.</li>
                                        <li>The type of `provider` is correctly set to `OllamaProvider` using the `isinstance()` function.</li>
                                        <li>An error message or exception is not raised when calling `get_provider(config)` with the correct configuration.</li>
                                        <li>The provider name 'ollama' is properly formatted and does not contain any typos.</li>
                                        <li>The config object passed to `get_provider(config)` has the required attributes (provider) set correctly.</li>
                                        <li>A custom error message or exception is not raised when calling `get_provider(config)` with the correct configuration.</li>
                                        <li>The provider name 'ollama' is properly formatted and does not contain any leading or trailing whitespace.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 245, 247, 249, 252-253, 255)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_available_caches_result</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the LlmProvider defaults implementation provides a valid cache result.</p>
                                <p><strong>Why Needed:</strong> The current implementation of LlmProvider does not provide a valid cache result, which can cause unexpected behavior in downstream applications.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `is_available()` method returns True for both cached and uncached configurations.</li>
                                        <li>The `checks` attribute is incremented correctly when the `_check_availability()` method is called.</li>
                                        <li>The `is_available()` method returns False for a cache configuration that does not have any available caches.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 107-108, 110-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_model_name_defaults_to_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'get_model_name' method of the ConcreteProvider class returns the name set in the test model configuration.</p>
                                <p><strong>Why Needed:</strong> Without this default configuration, the LLM model may not be properly initialized or configured.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>the 'model' attribute is set to 'test-model'</li>
                                        <li>the 'name' attribute of the provider object is equal to 'test-model'</li>
                                        <li>the provider's get_model_name() method returns 'test-model'</li>
                                        <li>the configuration's model attribute has been successfully loaded</li>
                                        <li>the configuration's name attribute matches the expected default value</li>
                                        <li>the provider's name attribute does not match the expected default value</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 136)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_rate_limits_defaults_to_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_rate_limits` method of a `ConcreteProvider` instance returns `None` when no rate limits are specified.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the default rate limit for LLM providers is not properly set to None when no custom rate limits are provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_rate_limits()` method of the provider instance returns an empty list or None.</li>
                                        <li>The `rate_limit` attribute of the provider instance does not contain any valid numerical values.</li>
                                        <li>The `max_rate` and `min_rate` attributes of the provider instance are set to a default value (e.g., 1.0) that is not compatible with the provided rate limits.</li>
                                        <li>The `rate_limits` attribute of the provider instance contains invalid or unsupported types (e.g., strings, booleans).</li>
                                        <li>The `max_rate_limit` and `min_rate_limit` attributes of the provider instance are set to a default value that exceeds the maximum allowed rate limit for the specific LLM model.</li>
                                        <li>The `rate_limits` attribute is not updated when custom rate limits are provided in the configuration.</li>
                                        <li>The `get_rate_limits()` method raises an exception (e.g., ValueError) if no valid rate limits can be determined from the provider instance.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 128)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_is_local_defaults_to_false</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that `is_local()` returns False when default defaults are used.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of default defaults being used.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` object is not local.</li>
                                        <li>The `provider` object has a non-local configuration.</li>
                                        <li>The `provider.is_local()` method returns False.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestHashSource::test_consistent_hash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the consistency of a cache with a single source.</p>
                                <p><strong>Why Needed:</strong> To ensure that the cache maintains its expected behavior when using the same source function.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The hash value of the source function should be the same every time it is called.</li>
                                        <li>The hash value of the source function should not change after multiple calls to the function.</li>
                                        <li>The cache should store the source function's return values with their corresponding hashes.</li>
                                        <li>The cache should maintain a consistent order of insertion for functions that have different hashes.</li>
                                        <li>The cache should handle function calls correctly, including caching the result of each call.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestHashSource::test_different_source_different_hash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `hash_source` function with different sources.</p>
                                <p><strong>Why Needed:</strong> Prevents a bug where two functions with the same source code but different names produce the same hash value.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `hash_source` function should return a different hash value for two different source strings.</li>
                                        <li>The `hash_source` function should not be able to deduce the source of a function from its name.</li>
                                        <li>The `hash_source` function should raise an error if given the same input multiple times.</li>
                                        <li>The `hash_source` function should preserve the original source code when hashing.</li>
                                        <li>The `hash_source` function should return different hash values for functions with the same name but different parameter lists.</li>
                                        <li>The `hash_source` function should not be able to deduce the source of a function from its docstring.</li>
                                        <li>The `hash_source` function should raise an error if given a function that does not have a `__name__` attribute.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestHashSource::test_hash_length</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the length of the hash generated by `hash_source`.</p>
                                <p><strong>Why Needed:</strong> Prevent a potential issue where the hash length is not consistent across different inputs.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The hash should be at least 16 characters long.</li>
                                        <li>The hash should be exactly 16 characters long (e.g., 'aabbccdd').</li>
                                        <li>No leading zeros are allowed in the hash.</li>
                                        <li>No trailing zeros are allowed in the hash.</li>
                                        <li>The hash does not contain any repeated characters.</li>
                                        <li>All characters in the hash are unique.</li>
                                        <li>The hash is a valid SHA-256 hash (e.g., '1234567890abcdef').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_clear</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that clearing the cache removes all entries.</p>
                                <p><strong>Why Needed:</strong> Prevents a bug where some cache entries are not properly cleared after test::a and test::b are removed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The number of cache entries should be exactly 2.</li>
                                        <li>test::a should not be present in the cache.</li>
                                        <li>test::b should not be present in the cache.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 129, 132-136, 141)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_does_not_cache_errors</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that annotations with errors do not get cached.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where error annotations are incorrectly cached.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation 'error' in the cache key 'test::foo' should be None.</li>
                                        <li>The annotation 'abc123' in the cache key 'test::foo' should not match any existing value.</li>
                                        <li>No error message should be stored in the cache for annotations with errors.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 39-41, 53, 55-56, 86, 88, 118-119, 121)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_get_missing</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case 'test_get_missing' verifies that the function returns None for missing entries in the cache.</p>
                                <p><strong>Why Needed:</strong> The test prevents a potential bug where the function does not handle missing cache entries correctly, potentially leading to incorrect results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `cache.get()` should return `None` when called with an invalid key.</li>
                                        <li>The function `cache.get()` should raise a `KeyError` when called with an invalid key.</li>
                                        <li>The function `cache.get()` should not modify the cache when called with an existing key that is missing from it.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 39-41, 53, 55-56, 118-119, 121)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_set_and_get</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that annotations are stored and retrieved correctly from the cache.</p>
                                <p><strong>Why Needed:</strong> Prevents bypass attacks by ensuring that annotations are cached before they can be used to bypass security measures.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Check if the annotation is set with the correct key</li>
                                        <li>Check if the annotation's scenario matches the expected value</li>
                                        <li>Check if the confidence of the annotation is as expected</li>
                                        <li>Verify that the cache returns a non-None result for the given key</li>
                                        <li>Verify that the retrieved annotation has the correct confidence level</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 39-41, 53, 55, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorCollectionErrors::test_collection_error_structure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that a collection error has the correct 'nodeid' and 'message' attributes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where a CollectionError is incorrectly structured, potentially leading to incorrect handling or reporting of errors in the collector.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'nodeid' attribute of the error object should be equal to 'test_bad.py'.</li>
                                        <li>The 'message' attribute of the error object should be equal to 'SyntaxError'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorCollectionErrors::test_get_collection_errors_initially_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that an empty collection is returned when the `get_collection_errors` method is called on a newly created `TestCollector` instance with an empty configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where an empty collection is not immediately recognized as an error.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_collection_errors()` method returns an empty list when the input collection is empty.</li>
                                        <li>An empty collection is considered an error and should be handled accordingly in subsequent steps.</li>
                                        <li>A test asserting that an empty collection is returned is necessary to ensure the method behaves as expected in this scenario.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_context_override_default_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the default value of llm_context_override in TestCollectorMarkerExtraction.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the default value of llm_context_override is not set correctly, potentially leading to incorrect results or unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The llm_context_override attribute should be None for the given TestCaseResult.</li>
                                        <li>The llm_context_override attribute should not be set to any other value than None for the given TestCaseResult.</li>
                                        <li>If llm_context_override is set to a different value, it should be immediately overridden by the actual context.</li>
                                        <li>The default value of llm_context_override should be None for the given TestCaseResult.</li>
                                        <li>If the default value of llm_context_override is not None, it should be checked and verified in subsequent tests.</li>
                                        <li>The test should verify that the default value of llm_context_override is correctly set to None for the given TestCaseResult.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_opt_out_default_false</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the default value of llm_opt_out is correctly set to False for a test case.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the default value of llm_opt_out is incorrectly set to True.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The llm_opt_out attribute should be set to False.</li>
                                        <li>The llm_opt_out attribute should not be set to True.</li>
                                        <li>The TestCaseResult object should have a llm_opt_out attribute with the correct value (False).</li>
                                        <li>The TestCaseResult object should not have a llm_opt_out attribute with an incorrect value (True).</li>
                                        <li>The llm_opt_out attribute should be correctly initialized in the TestCaseResult object.</li>
                                        <li>The TestCaseResult object should not have any other attributes that are not relevant to this test.</li>
                                        <li>The TestCaseResult object should pass all assertions without raising any exceptions.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorOutputCapture::test_capture_disabled_by_default</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the output capture is disabled by default.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the output capture was enabled by default.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.capture_failed_output should be set to False</li>
                                        <li>config.capture_enabled should be set to False</li>
                                        <li>output_capture_enabled should not be True</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorOutputCapture::test_capture_max_chars_default</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the default value of `capture_output_max_chars` is set to 4000.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the default max chars value is not correctly set to prevent excessive output.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert config.capture_output_max_chars == 4000</li>
                                        <li>assert config.capture_output_max_chars != 10000</li>
                                        <li>assert config.capture_output_max_chars != 5000</li>
                                        <li>assert config.capture_output_max_chars != 2000</li>
                                        <li>assert config.capture_output_max_chars != 3000</li>
                                        <li>assert config.capture_output_max_chars == 4000</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorXfailHandling::test_xfail_failed_is_xfailed</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'xfail failures should be recorded as xfailed' verifies that failed xfail tests are correctly marked as xfailed in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that failed xfail tests are properly recorded and marked as such, preventing incorrect reporting of failure status.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'passed', 'skipped' and 'duration' fields of the SimpleNamespace object `report` should be set to False.</li>
                                        <li>The 'outcome' field of the SimpleNamespace object `result` should be set to 'xfailed'.</li>
                                        <li>The 'wasxfail' field of the SimpleNamespace object `report` should be set to 'expected failure'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212, 216, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorXfailHandling::test_xfail_passed_is_xpassed</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'xfail passes should be recorded as xpassed' verifies that the test collector correctly records and reports xfail tests as passed.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where an unexpected pass in a test is not properly reported as failed, potentially masking issues with test behavior or reporting.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `result.outcome` of the `test_unexpected_pass` node should be 'xpassed' after calling the `handle_runtest_logreport` method on the `TestCollector` instance.</li>
                                        <li>The `wasxfail` attribute of the `test_xfail.py::test_unexpected_pass` node should match the expected failure message.</li>
                                        <li>The `duration` and `longrepr` attributes of the `test_xfail.py::test_unexpected_pass` node should be set to 0.01 seconds and an empty string respectively after calling the `handle_runtest_logreport` method on the `TestCollector` instance.</li>
                                        <li>The `skipped` attribute of the `test_xfail.py::test_unexpected_pass` node should remain False after calling the `handle_runtest_logreport` method on the `TestCollector` instance.</li>
                                        <li>The `failed` attribute of the `test_xfail.py::test_unexpected_pass` node should be set to False after calling the `handle_runtest_logreport` method on the `TestCollector` instance.</li>
                                        <li>The `passed` attribute of the `test_xfail.py::test_unexpected_pass` node should remain True after calling the `handle_runtest_logreport` method on the `TestCollector` instance.</li>
                                        <li>After calling the `handle_runtest_logreport` method, the `config` object passed to the `TestCollector` constructor should not be modified.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212-214)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestTestCollector::test_create_collector</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `TestCollector` class initializes correctly and returns an empty collection.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the collector is initialized with incorrect or missing configuration settings, potentially leading to incorrect results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>collector.results == {}</li>
                                        <li>collector.collection_errors == []</li>
                                        <li>collector.collected_count == 0</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestTestCollector::test_get_results_sorted</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `get_results` method of TestCollector to ensure it returns sorted results by node ID.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where unsorted results are returned due to a bug in the sorting logic.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The list of node IDs returned by the `get_results` method is sorted in ascending order.</li>
                                        <li>The first element of the list is 'a_test.py::test_a'.</li>
                                        <li>The second element of the list is 'z_test.py::test_z'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestTestCollector::test_handle_collection_finish</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `handle_collection_finish` method correctly tracks collected and deselected counts.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where items are not properly tracked as collected or deselected after collection finish.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `collected_count` attribute of the collector should be equal to the number of items that were collected.</li>
                                        <li>The `deselected_count` attribute of the collector should be equal to the number of items that were deselected.</li>
                                        <li>The `collected_count` and `deselected_count` attributes should match the expected values after calling `handle_collection_finish` with a list of collected and deselected items.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 78-79, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_disabled_via_handle_report</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'Should not capture if config disabled (integration via handle_runtest_logreport)' verifies that the test does not capture output when `capture_failed_output` is set to False.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the test captures output even though `capture_failed_output` is set to False, which could lead to unexpected behavior or false positives in the test results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'nodeid' attribute of the report object should be empty.</li>
                                        <li>The 'outcome' attribute of the report object should be 'failed'.</li>
                                        <li>The 'when' attribute of the report object should be set to 'call'.</li>
                                        <li>The 'passed' attribute of the report object should be False.</li>
                                        <li>The 'failed' attribute of the report object should be True.</li>
                                        <li>The 'skipped' attribute of the report object should be False.</li>
                                        <li>The 'capstdout' attribute of the report object should be set to 'output'.</li>
                                        <li>The 'wasxfail' attribute of the report object should not exist.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stderr</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `test_capture_output_stderr` function captures stderr correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the `test_capture_output_stderr` function does not capture stderr.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `captured_stderr` attribute of the `TestCaseResult` object is set to 'Some error'.</li>
                                        <li>The `report.capstderr` method is called with an argument equal to 'Some error'.</li>
                                        <li>The `collector._capture_output` function is called with a result and report object that have been populated with the expected values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264, 268-269)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stdout</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `test_capture_output_stdout` function captures stdout correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the collector does not capture stdout when it should.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `captured_stdout` attribute of the `TestCaseResult` object is set to 'Some output'.</li>
                                        <li>The `report.capstdout` method returns 'Some output' as expected.</li>
                                        <li>The `report.capstderr` method does not interfere with the captured stdout.</li>
                                        <li>The collector correctly captures stdout even when it fails and reports an error.</li>
                                        <li>The `test_capture_output_stdout` function is able to distinguish between captured stdout and reported errors.</li>
                                        <li>The test result indicates that the collector successfully captured stdout.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_truncated</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `TestCollector` truncates output exceeding the specified maximum characters.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the collector fails to capture and display the entire output of a test, potentially leading to misleading results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The captured stdout should be truncated at 10 characters.</li>
                                        <li>The captured stderr is empty.</li>
                                        <li>The `captured_stdout` attribute of the `TestCaseResult` object should contain only the first 10 characters of the output.</li>
                                        <li>The collector does not fail when capturing output that exceeds the maximum characters.</li>
                                        <li>The collector correctly truncates output and displays it in the report.</li>
                                        <li>The captured stderr is empty even if the output exceeds the maximum characters.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_create_result_with_item_markers</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test creates a result with item markers and verifies the expected behavior.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the collector correctly extracts item markers from an item and returns them in the expected format.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `param_id` of the extracted marker is set to 'param1'.</li>
                                        <li>The value of `llm_opt_out` is set to True.</li>
                                        <li>The value of `llm_context_override` is set to 'complete'.</li>
                                        <li>A list of requirements is returned with values ['REQ-1', 'REQ-2'].</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">35 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163-164, 167-169, 171, 181-182, 185-189, 198-200, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_repr_crash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `collectors` module's ability to handle ReprFileLocation in error reports.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential crash when encountering ReprFileLocation in error reports, ensuring the collector can recover and continue processing other errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_error` method of `TestCollector` should return 'Crash report' when called with an instance of `ReprFileLocation`.</li>
                                        <li>The `longrepr` attribute of `Report` should have a value that is a string representation of 'Crash report'.</li>
                                        <li>The `__str__` method of `Report.longrepr` should be able to return the expected string 'Crash report' when called. If this assertion fails, it may indicate a bug in the collector or its dependencies.</li>
                                        <li>If an instance of `ReprFileLocation` is passed to `_extract_error`, it should not cause a crash report.</li>
                                        <li>The `collectors` module should be able to recover from errors and continue processing other errors without crashing.</li>
                                        <li>If an error occurs during the collection process, the test should fail with a meaningful error message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_string</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `_extract_error` method returns a string 'longrepr' when called with a `report` object containing a 'longrepr' attribute.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the collector might not correctly extract error strings from reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report.longrepr` attribute is set to 'Some error occurred'.</li>
                                        <li>The `_extract_error` method returns 'Some error occurred'.</li>
                                        <li>The extracted string matches the expected value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_fallback</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `_extract_skip_reason` method returns `None` when no longrepr is provided.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails if there are no longreprs to extract skip reasons from.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_skip_reason` method does not raise an exception or return a specific value when `report.longrepr` is `None`.</li>
                                        <li>The `_extract_skip_reason` method returns `None` instead of raising an exception or returning a default value when `report.longrepr` is `None`.</li>
                                        <li>The `_extract_skip_reason` method checks for the existence of `report.longrepr` before attempting to extract skip reasons and raises an exception if it does not exist.</li>
                                        <li>When `report.longrepr` is `None`, the `_extract_skip_reason` method correctly returns `None` without raising an exception or returning a default value.</li>
                                        <li>The `_extract_skip_reason` method checks for the existence of `report.longrepr` before attempting to extract skip reasons and raises an exception if it does not exist.</li>
                                        <li>When `report.longrepr` is `None`, the `_extract_skip_reason` method correctly returns `None` without raising an exception or returning a default value.</li>
                                        <li>The `_extract_skip_reason` method checks for the existence of `report.longrepr` before attempting to extract skip reasons and raises an exception if it does not exist.</li>
                                        <li>When `report.longrepr` is `None`, the `_extract_skip_reason` method correctly returns `None` without raising an exception or returning a default value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250, 252)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_string</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test `test_extract_skip_reason_string` verifies that the `_extract_skip_reason` method returns a string as expected.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues where the method does not return the correct skip reason string, potentially leading to incorrect reporting or debugging.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The method should return 'Just skipped' as the skip reason string.</li>
                                        <li>The method should extract the `longrepr` attribute from the report object and return it as a string.</li>
                                        <li>The method should not raise an exception if no longrepr is available.</li>
                                        <li>The method should handle cases where the report object does not have a `longrepr` attribute.</li>
                                        <li>The method should not modify the original report object.</li>
                                        <li>The method should preserve the original type and behavior of the report object.</li>
                                        <li>The method should return a string that can be used as a valid reason for skipping in the collector.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_tuple</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that extract skip reason tuple works correctly.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to incorrect handling of tuples with longrepr messages.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report.longrepr` attribute should contain a tuple with `(file, line, message)` as its first element.</li>
                                        <li>The `message` field within this tuple should be 'Skipped for reason'.</li>
                                        <li>The `longrepr` value should match the expected string when converted to a Python literal.</li>
                                        <li>The `report.longrepr` attribute should contain the entire tuple as its second element.</li>
                                        <li>The `message` field within this tuple should be 'Skipped for reason'.</li>
                                        <li>The `longrepr` value should match the expected string when converted to a Python literal.</li>
                                        <li>The `report.longrepr` attribute should contain the entire tuple as its second element.</li>
                                        <li>The `message` field within this tuple should be 'Skipped for reason'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_collection_report_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> When the `handle_collection_report` method is called with a report that indicates collection failure, it should record this error.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential data loss due to unhandled collection errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of `collector.collection_errors` should be equal to 1.</li>
                                        <li>The nodeid in the first `collector.collection_errors` item should match 'test_broken.py'.</li>
                                        <li>The message in the first `collector.collection_errors` item should match 'SyntaxError'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 58, 60-65, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_rerun</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'handle_runtest_rerun' verifies that the TestCollector handles rerun attribute correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the TestCollector does not handle reruns correctly, potentially leading to incorrect results or failures.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>res.rerun_count should be equal to 1 (the expected number of reruns).</li>
                                        <li>res.final_outcome should be 'failed' (indicating that the test failed after rerunning).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_setup_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> TestCollectorReportHandling::test_handle_runtest_setup_failure verifies that a setup error is recorded in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where TestCollector handles runtest log reports as if they were successful, potentially leading to incorrect reporting of setup errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>res.outcome should be 'error'</li>
                                        <li>res.phase should be 'setup'</li>
                                        <li>res.error_message should be 'Setup failed'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99-103, 109-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_teardown_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case 'Should record error if teardown fails after pass' verifies that the collector correctly records an error when a teardown operation fails.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the collector handles teardown failures properly and reports them as errors in the results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert res.outcome == 'error'</li>
                                        <li>assert res.phase == 'teardown'</li>
                                        <li>assert res.error_message == 'Cleanup failed'</li>
                                        <li>assert call_report.wasxfail</li>
                                        <li>assert teardown_report.wasxfail</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">38 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-115, 124, 127-128, 130, 132-133, 135-137, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_model_parsing_edge_cases</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the GeminiProvider's _parse_preferred_models method with edge cases to ensure correct behavior.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case a new model is added to the preferred list without updating the parsing logic.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return an empty list when the 'model' parameter is None.</li>
                                        <li>The function should return an empty list when the 'model' parameter is set to 'All'.</li>
                                        <li>The function should not throw any errors or exceptions when the 'model' parameter is None and the preferred models are already empty.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 134, 136-139, 141-142, 385, 387, 417-424)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_rate_limiter_edge_math</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter does not allow excessive tokens when there are available tokens but no requests.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the rate limiter allows too many tokens when there are enough available tokens, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert limiter.next_available_in(60) > 0</li>
                                        <li>assert limiter.next_available_in(10) == 0</li>
                                        <li>assert limiter.record_tokens(50) < 100</li>
                                        <li>assert len(limiter.tokens) > 0</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">35 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_models_to_dict_variants</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `to_dict()` method of `SourceCoverageEntry` and `LlmAnnotation` correctly returns their respective values.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in coverage booster models where the coverage percentage is not being accurately calculated.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `coverage_percent` attribute of `SourceCoverageEntry` should be equal to 50.0.</li>
                                        <li>The `error` attribute of `LlmAnnotation` should be 'timeout'.</li>
                                        <li>The `duration` attribute of `RunMeta` should be equal to 1.0.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 71-78, 104-107, 109, 111-113, 115, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_create_mapper</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `CoverageMapper` instance should be initialized with the provided configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `CoverageMapper` instance's configuration is not properly set.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert mapper.config is config</li>
                                        <li>assert mapper.warnings == []</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 44-45)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_get_warnings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `get_warnings` method in the `CoverageMapper` class should be able to retrieve a list of warnings from the coverage report.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues where the function returns an incorrect type (in this case, a list) when it's expected to return a specific type (a list).</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_warnings` method is called on an instance of `CoverageMapper`.</li>
                                        <li>A `Config` object is created and passed to the `CoverageMapper` instance.</li>
                                        <li>The `get_warnings` method is called on the `CoverageMapper` instance.</li>
                                        <li>The result of the `get_warnings` method is checked to be a list.</li>
                                        <li>The type of the result is checked to be a list.</li>
                                        <li>A warning message or error is expected to be returned from the `get_warnings` method.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 44-45, 308)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_map_coverage_no_coverage_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests coverage map function with no coverage file.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function returns an empty dictionary when there is no coverage file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `Path.exists` mock returns False.</li>
                                        <li>The `glob.glob` mock returns an empty list.</li>
                                        <li>The `map_coverage` function should return an empty dictionary.</li>
                                        <li>There should be at least one warning in the `warnings` list.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_all_phases</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `CoverageMapper` extracts all phases when `include_phase=all`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the coverage is not extracted for all phases when `include_phase=all`.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The method `_extract_nodeid` should return the correct node ID for each phase.</li>
                                        <li>The method `_extract_nodeid` should handle cases where the phase name contains multiple words (e.g., 'test_foo|run').</li>
                                        <li>The method `_extract_nodeid` should correctly extract the node ID for phases that are not explicitly mentioned in the test code (e.g., `test.py::test_foo|teardown`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_empty_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `extract_nodeid` method returns `None` when given an empty context.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `extract_nodeid` method does not handle empty contexts correctly, potentially leading to incorrect coverage metrics or unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_nodeid` method should return `None` for an empty string.</li>
                                        <li>The `_extract_nodeid` method should return `None` for a `None` value as context.</li>
                                        <li>The test should fail when given an empty context, indicating that the `extract_nodeid` method is not handling it correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 44-45, 216-217)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_filters_setup</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the function `extract_nodeid_filters_setup` to ensure it correctly filters out setup phase when `include_phase` is set to 'run'.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not filter out setup phase, potentially leading to incorrect coverage analysis.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `_extract_nodeid` should return None for node IDs that start with `test.py::test_foo|setup`.</li>
                                        <li>The function `_extract_nodeid` should raise an error when called with a non-string argument.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 44-45, 216, 220, 224-225, 228-230)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_with_run_phase</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `extract_nodeid` method extracts the correct node ID from a run phase context.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the node ID extraction logic is correct and consistent across different phases.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The extracted node ID matches the expected value (`test.py::test_foo`) for the given module and function name.</li>
                                        <li>The extracted node ID does not contain any leading or trailing whitespace.</li>
                                        <li>The extracted node ID does not contain any invalid characters (e.g., special regex patterns).</li>
                                        <li>The extracted node ID is present in the `run` phase context.</li>
                                        <li>The extracted node ID is present for all modules and functions within the given module.</li>
                                        <li>No exceptions are raised when extracting the node ID from a run phase context.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_full_logic</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test should extract contexts for full logic coverage of _extract_contexts method.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression that would occur if the _extract_contexts method did not cover all paths in _extract_contexts.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mocked context_by_lineno returns a dictionary with test_one and test_two as keys, where each key has multiple values.</li>
                                        <li>the function should return a list of files that have app.py as their file path.</li>
                                        <li>the line count for the returned files in test_one should be 2 (lines 1 and 2).</li>
                                        <li>the line count for the returned files in test_two should not be affected by the mock data.</li>
                                        <li>mocked contexts_by_lineno does not return any additional context files that are not already included in the result.</li>
                                        <li>the function should correctly handle cases where there are multiple lines of code with the same file path (e.g., app.py::test_one|run).</li>
                                        <li>the function should correctly handle cases where a file has no matching contexts (e.g., README.md::test_three|run).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">57 lines (ranges: 44-45, 118, 121-122, 127, 131-135, 137-140, 144, 148, 150, 152-153, 156, 160-163, 165, 167-168, 173, 176, 178-184, 187-189, 191-194, 196, 199-200, 202, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">13 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65-67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_no_contexts</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `extract_contexts` method returns an empty dictionary when there are no test contexts.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where the coverage map is incorrectly populated with context information for files without any test contexts.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_data.contexts_by_lineno.return_value == {}</li>
                                        <li>mock_data.measured_files.return_value == ['app.py']</li>
                                        <li>result == {}</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 44-45, 118, 121-122, 127, 131-135, 144-146)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_nodeid_variants</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `CoverageMapper` extracts node IDs for tests with missing lines in different phases.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the `CoverageMapper` incorrectly filters out node IDs from tests with missing lines in certain phases.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_nodeid` method returns the expected node ID for each test.</li>
                                        <li>The `_extract_nodeid` method correctly ignores node IDs from tests with missing lines in certain phases.</li>
                                        <li>The `CoverageMapper` uses the correct phase to filter out node IDs from tests.</li>
                                        <li>The `CoverageMapper` ignores node IDs from tests without a specified phase.</li>
                                        <li>The `CoverageMapper` does not incorrectly report node IDs for tests with missing lines in different phases.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231-234, 236, 239)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_no_files</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the test_load_coverage_data_no_files function correctly handles the case when no coverage files exist.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the test would fail due to missing coverage data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return None for _load_coverage_data() without any .coverage files.</li>
                                        <li>There should be exactly one warning message with code 'W001' when no .coverage files exist.</li>
                                        <li>The warnings list should contain only one item with the specified code.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 44-45, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_read_error</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test should handle errors reading coverage files with a corrupt .coverage file.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the CoverageMapper can correctly handle corrupted coverage data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function _load_coverage_data() returns None when an error occurs during read.</li>
                                        <li>Any warnings generated by mapper.warnings contain the message 'Failed to read coverage data'.</li>
                                        <li>The function _load_coverage_data() raises an Exception with the message 'Corrupt coverage file' when a corrupt .coverage file is encountered.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 44-45, 72-73, 83, 86, 88, 92, 94-96, 107-111, 114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_with_parallel_files</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test should handle parallel coverage files from xdist and verify that the CoverageMapper correctly updates its internal data structures.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the CoverageMapper class, which is responsible for handling parallel coverage files from xdist. Without this test, the mapper may not update its internal data structures correctly when dealing with parallel files, leading to incorrect coverage data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `update` method of the `CoverageData` instance should be called at least twice during the `_load_coverage_data` process.</li>
                                        <li>The `update` method of the `CoverageData` instance should not be called more than once during the `_load_coverage_data` process.</li>
                                        <li>The number of times the `update` method is called for each mock CoverageData instance should match the expected number of parallel coverage files.</li>
                                        <li>The `update` method should only be called when a new parallel coverage file is created and removed from the temporary directory.</li>
                                        <li>The `update` method should not be called when the existing parallel coverage file is updated or deleted in the temporary directory.</li>
                                        <li>The `update` method should call the original mock CoverageData instance's `update` method for each mock instance.</li>
                                        <li>The `update` method should update the internal data structures of the CoverageMapper correctly after calling its side effect instances.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 44-45, 72-73, 83, 86, 88, 92, 94, 98, 101-104, 106)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_coverage_no_data</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `map_coverage` method returns an empty dictionary when `_load_coverage_data` returns None.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to a missing coverage map being returned from `_load_coverage_data`.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return an empty dictionary when no data is loaded.</li>
                                        <li>The function should not raise any exceptions when no data is loaded.</li>
                                        <li>The function should handle the case where `None` is returned by `_load_coverage_data` correctly.</li>
                                        <li>The function should preserve the original coverage map values.</li>
                                        <li>The function should ignore missing keys in the coverage map.</li>
                                        <li>The function should return a dictionary with default values (e.g., `{}`) when no data is loaded.</li>
                                        <li>The function should not throw an exception when `None` is returned by `_load_coverage_data`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 44-45, 58-60)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_analysis_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test coverage map source coverage analysis error.</p>
                                <p><strong>Why Needed:</strong> Prevents test failure due to analysis errors during source coverage mapping.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The mock analysis2 function should be called with an Exception exception.</li>
                                        <li>The mocked mock_data.measured_files.return_value should return ['app.py'].</li>
                                        <li>The mock_cov.get_data.return_value should raise an Exception exception.</li>
                                        <li>The entries list should have zero length after skipping files with errors.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 44-45, 243-244, 246-248, 250, 252-254, 259, 261, 263-268, 271, 299-300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_comprehensive</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the test maps all paths in `map_source_coverage` to a comprehensive coverage report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that all paths are covered, even if analysis2 is not able to provide complete information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `entries[0].file_path` should return the path of the source file being tested (`'app.py'`).</li>
                                        <li>The function `entries[0].statements` should return the number of statements in the source file (`3`).</li>
                                        <li>The function `entries[0].covered` should return the percentage of covered lines (`2`).</li>
                                        <li>The function `entries[0].missed` should return the number of missed lines (`1`).</li>
                                        <li>The function `entries[0].coverage_percent` should return the percentage of covered lines (`66.67`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">32 lines (ranges: 44-45, 243-244, 246-248, 250, 252, 259-261, 273, 276-279, 281-283, 285-293, 295, 299-300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors.py::test_make_warning</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `make_warning` factory function to ensure it correctly returns a WarningCode.W001_NO_COVERAGE instance with the specified detail.</p>
                                <p><strong>Why Needed:</strong> To prevent a bug where an unknown warning is returned without any additional information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `message` attribute of the returned WarningCode.W001_NO_COVERAGE instance contains the expected string 'No .coverage file found'.</li>
                                        <li>The `detail` attribute of the returned WarningCode.W001_NO_COVERAGE instance matches the specified value 'test-detail'.</li>
                                        <li>The `code` attribute of the returned WarningCode.W001_NO_COVERAGE instance is set to WarningCode.W001_NO_COVERAGE.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors.py::test_warning_code_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that warning codes have correct values.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the warning code values are not correctly set.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>{'message': 'Assertion error', 'value': 'W001'}</li>
                                        <li>{'message': 'Assertion error', 'value': 'W101'}</li>
                                        <li>{'message': 'Assertion error', 'value': 'W201'}</li>
                                        <li>{'message': 'Assertion error', 'value': 'W301'}</li>
                                        <li>{'message': 'Assertion error', 'value': 'W401'}</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors.py::test_warning_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the warning to dict method.</p>
                                <p><strong>Why Needed:</strong> Prevent a potential bug where Warning objects are not properly converted to dictionaries.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'code' key should be present in the dictionary and have the correct value.</li>
                                        <li>The 'message' key should also be present in the dictionary with the correct value.</li>
                                        <li>The 'detail' key should be present in the dictionary if it exists, otherwise its value should be an empty string.</li>
                                        <li>All keys in the dictionary should match the expected keys according to WarningCode.</li>
                                        <li>If a Warning object has no detail, then the 'message' and 'code' keys should both be missing from the dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 70-72, 74-76)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_known_code</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `make_warning` function with known code.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the warning is not correctly generated for known code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `make_warning` should return an instance of `WarningCode.W101_LLM_ENABLED` with the correct message.</li>
                                        <li>The warning message should be set to `WARNING_MESSAGES[WarningCode.W101_LLM_ENABLED]`.</li>
                                        <li>The detail attribute should be set to `None` for this specific case.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_unknown_code</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test MakeWarningUnknownCode verifies that missing WarningCode.W001_NO_COVERAGE is handled correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the fallback message for unknown code is not used when an enum is allowed in the typed function.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The expected message 'Unknown warning.' is returned when making a warning with WarningCode.W001_NO_COVERAGE.</li>
                                        <li>The old message 'Warning: Unknown warning.' is restored after the test.</li>
                                        <li>The WARNING_MESSAGES dictionary is updated correctly to reflect the new fallback message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_with_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test makes a warning when invalid configuration is provided with detail.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test does not create a warning for an invalid configuration with detailed error message.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>w.code == WarningCode.W301_INVALID_CONFIG</li>
                                        <li>w.detail == 'Bad value'</li>
                                        <li>assert w is not None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestWarningCodes::test_codes_are_strings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that all WarningCode enum values are strings.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the WarningCode enum is not properly initialized with string values.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>code.value should be an instance of str.</li>
                                        <li>code.value should start with 'W'.</li>
                                        <li>All WarningCode enum members should have a value that starts with 'W' and is a string.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_no_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that Warning.to_dict() returns a dictionary without 'detail' key when no detail is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents the warning 'WarningDataClass: warning_to_dict_no_detail' because it ensures that the Warning object's attributes are serialized correctly to a dictionary without including any additional details.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'code' attribute of the Warning object is set to 'W001'.</li>
                                        <li>The 'message' attribute of the Warning object is set to 'No coverage'.</li>
                                        <li>The 'detail' key is not present in the serialized dictionary.</li>
                                        <li>The length of the serialized dictionary is 2 (i.e., it only contains 'code' and 'message').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 70-72, 74, 76)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_with_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the warning to dictionary conversion with detail.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where warnings are not properly serialized to dictionaries, potentially causing issues in downstream data processing or logging.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `to_dict()` method of the Warning class returns a dictionary with the correct keys and values.</li>
                                        <li>The 'code' key in the dictionary contains the warning code.</li>
                                        <li>The 'message' key in the dictionary contains the warning message.</li>
                                        <li>The 'detail' key in the dictionary contains the detailed warning message.</li>
                                        <li>The 'WarningCode.W001_NO_COVERAGE' value is correctly assigned to the 'code' key.</li>
                                        <li>The 'Check setup' value is correctly assigned to the 'detail' key.</li>
                                        <li>The resulting dictionary has the correct structure and content.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 70-72, 74-76)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestIsPythonFile::test_non_python_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `is_python_file` function returns False for non-.py files.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function incorrectly identifies Python files as non-Python files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return `False` when given a file name without a `.py` extension (e.g., `foo/bar.txt`).</li>
                                        <li>The function should return `False` when given a file name with a `.pyc` extension (e.g., `foo/bar.pyc`).</li>
                                        <li>The function should not incorrectly identify files that are actually Python files but have a non-Python file extension (e.g., `foo/bar.py`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 79)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestIsPythonFile::test_python_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `is_python_file` function returns True for a `.py` file.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function incorrectly identifies non-`.py` files as Python files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return `True` when given a path to a `.py` file.</li>
                                        <li>The function should raise an error or return a specific value when given a non-`.py` file path.</li>
                                        <li>The function should correctly handle relative paths for `.py` files.</li>
                                        <li>The function should not incorrectly identify other types of files as Python files.</li>
                                        <li>The function should be able to handle multiple extension checks (e.g., `.txt`, `.json`) in a single call.</li>
                                        <li>The function should raise an error when given a file with a different encoding than UTF-8.</li>
                                        <li>The function should correctly handle cases where the file is empty or contains only whitespace.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 79)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestMakeRelative::test_makes_path_relative</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_makes_path_relative' verifies that making a path relative to the test directory results in an absolute path.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when creating files outside of the test directory.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file is created with the correct parent directory.</li>
                                        <li>The file's path is not the same as its original path.</li>
                                        <li>The file is written to the correct location (subdir/file.py).</li>
                                        <li>The file's path is absolute (i.e., it does not start with a slash).</li>
                                        <li>The file's parent directory exists and can be created without raising an error.</li>
                                        <li>The file's parent directory is not already in the test directory.</li>
                                        <li>The file's original path is not the same as its relative path.</li>
                                        <li>The file's relative path starts with 'subdir/'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `make_relative` function returns a normalized path when there is no base.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where an absolute path would be returned instead of a relative one.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of `make_relative('foo/bar')` should be 'foo/bar'.</li>
                                        <li>The function does not return the same result when given an absolute path like 'foo/absolute/path'.</li>
                                        <li>If the input is a directory, the function returns the relative path to that directory.</li>
                                        <li>If the input is a file, the function returns the relative path from the current working directory.</li>
                                        <li>The function handles cases where the base directory is empty or None.</li>
                                        <li>The function does not return an error when given an invalid input (e.g., non-string base)</li>
                                        <li>The function preserves the original case of the input path (e.g., 'foo' instead of 'Foo').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 30, 33, 36, 39, 42, 55-56)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestNormalizePath::test_already_normalized</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests that a normalized path is returned for an already-normalized input.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `normalize_path` function would incorrectly return the original path if it's already normalized.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `normalize_path` function should return the same result as calling `pathlib.PurePath('foo/bar')`.</li>
                                        <li>The `normalize_path` function should not modify the input path.</li>
                                        <li>The `normalize_path` function should handle paths with leading or trailing slashes correctly.</li>
                                        <li>The `normalize_path` function should preserve the original directory hierarchy.</li>
                                        <li>The `normalize_path` function should raise an error if the input is not a string or a Path object.</li>
                                        <li>The `normalize_path` function should return an empty string for an empty path.</li>
                                        <li>The `normalize_path` function should handle symbolic links correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestNormalizePath::test_forward_slashes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `normalize_path` function correctly converts forward slashes in file paths to forward slashes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not handle forward slashes correctly, potentially leading to incorrect path comparisons or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should convert '\"' to '/\'.</li>
                                        <li>The function should convert '/foo/bar' to 'foo/bar'.</li>
                                        <li>The function should preserve the original directory structure and only replace forward slashes with forward slashes.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not correctly handle paths with trailing slashes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input path should be stripped of any trailing slash before normalization.</li>
                                        <li>Normalization should return the same path if it already did not have a trailing slash.</li>
                                        <li>The function should raise an error when given a path with no leading directory or file name.</li>
                                        <li>The function should correctly handle paths like 'foo/../bar/' and 'foo/bar/'.</li>
                                        <li>Normalization of empty strings should return an empty string.</li>
                                        <li>Normalization of absolute paths (like '/home/user/foo/') should work as expected.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_custom_exclude_patterns</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the `should_skip_path` function correctly skips custom paths based on provided patterns.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the function does not skip custom paths as intended, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>custom pattern 'test*' should be matched and the function should return True for `tests/conftest.py`</li>
                                        <li>custom pattern 'test*' should not match and the function should return False for `src/module.py`</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116-117, 119-121, 123)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_normal_path</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_fs.py::TestShouldSkipPath::test_normal_path</p>
                                <p><strong>Why Needed:</strong> To prevent skipping of normal file system paths.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert should_skip_path('src/module.py') == False</li>
                                        <li>assert not should_skip_path('non-existent-path.txt') == True</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_git</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `should_skip_path` function correctly identifies `.git` directories.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the function incorrectly skips non-`.git` directories, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert should_skip_path('.git/objects/foo') is True</li>
                                        <li>assert should_skip_path('non_git_directory') is False</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_pycache</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `should_skip_path` function correctly identifies __pycache__ directories.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where the test would incorrectly skip non-pycache directories.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return True for directories with a name starting with '__pycache__' and containing '.pyc'.</li>
                                        <li>The function should not return True for directories without a name starting with '__pycache__' or containing '.pyc'.</li>
                                        <li>The function should correctly handle cases where the directory name is not exactly 'foo/__pycache__/bar.pyc'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_venv</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_fs.py::TestShouldSkipPath::test_skips_venv</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the `should_skip_path` function incorrectly identifies venv directories as being to be skipped.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `should_skip_path` function should return True for venv directories and False for regular Python libraries.</li>
                                        <li>The `should_skip_path` function should not return True for `.venv` directories or any other directory that is a subdirectory of the current working directory.</li>
                                        <li>The test should verify that the `should_skip_path` function correctly handles different types of Python library directories.</li>
                                        <li>The test should check that the `should_skip_path` function does not incorrectly identify venv directories as being to be skipped in certain scenarios.</li>
                                        <li>The test should ensure that the `should_skip_path` function is able to handle nested directory structures correctly.</li>
                                        <li>The test should verify that the `should_skip_path` function returns False for all other Python library directories.</li>
                                        <li>The test should check that the `should_skip_path` function does not return True for any other type of directory that is a subdirectory of the current working directory.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_pruning</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that pruning clears request and token usage counts after a past request.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the rate limiter incorrectly clears request and token usage counts for requests made in the past.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of _request_times should be greater than 0 after calling _prune()</li>
                                        <li>The length of _token_usage should be greater than 0 after calling _prune()</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 39-42, 81-85, 87-88)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_rpm_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter prevents requests from exceeding a certain threshold (in this case, 1 RPM)</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where multiple requests are made within a short time frame and exceed the rate limit, causing the API to become unavailable.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `next_available_in` method returns a value greater than 0</li>
                                        <li>The `next_available_in` method returns a value less than or equal to 60.0</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95, 97, 100-102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_tpm_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter prevents a regression when tokens are not yet available.</p>
                                <p><strong>Why Needed:</strong> This test verifies that the rate limiter correctly handles cases where tokens are not yet available, preventing potential regressions.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The next_available_in method should return a value greater than 0 after waiting for 10 tokens.</li>
                                        <li>The _token_usage list should contain exactly two elements when the last token is recorded.</li>
                                        <li>The limiter._token_usage list should not be empty before and after recording the second token.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">33 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-94, 100-101, 103, 105, 107-108, 110-114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_wait_for_slot</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `wait_for_slot` method of `_GeminiRateLimiter` sleeps when a request is made.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the rate limiter does not sleep after a request, potentially leading to unexpected behavior or performance issues.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `wait_for_slot` method should be called with a mock `time.sleep` function.</li>
                                        <li>The `wait_for_slot` method should assert that it was called with the correct number of arguments (1).</li>
                                        <li>The `wait_for_slot` method should not call `mock_sleep` immediately, but instead after waiting for the specified amount of time.</li>
                                        <li>The `wait_for_slot` method should not return any value.</li>
                                        <li>The `wait_for_slot` method should raise an exception if it is called with a negative number of arguments (requests per minute).</li>
                                        <li>The `wait_for_slot` method should raise an exception if it is called with a non-integer argument (number of requests per minute).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">31 lines (ranges: 39-42, 45-46, 48, 52-54, 58-59, 61-63, 73, 76-78, 81-82, 84, 87-88, 92-93, 95, 97, 100-102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_record_zero_tokens</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the rate limiter records zero tokens when no tokens are available.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the rate limiter does not record tokens even when there are none available.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_token_usage` list of the rate limiter should be empty after calling `record_tokens(0)`.</li>
                                        <li>The length of `_token_usage` should be 0.</li>
                                        <li>No exception should be raised when no tokens are available for recording.</li>
                                        <li>The rate limiter's internal state should reflect that no tokens were recorded.</li>
                                        <li>_token_usage should not contain any token usage data.</li>
                                        <li>The rate limiter's `tokens_per_minute` attribute should still be valid and unchanged.</li>
                                        <li>The `_GeminiRateLimiter` instance should still have a valid `limits` object.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39-42, 66-67)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_requests_per_day_exhaustion</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the rate limiter to prevent exceeding daily requests.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the rate limiter is exceeded by more than one request per day, causing an error.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `wait_for_slot` should raise `_GeminiRateLimitExceeded` with the correct message 'requests_per_day'.</li>
                                        <li>The function `record_request` does not return any value.</li>
                                        <li>The function `wait_for_slot` should wait for a slot to become available after each request, and raise an error if no slots are available within the specified time frame.</li>
                                        <li>_GeminiRateLimitExceeded is raised with the correct message 'requests_per_day'.</li>
                                        <li>The rate limiter's daily limit is correctly checked before allowing another request.</li>
                                        <li>The function `wait_for_slot` does not wait for a slot to become available after each request, and raises an error if no slots are available within the specified time frame.</li>
                                        <li>_GeminiRateLimitExceeded is raised with the correct message 'requests_per_day'.</li>
                                        <li>The rate limiter's daily limit is correctly checked before allowing another request, and the function `wait_for_slot` waits for a slot to become available after each request.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">24 lines (ranges: 32-34, 39-42, 45-46, 48-50, 58-60, 73, 76-78, 81-82, 84, 87-88)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_tpm_fallback_wait</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the test_gemini_limiter_tpm_fallback_wait function covers the case when TPM wait time fallback occurs.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the rate limiter fails to detect and handle cases when the TPM is not available for a long enough period, leading to unexpected behavior or errors in the application.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `_GeminiRateLimitConfig(tokens_per_minute=10)` sets up the correct rate limit configuration for the test environment.</li>
                                        <li>The `limiter.record_tokens(10)` call fills up the TPM with tokens as expected.</li>
                                        <li>The calculation of `wait = limiter._seconds_until_tpm_available(now, 5)` returns a non-zero value indicating that the TPM is not available within the specified time frame.</li>
                                        <li>The assertion `assert wait > 0` ensures that the function waits for at least some time before checking if the TPM is available again.</li>
                                        <li>The line `# Line 116 hit because tokens_used + request_tokens > limit AND token_usage is not empty` verifies that the rate limiter correctly detects and handles cases where the TPM is not available.</li>
                                        <li>The expected behavior of filling up the TPM with tokens as described in the test scenario is verified through the assertions.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">24 lines (ranges: 39-42, 66, 68-70, 81-82, 84, 87-88, 100-101, 103, 105, 107-108, 110-114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_provider_rpm_cooldown</span>
                        <div class="test-meta">
                            <span>614ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that RPM rate limit cooldown handling is properly enforced.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the RPM rate limit cooldown is not set correctly, leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'models/gemini-pro' model should be in the cooldowns dictionary with a value greater than 1000.0 seconds.</li>
                                        <li>The cooldown value for 'models/gemini-pro' should be greater than 1000.0 seconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">23 lines (ranges: 52-53, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">117 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-215, 217-223, 225-227, 233-234, 238-240, 242-243, 274-277, 280, 282-290, 292-295, 297-298, 300-301, 346, 348-350, 352-353, 381-382, 385-386)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_rate_limit_retry</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the GeminiProvider annotates a rate limit retry scenario correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when the provider encounters a rate limit and retries after a certain delay.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation should have the correct scenario 'Recovered Scenario',</li>
                                        <li>The mock post call count should be 2 (1 for the first failed request, 1 for the second successful one),</li>
                                        <li>The provider's _parse_response method should return a Mock object with the expected scenario and error.</li>
                                        <li>The provider's _annotate_internal method should not raise an exception when encountering a rate limit retry scenario.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">181 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-215, 217-222, 225-227, 233-234, 238-240, 242-243, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330-333, 335-341, 343, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_success</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that _annotate_internal returns the correct LlmAnnotation for a successful annotation.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where _parse_response might expect an incorrect format of response from _call_gemini.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The scenario of 'Success Scenario' is correctly extracted from the annotation.</li>
                                        <li>The error in the annotation is None.</li>
                                        <li>The annotation does not contain any errors.</li>
                                        <li>The annotation's scenario matches the expected value.</li>
                                        <li>The annotation does not contain any invalid or unexpected data.</li>
                                        <li>The _parse_response function returns a Mock object with the correct scenario and no error.</li>
                                        <li>The _annotate_internal function correctly calls _parse_response to extract the annotation.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">173 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-223, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-343, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_availability</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `GeminiProvider` class correctly checks availability based on environment variables.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `GeminiProvider` class does not handle environment variable changes properly, potentially causing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_check_availability()` method of the `GeminiProvider` class should return `False` when no environment variables are set.</li>
                                        <li>The `_check_availability()` method of the `GeminiProvider` class should return `True` when environment variable `GEMINI_API_TOKEN` is set to a valid value.</li>
                                        <li>Environment variable changes should not affect the availability check result of the `GeminiProvider` class.</li>
                                        <li>Setting `GEMINI_API_TOKEN` environment variable before creating an instance of `GeminiProvider` should not change its availability check result.</li>
                                        <li>Creating an instance of `GeminiProvider` with a different provider name (`gemini`) should not affect its availability check result.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 134, 136-139, 141-142, 266-267, 269)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpd_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter prevents exceeding the daily limit of 1 request per day.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the rate limiter allows more than one request to be processed within a single day, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The next_available_in method returns None after 100 requests have been recorded.</li>
                                        <li>The rate limiter does not allow any further requests to be processed until the daily limit is reached again (i.e., 101st request).</li>
                                        <li>The rate limiter prevents more than one request from being processed within a single day, ensuring consistent and predictable behavior.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 39-42, 45-46, 48-50, 73, 76-78, 81-82, 84, 87-88)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpm_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter does not block subsequent requests after the first two have passed.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where subsequent requests are blocked due to insufficient available time.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The next_available_in method should return 0.0 for the first two requests.</li>
                                        <li>The next_available_in method should return 0.0 after recording the third request.</li>
                                        <li>The wait value should be greater than 0 and less than or equal to 60.0 seconds.</li>
                                        <li>_GeminiRateLimitConfig(requests_per_minute=2) should not have been called before the first two requests.</li>
                                        <li>_GeminiRateLimiter(limits) should have created a rate limiter instance with the specified limits.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">27 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-97, 100-102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeConfigHash::test_different_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that different configurations produce different hashes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the same configuration produces the same hash, potentially leading to inconsistencies in data storage or retrieval.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `compute_config_hash` should return a different value for two different configurations.</li>
                                        <li>The hash of `config1` should not be equal to the hash of `config2`.</li>
                                        <li>The hash of `config1` should not be equal to the hash of `Config(provider='ollama')` (the second config is created with provider 'ollama' but its actual provider is still 'none').</li>
                                        <li>The hash of `Config(provider='ollama')` should not be equal to the hash of `Config(provider='none')` (the first config is created with provider 'none' but its actual provider is still 'ollama').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 96-101, 103-104)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeConfigHash::test_returns_short_hash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies the length of the computed hash is 16 characters.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where the hash might be too long, potentially causing issues with storage or transmission.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the computed hash should be exactly 16 characters.</li>
                                        <li>The hash value should not exceed 15 characters to prevent truncation errors.</li>
                                        <li>The hash value should start with '0x' prefix if it's a hexadecimal string.</li>
                                        <li>The hash value should contain only alphanumeric characters and underscores.</li>
                                        <li>No leading zeros are allowed in the hash value.</li>
                                        <li>The hash value should be at least 16 characters long but no more than 32 characters long.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 96-101, 103-104)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeFileSha256::test_consistent_with_bytes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the computed SHA-256 hash of a file matches its content hash.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the file's content changes but the file hash remains the same.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The computed SHA-256 hash of the file should be equal to the content hash.</li>
                                        <li>The content hash of the file should match the expected value.</li>
                                        <li>The file hash should not change even if the file's content is modified.</li>
                                        <li>The file hash should remain consistent across different runs of the test</li>
                                        <li>The computed SHA-256 hash of a file with a different content but same path should be equal to the content hash.</li>
                                        <li>The content hash of a file with a different content but same path and different name should also be equal to the content hash.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 32, 44-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeFileSha256::test_hashes_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the hashing function on a file with known contents.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the hashing function fails to correctly hash files with non-ASCII characters or other special cases.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the computed SHA256 hash should be 64 bytes.</li>
                                        <li>The first byte of the hash should match 'hello'.</li>
                                        <li>The second byte of the hash should match 'world'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 44-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeHmac::test_different_key</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_different_key': Verifies that different keys produce different signatures.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the same key is used for multiple computations, potentially leading to unexpected signature differences.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that two different keys produce distinct HMAC signatures for the same input.</li>
                                        <li>Check if the computed signature for 'key1' is not equal to the computed signature for 'key2'.</li>
                                        <li>Ensure the first key produces a unique signature, and the second key does not.</li>
                                        <li>Verify the difference in signatures between 'key1' and 'key2'.</li>
                                        <li>Confirm that the order of keys does not affect the generated signature.</li>
                                        <li>Test if using different keys results in different HMAC values for the same input.</li>
                                        <li>Analyze the behavior when multiple keys are used for a single computation.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 61)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeHmac::test_with_key</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the length of the HMAC signature for a given content and secret key.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where an attacker could exploit the length of the HMAC signature to deduce the secret key.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the HMAC signature should be 64 bytes.</li>
                                        <li>The length of the HMAC signature is not less than 32 bytes.</li>
                                        <li>The length of the HMAC signature is not greater than 128 bytes.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 61)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeSha256::test_consistent</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the SHA-256 hash of the same input produces the same output.</p>
                                <p><strong>Why Needed:</strong> Prevents a bug where different inputs produce different hashes, potentially leading to unexpected behavior or data corruption.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `compute_sha256(b'...'` should return the same hash for two identical bytes objects.</li>
                                        <li>The function `compute_sha256(b'...'` should raise an exception if the input is not a bytes object.</li>
                                        <li>The function `compute_sha256(b'...'` should produce the same hash as the built-in `hash()` function for the input.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeSha256::test_length</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The hash function should produce a SHA-256 hash of the input string "test" which is expected to have a length of 64 characters.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the hash length is not as expected due to incorrect implementation or configuration of the compute_sha256 function.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output of the compute_sha256 function should be a bytes object containing 64 hexadecimal characters.</li>
                                        <li>The hexadecimal representation of the hash should match the expected value (e.g., '48656c6c6f20576f726c64')</li>
                                        <li>The length of the resulting string should be exactly 64 characters</li>
                                        <li>The output string should not contain any null bytes ( ) or other non-hexadecimal characters</li>
                                        <li>The hexadecimal representation of the hash should have a total length of 64 characters (16 bytes)</li>
                                        <li>The hash should be a valid SHA-256 hash (e.g., it should start with '48656c6c6f20576f726c64')</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestGetDependencySnapshot::test_includes_pytest</span>
                        <div class="test-meta">
                            <span>67ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `get_dependency_snapshot` function includes the 'pytest' package.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the 'pytest' package is not included in the dependency snapshot.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'pytest' package should be present in the dependency snapshot.</li>
                                        <li>The 'pytest' package should be listed as an item in the dependency snapshot.</li>
                                        <li>The presence of 'pytest' in the dependency snapshot indicates that it is available for installation.</li>
                                        <li>Including 'pytest' in the dependency snapshot ensures its availability for testing purposes.</li>
                                        <li>The absence of 'pytest' in the dependency snapshot may indicate a missing or outdated package.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 113-114, 116-121)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestGetDependencySnapshot::test_returns_dict</span>
                        <div class="test-meta">
                            <span>73ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `get_dependency_snapshot()` function should return a dictionary.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function returns an incorrect data type (e.g., list instead of dict).</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>snapshot is indeed a dictionary.</li>
                                        <li>the 'get_dependency_snapshot()' function is called and its result is stored in the `snapshot` variable.</li>
                                        <li>assert isinstance(snapshot, dict) is used to verify that the returned value matches the expected type.</li>
                                        <li>if snapshot were not a dictionary, this test would fail.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 113-114, 116-121)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_loads_key</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loads HMAC key from file.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the loaded key is not correctly decoded from the file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `load_hmac_key` function should return the expected value of the HMAC key.</li>
                                        <li>The `load_hmac_key` function should handle cases where the file contents are not valid JSON.</li>
                                        <li>The `load_hmac_key` function should raise an error if the file does not exist or is not a valid file.</li>
                                        <li>The `load_hmac_key` function should correctly decode the HMAC key from the file even if it's missing the expected header.</li>
                                        <li>The `load_hmac_key` function should handle cases where the file contains multiple keys.</li>
                                        <li>The `load_hmac_key` function should ignore any extra data in the file that is not related to the HMAC key.</li>
                                        <li>The `load_hmac_key` function should correctly handle cases where the file has a different encoding than UTF-8.</li>
                                        <li>The `load_hmac_key` function should raise an error if the file contains invalid JSON.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 73, 76-77, 80-81)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_missing_key_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_missing_key_file' verifies that the function returns None when a missing key file is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function fails to return an expected result (None) when a key file does not exist.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return `None` when a key file with the specified path (`/nonexistent.key`) is provided.</li>
                                        <li>The function should raise a `KeyError` exception if the key file exists but cannot be loaded due to permission issues or other reasons.</li>
                                        <li>The test should verify that the function correctly handles cases where the key file does not exist, without attempting to load it.</li>
                                        <li>The function's return value should match the expected result (None) in all scenarios.</li>
                                        <li>The test should cover different paths for the missing key file (e.g., `/nonexistent.key`, `/nonexistent2.key`)</li>
                                        <li>The function's behavior should be consistent across different Python versions and platforms</li>
                                        <li>The test should report an error message indicating that the key file was not found, rather than raising a cryptic exception</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 73, 76-78)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_no_key_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the function returns `None` when no key file is provided.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function does not handle the case when no key file is configured.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return `None` without attempting to load any HMAC keys.</li>
                                        <li>No error or exception should be raised when loading an empty configuration.</li>
                                        <li>The function's behavior should be consistent across different test environments.</li>
                                        <li>No exceptions should be thrown when calling the `load_hmac_key` method with a `Config` object that does not contain an HMAC key.</li>
                                        <li>The function's return value should be `None` instead of raising an exception or returning an error message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 73-74)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_aggregation_defaults</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that aggregation defaults are set correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the default aggregation policy is not applied correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.aggregate_dir should be None.</li>
                                        <li>config.aggregate_policy should be 'latest'.</li>
                                        <li>config.aggregate_include_history should be False.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_capture_failed_output_default_false</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `capture_failed_output` default value is set to `False` for the integration gate.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `capture_failed_output` default value might be incorrectly set to `True`, leading to unexpected behavior in the integration tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config.capture_failed_output` attribute is not equal to `False`.</li>
                                        <li>The `config.capture_failed_output` attribute is not equal to `None`.</li>
                                        <li>The `capture_failed_output` default value is set to `False`.</li>
                                        <li>The `capture_failed_output` default value does not match the expected behavior in all test environments.</li>
                                        <li>The `capture_failed_output` default value is not overridden by any environment variables or configuration files.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_context_mode_default_minimal</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the default context mode for integration gate.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the context mode is not set to 'minimal' by default.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_default_config()` returns an instance of `Config` with an `llm_context_mode` attribute equal to 'minimal'.</li>
                                        <li>The value of `config.llm_context_mode` is indeed 'minimal'.</li>
                                        <li>If the context mode is not set to 'minimal', a different default configuration would be used.</li>
                                        <li>Setting the context mode to another value (e.g., 'default') would prevent this test from passing.</li>
                                        <li>The `llm_context_mode` attribute of the `Config` instance is not changed by setting it to an invalid value.</li>
                                        <li>If the context mode is set to a valid value, such as 'minimal', the function returns the expected result.</li>
                                        <li>A different configuration with a different context mode would be used if the default is not set correctly.</li>
                                        <li>The test would fail if the `llm_context_mode` attribute of the `Config` instance is changed after setting it to an invalid value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_llm_not_enabled_by_default</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the LLM is not enabled by default in the configuration.</p>
                                <p><strong>Why Needed:</strong> The test prevents a potential bug where the LLM is enabled by default, potentially causing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.is_llm_enabled() == False</li>
                                        <li>config.get_llm_enabled_value() == False</li>
                                        <li>get_default_config().llm_enabled() == False</li>
                                        <li>get_default_config().get_llm_enabled_value() == False</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 107, 147, 224, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_omit_tests_default_true</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `get_default_config()` function returns a configuration object with an `omit_tests_from_coverage` attribute set to `True`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the default configuration does not include tests by omission.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config` variable is of type `dict` and has an `omit_tests_from_coverage` key with value `True`.</li>
                                        <li>The `config` dictionary contains the expected keys: `omit_tests_from_coverage`.</li>
                                        <li>The `config` dictionary does not have any other keys that could be causing issues.</li>
                                        <li>The `config` dictionary is a valid Python object.</li>
                                        <li>The `get_default_config()` function returns a configuration object with an `omit_tests_from_coverage` attribute set to `True`.</li>
                                        <li>The `config` variable has the expected value for the `omit_tests_from_coverage` key.</li>
                                        <li>The test does not fail when the default configuration includes tests by omission.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_provider_default_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the default provider setting when it is set to None.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the provider is not set to 'none' in case of privacy requirements.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.provider should be equal to 'none'</li>
                                        <li>config.provider should not be equal to any other value</li>
                                        <li>config.provider should have a default value of 'none'</li>
                                        <li>get_default_config() should return an object with provider set to 'none'</li>
                                        <li>assert config.provider is None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_secret_exclude_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that secret files are excluded by default from the LLM context.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where sensitive configuration files like 'secret' or '.env' might be inadvertently included in the LLM context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_default_config()` returns an object with a list of exclude globs.</li>
                                        <li>Any string containing 'secret' is found in the excludes list.</li>
                                        <li>Any string containing '.env' is found in the excludes list.</li>
                                        <li>The excludes list does not contain any strings that start with 'secret' or '.env'.</li>
                                        <li>The function `get_default_config()` returns a list of globs that are not empty.</li>
                                        <li>The excludes list contains only non-empty globs.</li>
                                        <li>Any string containing 'secret' is found in the excludes list after filtering out empty globs.</li>
                                        <li>Any string containing '.env' is found in the excludes list after filtering out empty globs.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_deterministic_output</span>
                        <div class="test-meta">
                            <span>6ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the output of the full pipeline is deterministic, i.e., the nodes are reported in a consistent order.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the order of reports changes due to external factors or system updates.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>nodeids should be sorted consistently.</li>
                                        <li>nodeid 'z_test.py::test_z' should come first in the list.</li>
                                        <li>nodeid 'a_test.py::test_a' should come second in the list.</li>
                                        <li>nodeid 'm_test.py::test_m' should come third in the list.</li>
                                        <li>The order of nodes is not affected by external factors or system updates.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">78 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">117 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_empty_test_suite</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that an empty test suite produces a valid report.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the test suite is empty, potentially causing invalid reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total count of tests in the report should be zero.</li>
                                        <li>The summary section of the report should have a total count of zero.</li>
                                        <li>All other sections of the report should not contain any data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_html_report_generation</span>
                        <div class="test-meta">
                            <span>30ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the full pipeline generates an HTML report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the HTML report is not generated correctly due to incorrect configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The HTML report should be created at the specified path.</li>
                                        <li>The content of the HTML report should contain the string '<html'.</li>
                                        <li>The content of the HTML report should contain the string 'test_pass'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">113 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_json_report_generation</span>
                        <div class="test-meta">
                            <span>53ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the full pipeline generates a valid JSON report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the full pipeline fails to generate a valid JSON report due to incorrect configuration or missing dependencies.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'report_json' and 'report_html' paths are created in the test directory.</li>
                                        <li>A JSON file named 'report.json' is written with the correct schema version, summary statistics, and number of tests.</li>
                                        <li>The total number of passed tests is 1, failed tests is 1, and skipped tests is 1.</li>
                                        <li>The 'schema_version' field matches the expected value of SCHEMA_VERSION.</li>
                                        <li>The 'summary' section contains the correct data: total tests = 3, passed tests = 1, failed tests = 1, skipped tests = 1.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/_git_info.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 2-3)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">78 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">133 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-322, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_report_root_has_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the ReportRoot class has required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the report root is missing essential metadata.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'schema_version' field should be present in the data.</li>
                                        <li>The 'run_meta' field should be present in the data.</li>
                                        <li>The 'summary' field should be present in the data.</li>
                                        <li>The 'tests' field should be present in the data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_aggregation_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that `RunMeta` has the required 'aggregation_fields' key.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where `is_aggregated` is False, potentially causing incorrect aggregation behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'is_aggregated' key should be present in the data.</li>
                                        <li>The 'run_count' key should be present in the data.</li>
                                        <li>The value of 'aggregation_policy' should only include when `is_aggregated` is True.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_status_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'RunMeta has run status fields' verifies that the RunMeta object contains status fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the RunMeta object is not properly initialized with status fields.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'exit_code' field should be present in the data.</li>
                                        <li>The 'interrupted' field should be present in the data.</li>
                                        <li>The 'collect_only' field should be present in the data.</li>
                                        <li>The 'collected_count' field should be present in the data.</li>
                                        <li>The 'selected_count' field should be present in the data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_schema_version_defined</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the schema version is defined and conforms to a semver-like format.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the schema version is not defined or does not conform to a semver-like format.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The schema version should be present in the test.</li>
                                        <li>The schema version should be a string that represents a valid semver-like format (e.g., '1.2.3').</li>
                                        <li>The schema version should contain at least one dot (.) character, indicating it is defined.</li>
                                        <li>The schema version should not start with a zero (0).</li>
                                        <li>The schema version should not end with a trailing period (.).</li>
                                        <li>The schema version should be a valid semver number (e.g., '1.2.3').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_test_case_has_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `TestSchemaCompatibility` class is tested to ensure that the `test_case_has_required_fields` method verifies the presence of required fields in a test case.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where a test case may not have all necessary fields, potentially causing issues with data validation or schema compatibility.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'nodeid' field is present in the `data` dictionary.</li>
                                        <li>The 'outcome' field is present in the `data` dictionary.</li>
                                        <li>The 'duration' field is present in the `data` dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_gemini_returns_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_provider` function correctly returns a `GeminiProvider` instance when the `provider` parameter is set to `'gemini'`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `get_provider` function incorrectly returns an incorrect provider type.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `__class__.__name__` attribute of the returned provider should be `GeminiProvider`.</li>
                                        <li>The `provider` parameter is set to `'gemini'`.</li>
                                        <li>The `get_provider` function correctly returns a `GeminiProvider` instance when the `provider` parameter is set to `'gemini'`.</li>
                                        <li>The `__class__.__name__` attribute of the returned provider matches the expected value (`GeminiProvider`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 245, 247, 249, 252, 257, 262-263, 265)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_litellm_returns_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_provider` function correctly returns an instance of LiteLLMProvider when a specific provider is specified.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the correct provider is not returned if an incorrect or unsupported provider is provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider.__class__ == 'LiteLLMProvider'</li>
                                        <li>provider.name == 'liteellm'</li>
                                        <li>provider.model == 'gpt-3.5-turbo'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52-53, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_none_returns_noop</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> test_get_provider_with_none_config_returns_noop</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the LLM is not initialized correctly with a 'none' provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_provider` function should return an instance of `NoopProvider` when given a configuration with 'provider='none'.</li>
                                        <li>The `provider` attribute of the returned `NoopProvider` instance should be set to `'none'`.</li>
                                        <li>The `config` object passed to `get_provider` has a valid `provider` key.</li>
                                        <li>The `provider` value in the `config` object is correctly converted to a string.</li>
                                        <li>The `provider` attribute of the resulting `NoopProvider` instance does not contain any non-'none' characters.</li>
                                        <li>The `provider` attribute of the resulting `NoopProvider` instance has the correct type (str).</li>
                                        <li>The `provider` attribute of the resulting `NoopProvider` instance is set to a string value without any quotes.</li>
                                        <li>The `provider` attribute of the resulting `NoopProvider` instance does not contain any non-string characters.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_ollama_returns_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the OllamaProvider class is returned when the 'provider' parameter is set to 'ollama'.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where an incorrect provider type is returned.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider should be of type OllamaProvider.</li>
                                        <li>The provider's __class__ attribute should match 'OllamaProvider'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 245, 247, 249, 252-253, 255)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_unknown_raises</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that unknown providers are correctly identified and cause a ValueError.</p>
                                <p><strong>Why Needed:</strong> To prevent unexpected behavior when using an unknown provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider(config)` should be called with a valid provider.</li>
                                        <li>A ValueError exception should be raised when the provider is unknown.</li>
                                        <li>The error message should contain the string 'unknown'.</li>
                                        <li>The error message should be case-insensitive (e.g., 'Unknown Provider' instead of 'unknown').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 245, 247, 249, 252, 257, 262, 267)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestLlmProviderContract::test_noop_implements_interface</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `NoopProvider` implements the LlmProvider interface.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where `NoopProvider` is not implementing all required methods of LlmProvider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Should have required methods: annotate, is_available, get_model_name, config</li>
                                        <li>The provider should be able to provide the model name from its configuration</li>
                                        <li>The provider should be able to return a boolean indicating if it's available</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestNoopProvider::test_annotate_returns_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the annotate method returns an empty LlmAnnotation object when no annotation is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the NoopProvider does not return any annotation even if it's supposed to.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>annotation is of type LlmAnnotation</li>
                                        <li>annotation scenario is empty</li>
                                        <li>annotation why_needed is empty</li>
                                        <li>annotation key_assertions are empty</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestNoopProvider::test_get_model_name_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `get_model_name` method of the `NoopProvider` class is called with an empty configuration.</p>
                                <p><strong>Why Needed:</strong> Without this test, a bug or regression may occur where the `get_model_name` method returns an empty string when given an empty configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert provider.get_model_name() == ''</li>
                                        <li>assert isinstance(provider.get_model_name(), str)</li>
                                        <li>assert provider.get_model_name().startswith('')</li>
                                        <li>assert provider.get_model_name().endswith('')</li>
                                        <li>assert len(provider.get_model_name()) == 0</li>
                                        <li>assert provider.get_model_name() != 'noop'  # This is not the expected result</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 66)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestNoopProvider::test_is_available</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the NoopProvider instance is available.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the provider might not be available due to configuration issues or other internal reasons.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `is_available()` method should return True for any valid configuration.</li>
                                        <li>The `is_available()` method should raise an exception if there are any invalid configurations.</li>
                                        <li>The `is_available()` method should call the underlying provider's `__call__` method without raising any exceptions.</li>
                                        <li>The `is_available()` method should not throw a `ValueError` exception when called with a valid configuration.</li>
                                        <li>The `is_available()` method should not raise an exception when called with an invalid configuration.</li>
                                        <li>The NoopProvider instance should be created successfully for each test case.</li>
                                        <li>The provider's underlying implementation should be available without any issues.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 107, 110-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 58)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_emits_summary</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the annotation summary is printed when annotations run.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the annotation summary is not printed, potentially causing confusion or errors in the testing process.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider` from `pytest_llm_report.llm.annotator` is called with a valid configuration.</li>
                                        <li>The `test_case` nodeid matches the expected scenario.</li>
                                        <li>The annotation summary is printed when annotations run successfully.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_reports_progress</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test LLM annotator progress reporting for multiple tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the LLM annotation progress is not reported correctly when running multiple tests simultaneously.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The LLM annotation should report progress for each test.</li>
                                        <li>The LLM annotation should display the correct test ID in its message.</li>
                                        <li>The LLM annotation should append a string to the messages list indicating the start of annotations for the current test.</li>
                                        <li>The LLM annotation should not append any additional strings beyond the initial message.</li>
                                        <li>The progress callback should be called with the correct number of tests annotated.</li>
                                        <li>The progress callback should call the provider with the correct test ID.</li>
                                        <li>The provider should have a valid cache directory.</li>
                                        <li>The test result outcome should be 'passed' for this test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_respects_opt_out_and_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM annotations respect opt-out and limit settings.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring LLM annotations do not skip opt-out tests or exceed the maximum number of tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm_opt_out' attribute is set to True for tests with this scenario.</li>
                                        <li>The LLM annotation is only called for tests where 'llm_opt_out' is False.</li>
                                        <li>No LLM annotations are called for tests where 'llm_opt_out' is True or the maximum number of tests has been reached.</li>
                                        <li>The provider function returns a FakeProvider instance that calls the correct provider.</li>
                                        <li>The LLM annotation is not called for test with this scenario and llm_opt_out=False.</li>
                                        <li>No LLM annotations are called for all tests where llm_opt_out=True or the maximum number of tests has been reached.</li>
                                        <li>The 'llm_max_tests' attribute is set to 1 in the config, which limits the number of tests to 1.</li>
                                        <li>The LLM annotation is only called once per test with this scenario and llm_opt_out=False.</li>
                                        <li>No LLM annotations are called for all tests where llm_opt_out=True or the maximum number of tests has been reached.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 45, 48-49, 56-57, 59, 61-62, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_respects_rate_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM annotations respect the requests-per-minute rate limit.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where LLM annotations may not respect the requests-per-minute rate limit, leading to inaccurate or delayed results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider's calls should match the expected list of node IDs.</li>
                                        <li>The sleep function should be called twice with values 2.0 and 4.0 respectively.</li>
                                        <li>The calls should not exceed the configured requests-per-minute rate limit (30).</li>
                                        <li>The calls should include all nodes specified in the tests (tests/test_a.py::test_a and tests/test_b.py::test_b).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-173, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_skips_unavailable_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests for annotating tests with unavailable providers should be skipped.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that annotation is skipped when a provider is unavailable.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `is_available` method of the `UnavailableProvider` class returns False.</li>
                                        <li>The `get_provider` function from `pytest_llm_report.llm.annotator` calls the `is_available` method of the `UnavailableProvider` class with the provided configuration.</li>
                                        <li>When a provider is unavailable, the `is_available` method should return False.</li>
                                        <li>The test should fail when an unavailable provider is used to annotate tests.</li>
                                        <li>The message 'is not available' should be printed when an unavailable provider is used to annotate tests.</li>
                                        <li>The `annote` function from `pytest_llm_report.llm.annotator` should call the `get_provider` function with a configuration that includes the `UnavailableProvider` class.</li>
                                        <li>When an unavailable provider is used to annotate tests, the `annote` function should not be able to proceed without skipping the annotation.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_uses_cache</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that annotations are cached between runs and that the annotation is used when it should be.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where annotations are not being used as expected due to caching issues.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider.calls` assertion checks if the provider was called before annotating tests.</li>
                                        <li>The `test.llm_annotation` assertion checks if the annotation is set correctly and matches the scenario.</li>
                                        <li>The `provider_next.annotate` assertion checks if the annotation is not called when it should be.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">30 lines (ranges: 39-41, 53, 55-56, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-81, 87-92, 97-98, 100, 102, 104, 115-122, 127, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `test_required_fields` function checks for the presence of 'scenario' and 'why_needed' keys in the ANNOTATION_JSON_SCHEMA.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the schema is not correctly validated if the required fields are missing or empty.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'scenario' key should be present in the ANNOTATION_JSON_SCHEMA.</li>
                                        <li>The 'why_needed' key should also be present in the ANNOTATION_JSON_SCHEMA.</li>
                                        <li>If 'scenario' and 'why_needed' keys are not present, the test will fail with an error message indicating a required field is missing.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_from_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the AnnotationSchema.from_dict method correctly parses a dictionary into an annotation.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential bugs or regressions in the AnnotationSchema class where it may not be able to parse dictionaries correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>checks password</li>
                                        <li>checks username</li>
                                        <li>schema.scenario matches 'Tests user login'</li>
                                        <li>schema.why_needed matches 'Prevents auth bypass'</li>
                                        <li>len(schema.key_assertions) is equal to 2</li>
                                        <li>schema.confidence is greater than or equal to 0.95</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the AnnotationSchema handles an empty input by creating a schema from an empty dictionary.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the AnnotationSchema is not correctly handling empty inputs, potentially leading to incorrect validation or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>schema.scenario = "" (empty string)</li>
                                        <li>schema.why_needed = "" (empty string)""</li>
                                        <li>assert schema.scenario == "" (checks if the assertion matches an expected value)</li>
                                        <li>assert schema.why_needed == "" (checks if the assertion matches an expected value)</li>
                                        <li>assert isinstance(schema, AnnotationSchema) (checks if the assertion is a valid instance of AnnotationSchema)</li>
                                        <li>schema.from_dict({}) should raise a ValueError or return None (checks if the from_dict method raises an exception or returns None)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_partial</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the AnnotationSchema.from_dict method correctly handles a partial input scenario.</p>
                                <p><strong>Why Needed:</strong> This test prevents bugs or regressions where the AnnotationSchema does not handle partial inputs correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The schema's 'scenario' property is set to 'Partial only'.</li>
                                        <li>The schema's 'why_needed' property is empty, indicating that no specific bug or regression was prevented by this test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_has_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the schema has required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the schema is not properly defined with required fields, potentially leading to incorrect validation of contract data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'scenario' field should be present in the schema's properties.</li>
                                        <li>The 'why_needed' field should also be present in the schema's properties.</li>
                                        <li>The 'key_assertions' field should contain assertions about the required fields within the schema.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_llm_contract.py::TestAnnotationSchema::test_schema_to_dict</p>
                                <p><strong>Why Needed:</strong> Prevents regression of bug Y when schema is serialized to dict.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>data['scenario'] == 'Tests feature X'</li>
                                        <li>data['why_needed'] == 'Prevents bug Y'</li>
                                        <li>data['key_assertions'] in data</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 90-92, 94-96, 98)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_from_factory</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `NoopProvider` is returned when a factory configuration with 'provider' set to 'none' is used.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the `NoopProvider` is not returned for provider='none'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider(config)` returns an instance of `NoopProvider` when the configuration has a 'provider' set to 'none'.</li>
                                        <li>The `NoopProvider` instance is correctly created and assigned to the variable `provider`.</li>
                                        <li>The `isinstance(provider, NoopProvider)` assertion passes, indicating that the correct class is returned.</li>
                                        <li>The test does not fail when using a factory configuration with 'provider' set to 'none'.</li>
                                        <li>The `get_provider(config)` function handles cases where the provider is not specified correctly.</li>
                                        <li>No exceptions are raised during the execution of this test.</li>
                                        <li>The test covers all possible scenarios for the given scenario.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_is_llm_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `test_noop_is_llm_provider` test verifies that the `NoopProvider` class correctly inherits from `LlmProvider`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `NoopProvider` class is mistakenly implemented as an LLM provider instead of a no-op.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` variable should be an instance of `LlmProvider`.</li>
                                        <li>The `provider` variable should not have any attributes or methods other than those inherited from `LlmProvider`.</li>
                                        <li>The `provider` variable should not inherit any attributes or methods from the `NoopProvider` class.</li>
                                        <li>The `provider` variable's type should be correctly set to `LlmProvider` using the `isinstance()` function.</li>
                                        <li>Any additional attributes or methods in the `provider` object should be removed after inheritance.</li>
                                        <li>The `provider` object should not have any unexpected behavior when accessed through its methods.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_returns_empty_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The NoopProvider function should return an empty annotation when the provided test case does not match any known node IDs.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the NoopProvider returns incorrect annotations for tests that do not match any known node IDs.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert result.scenario == "" (empty string)</li>
                                        <li>assert result.why_needed == "" (empty string)</li>
                                        <li>assert result.key_assertions == [] (no key assertions performed)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_annotate_returns_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `annotate` method of `NoopProvider` returns an LlmAnnotation-like object with the correct attributes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the `annotate` method does not return the expected annotation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result has the attribute 'scenario' and it is set to 'Annotate returns LlmAnnotation-like object.'</li>
                                        <li>The result has the attribute 'why_needed' and it is set to 'This test prevents a potential regression where the `annotate` method does not return the expected annotation.'</li>
                                        <li>The result has the attribute 'key_assertions' and it contains the expected checks</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_handles_empty_code</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the ProviderContract handles an empty code by returning a valid result.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where an empty code would cause the contract to fail.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider should return a non-empty result for an empty code.</li>
                                        <li>The provider should not raise any errors or exceptions when handling empty code.</li>
                                        <li>The provider should be able to correctly annotate the test with a valid outcome.</li>
                                        <li>The annotation should include the nodeid and outcome of the test.</li>
                                        <li>The configuration should not affect the behavior of the provider.</li>
                                        <li>The result should not be None, indicating that the contract handled the empty code successfully.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_handles_none_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `provider` handles a `None` context gracefully by annotating a `TestCaseResult` with an empty string.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the provider might throw an error when handling a `None` context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider.annotate()` method should return `None` instead of raising an exception.</li>
                                        <li>The `provider.annotate()` method should not raise an exception if the input `test` is `None`.</li>
                                        <li>The `provider.annotate()` method should correctly handle the case where the input `test` is `None` and returns a valid `TestCaseResult` object.</li>
                                        <li>The `provider.annotate()` method should not throw any exceptions when handling a `None` context.</li>
                                        <li>The `provider.annotate()` method should preserve the original value of the `nodeid` attribute in the `TestCaseResult` object.</li>
                                        <li>The `provider.annotate()` method should preserve the original value of the `outcome` attribute in the `TestCaseResult` object.</li>
                                        <li>The `provider.annotate()` method should not modify the original values of these attributes in the `TestCaseResult` object.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_has_annotate_method</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> All providers should have an annotate method.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where providers might not be able to annotate data correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider_name in ['none', 'ollama', 'litellm', 'gemini']</li>
                                        <li>hasattr(provider, 'annotate')</li>
                                        <li>callable(provider.annotate)</li>
                                        <li>provider.annotate() should return a callable object</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 52-53, 245, 247, 249-250, 252-253, 255, 257-258, 260, 262-263, 265)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_handles_context_too_large</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `annotate` method of the `GeminiProvider` class is being tested when it handles a context that is too large.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `annotate` method fails with an exception due to an excessive memory usage in cases with very large contexts.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `context_size` attribute of the `GeminiProvider` instance should be less than or equal to 1000.</li>
                                        <li>The `annotate` method should not raise a `MemoryError` when called with a context that exceeds 1000 bytes in size.</li>
                                        <li>The `context_size` attribute of the `GeminiProvider` instance should be updated correctly after calling the `annotate` method.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52-53, 72, 75-76, 80, 165, 167, 175)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">155 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-221, 233, 245-248, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417-418, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_dependency</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The LiteLLM provider should report a missing dependency error when the required package is not installed.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider incorrectly reports an installation issue without providing any useful information about the actual problem.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert annotation.error == 'litellm not installed. Install with: pip install litellm'</li>
                                        <li>provider.annotate(test, 'def test_case(): assert True')</li>
                                        <li>test.test_case() should raise a mock_import_error('litellm')</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 134, 136-139, 141-142, 160-164)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_token</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the GeminiProvider annotates missing tokens correctly.</p>
                                <p><strong>Why Needed:</strong> To prevent a TypeError when trying to annotate a test with an undefined API token.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation should raise a ValueError indicating that GEMINI_API_TOKEN is not set.</li>
                                        <li>The annotation should include a message explaining the expected behavior (i.e., 'GEMINI_API_TOKEN is not set')</li>
                                        <li>The annotation should provide a clear indication of what needs to be set (API token) before annotating a test</li>
                                        <li>The annotation should return an error code indicating that GEMINI_API_TOKEN is missing</li>
                                        <li>The annotation should include the actual API token value if it exists</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 134, 136-139, 141-142, 160-161, 167-169)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_records_tokens</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify Gemini provider annotates records tokens correctly.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in token usage logging.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'annotate' method of the GeminiProvider is called with a test function that checks for token usage.</li>
                                        <li>The 'annotate' method logs usage metadata, including the total number of tokens recorded.</li>
                                        <li>The 'annotate' method verifies if the limiter has at least one record of token usage.</li>
                                        <li>The limiter's token usage is verified to be 1 token with a count of 123.</li>
                                        <li>The rate limits logic is tested by verifying it ran without error.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">183 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-223, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-343, 346, 348-350, 352-355, 360-366, 368, 370-371, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_retries_on_rate_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `annotate_retries_on_rate_limit` method of the `GeminiProvider` class should retry annotating requests when rate limiting is exceeded.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the `annotate_retries_on_rate_limit` method does not retry after exceeding the rate limit, potentially causing the service to become unresponsive or fail with an error.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `rate_limit` attribute of the `GeminiProvider` instance is set correctly before calling `annotate_retries_on_rate_limit`.</li>
                                        <li>The `annotate_retries_on_rate_limit` method retries annotating requests when rate limiting is exceeded. The retry attempts are limited to a reasonable number of times.</li>
                                        <li>The `rate_limit` attribute is reset after each retry attempt, allowing the service to recover from rate limiting issues. If `rate_limit` is not reset, it may lead to infinite retries and potential service degradation.</li>
                                        <li>The `annotate_retries_on_rate_limit` method does not raise an exception when rate limiting is exceeded, preventing the test from failing due to expected behavior.</li>
                                        <li>The `annotate_retries_on_rate_limit` method uses a reasonable number of retry attempts (e.g., 3-5) before giving up and returning without annotating the request. This helps prevent overwhelming the service with retries and potential denial-of-service attacks.</li>
                                        <li>The `rate_limit` attribute is not reset after each retry attempt, allowing the service to recover from rate limiting issues. If `rate_limit` is not reset, it may lead to infinite retries and potential service degradation.</li>
                                        <li>The `annotate_retries_on_rate_limit` method does not log any relevant information when rate limiting is exceeded, preventing the test from detecting potential issues or errors in the service.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">181 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 233-234, 238-240, 242-243, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330-333, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-369, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_rotates_models_on_daily_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `annotate` method of the `GeminiProvider` class rotates models on a daily limit when used with the `rotate_models_on_daily_limit` fixture.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the model rotation is not applied correctly due to an incorrect implementation of the `rotate_models_on_daily_limit` fixture.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `annotate` method should rotate models on the daily limit.</li>
                                        <li>The `rotate_models_on_daily_limit` fixture should be able to rotate models without any issues.</li>
                                        <li>The model rotation is applied correctly and does not exceed the daily limit.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">177 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419-420, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_skips_on_daily_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the annotate function skips annotations when the daily limit is exceeded.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the annotate function does not skip annotations due to the daily limit being reached.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotate function should skip any annotations that would exceed the daily limit.</li>
                                        <li>Any annotations that are created after the daily limit has been exceeded should be skipped.</li>
                                        <li>The annotate function should not attempt to create new annotations when the daily limit is reached.</li>
                                        <li>The annotation count should decrease by 1 when the daily limit is exceeded.</li>
                                        <li>The total number of annotations created should be less than or equal to the daily limit after exceeding it.</li>
                                        <li>The annotate function should throw an exception when the daily limit is exceeded and no new annotations are allowed.</li>
                                        <li>Any exceptions thrown during annotation creation should not propagate up the call stack.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">184 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-208, 210-211, 213-214, 217-222, 225-227, 252-254, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_success_with_mock_response</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LiteLLM provider annotates successful responses correctly.</p>
                                <p><strong>Why Needed:</strong> Prevents regressions caused by incorrect annotation of failed responses.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotated response contains the expected scenario and why-need information.</li>
                                        <li>The annotated response includes the correct key assertions.</li>
                                        <li>The confidence level is set to a reasonable value (0.8 in this case).</li>
                                        <li>The captured model is correctly identified as 'gpt-4o'.</li>
                                        <li>The system role is correctly associated with the message 'def test_login()'.</li>
                                        <li>The tests/test_auth.py::test_login message is present in the response.</li>
                                        <li>The successful login scenario is included in the response content.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">177 lines (ranges: 39-42, 45-46, 48-49, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_exhausted_model_recovers_after_24h</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the exhausted model recovers after 24 hours.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the model does not recover from exhaustion within 24 hours.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The model's performance metrics (e.g., accuracy, F1 score) should return to normal or near-normal values after 24 hours.</li>
                                        <li>The model's inference time should decrease significantly after 24 hours.</li>
                                        <li>The model's memory usage should decrease significantly after 24 hours.</li>
                                        <li>The model's warnings and errors should be cleared after 24 hours.</li>
                                        <li>The model's training data should not have been exhausted within the last 24 hours (if applicable).</li>
                                        <li>The model's inference requests should be able to complete successfully without any timeouts or exceptions.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">190 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-188, 190-191, 193-194, 196, 200-208, 210-211, 213-214, 217-222, 225-227, 252-254, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_fetch_available_models_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'fetch_available_models' method of the Gemini provider returns an error when no models are available.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the 'fetch_available_models' method returns an error instead of raising a meaningful exception.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assertRaisesRegex with 'GeminiError' and 'No models available.'</li>
                                        <li>assertRaisesRegex with 'GeminiError' and 'No models found.'</li>
                                        <li>assertRaisesRegex with 'GeminiError' and 'Model not found in database.'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 134, 136-139, 141-142, 280, 282-283, 286-290, 292-295, 297-298, 300-301, 346, 348-350, 352-355, 360-363, 374-377, 385, 387, 391-392, 396-402, 405, 408-410, 412-414, 417-418, 428, 430-432, 435-436)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_model_list_refreshes_after_interval</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The model list should refresh after a specified interval.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when the model is not refreshed immediately after an interval.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>model_list is updated with new models after the specified interval.</li>
                                        <li>no exception is raised if no models are available to update.</li>
                                        <li>models are only added to the list if they have been trained within the specified interval.</li>
                                        <li>the model list size does not exceed a certain threshold.</li>
                                        <li>the test can be run in parallel without affecting each other's results.</li>
                                        <li>the refresh interval can be adjusted dynamically based on system load.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">169 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-369, 374-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_handles_completion_error</span>
                        <div class="test-meta">
                            <span>6.00s</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the LiteLLMProvider annotates completion errors in the annotation.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the LiteLLM provider does not surface completion errors in annotations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert 'boom' in annotation.error</li>
                                        <li>annotation.error is an instance of RuntimeError</li>
                                        <li>annotation.error contains the string 'boom'</li>
                                        <li>annotation.error is raised by fake_completion()</li>
                                        <li>fake_completion raises a RuntimeError</li>
                                        <li>fake_completion() does not raise any other exception</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 78-79, 81-82, 84-85)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_invalid_key_assertions</span>
                        <div class="test-meta">
                            <span>6.00s</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LiteLLMProvider rejects invalid key_assertions payloads.</p>
                                <p><strong>Why Needed:</strong> To prevent the test from passing when an invalid key_assertions payload is provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'key_assertions' parameter must be a list.</li>
                                        <li>Invalid response: key_assertions must be a list</li>
                                        <li>Key assertion error message should include the expected format.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207-209)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69, 73, 76, 81-82, 84-85)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_missing_dependency</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The LiteLLMProvider annotates a missing dependency in the provided test case.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider does not report an error for a missing dependency, potentially leading to silent failures or incorrect results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>annotation.error == 'litellm not installed. Install with: pip install litellm'</li>
                                        <li>provider.annotate(test, "def test_case(): assert True")</li>
                                        <li>test.test_case() is False</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 37-41)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_success_with_mock_response</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the LiteLLM provider annotates a successful response with mock data.</p>
                                <p><strong>Why Needed:</strong> Prevents regression due to fake completion of LiteLLMProvider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation contains the correct scenario.</li>
                                        <li>The annotation contains the correct why needed message.</li>
                                        <li>The annotation contains the correct key assertions.</li>
                                        <li>The annotation has a confidence level of 0.8.</li>
                                        <li>The captured model is 'gpt-4o'.</li>
                                        <li>The captured messages contain the expected system role and function calls.</li>
                                        <li>The captured messages contain the expected test login function call.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69-70)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_is_available_with_module</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the LiteLLM provider detects installed modules.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the provider does not detect installed modules.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `is_available()` method of the `LiteLLMProvider` class should return True when the 'litellm' module is available in the system's modules.</li>
                                        <li>The `is_available()` method should raise an error if the 'litellm' module is not installed or not found in the system's modules.</li>
                                        <li>The provider should correctly detect the presence of the 'litellm' module even if it is not a standard Python package.</li>
                                        <li>The provider should handle cases where the 'litellm' module is installed but not imported as a module (e.g., as a package)</li>
                                        <li>The provider should raise an error when trying to import the 'litellm' module as a module, indicating that it is not available</li>
                                        <li>The provider should correctly handle cases where the 'litellm' module has been removed from the system's modules</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 107, 110-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 94-95, 97)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_fallbacks_on_context_length_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the annotate method handles context length errors correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the annotate method fails to handle context length errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>If the input `context` is longer than the maximum allowed length, the function should return an error message.</li>
                                        <li>The function should raise a ValueError with a meaningful error message when the input `context` is too long.</li>
                                        <li>The function should not silently ignore the input `context` and instead raise an exception.</li>
                                        <li>The function should provide a clear and descriptive error message that explains why the context length was exceeded.</li>
                                        <li>The function should handle cases where the input `context` is an empty string or None.</li>
                                        <li>The function should return an error message with a specific format (e.g., 'Context too long: ...')</li>
                                        <li>The function should raise an exception with a specific error code (e.g., 'EXC_CONTEXT_LENGTH_ERROR')</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">33 lines (ranges: 52-53, 72, 75-76, 78, 165, 167-173, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-60, 62-63, 66-67)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_handles_call_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test OllamaProvider::test_annotate_handles_call_error verifies that the annotation of a call error to Ollama provider prevents regression.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case of call errors to Ollama provider, ensuring the correctness of annotations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation should indicate that the call was not successful and returned an error.</li>
                                        <li>The error message should be 'Failed after 3 retries. Last error: boom'.</li>
                                        <li>The test should pass even if the system prompt is different from the Ollama provider's system prompt.</li>
                                        <li>The test should fail with a non-zero exit code (e.g., 1) when the call to Ollama provider fails.</li>
                                        <li>The annotation should not be affected by the number of retries.</li>
                                        <li>The annotation should indicate that the call was not successful and returned an error even if the system prompt is different from the Ollama provider's system prompt.</li>
                                        <li>The test should fail with a non-zero exit code (e.g., 1) when the call to Ollama provider fails, regardless of the number of retries.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-59, 71-72, 74-75, 77-78)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_missing_httpx</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The Ollama provider should report an error when annotating a function that uses the missing httpx dependency.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider incorrectly reports a non-existent dependency, potentially leading to incorrect or misleading error messages.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation message should include the correct error message for installing httpx.</li>
                                        <li>The annotation message should not be empty.</li>
                                        <li>The annotation message should contain the exact string 'httpx not installed.'</li>
                                        <li>The annotation message should not contain any other relevant information that could lead to incorrect error messages.</li>
                                        <li>The annotation message should not be too long and only include the necessary information for installation.</li>
                                        <li>The annotation message should not contain any typos or grammatical errors.</li>
                                        <li>The annotation message should provide a clear and concise explanation of what needs to be installed.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 40-44)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_success_full_flow</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the full annotation flow of Ollama provider with mocked HTTP.</p>
                                <p><strong>Why Needed:</strong> Prevents authentication-related bugs in the annotator.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify status code and validate token response</li>
                                        <li>Check if the response contains a valid JSON object</li>
                                        <li>Assert that the 'response' key is present in the JSON object</li>
                                        <li>Verify that the 'why_needed' field matches the expected reason for failure</li>
                                        <li>Validate that the 'key_assertions' list includes all necessary checks</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-60, 62-63, 114, 116-123, 127-130, 132, 134-135)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_success</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The Ollama provider makes a successful API call to generate text.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the Ollama provider fails to make a correct API call.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider._call_ollama` method returns the expected response from the Ollama model.</li>
                                        <li>The `captured.json['model']` attribute matches the provided `model` in the configuration.</li>
                                        <li>The `captured.json['prompt']` attribute matches the provided `prompt` in the configuration.</li>
                                        <li>The `captured.json['system']` attribute matches the provided `system` in the configuration.</li>
                                        <li>The `captured.json['stream']` attribute is set to `False` as expected.</li>
                                        <li>The `timeout` attribute is set to 60 seconds as expected.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 114, 116-123, 127-130, 132, 134-135)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_uses_default_model</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the default model is used when not specified for Ollama provider.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the default model is not used as expected.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The captured response from the API should contain the default model.</li>
                                        <li>The captured response from the API should be 'ok'.</li>
                                        <li>The captured response from the API should have a 'model' key with value 'llama3.2'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 114, 116-123, 127-130, 132, 134-135)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the Ollama provider returns False when the server is unavailable.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the provider fails to return an error when the server is not running.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function _check_availability() of the OllamaProvider instance should raise a ConnectionError exception.</li>
                                        <li>The function _check_availability() of the OllamaProvider instance should return False.</li>
                                        <li>The function _check_availability() of the OllamaProvider instance should not have any other return value.</li>
                                        <li>The function _check_availability() of the OllamaProvider instance should not raise a TypeError exception.</li>
                                        <li>The function _check_availability() of the OllamaProvider instance should not be able to return True.</li>
                                        <li>The function _check_availability() of the OllamaProvider instance should not have any side effects.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 87-88, 90-91, 93-94)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_non_200</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the Ollama provider returns False for non-200 status codes.</p>
                                <p><strong>Why Needed:</strong> To prevent a regression where the provider incorrectly reports availability when it's not available (status code 500).</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert provider._check_availability() is False</li>
                                        <li>assert FakeResponse().status_code == 500</li>
                                        <li>assert config.provider != 'ollama'</li>
                                        <li>assert isinstance(provider, OllamaProvider)</li>
                                        <li>assert isinstance(config, Config)</li>
                                        <li>assert isinstance(fake_httpx, SimpleNamespace)</li>
                                        <li>assert fake_get.__name__ == 'fake_get'</li>
                                        <li>assert fake_get.__doc__ is None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 87-88, 90-92)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_success</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the Ollama provider checks availability via /api/tags endpoint successfully.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the provider fails to check availability when it's not available.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The '/api/tags' URL is present in the provided host.</li>
                                        <li>The response status code is 200 (OK) for the '/api/tags' endpoint.</li>
                                        <li>The 'ollama_host' configuration parameter is set correctly to 'http://localhost:11434'.</li>
                                        <li>The provider's `_check_availability()` method returns True.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 87-88, 90-92)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_is_local_returns_true</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The Ollama provider is correctly identified as local.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider might be incorrectly identified as non-local.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider is an instance of OllamaProvider</li>
                                        <li>is_local() returns True for the provided config</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_json</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `OllamaProvider` class throws an error when parsing a response with invalid JSON.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the Ollama provider incorrectly reports valid responses as having an error.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `annotation.error` attribute is set to 'Failed to parse LLM response as JSON'.</li>
                                        <li>The `provider._parse_response('not-json')` method returns an instance of `OllamaProviderError`.</li>
                                        <li>The `annotation.error` attribute contains the string 'Failed to parse LLM response as JSON'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 52-53, 186-187, 190-192)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-52, 55)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_key_assertions</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the OllamaProvider rejects invalid key_assertions payloads in its _parse_response method.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the provider incorrectly accepts or ignores invalid key_assertions payloads.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>- The 'key_assertions' field must be a list.</li>
                                        <li>- Invalid values are not allowed in this field.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207-209)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The provided test verifies that the Ollama provider correctly extracts JSON from markdown code fences.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider fails to parse JSON in code fences, potentially leading to incorrect or incomplete annotations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The response is not empty.</li>
                                        <li>The response contains valid JSON syntax.</li>
                                        <li>The response does not contain any non-JSON characters (e.g. whitespace, special characters).</li>
                                        <li>The response does not contain any invalid JSON syntax (e.g. missing or mismatched brackets, quotes).</li>
                                        <li>The response is a valid JSON object (i.e. an object with a 'text' property and optional 'metadata' properties).</li>
                                        <li>The provider correctly handles nested objects and arrays within the JSON.</li>
                                        <li>The provider correctly handles quoted strings within the JSON.</li>
                                        <li>The provider does not attempt to parse any invalid or malformed JSON.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 38, 42-44, 46-47)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_plain_fence</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The provided test verifies that the Ollama provider correctly parses a JSON response in a plain fence without any language specification.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider fails to extract JSON from plain fences with no specified language.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>The extracted JSON should be valid JSON syntax without any extra characters or whitespace.</li>
                                        <li>The response should not contain any language-specific keywords or phrases.</li>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>The provider's error message should indicate that no language was specified for the fence.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 38, 42-44, 46-47)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_success</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Ollama provider parses valid JSON responses and verifies correct configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents bugs in the Ollama provider by ensuring it correctly configures itself with a valid response.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert annotation.scenario == 'Tests feature'</li>
                                        <li>assert annotation.why_needed == 'Stops bugs'</li>
                                        <li>assert annotation.key_assertions == ['assert a', 'assert b']</li>
                                        <li>assert annotation.confidence == 0.8</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestArtifactEntry::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `CoverageEntry` correctly serializes to a dictionary.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the serialization of `CoverageEntry` is incorrect.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key in the serialized dictionary should be exactly 'src/foo.py'.</li>
                                        <li>The 'line_ranges' key in the serialized dictionary should match the provided string.</li>
                                        <li>The 'line_count' key in the serialized dictionary should be equal to the expected value of 10.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 254-257)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestCollectionError::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `to_dict()` method of CoverageEntry to ensure it correctly serializes a coverage entry.</p>
                                <p><strong>Why Needed:</strong> This test prevents regressions where the `to_dict()` method fails to serialize a coverage entry with invalid or missing data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `file_path` key in the serialized dictionary should match the original value.</li>
                                        <li>The `line_ranges` key in the serialized dictionary should match the original value.</li>
                                        <li>The `line_count` key in the serialized dictionary should match the original value.</li>
                                        <li>Any missing keys (e.g. `coverage_type`, `start_line`, etc.) should be ignored or raise an error.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 207-209)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestCoverageEntry::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test CoverageEntry to_dict serialization correctness.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the coverage entry is not properly serialized to JSON.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key should match the expected value.</li>
                                        <li>The 'line_ranges' key should contain the correct ranges and values.</li>
                                        <li>The 'line_count' key should match the expected value.</li>
                                        <li>A KeyError should be raised if the 'file_path', 'line_ranges', or 'line_count' keys are missing from the dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 40-43)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_empty_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> An empty annotation should be created with default values.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where an empty annotation would have no effect on the model's performance.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>annotation.scenario == "" (empty string)</li>
                                        <li>annotation.why_needed == "Empty annotation should have default values." (description of why it needs to be tested)</li>
                                        <li>annotation.key_assertions == [] (expected empty list for key assertions)</li>
                                        <li>assert annotation.confidence is None (expected confidence to be None for an empty annotation)</li>
                                        <li>assert annotation.error is None (expected error to be None for an empty annotation)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_to_dict_minimal</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `to_dict` method of `LlmAnnotation` returns a dictionary with required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the minimal annotation is properly serialized without any optional or missing fields.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'scenario' field should be present in the dictionary.</li>
                                        <li>The 'why_needed' field should also be present in the dictionary.</li>
                                        <li>The 'key_assertions' field should not include the 'confidence' field, as it is an optional attribute.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 104-107, 109, 111, 113, 115)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_to_dict_with_all_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test to dictionary with all fields</p>
                                <p><strong>Why Needed:</strong> Prevents data loss due to missing fields in the output.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Asserts that the 'scenario' field is present and matches the expected value.</li>
                                        <li>Asserts that the 'confidence' field has a value greater than or equal to 0.95.</li>
                                        <li>Asserts that the 'context_summary' field contains the correct mode ('minimal') and bytes count (1000).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 104-107, 109-111, 113-115)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_default_report</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test default report schema version and empty lists.</p>
                                <p><strong>Why Needed:</strong> Prevents regression by ensuring the default report has a valid schema version and no collection errors or warnings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'schema_version' key in the report dictionary should be equal to SCHEMA_VERSION.</li>
                                        <li>The 'tests' key in the report dictionary should be an empty list.</li>
                                        <li>The 'warnings' key in the report dictionary should not exist (i.e., its value should be None).</li>
                                        <li>The 'collection_errors' key in the report dictionary should not exist (i.e., its value should be None).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_report_with_collection_errors</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Report with Collection Errors should include them.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the report does not include collection errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'collection_errors' key in the report dictionary should be present and contain exactly one error.</li>
                                        <li>The 'nodeid' value of the first error in the 'collection_errors' list should match the provided node id.</li>
                                        <li>All other values in the 'collection_errors' list (if any) should have a valid 'message' property.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">58 lines (ranges: 207-209, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508-510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_report_with_warnings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test reports the presence of warnings in a ReportRoot instance.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where warnings are not reported when there is no coverage.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the 'warnings' list should be exactly 1.</li>
                                        <li>The first element of the 'warnings' list should have code 'W001'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">60 lines (ranges: 229-231, 233, 235, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_tests_sorted_by_nodeid</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the output of `ReportRoot` is sorted by nodeid.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where tests are not sorted correctly by nodeid, potentially causing incorrect report generation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The list of nodeids in the output matches the expected order.</li>
                                        <li>Each nodeid appears exactly once in the list.</li>
                                        <li>All nodeids are present in the input data.</li>
                                        <li>Nodeids are in ascending order.</li>
                                        <li>No duplicate nodeids are present in the output.</li>
                                        <li>Nodeids are not empty.</li>
                                        <li>The test passes if all assertions pass.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">71 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportWarning::test_to_dict_with_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test `test_to_dict_with_detail` verifies that the `to_dict()` method of a `ReportWarning` object returns a dictionary with the correct detail.</p>
                                <p><strong>Why Needed:</strong> This test prevents a warning about missing coverage information in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'detail' key should be present in the returned dictionary.</li>
                                        <li>The value of the 'detail' key should match '/path/to/file'.</li>
                                        <li>The 'message' key is not included in the returned dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 229-231, 233-235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportWarning::test_to_dict_without_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_to_dict_without_detail' verifies that a ReportWarning object is created without detailed warnings.</p>
                                <p><strong>Why Needed:</strong> This test prevents the creation of unnecessary detailed warnings when no coverage information is available.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The warning object should be created with only the required code and message keys.</li>
                                        <li>The 'detail' key should not be present in the warning object.</li>
                                        <li>The 'message' key should contain the expected warning message.</li>
                                        <li>The 'code' key should match the expected warning code.</li>
                                        <li>The 'detail' value should be an empty string or None to indicate no coverage information is available.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 229-231, 233, 235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_aggregation_fields_present</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that RunMeta has aggregation fields.</p>
                                <p><strong>Why Needed:</strong> Prevent regression where RunMeta is missing aggregation fields, potentially leading to incorrect aggregation results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert d['run_id'] == 'run-123'</li>
                                        <li>assert d['run_group_id'] == 'group-456'</li>
                                        <li>assert d['is_aggregated'] is True</li>
                                        <li>assert d['aggregation_policy'] == 'merge'</li>
                                        <li>assert d['run_count'] == 3</li>
                                        <li>assert len(d['source_reports']) == 2</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 277-279, 281-283, 364-380, 382, 385, 387, 390, 393, 395, 397, 399-405, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_llm_fields_excluded_when_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM fields are excluded when annotations are disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the LLM fields are included even when annotations are disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm_annotations_enabled' key is not present in the data.</li>
                                        <li>The 'llm_provider' key is not present in the data.</li>
                                        <li>The 'llm_model' key is not present in the data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_llm_traceability_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test LLM traceability fields are included when enabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the LLM traceability fields are properly set to true when llm_annotations_enabled is True.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>data['llm_annotations_enabled'] is True</li>
                                        <li>data['llm_provider'] == 'ollama'</li>
                                        <li>data['llm_model'] == 'llama3.2:1b'</li>
                                        <li>data['llm_context_mode'] == 'complete'</li>
                                        <li>data['llm_annotations_count'] == 10</li>
                                        <li>data['llm_annotations_errors'] == 2</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407-419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_non_aggregated_excludes_source_reports</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `non_aggregated_excludes_source_reports` method of `RunMeta` does not include source reports.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where non-aggregated report results do contain source reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>source_reports is not included in the report dictionary</li>
                                        <li>is_aggregated is set to False for this meta</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_run_meta_to_dict_full</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test RunMeta to dict with all optional fields.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of missing or outdated metadata.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'git_sha' field should be present and have the expected value.</li>
                                        <li>The 'git_dirty' field should be True.</li>
                                        <li>The 'repo_version', 'repo_git_sha', and 'plugin_git_sha' fields should be present and have the expected values.</li>
                                        <li>The 'config_hash' field should be present and have the expected value.</li>
                                        <li>The length of the 'source_reports' list should be 1.</li>
                                        <li>All source reports should be dictionaries with the required keys (path, sha256, run_id).</li>
                                        <li>The 'pytest_invocation', 'pytest_config_summary', and 'run_id' fields should be present and have the expected values.</li>
                                        <li>The 'is_aggregated' field should be True.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">49 lines (ranges: 277-279, 281-283, 364-380, 382-405, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_run_status_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> TestRunMeta::test_run_status_fields verifies that RunMeta includes the necessary run status fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where RunMeta is missing required run status fields, potentially leading to incorrect or incomplete results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'exit_code' field should be set to 1.</li>
                                        <li>The 'interrupted' field should be True.</li>
                                        <li>The 'collect_only' field should be True.</li>
                                        <li>The 'collected_count' field should equal 10.</li>
                                        <li>The 'selected_count' field should equal 8.</li>
                                        <li>The 'deselected_count' field should equal 2.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSchemaVersion::test_schema_version_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the schema version is correctly formatted (semver),</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where an incorrect or malformed semver format is used.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The schema version should be split into three parts (e.g., '1.2.3').</li>
                                        <li>Each part of the schema version should consist only of digits (0-9).</li>
                                        <li>All parts of the schema version should be non-empty and not empty strings.</li>
                                        <li>If a part is an empty string, it should be ignored or handled appropriately.</li>
                                        <li>The first part of the schema version should be greater than 0.</li>
                                        <li>The second part of the schema version should be less than or equal to 99.</li>
                                        <li>The third part of the schema version should be less than or equal to 999.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSchemaVersion::test_schema_version_in_report_root</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `ReportRoot` class to ensure it includes the schema version in its report root.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the schema version is not included in the report root, potentially causing issues with downstream processing or reporting.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `schema_version` attribute of the `ReportRoot` object should be equal to `SCHEMA_VERSION`.</li>
                                        <li>The `to_dict()` method of the `ReportRoot` object should return a dictionary with a key named `schema_version` and a value equal to `SCHEMA_VERSION`.</li>
                                        <li>The `schema_version` property of the `ReportRoot` class should have a value equal to `SCHEMA_VERSION`.</li>
                                        <li>The `schema_version` attribute of the `ReportRoot` object should be present in its `to_dict()` method.</li>
                                        <li>The `schema_version` property of the `ReportRoot` class should not be `None` or an empty string.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSourceCoverageEntry::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `CoverageEntry.to_dict()` correctly serializes a CoverageEntry object.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the coverage entry data is not properly serialized, potentially leading to incorrect or incomplete coverage reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key in the serialized dictionary matches the expected value.</li>
                                        <li>The 'line_ranges' key in the serialized dictionary matches the expected value.</li>
                                        <li>The 'line_count' key in the serialized dictionary matches the expected value.</li>
                                        <li>All line ranges are correctly formatted (e.g. '1-3', '5, 10-15')</li>
                                        <li>No missing or extra line ranges are present in the output</li>
                                        <li>The coverage entry data is not empty</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 71-78)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSourceReport::test_to_dict_minimal</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `to_dict` method of `LlmAnnotation` returns a dictionary with required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the minimal annotation is not properly serialized without the 'confidence' field.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The dictionary should contain the keys 'scenario', 'why_needed', and 'key_assertions'.</li>
                                        <li>The value of 'scenario' should be present in the dictionary.</li>
                                        <li>The value of 'why_needed' should be present in the dictionary.</li>
                                        <li>The value of 'key_assertions' should be present in the dictionary.</li>
                                        <li>The value of 'confidence' should not be present in the dictionary when it is None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 277-279, 281, 283)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSourceReport::test_to_dict_with_run_id</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `SourceReport` object's `to_dict()` method returns the `run_id` attribute correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `run_id` is not included in the dictionary representation of the `SourceReport` object.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `run_id` attribute should be present and equal to 'run-1' when the `to_dict()` method is called on a `SourceReport` object with a `run_id` parameter.</li>
                                        <li>The `run_id` attribute should not be missing or incorrect when the `to_dict()` method is called on a `SourceReport` object without a `run_id` parameter.</li>
                                        <li>The `run_id` attribute should be present and equal to 'run-1' when the `to_dict()` method is called on a `SourceReport` object with an invalid or missing `run_id` value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 277-279, 281-283)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSummary::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `CoverageEntry` class correctly serializes a coverage report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the coverage report is not properly serialized.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key in the coverage report should match the provided file path.</li>
                                        <li>The 'line_ranges' key in the coverage report should contain the expected line ranges.</li>
                                        <li>The 'line_count' key in the coverage report should match the specified line count.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 449-457, 459, 461)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_minimal_result</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the minimal result of a TestCaseResult object.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression that might occur when creating a minimal result with an empty nodeid or outcome.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'nodeid' field should be set to the expected value.</li>
                                        <li>The 'outcome' field should be set to the expected value.</li>
                                        <li>The 'duration' field should be set to 0.0 (indicating no execution time).</li>
                                        <li>The 'phase' field should be set to 'call'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_models.py::TestTestCaseResult::test_result_with_coverage verifies that the test result includes a coverage list.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the test result accurately reflects the code's coverage.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'coverage' key in the result dictionary should contain exactly one entry.</li>
                                        <li>The 'file_path' value of the first 'coverage' entry should be 'src/foo.py'.</li>
                                        <li>All lines in the 'coverage' list should have a line count greater than zero.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 40-43, 161-165, 167, 169, 171, 173, 176-178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_llm_opt_out</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `result` object includes a flag indicating LLM opt-out.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the LLM is enabled by default and the test passes without it.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `llm_opt_out` in the `result` dictionary should be `True`.</li>
                                        <li>The key `'llm_opt_out'` exists in the `d` dictionary.</li>
                                        <li>The value of `llm_opt_out` is a boolean value (`True` or `False`).</li>
                                        <li>The test passes without LLM opt-out enabled.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180-182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_rerun</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_result_with_rerun' verifies that the TestCaseResult object includes rerun fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that reruns are included in the result.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `rerun_count` is 2.</li>
                                        <li>The value of `final_outcome` is 'passed'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 161-165, 167, 169, 171, 173-176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_without_rerun_excludes_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test `test_result_without_rerun_excludes_fields` verifies that the `result` dictionary does not contain 'rerun_count' and 'final_outcome' keys.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the result of a test is rerun, potentially hiding important information about its outcome.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'rerun_count' key should be absent from the `result` dictionary.</li>
                                        <li>The 'final_outcome' key should be absent from the `result` dictionary.</li>
                                        <li>The 'rerun_count' and 'final_outcome' keys should not be present in the `result` dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_default_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that default values are set correctly for the Config class.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the default configuration values are not set correctly, potentially leading to unexpected behavior or errors in the application.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>cfg.provider == 'none'</li>
                                        <li>cfg.llm_context_mode == 'minimal'</li>
                                        <li>cfg.llm_max_tests == 0</li>
                                        <li>cfg.llm_max_retries == 3</li>
                                        <li>cfg.llm_context_bytes == 32000</li>
                                        <li>cfg.llm_context_file_limit == 10</li>
                                        <li>cfg.llm_requests_per_minute == 5</li>
                                        <li>cfg.llm_timeout_seconds == 30</li>
                                        <li>cfg.llm_cache_ttl_seconds == 86400</li>
                                        <li>cfg.include_phase == 'run'</li>
                                        <li>cfg.aggregate_policy == 'latest'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_get_default_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that `get_default_config()` returns a default configuration with no provider.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the default configuration is not correctly set to 'none'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_default_config()` should return an instance of `Config`.</li>
                                        <li>The attribute `provider` on the returned object should be set to `'none'`.</li>
                                        <li>The value of `provider` should match the expected default provider ('none').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_is_llm_enabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `is_llm_enabled` check returns False for a provider without an LLM.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case the LLM is not enabled by default.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert Config(provider='none').is_llm_enabled() is False</li>
                                        <li>assert Config(provider='ollama').is_llm_enabled() is True</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_aggregate_policy</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-197, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_context_mode</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the validation of an invalid context mode.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where an invalid context mode is not properly validated and causes unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The configuration object has been created with an invalid llm_context_mode.</li>
                                        <li>An error message indicating 'Invalid llm_context_mode' was found in the validation result.</li>
                                        <li>The error message includes the specific value 'mega_max'.</li>
                                        <li>A single error is expected as a result of this validation.</li>
                                        <li>The error message does not contain any additional context or details.</li>
                                        <li>The test verifies that an error is raised when an invalid context mode is provided.</li>
                                        <li>The test verifies that the error message contains the correct information about the invalid context mode.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-189, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_include_phase</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the test validates for an invalid include phase.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the validation of invalid include phases is not properly handled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `validate()` returns exactly one error message.</li>
                                        <li>The error message contains the string 'Invalid include_phase 'lunch_break'.</li>
                                        <li>The error message includes the specified include phase value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-205, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_numeric_ranges</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validation of numeric constraints for TestConfig.</p>
                                <p><strong>Why Needed:</strong> Prevents regression when setting invalid numeric ranges for LLM.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm_context_bytes' value should be at least 1000.</li>
                                        <li>The 'llm_max_tests' value should be 0 (no limit) or positive.</li>
                                        <li>The 'llm_requests_per_minute' value should be at least 1.</li>
                                        <li>The 'llm_timeout_seconds' value should be at least 1.</li>
                                        <li>The 'llm_max_retries' value should be 0 or positive.</li>
                                        <li>All numeric constraints must be validated successfully.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209-218, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_valid_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Valid configuration is validated successfully without any errors.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential bugs where an invalid or malformed configuration could cause the application to crash or behave unexpectedly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `validate()` method of the `Config` object returns an empty list of errors when a valid configuration is provided.</li>
                                        <li>No error messages are printed to the console indicating that no errors were found in the configuration.</li>
                                        <li>The test does not fail if the configuration contains invalid or missing values, as it only checks for the absence of errors.</li>
                                        <li>The `validate()` method correctly identifies and returns an empty list when a valid configuration is provided.</li>
                                        <li>The function signature and docstring indicate that it should return an empty list in this case.</li>
                                        <li>The test does not fail if the configuration contains invalid or missing values, as it only checks for the absence of errors.</li>
                                        <li>The `validate()` method correctly identifies and returns an empty list when a valid configuration is provided.</li>
                                        <li>The function signature and docstring indicate that it should return an empty list in this case.</li>
                                        <li>The test does not fail if the configuration contains invalid or missing values, as it only checks for the absence of errors.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_aggregation_options</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loading aggregation options for the `load_config` function.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the aggregate policy is set to 'merge' instead of 'merge_run_id'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `aggregate_dir` should be 'aggr_dir'.</li>
                                        <li>The value of `aggregate_policy` should be 'merge'.</li>
                                        <li>The value of `aggregate_run_id` should be 'run-123'.</li>
                                        <li>The value of `aggregate_group_id` should be 'group-abc'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286-294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_config_invalid_int_ini</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the handling of invalid integer values in INI files.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test crashes due to an invalid integer value in the INI file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `load_config` should not crash when it encounters an invalid integer value in the INI file.</li>
                                        <li>The default value of `llm_max_retries` is correctly set to 3.</li>
                                        <li>The test does not verify that the fallback value is incorrect (i.e., `llm_report_max_retries` equals 'garbage').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263-267, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_coverage_source</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `llm_coverage_source` option is set to 'cov_dir' after loading the configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the coverage source is not correctly set even when the correct option is provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_pytest_config.option.llm_coverage_source</li>
                                        <li>cfg.llm_coverage_source == 'cov_dir'</li>
                                        <li>cfg.llm_coverage_source is not None</li>
                                        <li>cfg.llm_coverage_source is not empty</li>
                                        <li>cfg.llm_coverage_source does not contain 'default'</li>
                                        <li>cfg.llm_coverage_source contains 'cov_dir'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_defaults</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the default provider and report HTML are correctly loaded when no options are provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the configuration defaults to 'none' without any explicit option being set, potentially leading to unexpected behavior or errors in subsequent tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>cfg.provider == 'none'</li>
                                        <li>cfg.report_html is None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">24 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_cli_overrides_ini</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that CLI options override ini options.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the CLI overrides ini settings, potentially causing unexpected behavior or incorrect results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>ini_value is set to 'cli_report.html' for llm_report_html option</li>
                                        <li>llm_requests_per_minute value is set to 100</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">27 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259-261, 263, 270-272, 274, 276, 278, 280-282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_cli_retries</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `llm_max_retries` option is set to 9 when loading configuration from CLI.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `llm_max_retries` option is not correctly set to its default value of 5, leading to incorrect configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `llm_max_retries` option should be set to 9 when loading configuration from CLI.</li>
                                        <li>The `llm_max_retries` option should have a default value of 5.</li>
                                        <li>The test should fail if the `llm_max_retries` option is not set to 9 or has a different value than its default.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282-283, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_ini</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loads values from ini options with mock Pytest configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the `load_config` function relies on `getini` to retrieve ini values, which may not be available or properly configured.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>cfg.provider == 'ollama'</li>
                                        <li>cfg.model == 'llama3'</li>
                                        <li>cfg.llm_context_mode == 'balanced'</li>
                                        <li>cfg.llm_requests_per_minute == 10</li>
                                        <li>cfg.llm_max_retries == 5</li>
                                        <li>cfg.report_html == 'report.html'</li>
                                        <li>cfg.report_json == 'report.json'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">32 lines (ranges: 107, 147, 248, 251-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_aggregation_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Config with aggregation settings to ensure correct directory and policy.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the configuration is not set correctly for aggregation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregate_dir` attribute of the `Config` object should be set to `/reports`.</li>
                                        <li>The `aggregate_policy` attribute of the `Config` object should be set to 'merge'.</li>
                                        <li>The `aggregate_include_history` attribute of the `Config` object should be set to True.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_all_output_paths</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests Config with all output paths.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails if any of the report files are missing or corrupted.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report_html` attribute is set to 'report.html'.</li>
                                        <li>The `report_json` attribute is set to 'report.json'.</li>
                                        <li>The `report_pdf` attribute is set to 'report.pdf'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_capture_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `capture_failed_output` parameter of `Config` is set to `True`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the capture output is not properly handled when `capture_failed_output` is `False`.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.capture_failed_output is True</li>
                                        <li>assert config.capture_failed_output == True</li>
                                        <li>Verify that setting `capture_failed_output` to `False` does not prevent test execution.</li>
                                        <li>Test that the capture output is properly handled when `capture_failed_output` is `True`.</li>
                                        <li>Ensure that the assertion passes even if `capture_output_max_chars` is set to a lower value than 8000.</li>
                                        <li>Verify that the test can still pass without capturing any output.</li>
                                        <li>Check that the test does not raise an exception when `capture_failed_output` is `False`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_compliance_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the configuration of compliance settings.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the configuration is not set correctly for compliance settings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `metadata_file` attribute of the `Config` object is set to 'metadata.json'.</li>
                                        <li>The `hmac_key_file` attribute of the `Config` object is set to 'key.txt'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_coverage_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the configuration of coverage settings.</p>
                                <p><strong>Why Needed:</strong> Prevents a bug where coverage settings are not properly configured, leading to incorrect test results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.omit_tests_from_coverage is False</li>
                                        <li>config.include_phase == "all"</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_custom_exclude_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `llm_context_exclude_globs` option is correctly excluded custom Python files and log files.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the `llm_context_exclude_globs` option might be incorrectly or unexpectedly included in the list of exclude globs for custom Python files and log files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The string `*.pyc` should be present in the `llm_context_exclude_globs` list.</li>
                                        <li>The string `*.log` should be present in the `llm_context_exclude_globs` list.</li>
                                        <li>Custom Python files (e.g., `custom_file.py`) and log files (e.g., `custom_log.log`) should not be included in the `llm_context_exclude_globs` list.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_include_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `llm_context_include_globs` attribute includes the correct globs.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the include globs are not correctly configured, potentially leading to incorrect or missing files being included in the LLM context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `*.py` glob matches files with the `.py` extension.</li>
                                        <li>The `*.pyi` glob matches files with the `.pyi` extension.</li>
                                        <li>The `*.py` glob includes all Python source files (`.py`, `.pyi`) in the specified directory.</li>
                                        <li>The `*.pyi` glob includes all Python source files with the `.pyi` extension (e.g., `.pyc`) in the specified directory.</li>
                                        <li>Files not matching either `*.py` or `*.pyi` are excluded from the LLM context.</li>
                                        <li>The include globs are correctly configured to match the expected file types and directories.</li>
                                        <li>The test passes without any errors or warnings, indicating that the configuration is correct.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_invocation_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that `include_pytest_invocation` is set to `False` for the specified configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the `include_pytest_invocation` setting is incorrectly configured, potentially leading to unexpected behavior or errors during testing.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `include_pytest_invocation` attribute of the test configuration object is set to `False`.</li>
                                        <li>The `include_pytest_invocation` attribute does not match the expected value of `False` for this specific configuration.</li>
                                        <li>The `include_pytest_invocation` attribute is correctly initialized with a boolean value.</li>
                                        <li>A pytest invocation setting is present in the test configuration, but it is not set to `True` or `False` as intended.</li>
                                        <li>The `invocation_redact_patterns` list contains a regular expression that matches API keys, which may be causing issues during testing.</li>
                                        <li>The `include_pytest_invocation` attribute is not being used in this specific test case.</li>
                                        <li>A different configuration object has the same `include_pytest_invocation` value as the one tested here.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_execution_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the configuration of LLM execution settings.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in LLMS when using a large number of tests or high concurrency.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of llm_max_tests is set to 50.</li>
                                        <li>The value of llm_max_concurrency is set to 8.</li>
                                        <li>The value of llm_requests_per_minute is set to 12.</li>
                                        <li>The value of llm_timeout_seconds is set to 60 seconds.</li>
                                        <li>The value of llm_cache_ttl_seconds is set to 3600 seconds (1 hour).</li>
                                        <li>The cache directory is set to .cache.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_param_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `llm_include_param_values` parameter is set to `True` and that its maximum value is 200.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the LLM param settings are not properly configured, potentially leading to incorrect output or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.llm_include_param_values should be `True`</li>
                                        <li>config.llm_param_value_max_chars should be `200`</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the configuration of LLM settings provided by OLLAMA.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the model and context bytes are not correctly configured for optimal performance.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` attribute is set to 'ollama'.</li>
                                        <li>The `model` attribute is set to 'llama3.2'.</li>
                                        <li>The `llm_context_bytes` attribute is set to 64000 bytes.</li>
                                        <li>The `llm_context_file_limit` attribute is set to 20.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_repo_root_path</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `repo_root` attribute is set correctly to `/project`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `repo_root` attribute is not set correctly, potentially leading to issues with repository configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.repo_root</li>
                                        <li>is equal to Path('/project')</li>
                                        <li>config.repo_root is set to /project</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_valid_phase_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `Config` class with valid include phase values.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where invalid or missing include phases are passed to the `validate()` method, potentially causing validation errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `include_phase` attribute of each configuration object is not present in any error messages.</li>
                                        <li>No error messages are raised when an include phase value is valid (e.g., 'run', 'setup', or 'all').</li>
                                        <li>All included phases are correctly validated and do not cause any validation errors.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_exclude_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the default exclude globs are correctly set for the LLM context.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the default exclude globs do not include certain files or directories, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `Config().llm_context_exclude_globs` returns a list of strings that includes `*.pyc`, `__pycache__/*`, and `*secret*`, `*password*`.</li>
                                        <li>The test asserts the presence of these globs in the default exclude list.</li>
                                        <li>If any of the assert statements fail, it would indicate an issue with the default exclude globs being set correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_redact_patterns</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test default redact patterns for ConfigDefaultsMaximal test.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the default redact patterns are not correctly detected, potentially leading to incorrect configuration of the application.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The '--password' and '--token' pattern names should be found in the invocation_redact_patterns list.</li>
                                        <li>The 'api[_-]?key' pattern name should be found in the invocation_redact_patterns list.</li>
                                        <li>Any other patterns that start with '--api_' or end with '_key' should also be present in the invocation_redact_patterns list.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test default values of the test_config_defaults_maximal module.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case the default configuration settings are not correctly set.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider should be 'none'.</li>
                                        <li>The llm_context_mode should be 'minimal'.</li>
                                        <li>The llm_context_bytes should be 32000 bytes.</li>
                                        <li>The omit_tests_from_coverage flag should be True.</li>
                                        <li>The include_phase should be 'run'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigHelpersMaximal::test_is_llm_enabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the correct enabled status of LLM for different providers.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in LLM configuration settings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `Config` object with provider 'none' should return False when `is_llm_enabled()` is called.</li>
                                        <li>The `Config` object with provider 'ollama' should return True when `is_llm_enabled()` is called.</li>
                                        <li>The `Config` object with provider 'litellm' should return True when `is_llm_enabled()` is called.</li>
                                        <li>The `Config` object with provider 'gemini' should return True when `is_llm_enabled()` is called.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_aggregate_policy</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `validate` method of `Config` class for an invalid aggregate policy.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where an invalid aggregate policy is not properly validated and returns an error message.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `validate` method should return exactly one error for an invalid aggregate policy.</li>
                                        <li>The error message should contain the string 'Invalid aggregate_policy 'invalid''.</li>
                                        <li>The error message should be present in the first error found during validation.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-197, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_context_mode</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `validate` method of the `Config` class when an invalid context mode is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `validate` method returns multiple errors for an invalid context mode, making it harder to identify and fix the issue.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `validate` method should return exactly one error message for an invalid context mode.</li>
                                        <li>The error message should contain 'Invalid llm_context_mode 'invalid''.</li>
                                        <li>The test should fail when an invalid context mode is provided.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-189, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_include_phase</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validates the `include_phase` parameter with an invalid value.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an incorrect or missing error message for an invalid `include_phase` value.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `validate()` returns exactly one error message.</li>
                                        <li>The error message contains the string 'Invalid include_phase 'invalid'.</li>
                                        <li>The error message is not empty.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-205, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validates an invalid provider.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test does not catch and report invalid providers.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `validate()` should return exactly one error message for an invalid provider.</li>
                                        <li>The error message for an invalid provider should contain the exact phrase 'Invalid provider 'invalid''.</li>
                                        <li>The test should assert that there is only one error message in total.</li>
                                        <li>The first error message should be the entire string 'Invalid provider 'invalid'.</li>
                                        <li>The error message should not be empty.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_numeric_bounds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_numeric_bounds</p>
                                <p><strong>Why Needed:</strong> This test prevents regression of invalid numeric values in the config.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `validate()` should return an error for invalid numeric values.</li>
                                        <li>The function `validate()` should throw an error if 'llm_context_bytes' is not a non-negative integer.</li>
                                        <li>The function `validate()` should throw an error if 'llm_max_tests' is negative.</li>
                                        <li>The function `validate()` should throw an error if 'llm_requests_per_minute' is zero.</li>
                                        <li>The function `validate()` should throw an error if 'llm_timeout_seconds' is zero.</li>
                                        <li>The function `validate()` should return at least 4 errors for invalid numeric values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209-217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_valid_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that an invalid configuration returns an empty list.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function does not handle all possible input configurations correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `validate` method of the `Config` class should return an empty list for any valid configuration.</li>
                                        <li>Any invalid configuration should raise an exception or return a meaningful error message.</li>
                                        <li>The function should be able to handle different types of input configurations (e.g., dictionaries, lists).</li>
                                        <li>If the input is not a dictionary or list, the function should raise an `TypeError` or return an error message.</li>
                                        <li>The function should check for any invalid keys in the configuration and raise an exception if found.</li>
                                        <li>Any missing required keys should be raised with an appropriate error message.</li>
                                        <li>The function should handle nested configurations correctly (if applicable).</li>
                                        <li>If the input is not a valid dictionary or list, the function should return a meaningful error message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginConfigLoading::test_config_defaults</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `Config` object has default settings.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the configuration is missing or incorrectly configured due to missing plugin options.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `cfg` variable should be an instance of `Config`.</li>
                                        <li>The value of `cfg` should not be None.</li>
                                        <li>The type of `cfg` should be `Config`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">33 lines (ranges: 107, 147, 248, 251-259, 261, 263-265, 270, 272-276, 278, 280, 282, 286, 288, 290-292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginConfigLoading::test_markers_exist_in_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `pytestconfig` object is accessible within the test.</p>
                                <p><strong>Why Needed:</strong> Prevent a potential bug where the plugin configuration is inaccessible due to incorrect import or setup.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytestconfig` object should be an instance of `pytest.config.Config`.</li>
                                        <li>The `pytestconfig` object should not be `None`.</li>
                                        <li>The `pytestconfig` object's attributes (e.g. `markers`, `options`) should be accessible.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_llm_context_marker</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the context marker does not cause errors.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the LLM marker causes an error in the test execution.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert True is executed without any errors.</li>
                                        <li>assert False is executed with an error message indicating context marker issue.</li>
                                        <li>assert 'Context marker should not cause errors.' is printed to the console.</li>
                                        <li>assert 'LLM marker' or 'context override' is present in the test output.</li>
                                        <li>assert 'Opt-out, context override' is present in the test output.</li>
                                        <li>assert 'Captured stdout/stderr for failed tests (opt-in)' is present in the test output.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_llm_opt_out_marker</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">


                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_requirement_marker</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'requirement_marker' function is being tested to ensure it does not throw any errors.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the 'requirement_marker' function could be used in error conditions, causing unexpected behavior or crashes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'requirement_marker' function should not raise an exception when called with no arguments.</li>
                                        <li>The 'requirement_marker' function should not throw any errors when executed without any inputs.</li>
                                        <li>The 'requirement_marker' function should not cause any runtime errors when used correctly and in the correct context.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestReportGeneration::test_report_writer_integration</span>
                        <div class="test-meta">
                            <span>32ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the integration of report writer with pytest_llm_report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when integrating report writer into pytest_llm_report, ensuring that full report generation flow works correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify existence of 'report.json' file in tmp_path.</li>
                                        <li>Verify correct number of passed tests (1) and failed tests (1) in the JSON data.</li>
                                        <li>Verify presence of 'test_a.py' and 'test_b.py' in the HTML report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">79 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">131 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that collectreport skips when disabled and pytest_collectreport is mocked.</p>
                                <p><strong>Why Needed:</strong> The test prevents a regression where pytest_collectreport fails to report the plugin's collection even though it was disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mocked session.config.stash.get._enabled_key returns False</li>
                                        <li>pytest_collectreport() does not call _enabled_key with True</li>
                                        <li>mock_report.session.config.stash.get.asserts_called_with(_enabled_key, False) is False</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 387-388, 391, 395-397, 408-409, 415-416)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_enabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `pytest_collectreport` calls the collector when collectreport is enabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where `pytest_collectreport` does not call the collector when collectreport is enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_collectreport` function should be able to find and stash the `_collector_key` key in the session configuration.</li>
                                        <li>The `stash_get` function should return `True` for the `_enabled_key` key.</li>
                                        <li>The `handle_collection_report` method of the collector should be called with the stash value containing the `_collector_key` key.</li>
                                        <li>The `handle_collection_report` method of the collector should not throw an exception if it cannot find the stash value.</li>
                                        <li>The `stash_get` function should return `True` for all other keys in the session configuration.</li>
                                        <li>The `pytest_collectreport` function should be able to handle a mock report object correctly.</li>
                                        <li>The `mock_collector.handle_collection_report` call should not throw an exception if it cannot find the stash value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 387-388, 391, 395-397, 408-409, 415, 419-421)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_no_session</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that `pytest_collectreport` does not raise an exception when no session is available.</p>
                                <p><strong>Why Needed:</strong> Prevent regression in plugin behavior when a test has no active session.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `pytest_collectreport(mock_report)` should not be called with a `session` attribute that is missing.</li>
                                        <li>No exception should be raised if the `session` attribute is deleted from the mock report object.</li>
                                        <li>The `session` attribute of the mock report object should be `None` after deletion.</li>
                                        <li>The function `pytest_collectreport(mock_report)` should not raise an AttributeError when the session attribute is accessed.</li>
                                        <li>The function `pytest_collectreport(mock_report)` should not raise a TypeError if the session attribute is accessed with no arguments.</li>
                                        <li>The function `pytest_collectreport(mock_report)` should not raise a KeyError if the session attribute is accessed without checking for its existence first.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 387-388, 391, 395-397, 408, 412)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_session_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the behavior of `pytest_collectreport` when a session is `None`.</p>
                                <p><strong>Why Needed:</strong> Prevents potential `pytest_collectreport` exceptions that may occur when trying to collect reports for an empty session.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>...</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 387-388, 391, 395-397, 408, 412)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_llm_enabled_warning</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM enabled warning is raised when using the Ollama LLM report provider.</p>
                                <p><strong>Why Needed:</strong> To prevent a potential bug where the LLM report provider 'ollama' is enabled and raises an error when configured with pytest.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_llm_report_provider` option should be set to `'ollama'`.</li>
                                        <li>The `llm_report_html`, `llm_report_json`, `llm_report_pdf`, `llm_evidence_bundle`, `llm_dependency_snapshot`, `llm_requests_per_minute`, `llm_aggregate_dir`, `llm_aggregate_policy`, `llm_aggregate_run_id`, and `llm_aggregate_group_id` options should be set to `None`.</li>
                                        <li>The `llm_max_retries` option should also be set to `None`.</li>
                                        <li>The `rootpath` and `stash` options should not be set.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">44 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_validation_errors</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that validation errors raise UsageError when setting invalid config.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the plugin does not handle configuration errors properly and raises a UsageError instead of providing meaningful error messages.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocking `getini` method with an invalid key returns a dictionary containing 'llm_report_provider' as the only valid value.</li>
                                        <li>Setting `option.llm_report_html`, `option.llm_report_json`, `option.llm_report_pdf`, `option.llm_evidence_bundle`, `option.llm_dependency_snapshot`, `option.llm_requests_per_minute`, `option.llm_aggregate_dir`, `option.llm_aggregate_policy`, `option.llm_aggregate_run_id`, and `option.llm_aggregate_group_id` to None.</li>
                                        <li>Setting `llm_report_provider` to an invalid value raises a ValueError with the message 'configuration errors'.</li>
                                        <li>The `pytest_configure` function is called with a valid config object, but it does not raise a UsageError.</li>
                                        <li>Calling `pytest_configure` with an invalid config object should raise a UsageError.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">43 lines (ranges: 107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 248, 251-253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-199, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_worker_skip</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that configure skips on xdist workers.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the plugin might skip configuration due to an incorrect or incomplete worker input.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_config.addinivalue_line was not called before calling pytest_configure</li>
                                        <li>addinivalue_line is still called for markers before worker check</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 169-171, 173-175, 177-179, 183-184, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigureFallback::test_pytest_configure_fallback_load</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that fallback to load_config occurs when Config.load is missing in pytest_configure function.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to missing Config.load method call.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The Config.load() method should be called with no arguments.</li>
                                        <li>The Config.validate() method should return an empty list.</li>
                                        <li>The load_config() function should not be called directly.</li>
                                        <li>The option.llm_report_html and option.llm_max_retries attributes should remain unchanged.</li>
                                        <li>The option.llm_max_retries attribute should still be None after the test.</li>
                                        <li>The mock_load object returned by patch should have been called once with no arguments.</li>
                                        <li>The Config.load() method call should not raise an exception.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_all_ini_options</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loading all INI options for plugin configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the plugin fails to load configuration due to missing or invalid INI values.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `llm_report_provider` option is set correctly.</li>
                                        <li>The `llm_report_model` option is set correctly.</li>
                                        <li>The `llm_report_context_mode` option is set correctly.</li>
                                        <li>The `llm_report_requests_per_minute` option is set to 10.</li>
                                        <li>The `report_html` option is set to 'ini.html'.</li>
                                        <li>The `report_json` option is set to 'ini.json'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">31 lines (ranges: 107, 147, 248, 251-263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_cli_overrides_ini</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test CLI options override INI options.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the CLI options override INI options, ensuring that the correct report files are used even when the INI file is not present or does not contain the required configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>cfg.report_html == 'cli.html'</li>
                                        <li>cfg.report_json == 'cli.json'</li>
                                        <li>cfg.report_pdf == 'cli.pdf'</li>
                                        <li>cfg.report_evidence_bundle == 'bundle.zip'</li>
                                        <li>cfg.report_dependency_snapshot == 'deps.json'</li>
                                        <li>cfg.llm_requests_per_minute == 20</li>
                                        <li>cfg.aggregate_dir == '/agg'</li>
                                        <li>cfg.aggregate_policy == 'merge'</li>
                                        <li>cfg.aggregate_run_id == 'run-123'</li>
                                        <li>cfg.aggregate_group_id == 'group-abc'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">38 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259-263, 270-283, 286-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that terminal summary skips when plugin is disabled.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where terminal summary is not displayed when the plugin is disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocked stash.get() with _enabled_key and False returns None, as expected.</li>
                                        <li>Mocked stash.get() with _enabled_key and True does not return None, as expected.</li>
                                        <li>Mocked stash.get() with _enabled_key and an incorrect value (e.g., 1) should have been called once.</li>
                                        <li>The stash.get method is called only once, even if the plugin is enabled multiple times.</li>
                                        <li>The stash.get method is not called when the plugin is disabled or does not exist.</li>
                                        <li>The stash.get method returns None instead of raising an exception when it fails to get a value.</li>
                                        <li>The stash.get method should raise an exception when it fails to get a value, as expected.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 238, 242-243, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_worker_skip</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that terminal summary skips on xdist worker when configured to skip.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the plugin does not properly handle skipping in xdist workers.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_terminal_summary` function should return None for the given configuration.</li>
                                        <li>The `workerinput` attribute of the mock config object is set to 'gw0'.</li>
                                        <li>No output is produced from the test (i.e., no output is printed or displayed).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 238-239, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::testload_config</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test config loading from pytest objects (CLI + INI) to ensure it correctly sets report HTML.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the report HTML is not set correctly if the `pytest_llm_report` CLI option or INI file does not provide an `llm_report_html` value.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>cfg.report_html == 'out.html'</li>
                                        <li>mock_config.option.llm_report_html == 'out.html'</li>
                                        <li>cfg.getini('llm_report_html') == None</li>
                                        <li>cfg.rootpath == '/root'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270-283, 286-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_disabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test makereport skips when disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the plugin does not report any issues even though it should.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_item.config.stash.get returns False</li>
                                        <li>mock_call is called with no arguments</li>
                                        <li>mock_outcome.get_result returns a MagicMock object</li>
                                        <li>gen.send raises StopIteration and passes mock_outcome</li>
                                        <li>gen.next yields the mock call to the next hookwrapper</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 387-388, 391-392, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_enabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test makereport calls collector when enabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the collector is not called when makereport is enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `mock_collector` should be called with the `mock_report` argument when `makereport` is enabled.</li>
                                        <li>The `mock_collector` should have been called once with the `mock_report` argument.</li>
                                        <li>The `mock_collector` should not have been called without any arguments (i.e., when makereport is disabled).</li>
                                        <li>The `mock_collector` should be called before calling `next(gen)` to ensure it has a chance to handle the log report.</li>
                                        <li>The `mock_collector` should not be called after `next(gen)` because there's no need to handle the log report again.</li>
                                        <li>The `mock_collector` should have been called with the correct arguments (i.e., `mock_report`) when `makereport` is enabled.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that collection_finish is skipped when disabled for Pytest.</p>
                                <p><strong>Why Needed:</strong> To prevent a regression where the plugin's hooks are not executed correctly when collection_finish is disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_session.config.stash.get.assert_called_with(_enabled_key, False)</li>
                                        <li>pytest_collection_finish(mock_session) should be called with _enabled_key as 'pytest_collection_finish' and False as its argument</li>
                                        <li>mock_session.config.stash.get.return_value should not be True</li>
                                        <li>_enabled_key should be 'pytest_collection_finish'</li>
                                        <li>pytest_collection_finish should not be called with any arguments</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 387-388, 391, 395-397, 431-432)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_enabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that collection_finish is called when collection finish is enabled.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the collector does not get notified about the finished items in Pytest.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_collector.handle_collection_finish.assert_called_once_with(mock_session.items)</li>
                                        <li>mock_session.items[0].is_pytest_collection_finished() == False</li>
                                        <li>mock_session.items[1].is_pytest_collection_finished() == True</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 387-388, 391, 395-397, 431, 435-437)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that sessionstart skips when disabled and verifies the expected behavior.</p>
                                <p><strong>Why Needed:</strong> To prevent a regression where pytest_sessionstart fails to check the enabled status of the plugin.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_sessionstart` function should be called with the `_enabled_key` parameter set to `False`.</li>
                                        <li>The `get` method of the stash configuration should have been called with the `_enabled_key` parameter and a boolean value of `False`.</li>
                                        <li>The `assert_called_with` method of the mock session's `config.stash.get` method should have been called.</li>
                                        <li>The `pytest_sessionstart` function should not be called with any other parameters.</li>
                                        <li>The `get` method of the stash configuration should return a boolean value of `False`.</li>
                                        <li>The `assert_called_with` method of the mock session's `config.stash.get` method should have been called with `_enabled_key` and `False` as arguments.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 387-388, 391, 395-397, 448-449)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_enabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that sessionstart initializes collector when enabled.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the collector is not initialized when pytest_sessionstart is called with an enabled configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The key _collector_key should be present in the mock stash.</li>
                                        <li>The key _start_time_key should be present in the mock stash.</li>
                                        <li>A MagicMock object with type _collector_key and _start_time_key should be created.</li>
                                        <li>A MagicMock object with type _enabled_key should be set to True in the stash_dict.</li>
                                        <li>_enabled_key should have a value of True in the stash_dict.</li>
                                        <li>The pytest_sessionstart function should create a collector when called with an enabled configuration.</li>
                                        <li>The collector should be initialized correctly, i.e., it should contain both _collector_key and _start_time_key.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 387-388, 391, 395-397, 448, 452, 455, 457-458)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test pytest_addoption adds expected arguments to the parser.</p>
                                <p><strong>Why Needed:</strong> pytest_addoption prevents a potential bug where it does not add all required options to the parser.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that `--llm-report` is added as an option.</li>
                                        <li>Verify that `--llm-coverage-source` is also added as an option.</li>
                                        <li>Check if both options are present in the parsed arguments.</li>
                                        <li>Verify that the group name matches 'llm-report' and the report type matches 'LLM-enhanced test reports'.</li>
                                        <li>Ensure that the `addoption` method of the group returns a tuple with two elements, where the first element is the option string.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">99 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption_ini</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test pytest_addoption adds INI options for llm_report plugin.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the plugin does not add INI options for pytest.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `pytest_addoption(parser)` is called with a `MagicMock` object as the parser.</li>
                                        <li>The `addini.call_args_list` attribute of the parsed parser contains lines starting with 'llm_report_'.</li>
                                        <li>The string 'llm_report_html' is found in the ini calls.</li>
                                        <li>The string 'llm_report_json' is found in the ini calls.</li>
                                        <li>The string 'llm_report_max_retries' is found in the ini calls.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">99 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_coverage_calculation</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test coverage percentage calculation logic for terminal summary.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in coverage percentage calculation logic when using the terminal summary feature.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report_html` option is set to 'out.html' and the `CoverageMapper` class is mocked with a mock `Coverage` object.</li>
                                        <li>The `MockStash` dictionary is created with the provided stash data.</li>
                                        <li>The `patch` decorator ensures that the `pathlib.Path.exists` function returns True when it should return False, simulating coverage file existence.</li>
                                        <li>The `patch` decorator ensures that the `Coverage` class is mocked with a mock object and its methods are called correctly.</li>
                                        <li>The `MockStash` dictionary is loaded into the `CoverageMapper` instance using the `load` method.</li>
                                        <li>The `report` method of the `Coverage` object is called to generate the coverage report, which should return 85.5 as expected.</li>
                                        <li>The test verifies that the coverage percentage calculation logic works correctly by checking if the reported coverage matches the expected value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">58 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 307, 309-315, 317-318, 331-332, 337-338, 365-375, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_llm_enabled</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test terminal summary with LLM enabled runs annotations.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in terminal summary functionality when LLM is enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that the `pytest_terminal_summary_llm_enabled` test passes without any errors.</li>
                                        <li>Check if the correct configuration is passed to `pytest_terminal_summary`.</li>
                                        <li>Ensure that the LLM annotator is called with the provided config.</li>
                                        <li>Verify that the annotation is performed correctly and returns the expected result.</li>
                                        <li>Confirm that the report writer is patched correctly and can write the output.</li>
                                        <li>Test that the coverage mapper is not used in this scenario.</li>
                                        <li>Verify that the `pytest_llm_report.llm.annotator.annotate_tests` function is called only once.</li>
                                        <li>Check if the correct model name is retrieved from the provider.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">59 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331-332, 337-340, 343, 345, 348-350, 357-362, 365-375, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_no_collector</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test terminal summary creates collector if missing.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the plugin does not create a collector even when it is supposed to be missing.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>Mocking stash with only enabled key and no config key should not trigger the creation of a collector.</li>
                                        <li>Mocking stash with only enabled key but no config key should still create a collector.</li>
                                        <li>Mocking stash with only disabled key and no config key should not trigger the creation of a collector.</li>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>...</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_with_aggregation</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test terminal summary with aggregation enabled.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where the plugin does not aggregate terminal summaries correctly when report_html and report_json are set to out.html and out.json respectively.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The stash object returned by pytest_terminal_summary should support both get() and [] indexing.</li>
                                        <li>The aggregation function should be called once with the correct arguments (0, stash).</li>
                                        <li>The ReportWriter should write JSON and HTML files correctly.</li>
                                        <li>The aggregation report should contain the expected data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 238, 242, 246, 249-250, 252-253, 256-257, 259, 261-265, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummaryErrors::test_terminal_summary_coverage_error</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test coverage calculation error when loading a large number of files.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the plugin fails to calculate coverage due to an OSError during load.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `load` method of `CoverageMapper` should not raise an exception if it encounters a file that cannot be loaded.</li>
                                        <li>The `coverage.Coverage` object returned by `CoverageMapper` should contain the correct coverage statistics for all files.</li>
                                        <li>The `report_writer.ReportWriter` instance should not throw an error when writing to the report HTML file.</li>
                                        <li>The `pytest_terminal_summary` function should return a summary with the expected coverage statistics even if it encounters an error during load.</li>
                                        <li>The `pytest_llm_report.report_writer.ReportWriter` instance should be able to write the report without raising an exception.</li>
                                        <li>The `pytest_llm_report.options.Config` object returned by `Config` should contain the correct configuration settings for coverage calculation.</li>
                                        <li>The `pytest_terminal_summary` function should return a summary with the expected error message when it encounters an OSError during load.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">52 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 322-325, 331-332, 337-338, 365-375, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_balanced_context</span>
                        <div class="test-meta">
                            <span>7ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the ContextAssembler to assemble a balanced context for a test file.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when the llm_context_mode is set to 'balanced' and the test file contains unbalanced dependencies.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'utils.py' file should be present in the assembled context.</li>
                                        <li>The 'def util()' function should be found in the 'utils.py' file within the assembled context.</li>
                                        <li>The line ranges of '1-2' and line count of '2' should match the coverage entry for 'utils.py'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">51 lines (ranges: 33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-155, 158-159, 163, 191-192, 194)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_complete_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Assembling a complete context for the 'test_a.py' test file and verifying that the 'test_1' function is included in the source code.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the 'test_a.py' test file is not properly assembled into a complete context, potentially leading to incorrect test results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'test_1' function should be present in the source code of the assembled context.</li>
                                        <li>The 'test_1' function should be included in the assembly output.</li>
                                        <li>The 'test_a.py::test_1' path should match the expected location in the assembled context.</li>
                                        <li>The 'test_a.py::test_1' function name should be present in the source code of the assembled context.</li>
                                        <li>The 'test_a.py::test_1' function signature should match the expected signature in the assembled context.</li>
                                        <li>The assembly output should contain the 'test_1' function definition.</li>
                                        <li>The assembly output should not contain any errors or warnings related to the 'test_1' function.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">34 lines (ranges: 33, 49, 52, 55, 58, 60, 63, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 132-133, 180)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_minimal_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the ContextAssembler can assemble a minimal context for a test file with a single test function.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when using the 'minimal' llm_context_mode, as it ensures the assembler only creates a minimal context without any additional dependencies.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The source code of the test file is present in the assembled context.</li>
                                        <li>The context contains no references to external modules or functions.</li>
                                        <li>The test function `test_1` is present in the assembled context.</li>
                                        <li>The context has an empty dictionary as its value.</li>
                                        <li>No additional dependencies are included in the context.</li>
                                        <li>No external imports are used in the test function.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">30 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_balanced_context_limits</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the ContextAssembler with balanced context limits to ensure it correctly truncates long files.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the ContextAssembler does not truncate long files according to the specified context limit.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>ContextAssemblyResult: The assembled context contains only the truncated part of the file.</li>
                                        <li>The length of the assembled context is within the allowed limit (40 bytes).</li>
                                        <li>The file name in the assembled context does not exceed the specified limit (20 bytes + truncation message).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">34 lines (ranges: 33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-84, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-156, 158-159, 163, 191-192, 194)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_get_test_source_edge_cases</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the ContextAssembler with edge cases where a non-existent file is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues when using the ContextAssembler with files that do not exist, as it ensures the assembler correctly handles such scenarios.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `_get_test_source` returns an empty string for a non-existent file.</li>
                                        <li>The function `_get_test_source` includes the full path of the test file in its output.</li>
                                        <li>The function `_get_test_source` correctly identifies the 'test' keyword in the nested test name with parameters.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 33, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_should_exclude</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the ContextAssembler should exclude certain files from being processed by the LLM.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the ContextAssembler incorrectly excludes important files, leading to incorrect results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file 'test.pyc' is excluded because it has been compiled and its contents are not relevant to the LLM's processing.</li>
                                        <li>The file 'secret/key.txt' is excluded because it contains sensitive information that should be protected by the ContextAssembler.</li>
                                        <li>The file 'public/readme.md' is included in the exclusion list because it does not contain any sensitive or confidential information.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 33, 191-194)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_consecutive_lines</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'compress_ranges' function is tested to ensure consecutive lines are compressed correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where consecutive lines in the input list do not use range notation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert compress_ranges([1, 2, 3]) == '1-3'</li>
                                        <li>assert compress_ranges([4, 5, 6]) == '4-6'</li>
                                        <li>assert compress_ranges([-1, -2, -3]) == '-1-3'</li>
                                        <li>assert compress_ranges([]) == ''</li>
                                        <li>assert compress_ranges([1]) == '1'</li>
                                        <li>assert compress_ranges([1, 2]) == '1-2'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_duplicates</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the function correctly handles duplicate ranges.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly identifies non-duplicate ranges as duplicates.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert compress_ranges([1, 2, 2, 3, 3, 3]) == '1-3'</li>
                                        <li>assert compress_ranges([4, 5, 6, 7, 8, 9]) == None</li>
                                        <li>assert compress_ranges([1, 1, 2, 2, 3, 3]) == '1-3'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_empty_list</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `compress_ranges` function with an empty input list.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where an empty list is not correctly compressed to an empty string.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return an empty string for an empty input list.</li>
                                        <li>The function should handle the case of an empty list without raising any exceptions or errors.</li>
                                        <li>The output of `compress_ranges([])` should be a string representing an empty range (e.g., '[]') rather than None or some other value.</li>
                                        <li>Any additional ranges in the input list should not affect the output of `compress_ranges([])`.</li>
                                        <li>The function should handle lists containing multiple ranges correctly (e.g., `[1, 2]` and `[3, 4]`).</li>
                                        <li>If a non-empty range is present in the input list, it should be included in the compressed result.</li>
                                        <li>The function should not return an empty string for a list with one or more ranges (e.g., `[1, 5]` and `[2, 6]`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 29-30)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_mixed_ranges</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the function when given a mixed range of values.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in cases where ranges are present alongside single values.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should correctly group the ranges and singles into separate output strings.</li>
                                        <li>The function should handle ranges with inclusive or exclusive endpoints correctly.</li>
                                        <li>The function should not include any invalid range combinations (e.g., -1-3, etc.).</li>
                                        <li>The function should preserve the original order of elements within each range.</li>
                                        <li>The function should handle edge cases where there are no values in a range.</li>
                                        <li>The function should correctly identify ranges with overlapping endpoints.</li>
                                        <li>The function should not include any invalid or duplicate output strings.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_non_consecutive_lines</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'test_non_consecutive_lines' test verifies that non-consecutive lines are comma-separated.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where consecutive lines are not separated by commas.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input list should be sorted in ascending order.</li>
                                        <li>Each element in the list should be an integer.</li>
                                        <li>Non-consecutive elements should be separated by commas.</li>
                                        <li>Consecutive elements should be separated by spaces.</li>
                                        <li>All elements should be comma-separated, with no leading or trailing spaces.</li>
                                        <li>No non-integer values should be present in the input list.</li>
                                        <li>The output string should match the expected result.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_single_line</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'single_line' test verifies that the `compress_ranges` function does not attempt to compress a single-line string.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when the `compress_ranges` function is called with a single-element list of integers, as it may incorrectly try to apply range notation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input list should be empty or contain only one element.</li>
                                        <li>The output should be the original string '5'.</li>
                                        <li>No error message or exception should be raised.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 29, 33, 35-37, 39, 50, 52, 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_two_consecutive</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_ranges.py::TestCompressRanges::test_two_consecutive</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where two consecutive numbers are not compressed to range notation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function compresses the input list into '1-2' if it contains two consecutive numbers.</li>
                                        <li>If the input list does not contain two consecutive numbers, the function should return an error message or raise a meaningful exception.</li>
                                        <li>The function should handle edge cases where the input list is empty or only contains one number correctly.</li>
                                        <li>Two consecutive numbers in the input list should be treated as separate ranges.</li>
                                        <li>Non-consecutive numbers in the input list should not be compressed to range notation.</li>
                                        <li>The function should raise a meaningful exception when given an invalid input, such as non-numeric values or out-of-range numbers.</li>
                                        <li>If the input is a single number, the function should return that number instead of trying to compress it into a range.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_unsorted_input</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_unsorted_input' verifies that the function handles unsorted input correctly.</p>
                                <p><strong>Why Needed:</strong> The test prevents a potential bug where the function would incorrectly group ranges in an unsorted list.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Input is sorted before compression.</li>
                                        <li>Ranges are grouped by their correct order (e.g., '1-3' for [1, 2, 3]).</li>
                                        <li>Ranges with the same start value but different end values are correctly separated ('5').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_empty_string</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `expand_ranges` function with an empty string.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function returns incorrect results for empty input strings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Input string is empty (length: 0).</li>
                                        <li>Function should return an empty list (`[]`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 81-82)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_mixed</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> test_mixed verifies the expansion of mixed range values.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the function incorrectly expands single numbers into ranges.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input string should be expanded correctly to include all specified numbers in their respective ranges.</li>
                                        <li>Single numbers should not be included in the output if they do not fall within any range.</li>
                                        <li>Ranges should include all specified values without gaps or overlaps.</li>
                                        <li>Invalid characters (e.g., commas) should not affect the expansion of the input string.</li>
                                        <li>The function should handle edge cases where a single number is at the start/end of a range correctly.</li>
                                        <li>The function should handle ranges with overlapping values correctly.</li>
                                        <li>The function should raise an error for invalid input strings.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 81, 84-91, 93, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_range</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'expand_ranges' function is called with a string argument containing a range (e.g., '1-3') and it returns the expected result.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not expand ranges correctly, potentially leading to incorrect results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input string should be in the format 'start-end' (e.g., '1-3')</li>
                                        <li>The function should return a list of numbers from start to end (inclusive)</li>
                                        <li>The start value should be less than or equal to the end value</li>
                                        <li>All values in the range should be integers</li>
                                        <li>No negative numbers should be allowed in the range</li>
                                        <li>The function should handle edge cases where the input string is empty or contains only one character</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 81, 84-91, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_roundtrip</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `compress_ranges` and `expand_ranges` functions should be able to reverse their operations.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the order of elements in the compressed or expanded ranges is not preserved.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>original = [1, 2, 3, 5, 10, 11, 12, 15]</li>
                                        <li>compressed = compress_ranges(original)</li>
                                        <li>expanded = expand_ranges(compressed)</li>
                                        <li>assert expanded == original</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">27 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67, 81, 84-91, 93, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_single_number</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `expand_ranges` function returns a list containing only one element when given a single number.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the function incorrectly expands multiple numbers into lists with multiple elements.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input string should be a single number (e.g., '5')</li>
                                        <li>The output list should contain only one element (i.e., [5])</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 81, 84-87, 93, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestFormatDuration::test_milliseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the function formats duration as milliseconds for less than one second.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function does not correctly format durations for values less than 1s.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return '500ms' when given an input of 0.5 seconds.</li>
                                        <li>The function should return '1ms' when given an input of 0.001 seconds.</li>
                                        <li>The function should return '0ms' when given an input of 0.0 seconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestFormatDuration::test_seconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the function formats duration values correctly for seconds.</p>
                                <p><strong>Why Needed:</strong> The test prevents a potential bug where the function does not handle durations greater than or equal to 1 second correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>{'message': 'Format should be in seconds', 'description': 'The format of the output is correct'}</li>
                                        <li>{'message': "Output should contain 's' suffix", 'description': "The output contains a 's' suffix for durations greater than or equal to 1 second"}</li>
                                        <li>{'message': 'No negative duration values are supported', 'description': 'Negative duration values are not supported by this function'}</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestOutcomeToCssClass::test_all_outcomes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> All outcomes should map to CSS classes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential CSS class mismatch issue where 'all' outcomes are incorrectly mapped to non-existent or incorrect classes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>outcome-to-css_class('passed') == 'outcome-passed'</li>
                                        <li>outcome-to-css_class('failed') == 'outcome-failed'</li>
                                        <li>outcome-to-css_class('skipped') == 'outcome-skipped'</li>
                                        <li>outcome-to-css_class('xfailed') == 'outcome-xfailed'</li>
                                        <li>outcome-to-css_class('xpassed') == 'outcome-xpassed'</li>
                                        <li>outcome-to-css_class('error') == 'outcome-error'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 79-85, 87)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the case where 'outcome' is unknown in `outcome_to_css_class` function.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function returns an unexpected class for an unknown outcome.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return the default class 'outcome-unknown' when given an unknown outcome.</li>
                                        <li>The function should not raise any exceptions or errors when given an unknown outcome.</li>
                                        <li>The function should correctly handle cases where the outcome is not recognized by the `outcome_to_css_class` mapping.</li>
                                        <li>The function's output should be consistent with the expected default class for unknown outcomes.</li>
                                        <li>The function's behavior should not change between different Python versions or environments.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 79-85, 87)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_basic_report</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the report renders a complete HTML document with the expected elements.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the report does not render correctly due to missing or incorrect plugin and repository information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The presence of '<!DOCTYPE html>' in the rendered HTML</li>
                                        <li>The presence of 'Test Report' in the rendered HTML</li>
                                        <li>The presence of 'test::passed' in the rendered HTML</li>
                                        <li>The presence of 'test::failed' in the rendered HTML</li>
                                        <li>The presence of 'PASSED' and 'FAILED' in the rendered HTML</li>
                                        <li>The presence of 'Plugin:</strong> v0.1.0' in the rendered HTML</li>
                                        <li>The presence of 'Repo:</strong> v1.2.3' in the rendered HTML</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">52 lines (ranges: 65-67, 79-85, 87, 121-124, 126-127, 131-132, 141-143, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test renders coverage to ensure that the rendered HTML includes the specified file and line count.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the rendered HTML includes the expected information, which is critical for maintaining code coverage.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The rendered HTML should include the specified file (src/foo.py).</li>
                                        <li>The rendered HTML should include the correct number of lines (5).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">52 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-129, 131-132, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_llm_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the LLM annotation is included in the rendered HTML report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential security vulnerability where an attacker could manipulate the LLM annotations to bypass authentication.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The string 'Tests login flow' should be present in the rendered HTML report.</li>
                                        <li>The string 'Prevents auth bypass' should be present in the rendered HTML report.</li>
                                        <li>The LlmAnnotation object should contain a scenario attribute with value 'Tests login flow'.</li>
                                        <li>The LlmAnnotation object should contain a why_needed attribute with value 'Prevents auth bypass'.</li>
                                        <li>The html string should contain both 'Tests login flow' and 'Prevents auth bypass' strings.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-134, 136-137, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_source_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the inclusion of source coverage summary in the rendered HTML report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the source coverage information is not included in the rendered HTML report, potentially misleading users about the code's coverage status.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'Source Coverage' section should be present in the rendered HTML report.</li>
                                        <li>The 'src/foo.py' file path should be included in the 'Source Coverage' section.</li>
                                        <li>The source coverage percentage should be displayed as '80.0%' in the rendered HTML report.</li>
                                        <li>All statements in the source code should be covered by at least 8% of the total statements in the test.</li>
                                        <li>At least 2 statements in the source code should be missed, which corresponds to 20% of the total statements in the test.</li>
                                        <li>The 'covered_ranges' and 'missed_ranges' sections should contain ranges that accurately represent the covered and missed statements respectively.</li>
                                        <li>All statements in the source code should be covered by at least 80% of the total statements in the test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">63 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-132, 141-142, 145-153, 158-164, 166-172, 177, 192, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_xpass_summary</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the rendered summary includes both 'XFailed' and 'XPassed' entries.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the summary is missing 'XFailed' or 'XPassed' entries when using XPass tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The HTML string contains the exact text 'XFailed', indicating that it was included in the rendered summary.</li>
                                        <li>The HTML string contains the exact text 'XPassed', indicating that it was included in the rendered summary.</li>
                                        <li>The presence of both 'XFailed' and 'XPassed' entries is confirmed by the assertion 'XFailed' being present in the HTML string.</li>
                                        <li>The absence of either 'XFailed' or 'XPassed' entry is not detected by this test.</li>
                                        <li>The test verifies that the rendered summary includes only one of each status, as expected for XPass tests.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">50 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-132, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestComputeSha256::test_different_content</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test computes SHA-256 for different input strings and verifies the output is different.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the same input string produces the same hash value, potentially leading to incorrect reporting or analysis of data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `compute_sha256` correctly computes the SHA-256 hash for both input strings.</li>
                                        <li>The output of `compute_sha256(b'hello')` is different from the output of `compute_sha256(b'world')`.</li>
                                        <li>The computed hash values are unique and cannot be compared directly.</li>
                                        <li>The function handles non-string inputs correctly (e.g., bytes, integers).</li>
                                        <li>The test case covers a common edge case where input strings are different but have the same content.</li>
                                        <li>The output of `compute_sha256(b'hello')` is not equal to `compute_sha256(b'world')` even though they contain different characters.</li>
                                        <li>The function correctly handles duplicate input strings, as expected for different content.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 55)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestComputeSha256::test_empty_bytes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_empty_bytes' verifies that an empty bytes input produces consistent hash.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where an empty bytes input could lead to inconsistent hashes due to the SHA-256 algorithm's behavior with zero-length inputs.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Input should be an empty bytes object (b'')</li>
                                        <li>Hash of the input should be equal to hash of another identical input (hash1 == hash2)</li>
                                        <li>Length of the hash output should match the expected SHA-256 hex length (64)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 55)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_run_meta</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `build_run_meta` method of ReportWriter with a test case that includes version info.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the report writer does not include version information in the run metadata.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The duration of the test should be 60 seconds.</li>
                                        <li>The `pytest_version` attribute of the meta object should have a value.</li>
                                        <li>The `plugin_version` attribute of the meta object should be set to '0.1.0'.</li>
                                        <li>The `python_version` attribute of the meta object should also be set to a valid Python version.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_summary_all_outcomes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies the ReportWriter's ability to build a summary of all outcome types.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the `total` count is not accurate due to missing or incorrect outcome counts.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total number of outcomes should be equal to the sum of passed, failed, skipped, xfailed, xpassed, and error outcomes.</li>
                                        <li>Each outcome type (passed, failed, skipped, xfailed, xpassed, error) should appear exactly once in the summary.</li>
                                        <li>The `total` count should match the expected value based on the number of tests with each outcome type.</li>
                                        <li>The `passed`, `failed`, `skipped`, `xfailed`, and `xpassed` counts should be accurate for each test.</li>
                                        <li>The `error` count should also be accurate for each test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 156-158, 312, 314-315, 317-328, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_summary_counts</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `total` count of outcomes is accurate.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the total count of outcomes might be incorrect due to missing or incomplete tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>summary.total should equal 4 (the number of tests passed)</li>
                                        <li>summary.passed should equal 2 (the number of tests that passed)</li>
                                        <li>summary.failed should equal 1 (the number of tests that failed)</li>
                                        <li>summary.skipped should equal 1 (the number of tests that were skipped)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">13 lines (ranges: 156-158, 312, 314-315, 317-322, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_create_writer</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the functionality of creating a ReportWriter instance.</p>
                                <p><strong>Why Needed:</strong> To prevent a potential bug where the Writer's configuration or artifacts are not properly initialized.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config` attribute of the `ReportWriter` instance should be equal to the provided `Config` object.</li>
                                        <li>The `warnings` list of the `ReportWriter` instance should be empty.</li>
                                        <li>The `artifacts` list of the `ReportWriter` instance should be empty.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 156-158)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_assembles_tests</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that ReportWriter writes a report with all tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the report does not include all tests, potentially leading to incomplete or misleading reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the report.tests list should be equal to 2.</li>
                                        <li>The total value of report.summary.total should be equal to 2.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">93 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_includes_coverage_percent</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `ReportWriter` class writes a report with a total coverage percentage.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the coverage percentage is not included in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `coverage_total_percent` attribute of the `report.summary` object should be equal to the provided `coverage_percent` value.</li>
                                        <li>The `report.summary.coverage_total_percent` property should have a numerical value.</li>
                                        <li>The coverage percentage should be calculated correctly based on the report's data.</li>
                                        <li>The test should fail if the coverage percentage is not included in the report.</li>
                                        <li>The coverage percentage should be reported as a percentage value (e.g., 85.5%).</li>
                                        <li>The `report.summary.coverage_total_percent` property should update correctly after writing the report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">93 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-199, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_includes_source_coverage</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">92 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_merges_coverage</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test ReportWriter::test_write_report_merges_coverage verifies that the report writer merges coverage into tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case where multiple tests have overlapping code coverage.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The first test's coverage should be a single entry with file path 'src/foo.py'.</li>
                                        <li>The first test's coverage should only contain the specified file path.</li>
                                        <li>The number of entries in the report's first test's coverage should match the number of tests.</li>
                                        <li>The first test's coverage should have exactly one file path.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">94 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186-189, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_atomic_write_fallback</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the ReportWriterWithFiles class falls back to direct write if an atomic write fails and the 'os.replace' mock is used.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the atomic write operation fails, but instead of raising an exception, it falls back to direct write.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file 'report.json' should exist at the expected location.</li>
                                        <li>Any warnings with code 'W203' (indicating a permission issue) should be present in the report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">125 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506-507, 509-512, 515-516)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_creates_directory_if_missing</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the `ReportWriter` creates a directory if it doesn't exist.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the report writer fails to create an output directory when the input JSON file does not exist.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `tmp_path / 'subdir' / 'report.json'` path should exist after calling `writer.write_report(tests)`.</li>
                                        <li>The `tmp_path / 'subdir' / 'report.json'` path should be a directory and not an empty file after calling `writer.write_report(tests)`.</li>
                                        <li>The `tmp_path / 'subdir' / 'report.json'` path should have the correct permissions (e.g. read, write, execute) after calling `writer.write_report(tests)`.</li>
                                        <li>The `tmp_path / 'subdir' / 'report.json'` path should be a directory with the correct number of subdirectories and files after calling `writer.write_report(tests)`.</li>
                                        <li>The `tmp_path / 'subdir' / 'report.json'` path should not have any empty directories or files after calling `writer.write_report(tests)`.</li>
                                        <li>The `tmp_path / 'subdir' / 'report.json'` path should be a directory with the correct permissions (e.g. read, write, execute) when checking its existence using `assert tmp_path / 'subdir' / 'report.json'.exists()`.</li>
                                        <li>The `tmp_path / 'subdir' / 'report.json'` path should not exist after calling `writer.write_report(tests)` if the input JSON file does not exist.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">84 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">123 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-477, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_ensure_dir_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `test_ensure_dir_failure` function to ensure it correctly handles directory creation failures.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the report writer fails to capture warnings when creating a non-existent directory.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should raise an exception with code 'W201' (Permission denied) when attempting to create the directory.</li>
                                        <li>The `writer.warnings` list should contain at least one warning with code 'W201'.</li>
                                        <li>The `writer.warnings` list should not be empty after attempting to create the non-existent directory.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 156-158, 470-473, 480-484)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_git_info_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `get_git_info` function handles git command failures gracefully by returning `None` for both SHA and dirty flag when `git not found`,</p>
                                <p><strong>Why Needed:</strong> To prevent a test failure where the test relies on `get_git_info()` to return non-None values, even if it encounters a Git command failure.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return `None` for both `sha` and `dirty` when `git not found` is encountered.</li>
                                        <li>The function should handle the case where `git not found` raises an exception without raising it again.</li>
                                        <li>The function should still allow the test to continue running even if `get_git_info()` returns non-None values for other reasons (e.g., successful git command execution).</li>
                                        <li>The function should provide a clear indication that the git command failed, such as returning `None` or raising an exception with a meaningful error message.</li>
                                        <li>The function should not silently ignore the failure and return incorrect results.</li>
                                        <li>The function should handle the case where `git not found` is a known Git version (e.g., older versions) without introducing new bugs.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 67-73, 85-86)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_creates_file</span>
                        <div class="test-meta">
                            <span>30ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test `test_write_html_creates_file` verifies that the report writer creates an HTML file and includes expected content.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the report writer does not create an HTML file or contains incorrect expected content.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The HTML file should be created at `report.html` in the temporary directory.</li>
                                        <li>The HTML file should contain the expected tests, including 'test1' and 'test2'.</li>
                                        <li>The HTML file should display a message indicating that all tests passed ('PASSED') or failed with an error message ('XFailed').</li>
                                        <li>The HTML file should include messages for skipped tests ('Skipped'), Xfailed tests ('XFailed'), and Xpassed tests ('XPassed').</li>
                                        <li>The HTML file should contain the expected report header, including 'Test 1', 'Test 2', 'PASSED', 'FAILED', 'Skipped', 'XFailed', and 'XPassed'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">115 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_includes_xfail_summary</span>
                        <div class="test-meta">
                            <span>31ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the report writer includes xfail outcomes in the HTML summary.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where xfail outcomes are not included in the HTML summary.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'XFAILED' keyword is present in the HTML summary.</li>
                                        <li>The 'XFailed' keyword is present in the HTML summary.</li>
                                        <li>The 'XPASSED' keyword is present in the HTML summary.</li>
                                        <li>The 'XPassed' keyword is present in the HTML summary.</li>
                                        <li>The 'XFAILED' and 'XFailed' keywords are both present in the HTML summary.</li>
                                        <li>The 'XPassed' and 'XPassed' keywords are both present in the HTML summary.</li>
                                        <li>All xfail outcomes are included in the HTML summary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-326, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_json_creates_file</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the `ReportWriter` class successfully creates a JSON file with an associated hash.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the report writer fails to create a JSON file, potentially leading to data loss or inconsistencies.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report.json` file should be created in the specified path.</li>
                                        <li>At least one artifact should be tracked for the report.</li>
                                        <li>The number of artifacts should be greater than zero.</li>
                                        <li>The file should exist at the specified path.</li>
                                        <li>The `ReportWriter` class should successfully create a JSON file with an associated hash.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">78 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">117 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_creates_file</span>
                        <div class="test-meta">
                            <span>33ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test writes PDF file when Playwright is available.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the test fails due to missing or corrupted report files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report.pdf` path should be created.</li>
                                        <li>All artifacts in the report directory should have a path equal to the `report.pdf` path.</li>
                                        <li>The `report.pdf` file should exist at the expected location.</li>
                                        <li>Any artifacts with paths not matching the `report.pdf` path should be ignored.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">125 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 401, 410, 412, 414-423, 434-435, 437-443, 448, 453, 455, 458-462, 470-471)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_missing_playwright_warns</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that a warning is raised when missing Playwright is used for PDF output.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the report writer does not warn users when Playwright is missing for PDF output.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'report.pdf' file should not exist after the test.</li>
                                        <li>Any warning code W204_PDF_PLAYWRIGHT_MISSING should be present in the list of warnings.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">98 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 401-405, 408)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer_coverage_v2.py::test_report_writer_ensure_dir_creation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test ensures directory creation of report files.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where the report file is created without being written to.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `tmp_dir` exists after the test.</li>
                                        <li>Any warnings from the report writer are set to 'W202' (directory creation warning).</li>
                                        <li>The directory containing the report files (`r.html`) is deleted afterwards.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 156-158, 470-477)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer_coverage_v2.py::test_report_writer_metadata_skips</span>
                        <div class="test-meta">
                            <span>8ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the scenario where report_writer_metadata_skips verifies that metadata skips when reports are disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that metadata is skipped when reports are disabled, which helps maintain the expected behavior of the ReportWriter.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'start_time' key should be present in the metadata.</li>
                                        <li>Metadata should not contain the 'llm_model' key.</li>
                                        <li>The 'llm_model' key should be None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_schemas.py::TestAnnotationSchema::test_from_dict_full</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `from_dict` method correctly creates an annotation from a dictionary with all required fields.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in handling missing or invalid input data, ensuring consistent behavior across different scenarios.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert schema.scenario == 'Verify login'</li>
                                        <li>assert schema.why_needed == 'Catch auth bugs'</li>
                                        <li>assert schema.key_assertions == ['assert 200', 'assert token']</li>
                                        <li>assert schema.confidence == 0.95</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_schemas.py::TestAnnotationSchema::test_to_dict_full</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Should convert to dictionary with all fields.</p>
                                <p><strong>Why Needed:</strong> Prevent regression in authentication logic by ensuring correct data is passed to the API.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert data['scenario'] == 'Verify login'</li>
                                        <li>assert data['why_needed'] == 'Catch auth bugs'</li>
                                        <li>assert data['key_assertions'] == ['assert 200', 'assert token']</li>
                                        <li>assert data['confidence'] == 0.95</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 90-92, 94-98)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_report_created</span>
                        <div class="test-meta">
                            <span>79ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that an HTML report is created when the test function `test_simple` is executed.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the HTML report might not be generated correctly if the test function `test_simple` raises an exception.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file path of the generated report should exist.</li>
                                        <li>The content of the report should contain the string '<html',</li>
                                        <li>The string 'test_simple' should be present in the report's content.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">101 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_summary_counts_all_statuses</span>
                        <div class="test-meta">
                            <span>111ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_summary_counts_all_statuses</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the HTML summary counts are not accurate for all statuses.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'Total Tests' label should be included in the HTML summary.</li>
                                        <li>The 'Passed' label should have a count of 1.</li>
                                        <li>The 'Failed' label should have a count of 1.</li>
                                        <li>The 'Skipped' label should have a count of 1.</li>
                                        <li>The 'XFailed' label should have a count of 1.</li>
                                        <li>The 'XPassed' label should have a count of 1.</li>
                                        <li>The 'Errors' and 'Error' labels should be included in the HTML summary with correct counts.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 78-79, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212-214, 216, 227-228, 230-236, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">111 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-328, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_json_report_created</span>
                        <div class="test-meta">
                            <span>65ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The JSON report is created and contains the expected schema version, summary statistics, and test counts.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that a basic report generation functionality is working correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `schema_version` key in the report should be set to the current LLM Report JSON format.</li>
                                        <li>The `summary` key should contain the correct total, passed, and failed counts.</li>
                                        <li>The `passed` count should match the number of tests that passed.</li>
                                        <li>The `failed` count should match the number of tests that failed.</li>
                                        <li>The `report.json` file should exist in the test directory after running the test.</li>
                                        <li>The JSON data loaded from the report file should contain the expected keys and values.</li>
                                        <li>The `schema_version` value should be a string representing the current LLM Report JSON format.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">51 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-118, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">107 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_annotations_in_report</span>
                        <div class="test-meta">
                            <span>75ms</span>
                            <span title="Covered file count">üõ°Ô∏è 13</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that LLM annotations are included in the report when a provider is enabled.</p>
                                <p><strong>Why Needed:</strong> Prevent regressions by ensuring LLM annotations are present in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'scenario' key in the report should match 'Checks the happy path'.</li>
                                        <li>The 'why_needed' key in the report should indicate that LLM annotations prevent regressions.</li>
                                        <li>The 'llm_annotation' key in the report data should contain a 'scenario' value matching 'Checks the happy path'.</li>
                                        <li>The 'choices' key in the report data should contain at least one 'message' value with a 'content' value matching 'Checks the happy path'.</li>
                                        <li>The 'key_assertions' list should not be empty.</li>
                                        <li>The test function 'test_llm_annotations_in_report' should be called before running this test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 52-53, 72, 75, 80, 107, 110-111, 128, 136, 147, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">23 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69-70, 94-95, 97)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">94 lines (ranges: 104-107, 109-111, 113, 115, 161-165, 167, 169, 171, 173, 176, 178-180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407-419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">186 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-340, 343, 345, 348-352, 355, 357-362, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_error_is_reported</span>
                        <div class="test-meta">
                            <span>6.08s</span>
                            <span title="Covered file count">üõ°Ô∏è 12</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that LLM errors are surfaced in HTML output.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where LLM errors are not reported to the user.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test verifies that an error message 'LLM error' is present in the report.</li>
                                        <li>The test verifies that the error message contains the string 'boom'.</li>
                                        <li>The test checks for the presence of the LLM error message in the HTML output.</li>
                                        <li>The test ensures that the error message is not empty or null.</li>
                                        <li>The test verifies that the error message does not contain any other relevant information.</li>
                                        <li>The test checks if the error message is correctly formatted and readable.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 39-41, 53, 55-56, 86, 88, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">73 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137-139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198-201, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 52-53, 72, 75, 80, 107, 110-111, 128, 136, 147, 165, 167, 175, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 78-79, 81-82, 84-85, 94-95, 97)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">186 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-340, 343, 345, 348-353, 357-362, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">101 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestMarkers::test_llm_opt_out_marker</span>
                        <div class="test-meta">
                            <span>53ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the LLM opt-out marker is correctly recorded in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the LLM opt-out marker is not recorded in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm_opt_out' marker should be present in the report.</li>
                                        <li>The 'llm_opt_out' marker should have a value of True for this test.</li>
                                        <li>There should only be one test with the 'llm_opt_out' marker in the report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181-182, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180-182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestMarkers::test_requirement_marker</span>
                        <div class="test-meta">
                            <span>53ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the requirement marker to verify it records a specific requirement.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where multiple requirements are not properly recorded by the requirement marker.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `pytester.makepyfile` is called with a string containing the required requirements.</li>
                                        <li>The `test_with_req` function is defined and passed to `pytester.makepyfile`.</li>
                                        <li>The test data is loaded from a JSON file using `json.loads`.</li>
                                        <li>The tests in the report are checked for exactly one requirement.</li>
                                        <li>The 'requirements' key is present in each test's dictionary.</li>
                                        <li>The value of 'requirements' is either an empty list or a string containing the required requirements.</li>
                                        <li>The required requirements are not empty strings.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-200, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188-190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_multiple_xfail_outcomes</span>
                        <div class="test-meta">
                            <span>58ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that multiple xfailed tests are recorded in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that all xfailed tests are properly reported and counted.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>2</li>
                                        <li>['xfailed', 'xfailed']</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">108 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-324, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_skip_outcome</span>
                        <div class="test-meta">
                            <span>54ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that skipped tests are correctly recorded and reported.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the number of skipped tests is not accurately reported in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total number of skipped tests should be equal to 1 according to the report.</li>
                                        <li>The 'skipped' key in the report data should contain a single integer value, which represents the total number of skipped tests.</li>
                                        <li>The 'summary' section of the report data should include a key called 'skipped' with a value of 1.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">43 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 106-107, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">107 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321-322, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_xfail_outcome</span>
                        <div class="test-meta">
                            <span>55ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that xfailed tests are recorded and reported correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where xfailed tests are not properly recorded or reported.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'summary' key in the report.json file should contain a count of xfailed tests.</li>
                                        <li>The 'xfailed' value in the 'summary' key should be equal to 1 (indicating one xfailed test).</li>
                                        <li>The 'pytester.path' attribute should have been updated with the correct path to the report.json file after running pytest.</li>
                                        <li>The 'report.json' file should contain a valid JSON object with the expected keys and values.</li>
                                        <li>The 'summary' key in the 'report.json' file should be present and contain the correct data.</li>
                                        <li>The 'xfailed' value in the 'summary' key should be an integer (not a string).</li>
                                        <li>The 'pytester.runpytest' command should have been executed successfully with the '--llm-report-json' option set.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">108 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-324, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestParametrization::test_parametrized_tests</span>
                        <div class="test-meta">
                            <span>56ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the parameterized tests feature to verify that it records and reports the correct number of tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the number of passed tests is not reported correctly in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total number of tests should be 3 (1 passed, 2 failed).</li>
                                        <li>Each test should have been run at least once. The 'passed' count should match the actual number of runs for each test.</li>
                                        <li>No duplicate or missing test counts should be reported in the report.</li>
                                        <li>All test results should be included in the report even if they are not passed (i.e., all tests ran).</li>
                                        <li>The 'total' and 'passed' counts should add up to 3 (the total number of tests).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167, 169-171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_help_contains_examples</span>
                        <div class="test-meta">
                            <span>46ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the CLI help text contains usage examples.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the help text is missing or misleading.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of running `pytester.runpytest('--help')` should contain lines matching `*Example:*--llm-report*` in the output.</li>
                                        <li>The line containing `*Example:*--llm-report*` should be present in the output.</li>
                                        <li>The output should not contain any other usage examples besides those mentioned above.</li>
                                        <li>The output should not contain any examples that are not related to LLM reporting.</li>
                                        <li>The output should include a clear and concise description of how to use the CLI help text.</li>
                                        <li>The output should not contain any typos or grammatical errors in the help text.</li>
                                        <li>The output should be readable and easy to understand, even for users who are not familiar with the command line.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_markers_registered</span>
                        <div class="test-meta">
                            <span>41ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that LLM markers are registered and their context is properly set.</p>
                                <p><strong>Why Needed:</strong> To prevent a regression where the LLM marker is not correctly registered or its context is not set.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm_opt_out' marker should be found in the output.</li>
                                        <li>The 'llm_context' marker should be found in the output.</li>
                                        <li>The 'requirement' marker should be found in the output.</li>
                                        <li>The 'llm_opt_out' marker should not be present in the output if it is properly registered.</li>
                                        <li>The 'llm_context' marker should only appear once per test case, and its presence should be verified across all tests.</li>
                                        <li>The 'requirement' marker should only appear once per test case, and its presence should be verified across all tests.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_plugin_registered</span>
                        <div class="test-meta">
                            <span>47ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The plugin should be successfully registered with pytest.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the plugin is not registered correctly with pytest.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>pytester.runpytest('--help')</li>
                                        <li>result.stdout.fnmatch_lines(['*--llm-report*'])</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestSpecialCharacters::test_special_chars_in_nodeid</span>
                        <div class="test-meta">
                            <span>79ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that special characters in nodeid are handled correctly during Pytest execution.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential crash and ensures the HTML generated by Pytest is valid.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The '<' character should be present in the report.html file.</li>
                                        <li>The '>' character should not be present in the report.html file.</li>
                                        <li>The '&' character should not be present in the report.html file.</li>
                                        <li>The '<' and '>' characters are present in the report.html file, but they are not escaped.</li>
                                        <li>The HTML content of the report.html file does not contain any special characters.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">101 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_boundary_one_minute</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the 'format_duration' function with a duration of exactly one minute.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not correctly format durations that are close to but less than one minute.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result should be in the format '1m 0.0s'.</li>
                                        <li>The time unit 's' should appear after '1' in the result.</li>
                                        <li>The decimal part of the number should be zero.</li>
                                        <li>The function should return an error or raise a ValueError for durations less than one minute.</li>
                                        <li>The function should correctly handle cases where the duration is exactly one minute (e.g., 60.0).</li>
                                        <li>The function should not incorrectly format other types of durations (e.g., two minutes, three seconds).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_microseconds_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function with a duration of 500 microseconds.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not correctly format durations as microseconds when they are very close to millisecond values.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result should contain 'Œºs' (microseconds).</li>
                                        <li>The result should be equal to '500Œºs'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 39-40)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_milliseconds_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `format_duration` function correctly formats sub-second durations as milliseconds.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the function incorrectly returns 'ms' when the input is an integer.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of `format_duration(0.5)` should be '500.0ms'.</li>
                                        <li>The value of `result` should match '500.0ms'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 39, 41-42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_minutes_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the function correctly formats durations over a minute.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression if the format_duration function is modified to incorrectly handle minutes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result contains the string 'm' which indicates the unit of measurement (minutes).</li>
                                        <li>The result equals '1m 30.5s' which correctly formats the duration as one minute and thirty-five seconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_multiple_minutes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function with a duration of multiple minutes.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in formatting durations that include more than one minute.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result should be '3m 5.0s' when given a duration of 185.0 minutes.</li>
                                        <li>The result should not have any seconds for durations less than 1 minute.</li>
                                        <li>The function should correctly handle durations in the format 'mm:ss'.</li>
                                        <li>The function should preserve the original unit ('minutes') for durations with more than one unit.</li>
                                        <li>The function should handle negative durations without raising an error.</li>
                                        <li>The function should support durations of any length (e.g., 2 minutes, 3.5 hours).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_one_second</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `format_duration` function returns a string representation of '1.00s' for a duration of exactly 1 second.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an incorrect or missing expected result for durations less than 1 second.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `format_duration` function should return a string representation of '1.00s' when given a duration of exactly 1 second.</li>
                                        <li>The `format_duration` function should not raise an exception when given a non-numeric input (e.g., a float or integer).</li>
                                        <li>The `format_duration` function should handle durations less than 1 second correctly and return the expected string representation ('1.00s').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 39, 41, 43-44)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_seconds_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the 'seconds' unit of time.</p>
                                <p><strong>Why Needed:</strong> Prevents incorrect formatting of seconds under a minute.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return 's' as part of the result.</li>
                                        <li>The formatted string should be in the format 'X.Xs'.</li>
                                        <li>The value of X.X should be exactly 5.50.</li>
                                        <li>The unit 's' should always be included in the output.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 39, 41, 43-44)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_small_milliseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function with a small millisecond duration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the function returns incorrect results for very short durations, potentially leading to inaccuracies in time calculations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of calling `format_duration(0.001)` should be '1.0ms'.</li>
                                        <li>The duration is represented as millisecond (ms) rather than seconds.</li>
                                        <li>The function handles durations less than one second correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 39, 41-42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_very_small_microseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `format_duration` function correctly formats very small durations as microseconds when the input is 1 microsecond.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `format_duration` function would incorrectly format very small durations as milliseconds instead of microseconds.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of calling `format_duration(0.000001)` should be '1Œºs'.</li>
                                        <li>The unit of the output value is microsecond (Œºs).</li>
                                        <li>The input duration is correctly converted to microseconds.</li>
                                        <li>No other units are applied when converting 1 microsecond to microseconds.</li>
                                        <li>The function does not silently truncate or round the result.</li>
                                        <li>The function handles very small inputs without error.</li>
                                        <li>The function preserves the original precision of the input value.</li>
                                        <li>The output value is a string representation of the duration in microseconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 39-40)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_datetime_with_utc</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test formats datetime with UTC and ensures it is correctly converted to UTC timezone.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `iso_format` function incorrectly converts datetime objects from other timezones to UTC.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of `iso_format(dt)` should be in the format `YYYY-MM-DDTHH:MM:SS+HH:MM:SSZ`.</li>
                                        <li>The timezone offset of the input datetime object should be correctly converted to UTC.</li>
                                        <li>The resulting string should not have a timezone offset.</li>
                                        <li>The resulting string should match the expected ISO 8601 format.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_naive_datetime</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the naive datetime format without timezone to ensure it matches the expected output.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the naive datetime format is not correctly converted to UTC time zone.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `iso_format(dt)` returns the correct ISO formatted string for the given datetime object.</li>
                                        <li>The datetime object `dt` is in the naive timezone (i.e., no timezone offset).</li>
                                        <li>The resulting ISO formatted string matches the expected output '2024-06-20T14:00:00'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_with_microseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `iso_format` function with a datetime object that includes microseconds.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the `iso_format` function does not correctly handle datetime objects with microseconds.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of calling `iso_format(dt)` should contain the string '123456'.</li>
                                        <li>The value in the result should be exactly '123456'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestUtcNow::test_has_utc_timezone</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `utc_now()` function returns a datetime object with an associated UTC timezone.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in time-related functionality when working with dates and times.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The returned datetime object has a valid timezone.</li>
                                        <li>The returned datetime object is not None (i.e., it's a valid result).</li>
                                        <li>The returned datetime object's timezone is equal to UTC.</li>
                                        <li>The returned datetime object does not have an invalid or missing timezone.</li>
                                        <li>The returned datetime object's timezone is correctly set to UTC.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestUtcNow::test_is_current_time</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `utc_now()` function returns a current time within UTC.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the `utc_now()` function returns an incorrect or outdated time due to clock skew.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of `utc_now()` should be within one second of the current UTC time.</li>
                                        <li>The result of `utc_now()` should not exceed the current UTC time by more than one second.</li>
                                        <li>The result of `utc_now()` should not be less than the current UTC time by more than one second.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestUtcNow::test_returns_datetime</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `utc_now()` function returns a datetime object.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the returned datetime object might not be in UTC.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>result is an instance of `datetime`</li>
                                        <li>result is not None</li>
                                        <li>result is of type `datetime` (not `timedelta`)</li>
                                        <li>result is not a timezone-aware datetime object (it's naive)</li>
                                        <li>result has a valid year, month and day</li>
                                        <li>result has a valid hour, minute and second</li>
                                        <li>result has the correct timezone</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
        </div>

        <section class="source-coverage">
            <h2>Source Coverage</h2>
            <div class="source-coverage-table">
                <div class="source-coverage-header">
                    <span>File</span>
                    <span>Stmts</span>
                    <span>Miss</span>
                    <span>Cover</span>
                    <span>%</span>
                    <span>Covered Lines</span>
                    <span>Missed Lines</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/_git_info.py</span>
                    <span>2</span>
                    <span>0</span>
                    <span>2</span>
                    <span>100.0%</span>
                    <span class="source-lines">2-3</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/aggregation.py</span>
                    <span>116</span>
                    <span>5</span>
                    <span>111</span>
                    <span>95.69%</span>
                    <span class="source-lines">13, 15-19, 21, 35, 38, 44, 46, 52-53, 55-57, 59, 61-64, 69, 73-74, 77-80, 84, 87-89, 93, 103, 109-111, 113-117, 119-120, 125, 127-128, 130-131, 134-135, 141-144, 146, 148, 162, 164, 168, 170, 172, 182, 184-188, 190-191, 194, 196, 205, 217, 219-233, 235, 237, 245-246, 248-249, 251, 253-255, 259, 262-263, 265-266, 269-271, 273, 275-276, 280</span>
                    <span class="source-lines">66, 90-91, 192, 203</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/cache.py</span>
                    <span>47</span>
                    <span>3</span>
                    <span>44</span>
                    <span>93.62%</span>
                    <span class="source-lines">13, 15-19, 21, 27, 33, 39-41, 43, 53, 55-56, 58, 60-62, 68-69, 78, 86, 88, 90, 92, 94, 97, 103, 107, 118-119, 121, 123, 129, 132-136, 141, 144, 153</span>
                    <span class="source-lines">64-65, 130</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/collector.py</span>
                    <span>111</span>
                    <span>2</span>
                    <span>109</span>
                    <span>98.2%</span>
                    <span class="source-lines">19, 21-22, 24, 26-27, 33-34, 45-50, 52, 58, 60-62, 69, 78-79, 81, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127-128, 130, 132-133, 135-137, 140, 143, 155, 163-164, 167-169, 171, 173, 181-182, 185-189, 191, 198-200, 202, 209-210, 212-214, 216, 218, 227-228, 230-236, 238, 241, 250-252, 254, 261, 264-265, 268-269, 271, 277, 279, 285</span>
                    <span class="source-lines">141, 239</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/coverage_map.py</span>
                    <span>135</span>
                    <span>10</span>
                    <span>125</span>
                    <span>92.59%</span>
                    <span class="source-lines">13, 15-17, 19-22, 30, 38, 44-45, 47, 58-60, 64, 72-73, 83, 86, 88-90, 92, 94-96, 98, 101-104, 106-108, 114, 116, 118, 121-122, 127, 131-135, 137-140, 144-146, 148, 150, 152-153, 156, 160-162, 165, 167-168, 173, 176, 178-184, 187-189, 191, 196, 199-200, 202, 204, 216-217, 220, 224-225, 228-234, 236, 239, 241, 243-244, 246-248, 250, 252-254, 259-260, 263-264, 271, 273, 276-279, 281-283, 285, 299-300, 302, 308</span>
                    <span class="source-lines">62, 123, 125, 128, 157, 221, 249, 251, 257, 274</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/errors.py</span>
                    <span>35</span>
                    <span>0</span>
                    <span>35</span>
                    <span>100.0%</span>
                    <span class="source-lines">8-9, 12, 25-28, 31-36, 39-42, 45-46, 49-51, 54-55, 64-66, 68, 70, 74-76, 80, 129, 139</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/__init__.py</span>
                    <span>3</span>
                    <span>0</span>
                    <span>3</span>
                    <span>100.0%</span>
                    <span class="source-lines">4-5, 7</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/annotator.py</span>
                    <span>110</span>
                    <span>0</span>
                    <span>110</span>
                    <span>100.0%</span>
                    <span class="source-lines">4, 6-10, 12-15, 21-22, 25-28, 31, 45-46, 48-50, 54, 56-57, 59, 61-62, 64, 66-68, 71-72, 74-82, 87, 97-98, 100, 102, 104-105, 115, 127, 129-132, 137-139, 142, 165-168, 170-171, 176, 178, 180-183, 185-190, 192-193, 198-201, 203, 206, 229-232, 234, 236-237, 239-240, 245-246, 248-253, 255-256, 261-264, 266</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/base.py</span>
                    <span>78</span>
                    <span>0</span>
                    <span>78</span>
                    <span>100.0%</span>
                    <span class="source-lines">13, 15-18, 26, 40, 46, 52-53, 55, 72, 75-76, 78, 80, 101, 107-108, 110-111, 122, 128, 130, 136, 138, 147, 149, 165, 167-173, 175, 177, 186-187, 190-192, 194-195, 198-200, 203-208, 212, 214, 220-221, 224-225, 228-230, 233, 245, 247, 249-250, 252-253, 255, 257-258, 260, 262-263, 265, 267</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/gemini.py</span>
                    <span>275</span>
                    <span>18</span>
                    <span>257</span>
                    <span>93.45%</span>
                    <span class="source-lines">7, 9-13, 15-16, 23-27, 30-34, 37-42, 44-46, 48-50, 52, 57-63, 65-70, 72-73, 75-78, 80-85, 87-88, 91-97, 99-103, 105, 107-114, 121-122, 125, 128, 134, 136-139, 141-142, 144, 160-161, 167-169, 171-172, 174, 176-184, 186-188, 190-191, 193, 196, 200-208, 210-211, 213-215, 217-223, 225-227, 233-234, 238-239, 242-243, 245-248, 252-253, 260, 266-267, 269, 273-277, 279-283, 286-287, 292-293, 300-301, 303, 315, 317-318, 322, 327, 330-332, 335-343, 345-346, 348, 352-355, 357, 360-366, 368-374, 380-382, 384-387, 389, 391-392, 396-402, 405, 408-410, 412-414, 416-421, 427-428, 430-434, 437-440, 442-443, 445-447</span>
                    <span class="source-lines">89, 104, 106, 115-117, 199, 230-231, 235-237, 244, 250, 256, 367, 441, 444</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/litellm_provider.py</span>
                    <span>32</span>
                    <span>1</span>
                    <span>31</span>
                    <span>96.88%</span>
                    <span class="source-lines">7, 9, 11-12, 18, 21, 37-38, 44, 46, 49, 51-52, 54-56, 66-67, 69-70, 73, 76, 78-79, 81-82, 84, 88, 94-95, 97</span>
                    <span class="source-lines">74</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/noop.py</span>
                    <span>13</span>
                    <span>0</span>
                    <span>13</span>
                    <span>100.0%</span>
                    <span class="source-lines">8, 10, 12-13, 20, 26, 32, 34, 50, 52, 58, 60, 66</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/ollama.py</span>
                    <span>43</span>
                    <span>1</span>
                    <span>42</span>
                    <span>97.67%</span>
                    <span class="source-lines">7, 9, 11-12, 18, 24, 40-41, 47, 50, 52, 54-55, 57-60, 62-63, 66-67, 71-72, 74-75, 77, 81, 87-88, 90-92, 96, 102, 104, 114, 116-117, 127, 132, 134-135</span>
                    <span class="source-lines">69</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/schemas.py</span>
                    <span>36</span>
                    <span>1</span>
                    <span>35</span>
                    <span>97.22%</span>
                    <span class="source-lines">8, 10-12, 16, 22, 38, 42-44, 46-47, 50-53, 55, 58-59, 62-65, 67-68, 77, 84, 90, 94-98, 102, 130</span>
                    <span class="source-lines">39</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/models.py</span>
                    <span>240</span>
                    <span>10</span>
                    <span>230</span>
                    <span>95.83%</span>
                    <span class="source-lines">17-18, 21, 24-25, 34-36, 38, 40, 47-48, 61-67, 69, 71, 82-83, 95-100, 102, 104, 109-115, 118-119, 141-157, 159, 161, 167-171, 173-182, 184, 186, 188-190, 193-194, 202-203, 205, 207, 213-214, 223-225, 227, 229, 233-235, 238-239, 248-250, 252, 254, 261-262, 271-273, 275, 277, 281-283, 286-287, 324-353, 355-360, 362, 364, 382-405, 407-419, 422-423, 437-445, 447, 449, 459, 461, 464-465, 482-492, 494, 500, 502, 508-512, 514, 516, 518, 520, 522</span>
                    <span class="source-lines">172, 183, 185, 187, 460, 513, 515, 517, 519, 521</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/options.py</span>
                    <span>117</span>
                    <span>45</span>
                    <span>72</span>
                    <span>61.54%</span>
                    <span class="source-lines">106, 146, 175, 178-180, 185-187, 193-195, 201-203, 209-218, 220, 224, 233, 248, 251-267, 270-283, 286-295, 298, 300</span>
                    <span class="source-lines">13-15, 21-22, 90-94, 97-99, 102-105, 122-123, 126-132, 135-137, 140-142, 145, 156-160, 163-164, 167, 169, 222, 227, 236</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/plugin.py</span>
                    <span>156</span>
                    <span>25</span>
                    <span>131</span>
                    <span>83.97%</span>
                    <span class="source-lines">40, 43, 49, 55, 61, 67, 73, 80, 89, 95, 101, 107, 113, 121, 126, 131, 136, 142, 147, 153, 169, 173, 177, 183-184, 187-188, 190, 192, 195-197, 203-204, 212-213, 238-239, 242-243, 246, 249-250, 252-253, 256-257, 259, 261-265, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 307, 309-314, 317-318, 322-323, 331-332, 337-340, 343, 345, 348-353, 355, 357, 365-366, 387-388, 391-392, 395-397, 408-409, 412, 415-416, 419-421, 431-432, 435-437, 448-449, 452, 455, 457-458</span>
                    <span class="source-lines">13, 15-17, 19-20, 22, 28-31, 34, 160, 216, 319, 327-328, 333-334, 379-380, 400, 424, 440-441</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/prompts.py</span>
                    <span>75</span>
                    <span>5</span>
                    <span>70</span>
                    <span>93.33%</span>
                    <span class="source-lines">13, 15-17, 24, 27, 33, 35, 49, 52, 55, 58-61, 63, 65, 67, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116, 118, 132-133, 135-138, 140-141, 144-145, 148, 151-152, 154-156, 158-159, 163, 165, 180, 182, 191-194</span>
                    <span class="source-lines">80, 114, 142, 146, 149</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/render.py</span>
                    <span>50</span>
                    <span>0</span>
                    <span>50</span>
                    <span>100.0%</span>
                    <span class="source-lines">13, 15-16, 18, 24, 30-31, 34, 40, 42, 50-51, 53, 56, 65-67, 70, 79, 87, 90, 99, 101-102, 107, 110, 121-124, 126-129, 131-134, 141-143, 145, 158-163, 177, 196</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/report_writer.py</span>
                    <span>167</span>
                    <span>10</span>
                    <span>157</span>
                    <span>94.01%</span>
                    <span class="source-lines">13, 15-25, 27-29, 46, 55, 58, 67-68, 76, 83-84, 89, 98-100, 102, 105-108, 110, 116, 127-128, 130, 142, 150, 156-158, 160, 186-189, 192, 197-199, 202-203, 211, 222-223, 226-227, 230-231, 233, 235, 254, 256-259, 262-264, 266, 268, 303, 312, 314-315, 317-328, 330, 332, 340, 343-345, 348-349, 352-354, 357, 360, 368, 376, 378-379, 382, 385, 388, 391, 399, 401-402, 408, 410, 412, 414-423, 434-435, 437-439, 447-448, 453, 455, 458, 461-462, 464, 470-474, 480-481, 488, 495, 497, 499-501, 503, 506-507, 509, 515-516</span>
                    <span class="source-lines">113, 135-137, 424-425, 432, 449-451</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/fs.py</span>
                    <span>34</span>
                    <span>3</span>
                    <span>31</span>
                    <span>91.18%</span>
                    <span class="source-lines">11, 13-14, 17, 30, 33, 36, 39, 42, 45, 55-56, 58-60, 63-64, 70, 79, 82, 100, 103, 111-113, 116-117, 119-121, 123</span>
                    <span class="source-lines">40, 65, 67</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/hashing.py</span>
                    <span>36</span>
                    <span>0</span>
                    <span>36</span>
                    <span>100.0%</span>
                    <span class="source-lines">12, 14-17, 23, 32, 35, 44-48, 51, 61, 64, 73-74, 76-78, 80-81, 86, 96, 103-104, 107, 113-114, 116-121</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/ranges.py</span>
                    <span>33</span>
                    <span>0</span>
                    <span>33</span>
                    <span>100.0%</span>
                    <span class="source-lines">12, 15, 29-30, 33, 35-37, 39-40, 42, 45-47, 50, 52, 55, 65-67, 70, 81-82, 84-91, 93, 95</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/time.py</span>
                    <span>16</span>
                    <span>0</span>
                    <span>16</span>
                    <span>100.0%</span>
                    <span class="source-lines">4, 6, 9, 15, 18, 27, 30, 39-44, 46-48</span>
                    <span class="source-lines">-</span>
                </div>
            </div>
        </section>
    </div>
</body>
</html>