<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Test Report &bull; 387 tests</title>
    <!-- Optional: Inter font from rsms.me CDN. Falls back to system fonts if unavailable. -->
    <link rel="stylesheet" href="https://rsms.me/inter/inter.css">
    <style>
/* Modern Color Palette */
:root {
    --bg-color: #f8fafc;
    --text-primary: #1e293b;
    --text-secondary: #64748b;
    --border-color: #e2e8f0;
    --card-bg: #ffffff;
    --surface-muted: #f1f5f9;
    --primary-color: #3b82f6;
    color-scheme: light dark;

    /* Status Colors */
    --passed-bg: #dcfce7;
    --passed-text: #166534;
    --failed-bg: #fee2e2;
    --failed-text: #991b1b;
    --skipped-bg: #fef9c3;
    --skipped-text: #854d0e;
    --xfailed-bg: #ffedd5;
    --xfailed-text: #9a3412;
    --xpassed-bg: #f3e8ff;
    --xpassed-text: #6b21a8;
    --error-bg: #fee2e2;
    --error-text: #991b1b;
}

body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    background-color: var(--bg-color);
    color: var(--text-primary);
    line-height: 1.5;
    margin: 0;
    padding: 0;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 2rem;
}

/* Header */
header {
    margin-bottom: 2rem;
    border-bottom: 1px solid var(--border-color);
    padding-bottom: 1rem;
    display: flex;
    justify-content: space-between;
    align-items: center;
}

h1 {
    font-size: 1.875rem;
    font-weight: 700;
    color: var(--text-primary);
    margin: 0;
}

.meta {
    font-size: 0.875rem;
    color: var(--text-secondary);
}

/* Summary Grid */
.summary {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
    gap: 1rem;
    margin-bottom: 2rem;
}

.summary-card {
    background: var(--card-bg);
    border-radius: 0.5rem;
    padding: 1.5rem;
    box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.1), 0 1px 2px -1px rgb(0 0 0 / 0.1);
    text-align: center;
    border: 1px solid var(--border-color);
    transition: transform 0.2s;
}

.summary-card:hover {
    transform: translateY(-2px);
}

.summary-card .count {
    font-size: 2.25rem;
    font-weight: 700;
    line-height: 1;
    margin-bottom: 0.5rem;
}

.summary-card .label {
    text-transform: uppercase;
    font-size: 0.75rem;
    font-weight: 600;
    letter-spacing: 0.05em;
    color: var(--text-secondary);
}

/* Status Colors for Summary */
.summary-card.passed .count {
    color: var(--passed-text);
}

.summary-card.failed .count {
    color: var(--failed-text);
}

.summary-card.skipped .count {
    color: var(--skipped-text);
}

.summary-card.xfailed .count {
    color: var(--xfailed-text);
}

.summary-card.xpassed .count {
    color: var(--xpassed-text);
}

.summary-card.coverage .count {
    color: var(--primary-color);
}

/* Filters */
.filters {
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.5rem;
    border: 1px solid var(--border-color);
    margin-bottom: 1.5rem;
    display: flex;
    flex-direction: column;
    gap: 0.75rem;
}

.filter-input {
    flex: 1;
    padding: 0.5rem 1rem;
    border: 1px solid var(--border-color);
    border-radius: 0.375rem;
    font-size: 0.875rem;
    background: var(--card-bg);
    color: var(--text-primary);
}

.filter-input::placeholder {
    color: var(--text-secondary);
}

.filter-statuses {
    display: flex;
    flex-wrap: wrap;
    gap: 0.5rem;
}

.filter-chip {
    display: inline-flex;
    align-items: center;
    gap: 0.35rem;
    padding: 0.25rem 0.75rem;
    border-radius: 9999px;
    border: 1px solid var(--border-color);
    background: var(--surface-muted);
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.04em;
}

.filter-chip input {
    margin: 0;
}

.filter-chip.passed {
    background: var(--passed-bg);
    color: var(--passed-text);
}

.filter-chip.failed {
    background: var(--failed-bg);
    color: var(--failed-text);
}

.filter-chip.skipped {
    background: var(--skipped-bg);
    color: var(--skipped-text);
}

.filter-chip.xfailed {
    background: var(--xfailed-bg);
    color: var(--xfailed-text);
}

.filter-chip.xpassed {
    background: var(--xpassed-bg);
    color: var(--xpassed-text);
}

.filter-chip.error {
    background: var(--error-bg);
    color: var(--error-text);
}

/* Test List */
.test-list {
    display: flex;
    flex-direction: column;
    gap: 0.75rem;
}

.test-row {
    background: var(--card-bg);
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
    overflow: hidden;
}

.test-header {
    padding: 1rem;
    display: flex;
    align-items: center;
    gap: 1rem;
    cursor: pointer;
    background: var(--card-bg);
}

.test-header:hover {
    background: var(--surface-muted);
}

.status-badge {
    padding: 0.25rem 0.75rem;
    border-radius: 9999px;
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
}

.status-passed {
    background: var(--passed-bg);
    color: var(--passed-text);
}

.status-failed {
    background: var(--failed-bg);
    color: var(--failed-text);
}

.status-skipped {
    background: var(--skipped-bg);
    color: var(--skipped-text);
}

.status-xfailed {
    background: var(--xfailed-bg);
    color: var(--xfailed-text);
}

.status-xpassed {
    background: var(--xpassed-bg);
    color: var(--xpassed-text);
}

.status-error {
    background: var(--error-bg);
    color: var(--error-text);
}

.test-name {
    flex: 1;
    font-family: monospace;
    font-size: 0.9rem;
    color: var(--text-primary);
    word-break: break-all;
}

.test-meta {
    display: flex;
    gap: 1rem;
    align-items: center;
    color: var(--text-secondary);
    font-size: 0.875rem;
}

/* Details Section */
.test-details {
    padding: 0 1rem 1rem 1rem;
    border-top: 1px solid var(--border-color);
    background: var(--surface-muted);
}

.detail-section {
    margin-top: 1rem;
}

.detail-title {
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
    color: var(--text-secondary);
    margin-bottom: 0.5rem;
}

.coverage-item {
    font-family: monospace;
    font-size: 0.85rem;
    padding: 0.25rem 0;
    border-bottom: 1px solid var(--border-color);
    display: grid;
    grid-template-columns: minmax(200px, 2fr) minmax(120px, 1fr);
    gap: 1rem;
}

.coverage-list {
    background: var(--card-bg);
    border-radius: 0.375rem;
    border: 1px solid var(--border-color);
    overflow: hidden;
}

.source-coverage {
    margin-top: 2rem;
}

.source-coverage h2 {
    margin: 0 0 1rem;
    font-size: 1.5rem;
}

.source-coverage-table {
    display: grid;
    gap: 0.35rem;
}

.source-coverage-header,
.source-coverage-row {
    display: grid;
    grid-template-columns: minmax(200px, 2fr) repeat(4, minmax(60px, 0.5fr)) minmax(
            140px,
            1fr
        ) minmax(140px, 1fr);
    align-items: center;
    gap: 0.75rem;
    padding: 0.75rem 1rem;
    border-radius: 0.5rem;
}

.source-coverage-header {
    background: var(--surface-muted);
    font-size: 0.75rem;
    font-weight: 700;
    text-transform: uppercase;
    letter-spacing: 0.04em;
    color: var(--text-secondary);
}

.source-coverage-row {
    background: var(--card-bg);
    border: 1px solid var(--border-color);
    font-size: 0.85rem;
}

.source-path {
    font-family: monospace;
    word-break: break-word;
}

.source-lines {
    font-family: monospace;
    color: var(--text-secondary);
    word-break: break-word;
}

.llm-annotation {
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.375rem;
    border: 1px solid var(--border-color);
}

.llm-annotation p {
    margin: 0 0 0.5rem 0;
}

.llm-annotation p:last-child {
    margin-bottom: 0;
}

.llm-annotation ul {
    margin: 0.5rem 0 0;
    padding-left: 1.25rem;
}

.llm-annotation li {
    margin-bottom: 0.25rem;
}

.error-message {
    font-family: monospace;
    color: var(--failed-text);
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.375rem;
    border: 1px solid var(--failed-bg);
    white-space: pre-wrap;
    overflow-x: auto;
}

/* HTML5 Progress Bar for Coverage */
progress {
    width: 60px;
}

/* Utility: Hidden state for filtering */
.hidden {
    display: none !important;
}

/* Dark Mode Support */
@media (prefers-color-scheme: dark) {
    :root {
        --bg-color: #0f172a;
        --text-primary: #f1f5f9;
        --text-secondary: #94a3b8;
        --border-color: #334155;
        --card-bg: #1e293b;
        --surface-muted: #0b1220;
        --primary-color: #60a5fa;

        /* Status Colors - Adjusted for dark mode */
        --passed-bg: #14532d;
        --passed-text: #86efac;
        --failed-bg: #7f1d1d;
        --failed-text: #fca5a5;
        --skipped-bg: #713f12;
        --skipped-text: #fde047;
        --xfailed-bg: #7c2d12;
        --xfailed-text: #fdba74;
        --error-bg: #7f1d1d;
        --error-text: #fca5a5;
    }

    /* Adjust box shadows for dark mode */
    .summary-card {
        box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.3), 0 1px 2px -1px rgb(0 0 0 / 0.3);
    }
}

@media print {
    body {
        background: #ffffff;
        color: #0f172a;
    }

    .container {
        max-width: none;
        padding: 1rem 1.5rem;
    }

    header {
        border-bottom: 2px solid var(--border-color);
    }

    .filters {
        display: none;
    }

    .summary-card,
    .test-row {
        box-shadow: none;
    }

    .test-header {
        background: #ffffff;
    }

    .test-row {
        page-break-inside: avoid;
        break-inside: avoid;
    }

    .test-details {
        background: #ffffff;
    }

    .llm-annotation {
        background: var(--surface-muted);
    }

    progress {
        width: 80px;
    }
}

body.pdf-mode .filters {
    display: none;
}

body.pdf-mode .test-row {
    page-break-inside: avoid;
    break-inside: avoid;
}    </style>
    <script>
// pytest-llm-report interactive features

// Global state for filters
const activeStatuses = new Set(['passed', 'failed', 'skipped', 'xfailed', 'xpassed', 'error']);

// Filter tests based on search input and outcome filters
function filterTests() {
    const query = document.getElementById('searchInput').value.toLowerCase();
    document.querySelectorAll('.test-row').forEach(row => {
        const nodeid = row.querySelector('.test-name').textContent.toLowerCase();
        const statusMatch = row.dataset.status ? activeStatuses.has(row.dataset.status) : false;
        const matchesSearch = nodeid.includes(query);
        row.classList.toggle('hidden', !matchesSearch || !statusMatch);
    });
}

// Toggle visibility of status filters
function toggleStatus(checkbox) {
    const status = checkbox.dataset.status;
    if (checkbox.checked) {
        activeStatuses.add(status);
    } else {
        activeStatuses.delete(status);
    }
    filterTests();
}

// Initialize interactive features after DOM is ready
document.addEventListener('DOMContentLoaded', function () {
    'use strict';

    // Toggle dark mode on preference
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.documentElement.dataset.theme = 'dark';
    }

    // Default: expand all details
    document.querySelectorAll('details').forEach(details => {
        details.setAttribute('open', '');
    });

    const params = new URLSearchParams(window.location.search);
    if (params.get('pdf') === '1') {
        document.body.classList.add('pdf-mode');
    }
});    </script>
</head>
<body>
    <div class="container">
        <header>
            <div>
                <h1>Test Report</h1>
                <div class="meta">
                    Run ID: 21097912899-py3.12 &bull;
                    Generated: 2026-01-17 17:15:47 &bull;
                    Duration: 286.46s<br>
                    <strong>Plugin:</strong> v0.1.0
                        (2f498263985a34902252c53c11fb820445bd8f21)
[dirty]<br>
                    <strong>Repo:</strong> v0.1.1
                        (3e1297d90afc9d2cdf92f2e93b2e048a94409310)
<br>
                    <strong>LLM:</strong> ollama / llama3.2:1b
                        (minimal context,
                         386 annotated)
                </div>
            </div>
            <div style="text-align: right">
                <div style="font-size: 2rem; font-weight: 700; color: var(--primary-color)">
                    91.09%
                </div>
                <div class="meta">Total Coverage</div>
            </div>
        </header>

        <!-- Summary Cards -->
        <div class="summary">
            <div class="summary-card">
                <div class="count">387</div>
                <div class="label">Total Tests</div>
            </div>
            <div class="summary-card passed">
                <div class="count">387</div>
                <div class="label">Passed</div>
            </div>
            <div class="summary-card failed">
                <div class="count">0</div>
                <div class="label">Failed</div>
            </div>
            <div class="summary-card skipped">
                <div class="count">0</div>
                <div class="label">Skipped</div>
            </div>
            <div class="summary-card xfailed">
                <div class="count">0</div>
                <div class="label">XFailed</div>
            </div>
            <div class="summary-card xpassed">
                <div class="count">0</div>
                <div class="label">XPassed</div>
            </div>
            <div class="summary-card failed">
                <div class="count">0</div>
                <div class="label">Errors</div>
            </div>
        </div>

        <!-- Filters -->
        <div class="filters">
            <input type="text" id="searchInput" class="filter-input" placeholder="Search tests..." onkeyup="filterTests()">
            <div class="filter-statuses" aria-label="Filter by status">
                <label class="filter-chip passed">
                    <input type="checkbox" data-status="passed" checked onchange="toggleStatus(this)">
                    Passed
                </label>
                <label class="filter-chip failed">
                    <input type="checkbox" data-status="failed" checked onchange="toggleStatus(this)">
                    Failed
                </label>
                <label class="filter-chip skipped">
                    <input type="checkbox" data-status="skipped" checked onchange="toggleStatus(this)">
                    Skipped
                </label>
                <label class="filter-chip xfailed">
                    <input type="checkbox" data-status="xfailed" checked onchange="toggleStatus(this)">
                    XFailed
                </label>
                <label class="filter-chip xpassed">
                    <input type="checkbox" data-status="xpassed" checked onchange="toggleStatus(this)">
                    XPassed
                </label>
                <label class="filter-chip error">
                    <input type="checkbox" data-status="error" checked onchange="toggleStatus(this)">
                    Error
                </label>
            </div>
        </div>

        <!-- Test List -->
        <div class="test-list">
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_all_policy</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the aggregation of all policy when an aggregate directory is set.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where aggregating all policies would result in no tests being reported.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The aggregated report should contain both retained tests.</li>
                                        <li>The length of the aggregated report should be equal to the number of retained tests.</li>
                                        <li>Each retained test should have an outcome of 'passed'.</li>
                                        <li>All retained tests should be included in the aggregated report.</li>
                                        <li>The aggregate directory is set before aggregating reports.</li>
                                        <li>A temporary directory is created for each run.</li>
                                        <li>Reports are written to a file in the temporary directory with a unique filename.</li>
                                        <li>The aggregated result is not None.</li>
                                        <li>Both retained tests have an outcome of 'passed'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 52, 55-56, 59, 61-63, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134, 141, 146, 148-153, 155, 157-159, 170, 217, 219-223, 235, 245, 248-249, 251, 253, 276-279, 281)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_dir_not_exists</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the aggregate function returns None when the directory does not exist.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the aggregate function fails to handle cases where the aggregation directory does not exist.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregate` method should return `None` when the specified directory does not exist.</li>
                                        <li>The `aggregate` method should raise an exception or handle the error in some way when the directory does not exist.</li>
                                        <li>The test should verify that the aggregate function behaves correctly even if the directory is missing or inaccessible.</li>
                                        <li>The test should check for any specific error messages or behavior when the directory does not exist.</li>
                                        <li>The `aggregate` method should be able to handle cases where the directory exists but is empty or has no files.</li>
                                        <li>The test should verify that the aggregate function can correctly aggregate data from a non-existent directory.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 52, 55-57, 109-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_latest_policy</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `aggregate` function picks the latest policy for aggregation.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the `aggregate` function fails to pick the correct policy when there are multiple reports with different outcomes in a single run.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The outcome of the aggregated report is 'passed' (latest).</li>
                                        <li>There is only one test in the aggregated result.</li>
                                        <li>The aggregated run meta contains `is_aggregated=True` and `run_count=2`.</li>
                                        <li>The aggregated summary contains `passed=1` and `failed=0`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">77 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134, 141, 146, 148-153, 155, 157-159, 170, 182, 184-188, 190-191, 194, 217, 219-223, 235, 245, 248-249, 251, 253, 276-279, 281)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_no_dir_configured</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The aggregator function should not be able to aggregate without a specified directory.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the aggregator function fails to aggregate due to missing configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>agg.aggregate() is None</li>
                                        <li>mock_config.aggregate_dir is None</li>
                                        <li>agg.aggregate() does not raise an exception when called with None as directory</li>
                                        <li>mock_config.aggregate_dir is set to None before calling agg.aggregate()</li>
                                        <li>agg.aggregate() should be able to aggregate without a specified directory</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 44, 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_no_reports</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `aggregate` function should not produce any reports when no files are found.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the `aggregate` function would incorrectly report that there were no aggregations performed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>aggregator.aggregate() is None</li>
                                        <li>pathlib.Path.exists() returns True</li>
                                        <li>pathlib.Path.glob() returns an empty list</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52, 55-57, 109-110, 113-114, 170)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_with_coverage_and_llm_annotations</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that coverage and LLM annotations are properly deserialized and can be re-serialized after fix.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in core functionality by ensuring correct deserialization of LLM annotations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Coverage was correctly deserialized from the report.</li>
                                        <li>LLM annotation was correctly deserialized from the test case.</li>
                                        <li>The aggregated result can be re-serialized successfully with the updated LLM annotations.</li>
                                        <li>The coverage and LLM annotations were properly included in the serialized output.</li>
                                        <li>The confidence level of the LLM annotation was correctly set to 0.95.</li>
                                        <li>The scenario, why_needed, and key_assertions of the LLM annotation were correctly updated after deserialization.</li>
                                        <li>The test case's file path and line ranges were correctly preserved in the serialized output.</li>
                                        <li>The coverage entry for the module was correctly created with the specified file path and line ranges.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">81 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134-137, 141-144, 146, 148-153, 155, 157-159, 170, 182, 184-188, 194, 217, 219-223, 235, 245, 248-249, 251, 253, 276-279, 281)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">32 lines (ranges: 40-43, 104-107, 109-111, 113, 115, 161-165, 167, 169, 171, 173, 176-180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_with_source_coverage</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> test_aggregate_with_source_coverage verifies that the source coverage summary is deserialized correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in handling JSON reports with multiple files and source coverage data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `source_coverage` key should be present in each report.</li>
                                        <li>Each `source_coverage` value should have the required keys (file_path, statements, missed, covered, coverage_percent, covered_ranges, missed_ranges).</li>
                                        <li>All values in the `source_coverage` dictionary should be of type int or float.</li>
                                        <li>The `covered_ranges` and `missed_ranges` values should match the expected formats.</li>
                                        <li>The `coverage_percent` value should be a decimal number between 0 and 1.</li>
                                        <li>Each `file_path` value should start with 'src/'.</li>
                                        <li>All statements in the `source_coverage` dictionary should be integers.</li>
                                        <li>Missed statements should be less than or equal to missed ranges.</li>
                                        <li>The `aggreate_dir` attribute of the aggregator instance should not be None and point to a valid directory.</li>
                                        <li>The `aggregate()` method should return a non-None result.</li>
                                        <li>The `source_coverage` value in the result should be an instance of `SourceCoverageEntry`.</li>
                                        <li>The file path in the `SourceCoverageEntry` object should match the expected value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">66 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 148-155, 157-159, 170, 182, 184-186, 194, 217, 219-220, 235, 245, 248-249, 251, 253, 276-279, 281)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_load_coverage_from_source</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loading coverage from configured source file when option is not set.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the aggregator fails to load coverage data when the `llm_coverage_source` option is not specified.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that `_load_coverage_from_source()` returns `None` when `llm_coverage_source` is `None`.</li>
                                        <li>Verify that `_load_coverage_from_source()` raises a `UserWarning` when `llm_coverage_source` is `/nonexistent/coverage`.</li>
                                        <li>Verify that `_load_coverage_from_source()` successfully loads coverage data from the mock `.coverage` file.</li>
                                        <li>Verify that `mock_cov.report()` returns the correct coverage percentage (80.0) when called.</li>
                                        <li>Verify that `mock_mapper.map_source_coverage()` correctly maps source coverage to entries.</li>
                                        <li>Verify that `mock_cov.load()` is called once when `_load_coverage_from_source()` is run.</li>
                                        <li>Verify that `mock_cov.report()` was called during its execution.</li>
                                        <li>Verify that the correct entry is returned from `_load_coverage_from_source()` with a coverage percentage of 80.0.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 245-246, 248-249, 251, 253-257, 259, 262-263, 265-266, 269, 271-272, 274)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_recalculate_summary</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the aggregator recalculates the summary correctly when there are multiple test results.</p>
                                <p><strong>Why Needed:</strong> To prevent regression in case of multiple failed tests, where the total duration is calculated incorrectly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total number of tests passed should be equal to the original total.</li>
                                        <li>The number of failed tests should remain unchanged.</li>
                                        <li>The skipped tests should not affect the total count.</li>
                                        <li>The xfailed and xpassed counts should still be correct.</li>
                                        <li>The error count should also remain unchanged.</li>
                                        <li>The coverage percentage should be preserved.</li>
                                        <li>The total duration should match the original value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 217, 219-233, 235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_skips_invalid_json</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case verifies that skipping an invalid JSON file prevents a regression.</p>
                                <p><strong>Why Needed:</strong> This test ensures that the aggregation function behaves correctly when handling invalid JSON files, preventing potential regressions.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test verifies that only valid reports are counted in the aggregate result.</li>
                                        <li>The test checks if the missing_fields.json file is skipped by the aggregator.</li>
                                        <li>The test asserts that a UserWarning is raised with the message 'Skipping invalid report file' when an invalid JSON file is encountered.</li>
                                        <li>The test ensures that the count of valid reports remains unchanged after skipping an invalid JSON file.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">71 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119-120, 125, 127-128, 148-153, 155, 157-159, 162, 164-166, 168, 170, 182, 184-186, 194, 217, 219-220, 235, 245, 248-249, 251, 253, 276-279, 281)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation_maximal.py::TestAggregationMaximal::test_recalculate_summary_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the aggregator recalculates the summary correctly when given a list of tests with coverage totals and a latest summary.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the aggregator fails to recalculate the summary after receiving new data, potentially causing incorrect results or misleading reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total number of tests passed in the summary should be equal to the number of tests provided.</li>
                                        <li>The total duration of all tests passed in the summary should be less than or equal to the latest summary's total duration.</li>
                                        <li>The coverage total percent of all tests passed in the summary should be greater than or equal to the latest summary's coverage total percent.</li>
                                        <li>At least one test should have failed in the summary, as indicated by a 'failed' key.</li>
                                        <li>All tests should have been covered at least once in the summary, as indicated by a 'passed' key.</li>
                                        <li>The total duration of all tests passed in the summary should be greater than or equal to the latest summary's total duration.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 44, 217, 219-225, 235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_cached_tests_are_skipped</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `cached_tests_are_skipped` test verifies that cached tests are skipped by the annotator.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the annotator incorrectly skips cached tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocking `mock_provider`, `mock_cache`, and `mock_assembler` to isolate dependencies.</li>
                                        <li>Verifying that the annotated function returns an empty list for cached tests.</li>
                                        <li>Checking that the annotated function does not raise any exceptions when called with mocked dependencies.</li>
                                        <li>Asserting that the annotated function correctly skips cached tests by checking if it returns a non-empty list.</li>
                                        <li>Verifying that the test passes only when no cached tests are available.</li>
                                        <li>Checking that the test fails when there are cached tests available to be skipped.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-81, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies concurrent annotation functionality in the test_anotate_tests module.</p>
                                <p><strong>Why Needed:</strong> Prevents potential performance regressions or bugs that may arise from concurrent annotation requests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Ensures that multiple annotations are processed sequentially without any conflicts or delays.</li>
                                        <li>Verifies that cache invalidation is properly handled when annotations are updated concurrently.</li>
                                        <li>Checks for any synchronization issues that might occur due to concurrent access to the annotator's state.</li>
                                        <li>Demonstrates that the annotator can handle a large number of concurrent requests without significant performance degradation.</li>
                                        <li>Ensures that the annotator does not get stuck in an infinite loop when multiple annotations are processed concurrently.</li>
                                        <li>Verifies that cache invalidation is correctly propagated to subsequent annotation requests even after the initial request has completed.</li>
                                        <li>Checks for any potential issues related to thread safety or synchronization when accessing shared resources.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">64 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104-112, 129-135, 137, 139, 229-232, 234, 236-237, 239, 245-246, 248-253, 255, 261, 266)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation_handles_failures</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The annotator handles failures concurrently when multiple tests are executed simultaneously.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the annotator fails to handle concurrent failures, leading to inconsistent results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocking the `annotator` function with multiple instances of `mock_provider`, `mock_cache`, and `mock_assembler` should not cause any failures.</li>
                                        <li>The `capsys` fixture should capture and display all output from the annotator tests.</li>
                                        <li>No exceptions should be raised when executing the test suite concurrently.</li>
                                        <li>All annotations should be generated correctly even if multiple tests fail.</li>
                                        <li>The `annotator` function should handle failures by logging errors or returning an error message.</li>
                                        <li>No inconsistent results or errors should be reported by the annotator.</li>
                                        <li>The annotator's output should not be affected by concurrent test execution.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104-112, 129-135, 137-139, 229-232, 234, 236-237, 239, 245-246, 248-253, 255, 261-264, 266)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_progress_reporting</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `test_progress_reporting` function is used to verify the progress reporting mechanism in the annotator.</p>
                                <p><strong>Why Needed:</strong> This test prevents regressions that may occur when the progress reporting mechanism is not functioning correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocking `mock_provider`, `mock_cache`, and `mock_assembler` objects ensures they are properly initialized before use.</li>
                                        <li>Verifying that the progress bar updates correctly after each iteration of the test loop.</li>
                                        <li>Checking if the progress reporting messages are being displayed as expected on the console or output stream.</li>
                                        <li>Ensuring that the progress bar is not stuck at 100% for an extended period of time.</li>
                                        <li>Verifying that the progress reporting mechanism does not throw any exceptions when called with invalid arguments.</li>
                                        <li>Testing the progress reporting mechanism with different types of annotations (e.g., text, image, etc.) to ensure it works correctly for all cases.</li>
                                        <li>Checking if the progress bar is reset to 0% after each test iteration to maintain accurate tracking.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation</span>
                        <div class="test-meta">
                            <span>12.00s</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that sequential annotation is performed correctly.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the annotator does not process annotations in sequence.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_provider is called before mock_cache and before mock_assembler.</li>
                                        <li>mock_provider is called before any of its dependencies (mock_cache and mock_assembler).</li>
                                        <li>mock_cache is called after all of its dependencies (mock_provider, mock_assembler) have been called.</li>
                                        <li>mock_assembler is called only once, after all of its dependencies (mock_provider, mock_cache) have been called.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_skips_if_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `test_skips_if_disabled` test verifies that the annotator does not perform any action when the LLMS (Large Language Model Service) is disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the annotator might skip tests or annotations if the LLMS is disabled, potentially causing unintended consequences.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config` object is created with `provider='none'`, indicating that the provider is not enabled.</li>
                                        <li>No annotation is performed on any tests in the list `[]`.</li>
                                        <li>The annotator does not attempt to annotate any tests.</li>
                                        <li>There are no test annotations generated or saved.</li>
                                        <li>The `test_skips_if_disabled` function returns without performing any action.</li>
                                        <li>The `config` object remains unchanged after calling `annotate_tests()`.</li>
                                        <li>No error is raised when the LLMS is disabled, indicating that the annotator behaves as expected.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 45-46)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_skips_if_provider_unavailable</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The annotator should skip the annotation process when a provider is unavailable.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where an annotator might incorrectly annotate data due to a temporary network or provider issue.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_provider.return_value.__class__.__name__ == 'MockProvider'</li>
                                        <li>mock_provider.return_value.is_available() == False</li>
                                        <li>self.assertEqual(mock_provider.return_value.annotations, [])</li>
                                        <li>mock_provider.return_value.skip_annotation()</li>
                                        <li>self.assertEqual(mock_provider.return_value.skip_annotation(), True)</li>
                                        <li>mock_provider.return_value._provider is None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_concurrent_with_progress_and_errors</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that annotator reports progress and first error when annotated concurrently with progress and errors.</p>
                                <p><strong>Why Needed:</strong> To prevent regression in concurrent mode, where multiple annotations are performed simultaneously.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that the annotation process is reported correctly with both a success outcome and an error.</li>
                                        <li>Verify that the first error encountered during annotation is reported as expected.</li>
                                        <li>Check if the progress messages accurately reflect the number of tasks being processed.</li>
                                        <li>Ensure that LLM annotation messages are included in the progress messages.</li>
                                        <li>Verify that the annotated result matches the expected value (in this case, 2).</li>
                                        <li>Confirm that the first error message includes the relevant information about the task ID and outcome.</li>
                                        <li>Check if the progress messages include a clear indication of the number of tasks being processed.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 229-232, 234, 236-237, 239-242, 245-246, 248-253, 255-258, 261-264, 266)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_sequential_rate_limit_wait</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Should wait if rate limit interval has not elapsed.</p>
                                <p><strong>Why Needed:</strong> Prevents test failure due to incorrect sleep behavior when rate limit interval hasn't elapsed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The time.sleep function should be called with a delay of at least 0.1 seconds.</li>
                                        <li>The time.sleep function should be called with a delay of exactly 1 second.</li>
                                        <li>The time.sleep function should not be called with a delay less than 0.1 seconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">23 lines (ranges: 165-168, 170-171, 173-174, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_tests_cached_progress</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the annotator reports progress when caching tests.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where cached tests are not reported with progress.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `progress_msgs` list should contain messages indicating cache status.</li>
                                        <li>Each message in `progress_msgs` should start with '(cache):'.</li>
                                        <li>At least one message in `progress_msgs` should be present.</li>
                                        <li>Any message in `progress_msgs` should have the format 'test_cached'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">37 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-84, 97-98, 100, 127, 129-135, 137, 139)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_tests_provider_unavailable</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that when the provider is not available, it prints a message and returns without attempting to annotate tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the annotator does not attempt to annotate tests when the provider is unavailable.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mocks.is_available was called with False</li>
                                        <li>annotate_tests calls mock_provider.get_provider with mock.Provider</li>
                                        <li>mock_provider.get_provider returns a MagicMock instance</li>
                                        <li>mock_provider.is_available returns False</li>
                                        <li>assert 'not available. Skipping annotations' in captured.out</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_coverage_v2.py::test_base_parse_response_malformed_json_after_extract</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that extracting a malformed JSON from a response fails with an error message.</p>
                                <p><strong>Why Needed:</strong> Prevents the test from passing if the extracted JSON is valid, allowing for coverage of invalid cases.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `annotation` variable should be set to `jsonDecodeError`.</li>
                                        <li>The `error` attribute of `annotation` should contain a string 'Failed to parse LLM response as JSON'.</li>
                                        <li>The `annotation.error` attribute should have the correct type hint.</li>
                                        <li>The `annotation.error` attribute should not be `None`.</li>
                                        <li>The `annotation.error` attribute should be an instance of `str`.</li>
                                        <li>The `annotation.error` attribute should contain a string that starts with 'Failed to parse'.</li>
                                        <li>The `annotation.error` attribute should contain the string ' Failed to parse LLM response as JSON'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 186-187, 190-191, 194-195, 220-221)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_coverage_v2.py::test_base_parse_response_non_string_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `test_base_parse_response_non_string_fields` test verifies that non-string fields are handled correctly in the response data.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the parser incorrectly handles non-string fields as lists, leading to incorrect results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The correct scenario is set by the `scenario` key in the response data.</li>
                                        <li>The expected why needed value is correctly identified as 'list'.</li>
                                        <li>The correct key assertions are made using the `key_assertions` list.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203-207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_gemini_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `get_gemini_provider` function returns a `GeminiProvider` instance.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case the `Config` class is modified to use a different provider type (e.g., 'satellite') without updating the `get_provider` function.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The returned value of `provider` should be an instance of `GeminiProvider`.</li>
                                        <li>The `provider` variable should hold a reference to a `GeminiProvider` instance.</li>
                                        <li>The `provider` attribute of the `config` object should have been set correctly using the `Config` class.</li>
                                        <li>The `get_provider` function should return a `GeminiProvider` instance when called with a valid configuration.</li>
                                        <li>If the `Config` class is modified to use a different provider type (e.g., 'satellite'), the test should fail and provide a clear error message.</li>
                                        <li>The `Config` class should be updated to include the correct provider type for the new provider.</li>
                                        <li>The `get_provider` function should be updated to handle cases where the provider is not found in the configuration.</li>
                                        <li>If an invalid provider type is passed to the `Config` constructor, the test should raise a meaningful error message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 245, 247, 249, 252, 257, 262-263, 265)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_invalid_provider</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that a ValueError is raised when an unknown LLM provider is specified.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the program incorrectly accepts an invalid LLM provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider()` raises a `ValueError` with the message 'Unknown LLM provider: invalid'.</li>
                                        <li>The error message includes the string 'invalid' to identify the unknown provider.</li>
                                        <li>The test verifies that the `pytest.raises()` matcher is used correctly to catch the ValueError.</li>
                                        <li>The test checks that the `match` parameter of the `pytest.raises()` matcher matches the expected error message.</li>
                                        <li>The test ensures that the `Config` object passed to `get_provider()` has an invalid provider.</li>
                                        <li>The test verifies that the `invalid` value is used as the provider in the `Config` object.</li>
                                        <li>The test checks that the `get_provider()` function raises a `ValueError` with the specified error message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 245, 247, 249, 252, 257, 262, 267)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_litellm_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `get_litellm_provider` function returns a correct instance of `LiteLLMProvider`.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an incorrect provider being returned.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` attribute of the returned `LiteLLMProvider` object is set to 'litellm'.</li>
                                        <li>The `provider` attribute of the returned `LiteLLMProvider` object is a string ('litellm').</li>
                                        <li>The `provider` attribute of the returned `LiteLLMProvider` object is an instance of `LiteLLMProvider`.</li>
                                        <li>The `provider` attribute of the returned `LiteLLMProvider` object is set to 'litellm' and has no additional attributes.</li>
                                        <li>The `provider` attribute of the returned `LiteLLMProvider` object is a string ('litellm') with an empty string as its value.</li>
                                        <li>The `provider` attribute of the returned `LiteLLMProvider` object is not None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52-53, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_noop_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that a NoopProvider is returned when the 'provider' parameter is set to 'none'</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where a valid provider is not found due to an incorrect or missing configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider` returns an instance of `NoopProvider` instead of another valid provider.</li>
                                        <li>The correct configuration for the 'provider' parameter is provided (in this case, 'none')</li>
                                        <li>A NoopProvider instance is created with the given configuration</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_ollama_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `get_ollama_provider` method returns an instance of OllamaProvider.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where an incorrect or malformed configuration is passed to the provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_ollama_provider` in the `Config` class correctly creates an instance of `OllamaProvider` with the provided provider.</li>
                                        <li>The method `get_provider` in the `Config` class correctly uses the provided configuration to create a valid Ollama provider.</li>
                                        <li>The returned value from `get_ollama_provider` is indeed an instance of `OllamaProvider` as expected.</li>
                                        <li>A malformed or incorrect configuration would result in an error being raised instead of creating an invalid provider.</li>
                                        <li>The correct type hinting for the `provider` parameter ensures that only valid providers are accepted.</li>
                                        <li>The method name and signature match the expected behavior, indicating a successful implementation.</li>
                                        <li>No other assertions are necessary as this test verifies the basic functionality of the `get_ollama_provider` method.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 245, 247, 249, 252-253, 255)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_available_caches_result</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `is_available()` method returns `True` for a provider with no available caches.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where a provider without any available caches would incorrectly return `False` when checking availability.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider.is_available() is True</li>
                                        <li>provider.checks == 1</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 107-108, 110-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_model_name_defaults_to_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the default model name is set to the configuration when a concrete provider is created.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the default model name is not set correctly in cases where a concrete provider is used.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_model_name()` method of the provider returns the value of `model` from the provided configuration.</li>
                                        <li>The `model` attribute of the provider instance has the same value as the `model` parameter passed to the `Config` constructor.</li>
                                        <li>The `provider.get_model_name()` call does not raise an exception if a concrete provider is used with a default model name.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 136)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_rate_limits_defaults_to_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_rate_limits` method returns `None` when no rate limits are specified in the configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the default rate limits are not properly initialized or set, potentially leading to unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.get('rate_limit') is None</li>
                                        <li>provider.get_rate_limits() == None</li>
                                        <li>assert isinstance(provider.get_rate_limits(), types.NoneType)</li>
                                        <li>assert provider.get_rate_limits().get('default_rate_limit') is None</li>
                                        <li>assert provider.get_rate_limits().get('max_rate_limit') is None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 128)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_is_local_defaults_to_false</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that `is_local()` returns False when the default is set to false.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the default value of `is_local()` is incorrectly reported as True.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider.is_local()` method should return False when `is_local_defaults_to_false` is called with a config that sets `local_defaults_to_false` to false.</li>
                                        <li>The `provider.is_local()` method should not raise an exception when `is_local_defaults_to_false` is called with a config that sets `local_defaults_to_false` to true.</li>
                                        <li>The `provider.is_local()` method should correctly handle the case where `local_defaults_to_false` is set to false in the config.</li>
                                        <li>The `provider.is_local()` method should not report any errors or warnings when `is_local_defaults_to_false` is called with a config that sets `local_defaults_to_false` to true.</li>
                                        <li>The `provider.is_local()` method should correctly handle the case where `local_defaults_to_false` is set to false in the config and the provider is created with it.</li>
                                        <li>The `provider.is_local()` method should not report any errors or warnings when `is_local_defaults_to_false` is called with a config that sets `local_defaults_to_false` to true and the provider is created with it.</li>
                                        <li>The `provider.is_local()` method should correctly handle the case where `local_defaults_to_false` is set to false in the config and the provider is created with it, without raising an exception.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestHashSource::test_consistent_hash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The function `hash_source` is called with a source code string that produces the same hash value.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where different source codes produce different hashes, which could lead to unexpected behavior in caching or other applications.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>source_code_is_same</li>
                                        <li>hash_value_is_not_different</li>
                                        <li>source_code_hash_is_consistent_with_source_code_hash</li>
                                        <li>source_code_hash_is_consistent_with_hash_of_source_code</li>
                                        <li>source_code_hash_is_the_same_as_hash_of_source_code</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestHashSource::test_different_source_different_hash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the behavior of `hash_source` when different sources are used.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where two functions with the same source code but different names could produce the same hash value, leading to unexpected behavior in caching.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `hash_source()` should return a different hash value for two different source strings.</li>
                                        <li>The function `hash_source()` should not be able to find a common prefix between two source strings.</li>
                                        <li>The function `hash_source()` should raise an error if the same source string is used multiple times.</li>
                                        <li>The function `hash_source()` should preserve the original order of source strings when comparing them for equality.</li>
                                        <li>The function `hash_source()` should not be able to cache a function with the same name as another function that uses different sources.</li>
                                        <li>A hash collision should occur between two different source strings and their corresponding cached functions.</li>
                                        <li>The function `hash_source()` should correctly handle source strings with multiple words or phrases separated by spaces.</li>
                                        <li>The function `hash_source()` should not cache a function if the same source string is used multiple times in the test suite.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestHashSource::test_hash_length</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the length of the hash generated by HashSource.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where the hash length is not consistent across different inputs.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The hash should be exactly 16 characters long.</li>
                                        <li>The hash length should remain constant regardless of the input.</li>
                                        <li>The hash should not be shorter than 16 characters but also not longer than 15 characters.</li>
                                        <li>The hash should have a consistent character distribution across all possible inputs.</li>
                                        <li>The hash should not be affected by the order of the characters in the input string.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_clear</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test clearing cache entries after adding some initial data.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case the test is run multiple times with different input data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that all cache entries are cleared after calling `clear()`.</li>
                                        <li>Ensure that no cached annotations are retrieved even after clearing the cache.</li>
                                        <li>Check if the cache directory remains as expected after clearing.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 129, 132-136, 141)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_does_not_cache_errors</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that annotations with errors are not cached.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of error annotations being cached.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation 'error' is present in the cache.</li>
                                        <li>The value associated with 'test::foo' is None after retrieval.</li>
                                        <li>The annotation type is correct ('error')</li>
                                        <li>The cache directory is set correctly using tmp_path / "cache".</li>
                                        <li>The error message is retrieved from the cache correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 39-41, 53, 55-56, 86, 88, 118-119, 121)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_get_missing</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case 'get_missing' verifies that the function returns None for missing entries.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not return an error when trying to retrieve a non-existent cache entry.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return `None` when given a key that does not exist in the cache.</li>
                                        <li>The function should raise a `KeyError` exception with a meaningful message when given a key that does not exist in the cache.</li>
                                        <li>The function should check if the cache is empty before trying to retrieve a non-existent entry.</li>
                                        <li>The function should return an error message indicating that the cache is missing a required entry.</li>
                                        <li>The function should handle cases where the cache directory is not writable or has incorrect permissions.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 39-41, 53, 55-56, 118-119, 121)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_set_and_get</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that annotations are correctly stored and retrieved from the cache.</p>
                                <p><strong>Why Needed:</strong> Prevents bypass by ensuring that annotations are persisted even after a test has completed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that the annotation is set correctly in the cache.</li>
                                        <li>Check that the annotation can be successfully retrieved from the cache.</li>
                                        <li>Ensure that the retrieved annotation matches the expected scenario and confidence level.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 39-41, 53, 55, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorCollectionErrors::test_collection_error_structure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that a collection error has the correct 'nodeid' and 'message' attributes.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where a collection error is incorrectly structured, potentially leading to incorrect or missing information being reported.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'nodeid' attribute of the CollectionError object should match the provided 'nodeid' value.</li>
                                        <li>The 'message' attribute of the CollectionError object should match the provided 'message' value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorCollectionErrors::test_get_collection_errors_initially_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the `get_collection_errors` method returns an empty list when the collection is initially empty.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the `get_collection_errors` method may return incorrect results or raise an exception due to an empty collection.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `collector.get_collection_errors()` function should return an empty list.</li>
                                        <li>No exceptions should be raised when the collection is initially empty.</li>
                                        <li>All errors in the collection should be ignored.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_context_override_default_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the default value of llm_context_override when it's not provided.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the default value of llm_context_override is set to None, potentially causing unexpected behavior in downstream code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The llm_context_override attribute is checked for being None.</li>
                                        <li>The TestCaseResult object has an llm_context_override attribute that matches the expected None value.</li>
                                        <li>The nodeid and outcome of the TestCaseResult match the expected values.</li>
                                        <li>The result.llm_context_override attribute is set to None, as expected.</li>
                                        <li>No exception is raised when calling llm_context_override on a TestCaseResult object.</li>
                                        <li>The llm_context_override attribute is not checked for being None in other test cases.</li>
                                        <li>The default value of llm_context_override is correctly set to None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_opt_out_default_false</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the default value of llm_opt_out for LLM Opt Out feature.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case where llm_opt_out defaults to False without explicit opt-out.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The llm_opt_out attribute is set to False.</li>
                                        <li>The TestCaseResult object has an llm_opt_out attribute that matches the expected value.</li>
                                        <li>The test passes if llm_opt_out is indeed False or if it's explicitly set to True.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorOutputCapture::test_capture_disabled_by_default</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the output capture feature is disabled by default.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the output capture feature was enabled by default.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.capture_failed_output should be set to False</li>
                                        <li>output_capture_enabled should not be True</li>
                                        <li>capture_mode should be 'disabled' or None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorOutputCapture::test_capture_max_chars_default</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'TestCollectorOutputCapture' test verifies that the default value of 'capture_output_max_chars' is 4000.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the default max chars value is not set correctly, potentially leading to unexpected behavior or errors in the application.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'capture_output_max_chars' configuration option should be set to 4000 by default.</li>
                                        <li>The current value of 'capture_output_max_chars' should match 4000.</li>
                                        <li>If the default max chars is not set, the test will fail with an error message indicating that it's not a valid value.</li>
                                        <li>The application may throw an exception or behave unexpectedly if the default max chars is not set correctly.</li>
                                        <li>The test ensures that the 'capture_output_max_chars' configuration option is properly initialized and configured.</li>
                                        <li>If the default max chars is not set, the test will fail with a clear and descriptive error message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorXfailHandling::test_xfail_failed_is_xfailed</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'xfail failures should be recorded as xfailed' verifies that failed test cases are correctly marked as xfailed in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where a failed test case is incorrectly marked as passed instead of xfailed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `results` dictionary contains an entry for the specified nodeid with a value of 'xfailed'.</li>
                                        <li>The `outcome` attribute of the result is set to 'xfailed'.</li>
                                        <li>The `wasxfail` attribute of the report is set to 'expected failure'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212, 216, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorXfailHandling::test_xfail_passed_is_xpassed</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that when an xfail is passed, it should be recorded as xpassed in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where an xfail is not properly handled and instead records it as failed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>the `results` dictionary contains a key with the value 'xpassed' for the nodeid 'test_xfail.py::test_unexpected_pass'.</li>
                                        <li>the `outcome` attribute of the result is set to 'xpassed'.</li>
                                        <li>the `wasxfail` attribute of the report is set to 'expected failure'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212-214)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestTestCollector::test_create_collector</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `create_collector` method of `TestCollector` class.</p>
                                <p><strong>Why Needed:</strong> The test ensures that a new `TestCollector` instance is created with an empty collection and no errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `results` attribute of the collector should be an empty dictionary.</li>
                                        <li>The `collection_errors` list should be an empty list.</li>
                                        <li>The `collected_count` attribute should be set to 0.</li>
                                        <li>A new instance of `TestCollector` should be created with a Config object.</li>
                                        <li>No errors should be reported by the collector during initialization.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestTestCollector::test_get_results_sorted</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_results` method returns a sorted list of node IDs from the collected results.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the order of node IDs in the results is not guaranteed to be consistent across different runs, potentially leading to incorrect analysis or reporting.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `nodeid` attribute of each result object contains the correct value.</li>
                                        <li>The list of node IDs returned by the `get_results` method is sorted in ascending order.</li>
                                        <li>No duplicate node IDs are present in the sorted list.</li>
                                        <li>All nodes with a 'passed' outcome are included in the sorted list.</li>
                                        <li>No nodes without an outcome ('failed') are included in the sorted list.</li>
                                        <li>The sorting is stable, meaning that if two results have the same `nodeid`, their original order is preserved.</li>
                                        <li>No duplicate node IDs are present in the sorted list of node IDs.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestTestCollector::test_handle_collection_finish</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `handle_collection_finish` method to ensure it correctly tracks collected and deselected counts.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the count of collected items is not updated correctly after the collection finish.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `collected_count` attribute should be set to 3 (the number of collected items).</li>
                                        <li>The `deselected_count` attribute should be set to 1 (the number of deselected items).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 78-79, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_disabled_via_handle_report</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the collector does not capture output when config is disabled and handle_report is used for integration via handle_runtest_logreport.</p>
                                <p><strong>Why Needed:</strong> To prevent capturing of output in scenarios where the `capture_failed_output` configuration is set to False, allowing for integration with handle_report for reporting purposes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The collector does not capture any output when run on a test that was previously failed and has its `capture_failed_output` config set to False.</li>
                                        <li>The collector's results do not contain any captured stdout data.</li>
                                        <li>No error message is emitted by the collector due to no captured stdout.</li>
                                        <li>The collector's `results` dictionary does not contain a key named 't' or any other relevant keys.</li>
                                        <li>The collector's `results` dictionary contains an empty string value for the 'output' key.</li>
                                        <li>The collector's `results` dictionary contains False values for all other keys (passed, failed, skipped).</li>
                                        <li>The collector's `results` dictionary does not contain a 't' key with a non-empty string value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stderr</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `capture_output` method captures stderr correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `capture_output` method does not capture stderr.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `captured_stderr` attribute of the `TestCaseResult` object should be set to 'Some error'.</li>
                                        <li>The `report.capstderr` attribute should contain the string 'Some error'.</li>
                                        <li>The `report.capstdout` attribute should be an empty string.</li>
                                        <li>The `collector._capture_output(result, report)` method should call `result.captured_stderr = 'Some error'`.</li>
                                        <li>The `collector._capture_output(result, report)` method should set `captured_stderr` to the captured stderr value.</li>
                                        <li>The `report.capstderr` attribute should be updated with the captured stderr value.</li>
                                        <li>The `collector._capture_output(result, report)` method should update the `report` object correctly.</li>
                                        <li>The `collector._capture_output(result, report)` method should not raise any exceptions.</li>
                                        <li>The `report` object should have a non-empty `capstderr` attribute after calling `_capture_output`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264, 268-269)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stdout</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `capture_output` method captures stdout correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the captured stdout is not properly recorded.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `captured_stdout` attribute of the `TestCaseResult` object should contain the expected output.</li>
                                        <li>The `report.capstdout` attribute should set the correct value for stdout.</li>
                                        <li>The `collector._capture_output(result, report)` method should record the captured stdout correctly.</li>
                                        <li>The `result.captured_stdout` attribute should be equal to the captured stdout.</li>
                                        <li>The `report.capstderr` attribute is not used in this test and can be safely ignored.</li>
                                        <li>The `collector._capture_output(result, report)` method does not modify the original output.</li>
                                        <li>The `collector._capture_output(result, report)` method does not record any additional information.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_truncated</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `test_capture_output_truncated` function truncates output exceeding max chars.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the collector fails to truncate output exceeding the maximum characters set in the configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The captured stdout should be truncated to '1234567890' if it exceeds the specified max_chars.</li>
                                        <li>The `captured_stdout` attribute of the `TestCaseResult` object should contain only the truncated output.</li>
                                        <li>The `report.capstdout` attribute should not exceed the maximum characters set in the configuration.</li>
                                        <li>The `report.capstderr` attribute is not used in this test and can be ignored for this test.</li>
                                        <li>The `collector._capture_output` method should call the `report.capstdout` method to update the captured stdout.</li>
                                        <li>The `result.captured_stdout` attribute should contain only the truncated output after calling `_capture_output`.</li>
                                        <li>The `report.capstderr` attribute should not be affected by the truncation of `captured_stdout`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_create_result_with_item_markers</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test creates a result with item markers.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the collector does not extract item markers correctly, leading to incorrect results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>item.get_closest_marker('llm_opt_out') returns MagicMock().</li>
                                        <li>item.get_closest_marker('llm_context') returns MagicMock().</li>
                                        <li>item.get_closest_marker('requirement') returns MagicMock().</li>
                                        <li>result.param_id is set to 'param1'.</li>
                                        <li>result.llm_opt_out is True.</li>
                                        <li>result.llm_context_override is set to 'complete'.</li>
                                        <li>result.requirements contains ['REQ-1', 'REQ-2'].</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">35 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163-164, 167-169, 171, 181-182, 185-189, 198-200, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_repr_crash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test should handle RePrFileLocation causing crash report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential crash when RePrFileLocation is used in the error representation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report.longrepr` attribute is set to 'Crash report'.</li>
                                        <li>The `report.longrepr.__str__.return_value` is set to 'Crash report'.</li>
                                        <li>The `collector._extract_error(report)` function returns 'Crash report' as expected.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_string</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `_extract_error` method returns the correct string for a maximal error message.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the `longrepr` attribute is not correctly propagated to the extracted error string.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `report.longrepr` should be equal to 'Some error occurred'.</li>
                                        <li>The method `_extract_error` should return the correct string for a maximal error message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_fallback</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `_extract_skip_reason` method returns `None` when no longrepr is provided.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the method does not handle cases with no longrepr correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_skip_reason` method should return `None` for an empty or missing `longrepr` attribute.</li>
                                        <li>The `_extract_skip_reason` method should not raise any exceptions when `longrepr` is `None`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250, 252)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_string</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test `test_extract_skip_reason_string` verifies that the `_extract_skip_reason` method returns a string when given a `report` object.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of unexpected report longrepr values, which could lead to incorrect skip reasons being returned.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report.longrepr` attribute is set to 'Just skipped'.</li>
                                        <li>The `_extract_skip_reason` method returns the expected string value.</li>
                                        <li>No other assertions are performed by this test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_tuple</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that extract skip reason tuple is called correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `extract_skip_reason` method does not handle tuples with more than two elements.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `longrepr` attribute of the report object contains the expected file, line and message.</li>
                                        <li>The `longrepr` attribute of the report object is a tuple containing the specified file, line and message.</li>
                                        <li>When a tuple with three elements is passed to `_extract_skip_reason`, it correctly extracts the skip reason from the tuple.</li>
                                        <li>When a tuple with more than two elements is passed to `_extract_skip_reason`, it raises an `AssertionError` with a meaningful error message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_collection_report_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> When the `handle_collection_report` method is called with a report that indicates a collection error, then it should record this error in the `collection_errors` list.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the collector might not handle collection errors correctly and instead silently ignore them.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `collection_errors` list should contain exactly one item with `nodeid = 'test_broken.py'` and `message = 'SyntaxError'`.</li>
                                        <li>The error message in the first `collection_errors` item should be 'SyntaxError'.</li>
                                        <li>All other items in the `collection_errors` list should have a different `nodeid` and/or an empty `message` field.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 58, 60-65, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_rerun</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'handle_runtest_rerun' verifies that the `rerun` attribute of a report is correctly set to 1 after rerunning the test.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in handling reruns, ensuring that reports with a `rerun` attribute are updated correctly when the test is rerun.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>res.rerun_count should be equal to 1</li>
                                        <li>res.final_outcome should be 'failed'</li>
                                        <li>report.wasxfail should not be present in report</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_setup_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Collector should handle run test setup failure correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the collector fails to record setup errors, potentially leading to incorrect reporting of test failures.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>res.outcome is set to 'error' as expected.</li>
                                        <li>res.phase is set to 'setup' as expected.</li>
                                        <li>res.error_message is set to 'Setup failed' as expected.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99-103, 109-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_teardown_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case: Handle runtest teardown failure</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of teardown failure after a pass.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `teardown` report is not recorded as an error.</li>
                                        <li>The `teardown` report has the correct phase ('teardown') and error message ('Cleanup failed').</li>
                                        <li>The `results` dictionary contains the expected outcome ('error'), phase, and error message for test 't::f'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">38 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-115, 124, 127-128, 130, 132-133, 135-137, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_model_parsing_edge_cases</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the GeminiProvider's _parse_preferred_models method with edge cases, specifically when no models are provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the GeminiProvider does not correctly handle scenarios where no models are specified in the configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'm1' and 'm2' models should be present in the parsed list of preferred models.</li>
                                        <li>The 'All' model should also be present in the parsed list of preferred models if it is specified in the configuration.</li>
                                        <li>An empty list of preferred models should be returned when no models are provided in the configuration.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 134, 136-139, 141-142, 385, 387, 417-424)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_rate_limiter_edge_math</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter does not allow excessive tokens when there are available slots but too many requests.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the rate limiter allows an edge case (excessive tokens) while still allowing enough available slots for other requests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert limiter.next_available_in(60) > 0</li>
                                        <li>assert limiter.next_available_in(10) == 0</li>
                                        <li>assert limiter.record_tokens(50) < 100</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">35 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_models_to_dict_variants</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `models_to_dict` method returns accurate coverage percentages for SourceCoverageEntry and LlmAnnotation objects.</p>
                                <p><strong>Why Needed:</strong> The test prevents regression in coverage reporting when using models to dict variants, as it ensures that the coverage percentage is always reported correctly even with errors or timeouts.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>d['coverage_percent'] == 50.0</li>
                                        <li>ann.to_dict()['error'] == 'timeout'</li>
                                        <li>meta.to_dict()['duration'] == 1.0</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 71-78, 104-107, 109, 111-113, 115, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_create_mapper</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `CoverageMapper` class to ensure it correctly initializes with a given configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents potential bugs or regressions where a `CoverageMapper` instance is created without a valid configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config` attribute of the `CoverageMapper` instance should be set to the provided `Config` object.</li>
                                        <li>The `warnings` attribute of the `CoverageMapper` instance should be an empty list.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 44-45)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_get_warnings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies the `get_warnings` method returns a list of warnings as expected.</p>
                                <p><strong>Why Needed:</strong> Prevents test failures due to incorrect handling of warnings in coverage reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_warnings` method is called on an instance of `CoverageMapper` with a valid configuration.</li>
                                        <li>The returned value is checked to be an instance of `list` as expected.</li>
                                        <li>A warning is extracted from the coverage report and added to the list of warnings.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 44-45, 308)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_map_coverage_no_coverage_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `map_coverage` method returns an empty dictionary when no coverage file is found.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to missing coverage data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `mapper.map_coverage()` method should return an empty dictionary when `Path.exists` and `glob.glob` return False.</li>
                                        <li>The `mapper.warnings` list should contain at least one warning message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_all_phases</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `CoverageMapper` extracts node IDs for all phases when `include_phase=all`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the coverage map does not include all phases when `include_phase=all`.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_nodeid()` method returns the expected node ID for each phase.</li>
                                        <li>The `_extract_nodeid()` method includes all phases in the coverage map.</li>
                                        <li>The `_extract_nodeid()` method excludes only the 'setup' phase from the coverage map when `include_phase=all`.</li>
                                        <li>The `CoverageMapper` class correctly handles the `include_phase=all` parameter.</li>
                                        <li>The test passes without any errors or warnings for this specific scenario.</li>
                                        <li>The test covers all possible cases where `include_phase=all` is used.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_empty_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `extract_nodeid` method with an empty context.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the method returns `None` for an empty context, potentially causing unexpected behavior or errors in downstream code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_nodeid` method should return `None` when passed an empty string.</li>
                                        <li>The `_extract_nodeid` method should return `None` when passed `None` as the context.</li>
                                        <li>The method should not throw any exceptions or raise errors for these inputs.</li>
                                        <li>The method's behavior should be consistent with its documentation and other tests.</li>
                                        <li>The test should verify that the method returns `None` in both cases, without attempting to extract a node ID from an empty string or None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 44-45, 216-217)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_filters_setup</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `test_extract_nodeid_filters_setup` test case filters out setup phase when `include_phase=run`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the coverage map includes nodes from the setup phase even though it's excluded.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_nodeid` method of the `CoverageMapper` class returns `None` for the given nodeid.</li>
                                        <li>The `include_phase` parameter is set to `'run'` in the test configuration.</li>
                                        <li>The coverage map does not include nodes from the setup phase when `include_phase=run`.</li>
                                        <li>The `test_foo` function is part of a module that has a setup phase, but it's excluded by default.</li>
                                        <li>The `test_foo` function should be filtered out from the coverage report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 44-45, 216, 220, 224-225, 228-230)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_with_run_phase</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `extract_nodeid` method extracts the correct `nodeid` from the run phase context.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the extracted `nodeid` is incorrect due to missing or incomplete information in the run phase context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_nodeid` method of the `CoverageMapper` class correctly extracts the `nodeid` from the provided string.</li>
                                        <li>The `nodeid` extracted from the run phase context matches the expected value (`test.py::test_foo`).</li>
                                        <li>The test does not fail when the input string is empty or contains only whitespace characters.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_full_logic</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test extracts contexts for full logic coverage of _extract_contexts method.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in coverage analysis when the _extract_contexts method is called with a file that has multiple test files but no other code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert 'test_app.py::test_one' in result</li>
                                        <li>assert 'test_app.py::test_two' in result</li>
                                        <li>assert len(one_cov) == 1 and one_cov[0].line_count == 2</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">57 lines (ranges: 44-45, 118, 121-122, 127, 131-135, 137-140, 144, 148, 150, 152-153, 156, 160-163, 165, 167-168, 173, 176, 178-184, 187-189, 191-194, 196, 199-200, 202, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">13 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65-67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_no_contexts</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_extract_contexts_no_contexts' verifies that the function correctly handles data with no test contexts by returning an empty dictionary.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the function incorrectly returns a non-empty dictionary for data without test contexts.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_data.measured_files.return_value == ['app.py']</li>
                                        <li>mock_data.contexts_by_lineno.return_value == {}</li>
                                        <li>result == {}</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 44-45, 118, 121-122, 127, 131-135, 144-146)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_nodeid_variants</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test extracts node ID variants for setup and teardown phases.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in coverage analysis by ensuring that all nodes are covered during both setup and teardown phases.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function _extract_nodeid returns the expected node ID for each test file.</li>
                                        <li>The function _extract_nodeid filters out nodes that do not belong to the specified phase (setup or teardown).</li>
                                        <li>The function _extract_nodeid does not return any nodes when there are no matching phases (e.g., 'test.py::test_no_phase').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231-234, 236, 239)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_no_files</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the test_load_coverage_data_no_files function correctly handles the case when no coverage files exist.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the CoverageMapper class does not handle the case when there are no coverage files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return None and have exactly one warning.</li>
                                        <li>The first warning should be for code 'W001'.</li>
                                        <li>No other warnings should be present in the result.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 44-45, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_read_error</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Load Coverage Data Read Error: Tests the function _load_coverage_data() when it encounters an error while reading a coverage file.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to unexpected errors in coverage data loading, ensuring the function remains reliable and handles edge cases correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function _load_coverage_data() should return None when it encounters an error while reading a coverage file.</li>
                                        <li>Any warnings generated by the function should contain the message 'Failed to read coverage data'.</li>
                                        <li>The function should not raise any exceptions during execution, maintaining its robustness and reliability.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 44-45, 72-73, 83, 86, 88, 92, 94-96, 107-111, 114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_with_parallel_files</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test should handle parallel coverage files from xdist and verify that the CoverageMapper correctly updates its internal state.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the CoverageMapper fails to update its internal state when loading coverage data with parallel files from xdist, potentially leading to incorrect or missing coverage information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert mock_main_data.update.call_count >= 2</li>
                                        <li>assert mock_parallel_data1.call_count == 0</li>
                                        <li>assert mock_parallel_data2.call_count == 0</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 44-45, 72-73, 83, 86, 88, 92, 94, 98, 101-104, 106)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_coverage_no_data</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `map_coverage` method when it does not receive any coverage data.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an incorrect assumption about the behavior of `_load_coverage_data` when no data is available.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return an empty dictionary `{}` when `_load_coverage_data` returns None.</li>
                                        <li>No exception should be raised when `_load_coverage_data` returns None.</li>
                                        <li>The test should pass even if the `_load_coverage_data` method returns a non-None value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 44-45, 58-60)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_analysis_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `map_source_coverage` method skips files with errors during analysis.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where an error in the analysis2 function would cause all source code to be skipped.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_data.measured_files.return_value should return ['app.py']</li>
                                        <li>mock_cov.get_data.return_value should raise Exception('Analysis failed')</li>
                                        <li>entries should not contain any files with errors</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 44-45, 243-244, 246-248, 250, 252-254, 259, 261, 263-268, 271, 299-300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_comprehensive</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'Should exercise all paths in map_source_coverage' to ensure comprehensive coverage of source files.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the CoverageMapperMaximal class exercises all possible paths in the map_source_coverage configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `map_source_coverage` should return a list containing exactly one entry with the following properties: `file_path`, `statements`, `covered`, `missed`, and `coverage_percent`.</li>
                                        <li>The value of `file_path` in the returned entry should be 'app.py'.</li>
                                        <li>The number of statements in the returned entry should be 3.</li>
                                        <li>The value of `covered` in the returned entry should be 2.</li>
                                        <li>The number of missed files in the returned entry should be 1.</li>
                                        <li>The percentage of covered lines in the returned entry should be 66.67 (rounded to two decimal places).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">32 lines (ranges: 44-45, 243-244, 246-248, 250, 252, 259-261, 273, 276-279, 281-283, 285-293, 295, 299-300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors.py::test_make_warning</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `make_warning` factory function to ensure it correctly identifies and handles unknown warnings.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where an unknown warning is incorrectly classified as having no coverage.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `make_warning` factory function should return a Warning object with the correct code (W001_NO_COVERAGE) and message.</li>
                                        <li>The `message` attribute of the returned Warning object should contain the expected string 'Unknown warning.'</li>
                                        <li>The `detail` attribute of the returned Warning object should be set to the specified value 'test-detail'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors.py::test_warning_code_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that warning codes have correct values.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the warning code values are incorrect, which could lead to unexpected behavior or errors in downstream code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>{'message': 'assert WarningCode.W001_NO_COVERAGE.value == "W001"', 'expected_value': 'W001'}</li>
                                        <li>{'message': 'assert WarningCode.W101_LLM_ENABLED.value == "W101"', 'expected_value': 'W101'}</li>
                                        <li>{'message': 'assert WarningCode.W201_OUTPUT_PATH_INVALID.value == "W201"', 'expected_value': 'W201'}</li>
                                        <li>{'message': 'assert WarningCode.W301_INVALID_CONFIG.value == "W301"', 'expected_value': 'W301'}</li>
                                        <li>{'message': 'assert WarningCode.W401_AGGREGATE_DIR_MISSING.value == "W401"', 'expected_value': 'W401'}</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors.py::test_warning_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the Warning.to_dict() method to ensure it returns a dictionary with correct keys.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the Warning.to_dict() method does not return a dictionary with all required keys.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `to_dict()` should return a dictionary with keys 'code', 'message', and 'detail'.</li>
                                        <li>The value of 'code' should be set to the correct warning code.</li>
                                        <li>The value of 'message' should be set to the correct warning message.</li>
                                        <li>The value of 'detail' should be set to the correct detail message if it exists.</li>
                                        <li>If a detail message is present, its length should not exceed 50 characters.</li>
                                        <li>If no detail message is present, the 'detail' key should be empty.</li>
                                        <li>The function should raise an AssertionError with a meaningful error message if any of the assertions fail.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 70-72, 74-76)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_known_code</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that a warning with the standard message is created when known code is used.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where warnings are not correctly generated for known code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>w.code == WarningCode.W101_LLM_ENABLED</li>
                                        <li>w.message == WARNING_MESSAGES[WarningCode.W101_LLM_ENABLED]</li>
                                        <li>w.detail is None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_unknown_code</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test MakeWarning::test_make_warning_unknown_code verifies that the test uses a fallback message for unknown code.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the typed function would not provide a warning when given an unknown WarningCode.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test should restore the original message after restoring the missing code.</li>
                                        <li>The test should assert that the restored message is 'Unknown warning.'</li>
                                        <li>The test should not assert any other messages or values than 'Unknown warning.'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_with_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_make_warning_with_detail' verifies that a warning is created with the correct code and detail.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where a warning might not be created correctly due to an invalid configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `make_warning` returns a Warning object with the correct `code` attribute set to `WarningCode.W301_INVALID_CONFIG` and the correct `detail` attribute set to 'Bad value'.</li>
                                        <li>The function `make_warning` returns a Warning object with the correct `code` attribute set to `WarningCode.W301_INVALID_CONFIG`.</li>
                                        <li>The function `make_warning` returns a Warning object with the correct `detail` attribute set to 'Bad value'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestWarningCodes::test_codes_are_strings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Ensures that enum values are indeed strings and start with 'W' to prevent Warnings.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential warning when trying to use non-string enum values.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert isinstance(code.value, str)</li>
                                        <li>assert code.value.startswith('W')</li>
                                        <li>code.value should be a string</li>
                                        <li>code.value should start with 'W'</li>
                                        <li>WarningCode.values() should return only strings</li>
                                        <li>WarningCode.values() should include values starting with 'W'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_no_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the Warning class can be serialized into a dictionary without including detailed information.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the warning details are included in the serialization of the Warning object to a dictionary.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'code' key is present with value 'W001'</li>
                                        <li>The 'message' key is present with value 'No coverage'</li>
                                        <li>The 'code' and 'message' keys have the correct values</li>
                                        <li>The warning details are not included in the dictionary</li>
                                        <li>The dictionary has the expected structure</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 70-72, 74, 76)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_with_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the warning to dictionary conversion with detailed information.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where warnings are not properly serialized in dictionaries.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'code' key should be present and have the correct value ('W001')</li>
                                        <li>The 'message' key should be present and have the correct value ('No coverage')</li>
                                        <li>The 'detail' key should be present and have the correct value ('Check setup')</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 70-72, 74-76)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestIsPythonFile::test_non_python_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `is_python_file` function returns False for non-.py files.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function incorrectly identifies Python files as non-Python files, potentially leading to incorrect file classification and implications in downstream code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return `False` when given a non-.py file path (e.g., `foo/bar.txt`).</li>
                                        <li>The function should not return `False` for `.pyc` files (e.g., `foo/bar.pyc`).</li>
                                        <li>When the input is a valid Python file, the function should correctly identify it as a Python file.</li>
                                        <li>If an invalid path is passed to the function, it should raise an error or handle it in a way that makes sense for the application.</li>
                                        <li>The function should not have any side effects (e.g., modifying external state) when determining whether a file is Python or not.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 79)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestIsPythonFile::test_python_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `is_python_file` function returns True for a Python file.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function incorrectly identifies non-Python files as such.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should correctly identify `.py` files and return `True`.</li>
                                        <li>The function should not incorrectly identify other types of files (e.g. `.txt`, `.js`) as Python files.</li>
                                        <li>The function should handle file paths with leading or trailing whitespace correctly.</li>
                                        <li>The function should ignore case when comparing file extensions (e.g. `.Py` vs `.py`).</li>
                                        <li>The function should raise an error if the input is not a string or a valid file path.</li>
                                        <li>The function should be able to handle files with relative paths correctly (e.g. `./foo/bar.py`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 79)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestMakeRelative::test_makes_path_relative</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `make_relative` function correctly makes a path relative to the test directory.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not handle cases where the input file path is absolute.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should be able to create an intermediate directory if it doesn't exist and then move the file into that directory.</li>
                                        <li>The function should remove any existing intermediate directory before creating it.</li>
                                        <li>The function should preserve the original file name and extension.</li>
                                        <li>The function should handle cases where the input file path is absolute (e.g., `/path/to/file.py`)</li>
                                        <li>The function should not create an intermediate directory if the input file path is already relative (e.g., `./file.py`)</li>
                                        <li>The function should preserve the original file permissions and ownership.</li>
                                        <li>The function should handle cases where the test directory does not exist (i.e., `tmp_path` is None)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `make_relative` function returns a normalized path when there is no base.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues where an invalid or empty base directory causes unexpected behavior in the application.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>result == 'foo/bar'</li>
                                        <li>is not equal to 'foo' (should be normalized)</li>
                                        <li>is not equal to 'bar' (should be normalized)</li>
                                        <li>is not equal to 'foo/bar' (should be normalized)</li>
                                        <li>has a length of 3 (expected to have 2 parts: 'foo', '.', and '/')</li>
                                        <li>does not contain any leading slashes (expected to be normalized)</li>
                                        <li>does not contain any trailing slashes (expected to be normalized)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 30, 33, 36, 39, 42, 55-56)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestNormalizePath::test_already_normalized</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that a normalized path is returned for an already-normalized input.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `normalize_path` function would incorrectly return the original input if it's already normalized.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The normalized path should be the same as the original input.</li>
                                        <li>The function should not modify the input path.</li>
                                        <li>The function should handle paths with leading or trailing slashes correctly.</li>
                                        <li>The function should ignore any redundant separators (e.g., multiple dots in a file name).</li>
                                        <li>The function should preserve the directory structure of the input path.</li>
                                        <li>The function should raise an error if the input is not a string or a Path object.</li>
                                        <li>The function should handle paths with non-ASCII characters correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestNormalizePath::test_forward_slashes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `normalize_path` function for forward slashes.</p>
                                <p><strong>Why Needed:</strong> Prevents a bug where the function incorrectly converts forward slashes to backslashes in certain paths.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should correctly convert 'foo\bar' to 'foo/bar'.</li>
                                        <li>The function should not convert '\bar' to '/bar'.</li>
                                        <li>The function should handle multiple consecutive backslashes correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `normalize_path` function strips trailing slashes from paths.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where a path with a trailing slash is returned as is, potentially causing issues downstream.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input path does not end with a forward slash (`/`).</li>
                                        <li>The output path has no leading forward slashes (`/`).</li>
                                        <li>The function correctly handles paths with multiple levels of nesting (e.g., `foo/bar/baz`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_custom_exclude_patterns</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies whether the `should_skip_path` function correctly skips paths matching custom patterns.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not skip paths that should be excluded due to custom patterns.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `should_skip_path` is called with a path 'tests/conftest.py' and an exclude pattern ['test*']</li>
                                        <li>The function `should_skip_path` returns True for the path 'src/module.py'</li>
                                        <li>The function `should_skip_path` returns False for the path 'tests/conftest.py'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116-117, 119-121, 123)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_normal_path</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_fs.py::TestShouldSkipPath::test_normal_path</p>
                                <p><strong>Why Needed:</strong> To ensure that the 'should_skip_path' function correctly handles normal file system paths.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return True for a normal path (e.g. 'src/module.py').</li>
                                        <li>The function should not return False for a normal path (e.g. 'src/module.py').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_git</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `should_skip_path` function correctly identifies `.git` directories.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly skips non-`.git` directories.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert should_skip_path('.git/objects/foo') is True</li>
                                        <li>assert not should_skip_path('non_git_directory.txt')</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_pycache</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `should_skip_path` function correctly skips the `__pycache__` directory.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the `should_skip_path` function does not skip the `__pycache__` directory, causing unexpected behavior in tests that rely on it.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The path should be skipped by the `should_skip_path` function.</li>
                                        <li>The `__pycache__` directory is skipped by the `should_skip_path` function.</li>
                                        <li>The `should_skip_path` function returns True for paths within the `__pycache__` directory.</li>
                                        <li>The test case asserts that the path is not included in the cache.</li>
                                        <li>The `should_skip_path` function should be able to determine whether a path is cached or not.</li>
                                        <li>The `__pycache__` directory should be excluded from the cache by the `should_skip_path` function.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_venv</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_fs.py::TestShouldSkipPath::test_skips_venv verifies that the function `should_skip_path` correctly identifies venv directories.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the function `should_skip_path` incorrectly identifies venv directories as Python site packages, potentially leading to incorrect skipping of these directories.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert should_skip_path('venv/lib/python/site.py') is True</li>
                                        <li>assert should_skip_path('.venv/lib/python/site.py') is True</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_pruning</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the _GeminiRateLimiter's pruning behavior when a request is added in the past.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where requests made before a certain time threshold are not properly cleared from the rate limiter's cache.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of _request_times should be equal to 0 after pruning.</li>
                                        <li>The length of _token_usage should also be equal to 0 after pruning.</li>
                                        <li>The request times list should contain only one element (the time when the request was added).</li>
                                        <li>_prune() should not modify the request times list or token usage lists.</li>
                                        <li>The prune method should clear all requests and token usages from the rate limiter's cache.</li>
                                        <li>The test should pass without any assertion errors after pruning.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 39-42, 81-85, 87-88)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_rpm_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter prevents requests from exceeding the specified limit.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where requests are allowed to exceed the specified rate limit, potentially leading to unexpected behavior or performance issues.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `wait` variable should be greater than 0.</li>
                                        <li>The `wait` variable should not exceed 60.0 seconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95, 97, 100-102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_tpm_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter prevents a regression when the token limit is exceeded.</p>
                                <p><strong>Why Needed:</strong> This test verifies that the rate limiter correctly handles cases where the token limit is reached, preventing potential performance regressions.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The next available time point should be greater than 0.</li>
                                        <li>The total number of tokens used since the last update should still be 2.</li>
                                        <li>The _token_usage list should contain only two elements.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">33 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-94, 100-101, 103, 105, 107-108, 110-114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_wait_for_slot</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `wait_for_slot` method of `_GeminiRateLimiter` sleeps for a specified amount of time when a request is made.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues where requests are made too quickly and the rate limiter does not have enough time to process them.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `wait_for_slot` method should call `time.sleep` with the specified amount of time.</li>
                                        <li>The `wait_for_slot` method should assert that `mock_sleep` was called.</li>
                                        <li>The `wait_for_slot` method should not be able to make any requests while sleeping.</li>
                                        <li>The rate limiter's `record_request` method should have been called before making the request.</li>
                                        <li>The rate limiter's `record_request` method should have been called with a valid limit value.</li>
                                        <li>The rate limiter's `wait_for_slot` method should not be able to make any requests while waiting for the slot.</li>
                                        <li>The time.sleep function call should be within a reasonable delay (e.g. < 1 second).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">31 lines (ranges: 39-42, 45-46, 48, 52-54, 58-59, 61-63, 73, 76-78, 81-82, 84, 87-88, 92-93, 95, 97, 100-102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_record_zero_tokens</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter records zero tokens when no tokens are available.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the rate limiter does not record tokens for an extended period without reaching the limit.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_token_usage` list of the `GeminiRateLimiter` instance is empty after calling `record_tokens(0)`.</li>
                                        <li>The `len(_token_usage)` property of the `GeminiRateLimiter` instance is equal to 0.</li>
                                        <li>The rate limiter's internal state is updated correctly when no tokens are available for recording.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39-42, 66-67)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_requests_per_day_exhaustion</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the test raises a RateLimitExceeded exception when exceeding daily limit.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the rate limiter does not raise an error when exceeding the daily limit.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `wait_for_slot` should raise `_GeminiRateLimitExceeded` with a message indicating that requests per day have exceeded the limit.</li>
                                        <li>The function `record_request` should be called before calling `wait_for_slot` to set up the rate limiter.</li>
                                        <li>The error message should include the string 'requests_per_day' which is expected to be present in the match.</li>
                                        <li>The function `wait_for_slot` should not return immediately after raising the exception, but instead wait for the slot to become available.</li>
                                        <li>The function `record_request` should be called before calling `wait_for_slot` to set up the rate limiter.</li>
                                        <li>The error message should include the string 'requests_per_day' which is expected to be present in the match.</li>
                                        <li>The function `wait_for_slot` should raise `_GeminiRateLimitExceeded` with a message indicating that requests per day have exceeded the limit.</li>
                                        <li>The function `record_request` should be called before calling `wait_for_slot` to set up the rate limiter.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">24 lines (ranges: 32-34, 39-42, 45-46, 48-50, 58-60, 73, 76-78, 81-82, 84, 87-88)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_tpm_fallback_wait</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter waits for TPM availability when tokens are used beyond the limit.</p>
                                <p><strong>Why Needed:</strong> The test prevents a potential bug where the rate limiter does not wait for TPM availability even when tokens exceed the limit, leading to unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>limiter._seconds_until_tpm_available(now, 5) > 0</li>
                                        <li>tokens_used + request_tokens > limit</li>
                                        <li>token_usage is not empty</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">24 lines (ranges: 39-42, 66, 68-70, 81-82, 84, 87-88, 100-101, 103, 105, 107-108, 110-114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_provider_rpm_cooldown</span>
                        <div class="test-meta">
                            <span>566ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that RPM rate limit cooldown handling is properly implemented.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the RPM rate limit cooldown is not set correctly on the first call to _call_gemini.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'models/gemini-pro' model should be in the cooldowns dictionary with a value greater than 1000.0 seconds.</li>
                                        <li>The provider._cooldowns['models/gemini-pro'] should have been set correctly after the first call to _call_gemini.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">23 lines (ranges: 52-53, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">117 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-215, 217-223, 225-227, 233-234, 238-240, 242-243, 274-277, 280, 282-290, 292-295, 297-298, 300-301, 346, 348-350, 352-353, 381-382, 385-386)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_rate_limit_retry</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the GeminiProvider's _annotate_internal method correctly handles rate limiting and retry logic when encountering a 429 status code.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the GeminiProvider class, ensuring it can handle cases where the API returns a 429 status code due to rate limiting.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'annotation' object has the expected scenario ('Recovered Scenario') and number of calls to the 'mock_post' method (2).</li>
                                        <li>The 'annotation' object does not have an error attribute.</li>
                                        <li>The 'annotation' object's 'scenario' attribute is set to 'Recovered Scenario'.</li>
                                        <li>The 'annotation' object's 'error' attribute is None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">181 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-215, 217-222, 225-227, 233-234, 238-240, 242-243, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330-333, 335-341, 343, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_success</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the _annotate_internal method returns a valid LlmAnnotation object with the correct scenario and no error.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the _annotate_internal method fails to return an LlmAnnotation object due to incorrect response format from _call_gemini.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The scenario of the annotation is set to 'Success Scenario'.</li>
                                        <li>No error is returned in the annotation. The error should be None.</li>
                                        <li>The annotation has a valid scenario.</li>
                                        <li>_parse_response returns a Mock object with the correct scenario and no error.</li>
                                        <li>_call_gemini returns text that matches the expected response format.</li>
                                        <li>The _annotate_internal method calls _parse_response correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">173 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-223, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-343, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_availability</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the availability check of a GeminiProvider instance returns False when no environment variables are set.</p>
                                <p><strong>Why Needed:</strong> This test prevents a scenario where the availability check fails due to missing environment variables, potentially causing unexpected behavior or errors in downstream applications.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider's _check_availability() method should return False for the given configuration.</li>
                                        <li>The provider's _check_availability() method should not throw an exception when no environment variables are set.</li>
                                        <li>The provider's _check_availability() method should correctly handle the case where environment variables are not present but a valid API token is provided.</li>
                                        <li>The provider's _check_availability() method should return True for the given configuration with a valid API token.</li>
                                        <li>The provider's _check_availability() method should throw an exception when no environment variables are set and a valid API token is not provided.</li>
                                        <li>The provider's _check_availability() method should correctly handle the case where environment variables are present but a valid API token is not provided.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 134, 136-139, 141-142, 266-267, 269)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpd_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the rate limiter prevents a request from being blocked after reaching the limit.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where a user's requests are blocked due to exceeding the daily rate limit.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The next_available_in method returns None when the limit has been reached.</li>
                                        <li>The limiter does not block any subsequent requests until the limit is reset.</li>
                                        <li>The limiter allows for at most one request per day, even if there are multiple requests in a short period.</li>
                                        <li>The limiter does not prevent users from making multiple requests within a short time frame.</li>
                                        <li>The limiter resets after each request, allowing for new requests to be made without blocking.</li>
                                        <li>The limiter does not block requests that have already passed the limit.</li>
                                        <li>The limiter allows for partial requests (e.g., 50% of the daily limit) to still be counted towards the total.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 39-42, 45-46, 48-50, 73, 76-78, 81-82, 84, 87-88)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpm_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter does not block requests for a short period after the first two requests.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the rate limiter blocks all subsequent requests for an extended period after the initial two requests, potentially causing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>limiter.next_available_in(100) == 0.0</li>
                                        <li>limiter.record_request()</li>
                                        <li>assert limiter.next_available_in(100) == 0.0</li>
                                        <li>limiter.record_request()</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">27 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-97, 100-102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeConfigHash::test_different_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that different provider configurations result in different hash values.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the same configuration produces the same hash value, potentially due to a bug in the hashing algorithm or incorrect implementation of the Config class.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `compute_config_hash(config)` should return a different hash for two different configurations (`config1` and `config2`).</li>
                                        <li>The values returned by `compute_config_hash(config1)` and `compute_config_hash(config2)` should be distinct.</li>
                                        <li>If the same configuration produces the same hash value, it may indicate an issue with the hashing algorithm or Config class implementation.</li>
                                        <li>A different provider configuration should result in a different hash value for the same configuration.</li>
                                        <li>The test should pass if the function correctly computes the hash of each configuration.</li>
                                        <li>The test should fail if the function incorrectly computes the hash of one or both configurations.</li>
                                        <li>If the test is run multiple times, it should produce different results each time due to the random nature of hashing.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 96-101, 103-104)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeConfigHash::test_returns_short_hash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies the length of the computed hash is exactly 16 characters.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the hash might be too long, potentially causing issues with storage or transmission.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the computed hash should be exactly 16 characters.</li>
                                        <li>The hash value should not exceed 255 characters (the maximum allowed in Python).</li>
                                        <li>The hash value should not contain any non-ASCII characters.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 96-101, 103-104)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeFileSha256::test_consistent_with_bytes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the computed SHA-256 hash of a file matches its content hash.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the file's contents are changed but the file hash remains consistent.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The computed SHA-256 hash of the file should be equal to its content hash.</li>
                                        <li>The content hash calculated from the file's contents should match the expected value.</li>
                                        <li>The file hash should not change even if the file's contents are modified.</li>
                                        <li>The file hash should remain consistent across different runs of the test.</li>
                                        <li>The computed SHA-256 hash of a file with a known content hash should also be equal to that hash.</li>
                                        <li>The content hash calculated from a file with a known content hash should also match the expected value.</li>
                                        <li>The file hash should not change even if multiple files are written to the temporary directory.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 32, 44-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeFileSha256::test_hashes_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the correctness of computing a SHA-256 hash for a file.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues where the computed hash does not match the expected output due to incorrect or missing file contents.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the computed hash should be 64 bytes.</li>
                                        <li>The computed hash should contain all characters from the input data (in this case, 'hello world').</li>
                                        <li>Any non-existent characters in the input data should not affect the computed hash.</li>
                                        <li>The computed hash should match the expected output provided by the `compute_file_sha256` function.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 44-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeHmac::test_different_key</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifying that different keys result in unique HMAC signatures.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential security vulnerabilities where the same key is used for multiple computations, potentially leading to predictable or reproducible signatures.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output of `compute_hmac(b'content', b'key1')` should be different from `compute_hmac(b'content', b'key2')`.</li>
                                        <li>The output of `compute_hmac(b'content', b'key3')` should not match either of the above outputs.</li>
                                        <li>The output of `compute_hmac(b'content', b'key1')` and `compute_hmac(b'content', b'key2')` should be different from each other.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 61)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeHmac::test_with_key</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the length of the HMAC signature is correct.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the HMAC signature might be too short or malformed, potentially causing errors in downstream processing.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the HMAC signature should be exactly 64 bytes.</li>
                                        <li>The HMAC signature should not be shorter than 32 bytes (the minimum required by most cryptographic standards).</li>
                                        <li>The HMAC signature should not exceed 128 bytes (the maximum allowed for most cryptographic applications).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 61)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeSha256::test_consistent</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The function `compute_sha256` is expected to produce the same hash for two identical input strings.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where different inputs could produce different hashes, potentially leading to inconsistent results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>h1 = h2 (assertion of equality)</li>
                                        <li>h1.hex() == h2.hex() (equality of hash values in hexadecimal format)</li>
                                        <li>h1.digest() == h2.digest() (equality of hash values as a bytes object)</li>
                                        <li>compute_sha256(b'salt') != compute_sha256(b'other_salt') (inequality of hashes for different inputs)</li>
                                        <li>compute_sha256(b'test') == b'test' (equality of hash value for the same input string)</li>
                                        <li>compute_sha256(b'test2') != compute_sha256(b'test') (inequality of hashes for different inputs)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeSha256::test_length</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the length of the computed SHA-256 hash is 64 characters.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the hash length may not be as expected, potentially leading to incorrect identification of the input data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the hash should be exactly 64 hexadecimal characters.</li>
                                        <li>The hash string should contain all 256 possible hexadecimal digits (0-9, A-F, a-f).</li>
                                        <li>No padding bytes are present in the hash output.</li>
                                        <li>No leading zeros are present in the hash output.</li>
                                        <li>All characters in the hash output are hexadecimal digits (0-9, A-F, a-f).</li>
                                        <li>The hash is not empty.</li>
                                        <li>The hash does not contain any null bytes.</li>
                                        <li>The hash string contains only one character.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestGetDependencySnapshot::test_includes_pytest</span>
                        <div class="test-meta">
                            <span>81ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `get_dependency_snapshot()` function returns a snapshot including the 'pytest' package.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the 'pytest' package is not included in the dependency snapshot, potentially causing issues with downstream dependencies.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'pytest' package should be present in the snapshot.</li>
                                        <li>The 'pytest' package should be included in the snapshot's list of dependencies.</li>
                                        <li>The snapshot should contain a reference to the 'pytest' package.</li>
                                        <li>The snapshot should include all dependencies required by the 'pytest' package.</li>
                                        <li>The snapshot should not exclude any dependencies that are required by the 'pytest' package.</li>
                                        <li>The snapshot should be able to identify and report on the presence of the 'pytest' package.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 113-114, 116-121)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestGetDependencySnapshot::test_returns_dict</span>
                        <div class="test-meta">
                            <span>82ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `get_dependency_snapshot()` function should return a dictionary.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function returns an incorrect data type (e.g., a list instead of a dictionary).</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>snapshot is an instance of dict</li>
                                        <li>snapshot has no attributes other than __dict__</li>
                                        <li>all keys in snapshot are strings</li>
                                        <li>no missing keys exist in snapshot</li>
                                        <li>len(snapshot) == 0</li>
                                        <li>get_snapshot().keys() == set(['package1', 'package2'])</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 113-114, 116-121)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_loads_key</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `load_hmac_key` function correctly loads a key from a file.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the `load_hmac_key` function fails to load a key from a file due to incorrect file path or permissions.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file path should be correct and accessible.</li>
                                        <li>The file should exist before attempting to load it.</li>
                                        <li>The file should not be empty or contain only whitespace characters.</li>
                                        <li>The `load_hmac_key` function should raise an error if the file is missing or corrupted.</li>
                                        <li>The `load_hmac_key` function should correctly read the key from the file and return it as expected.</li>
                                        <li>The `load_hmac_key` function should not throw any exceptions when loading a non-existent key.</li>
                                        <li>The `load_hmac_key` function should handle file paths with trailing slashes correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 73, 76-77, 80-81)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_missing_key_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case: TestLoadHmacKey::test_missing_key_file verifies that the function returns None when a non-existent key file is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues where the hmac_key_file parameter is not properly validated or checked for existence before attempting to load the HMAC key.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return None if the config object does not contain a 'hmac_key_file' attribute that points to an existing key file.</li>
                                        <li>The function should raise a ValueError with a descriptive message indicating that the key file is nonexistent when the config object does not contain a 'hmac_key_file' attribute.</li>
                                        <li>The function should check if the provided key file exists before attempting to load it using the Config class's hmac_key_file method.</li>
                                        <li>The function should handle the case where the key file is located in a different directory than expected (e.g., /home/user/.nonexistent.key).</li>
                                        <li>The function should not attempt to load an HMAC key from a non-existent or invalid key file, preventing potential security vulnerabilities.</li>
                                        <li>The function should provide informative error messages when the key file is missing or invalid, helping with debugging and troubleshooting.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 73, 76-78)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_no_key_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the function returns None when no key file is configured.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function attempts to load an HMAC key without one being set.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `load_hmac_key` function should return `None` if no key file configuration is provided.</li>
                                        <li>The `Config` class should be able to create a default instance with no key file specified.</li>
                                        <li>The `load_hmac_key` function should raise an error when called without a valid key file configuration.</li>
                                        <li>The `key` variable should not hold any value in this case, as expected.</li>
                                        <li>The test should fail if the `load_hmac_key` function returns something other than `None`.</li>
                                        <li>The `assert` statement should raise an AssertionError with a meaningful message when the condition is not met.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 73-74)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_aggregation_defaults</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that aggregation defaults to sensible settings.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where aggregation is not configured correctly, leading to incorrect data retrieval or analysis.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.aggregate_dir is None</li>
                                        <li>config.aggregate_policy == 'latest'</li>
                                        <li>config.aggregate_include_history is False</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_capture_failed_output_default_false</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the default capture failed output is set to False.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where the default capture failed output is enabled by default.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.capture_failed_output should be False</li>
                                        <li>config.capture_failed_output is not True</li>
                                        <li>get_default_config().capture_failed_output is False</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_context_mode_default_minimal</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the context mode is set to 'minimal' by default.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the context mode is not set to 'minimal' when no specific configuration is provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.llm_context_mode == 'minimal'</li>
                                        <li>get_default_config().llm_context_mode == 'minimal'</li>
                                        <li>assert isinstance(config, dict) and config.get('context_mode') == 'minimal'</li>
                                        <li>assert get_default_config().context_mode == 'minimal'</li>
                                        <li>assert get_default_config().context_mode != 'none' or get_default_config().context_mode != 'default'</li>
                                        <li>get_default_config().context_mode is not None</li>
                                        <li>get_default_config().context_mode != 'none'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_llm_not_enabled_by_default</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that LLM is disabled by default in the configuration.</p>
                                <p><strong>Why Needed:</strong> Prevent regression where LLM is enabled by default.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `is_llm_enabled()` method returns False for the default config.</li>
                                        <li>The `is_llm_enabled()` method should return True if LLM is not enabled by default.</li>
                                        <li>The configuration does not have a default value for `llm_enabled` set to False.</li>
                                        <li>The `get_default_config()` function returns an instance with a default value of False for `llm_enabled`.</li>
                                        <li>The `is_llm_enabled()` method should raise an exception if the config is invalid or missing.</li>
                                        <li>The test case should fail when LLM is enabled by default in the configuration.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 107, 147, 224, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_omit_tests_default_true</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `TestConfigDefaults` class correctly sets `omit_tests_from_coverage` to `True` when `default` is set to `True`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the default behavior of omitting tests from coverage is not implemented correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `TestConfigDefaults` class should set `omit_tests_from_coverage` to `True` when `default` is `True`.</li>
                                        <li>The value of `omit_tests_from_coverage` should be `True` for the given configuration.</li>
                                        <li>The test should not omit tests from coverage by default, even if `default` is `True`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_provider_default_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the default provider setting when it is set to None.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the provider is not set to 'none' by default.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function get_default_config() returns an instance of Config with a provider attribute equal to 'none'.</li>
                                        <li>The config.provider property is accessed and compared to 'none'.</li>
                                        <li>An assertion error occurs if the config.provider is not 'none'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_secret_exclude_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that secret files are excluded by default from the LLM context.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where secret files might be inadvertently included in the LLM context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `llm_context_exclude_globs` configuration option is set to exclude 'secret' files.</li>
                                        <li>The `llm_context_exclude_globs` configuration option is set to exclude '.env' files.</li>
                                        <li>Any secret file names are present in the list of excluded globs.</li>
                                        <li>No secret file names are present in the list of excluded globs.</li>
                                        <li>The `llm_context_exclude_globs` configuration option is correctly set for LLM context.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_deterministic_output</span>
                        <div class="test-meta">
                            <span>7ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test reports are deterministic (sorted by nodeid) and the output is correct.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the order of reported tests changes due to changes in the `report_json` configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `nodeids` list returned from `data['tests']` should be sorted in ascending order.</li>
                                        <li>Each `testid` in `data['tests']` should have a corresponding `nodeid` in the same order.</li>
                                        <li>All `nodeids` in `data['tests']` should be present in the output report.</li>
                                        <li>The number of unique `nodeids` in `data['tests']` should match the total number of tests.</li>
                                        <li>Each test result should have a corresponding `testid` and `nodeid` pair.</li>
                                        <li>All test results should have a valid outcome ('passed' or 'failed').</li>
                                        <li>The output report should contain all reported tests, sorted by nodeid.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">78 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">117 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_empty_test_suite</span>
                        <div class="test-meta">
                            <span>6ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that an empty test suite produces a valid report.</p>
                                <p><strong>Why Needed:</strong> To prevent a regression where the test suite fails when there are no tests to run.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total count of tests in the report is zero.</li>
                                        <li>There is no summary data in the report.</li>
                                        <li>No error messages or warnings are present in the report.</li>
                                        <li>The report does not contain any failed tests.</li>
                                        <li>All test suites have been successfully written to the report.</li>
                                        <li>The report contains a valid JSON structure with the expected keys and values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_html_report_generation</span>
                        <div class="test-meta">
                            <span>31ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that a full pipeline generates an HTML report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the HTML report is not generated correctly due to a bug in the ReportWriter class.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file "report.html" exists at the specified path.</li>
                                        <li>The string '<html' is present in the content of the 'report.html' file.</li>
                                        <li>The keyword 'test_pass' is found in the content of the 'report.html' file.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">113 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_json_report_generation</span>
                        <div class="test-meta">
                            <span>54ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the full pipeline generates a valid JSON report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the integration gate, where it was previously possible to generate invalid or incomplete JSON reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The report is generated with a valid schema version.</li>
                                        <li>The total count of tests is accurate (3 in this case).</li>
                                        <li>At least one test passed and at least one failed.</li>
                                        <li>At least one test was skipped.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/_git_info.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 2-3)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">78 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">133 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-322, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_report_root_has_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the ReportRoot has required fields when created with valid schema version, run meta and summary.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where a missing or invalid report root is created without specifying the required fields.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'schema_version' field should be present in the data.</li>
                                        <li>The 'run_meta' field should be present in the data.</li>
                                        <li>The 'summary' field should be present in the data.</li>
                                        <li>The 'tests' field should be present in the data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_aggregation_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'RunMeta has aggregation fields' verifies that the test runs metadata includes aggregation fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring RunMeta includes aggregation fields in its metadata.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>is_aggregated is present in data</li>
                                        <li>run_count is present in data</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_status_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'RunMeta has run status fields' verifies that the RunMeta object contains the necessary status fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the RunMeta object is missing or incorrectly configured status fields, potentially leading to incorrect interpretation of run metadata.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'exit_code' field is present in the data.</li>
                                        <li>The 'interrupted' field is present in the data.</li>
                                        <li>The 'collect_only' field is present in the data.</li>
                                        <li>The 'collected_count' field is present in the data.</li>
                                        <li>The 'selected_count' field is present in the data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_schema_version_defined</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the schema version is correctly defined and contains a dot (.) separating major and minor versions.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the schema version is not properly defined or does not contain a valid dot separation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>SCHEMA_VERSION should be set to a string value.</li>
                                        <li>SCHEMA_VERSION should contain at least one dot (.) separating major and minor versions.</li>
                                        <li>The presence of leading dots in SCHEMA_VERSION is not allowed.</li>
                                        <li>The presence of trailing dots in SCHEMA_VERSION is not allowed.</li>
                                        <li>The schema version should be a valid semver-like string.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_test_case_has_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `TestSchemaCompatibility` function verifies that the `TestCaseResult` object has required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `TestCaseResult` object is missing required fields, potentially leading to incorrect results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>nodeid should be present in the `data` dictionary</li>
                                        <li>outcome should be present in the `data` dictionary</li>
                                        <li>duration should be present in the `data` dictionary</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_gemini_returns_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_provider` function returns a `GeminiProvider` instance when the `provider` parameter is set to 'gemini'.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `get_provider` function incorrectly returns a different provider type when the `provider` parameter is not 'gemini'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `__class__.__name__` attribute of the returned `GeminiProvider` instance should be equal to `'GeminiProvider'`.</li>
                                        <li>The `get_provider` function should return a `GeminiProvider` instance when the `provider` parameter is set to 'gemini'.</li>
                                        <li>The `get_provider` function should raise an error when the `provider` parameter is not set to 'gemini'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 245, 247, 249, 252, 257, 262-263, 265)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_litellm_returns_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `get_provider` function returns a correct LiteLLMProvider instance when the provider is set to 'litellm'.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the LLM model is not correctly identified as 'LiteLLMProvider' even if it's being used with the correct provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider.__class__.__name__ should be equal to 'LiteLLMProvider'.</li>
                                        <li>get_provider(config).model == 'gpt-3.5-turbo'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52-53, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_none_returns_noop</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> test_get_provider_with_none_provider returns NoopProvider.</p>
                                <p><strong>Why Needed:</strong> The test prevents a potential bug where the LLM is not properly initialized with a None provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider should be None</li>
                                        <li>should return NoopProvider instance</li>
                                        <li>should have no dependencies</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_ollama_returns_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that when using 'ollama' as a provider, OllamaProvider is returned.</p>
                                <p><strong>Why Needed:</strong> This test prevents the regression of the previous one where 'ollama' was not correctly identified as an OllamaProvider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider.__class__.__name__ should be equal to 'OllamaProvider'.</li>
                                        <li>The provider instance is an instance of OllamaProvider.</li>
                                        <li>The provider's class name matches the expected one (OllamaProvider).</li>
                                        <li>The provider is not None or empty.</li>
                                        <li>The provider has a valid model attribute.</li>
                                        <li>The provider does not have any invalid attributes.</li>
                                        <li>The provider does not raise any exceptions during initialization.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 245, 247, 249, 252-253, 255)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_unknown_raises</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case: Unknown provider raises ValueError when trying to retrieve a provider.</p>
                                <p><strong>Why Needed:</strong> This test prevents the unknown provider from being used without proper configuration, ensuring that a ValueError is raised with a descriptive error message.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider` should throw a ValueError when an unknown provider is specified.</li>
                                        <li>The error message should contain the string 'unknown'.</li>
                                        <li>The error message should be in lowercase to ensure case-insensitive matching.</li>
                                        <li>The error message should include the word 'unknown' explicitly.</li>
                                        <li>The test should fail with a ValueError exception when the `get_provider` function is called with an unknown provider.</li>
                                        <li>The test should not pass if the `get_provider` function returns an unknown provider without raising a ValueError.</li>
                                        <li>The test should verify that the error message is not empty and does not contain any other information.</li>
                                        <li>The test should be able to reproduce the issue on multiple runs of the test suite.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 245, 247, 249, 252, 257, 262, 267)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestLlmProviderContract::test_noop_implements_interface</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that NoopProvider implements LlmProvider contract.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in LlmProvider implementation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider should have the required methods: annotate, is_available, get_model_name, and config.</li>
                                        <li>The provider should not raise an exception when these methods are called.</li>
                                        <li>The provider should be able to access its configuration.</li>
                                        <li>The provider should be able to call its annotation method.</li>
                                        <li>The provider should be able to check if it's available.</li>
                                        <li>The provider should have a valid model name.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestNoopProvider::test_annotate_returns_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the annotate method returns an empty LlmAnnotation object when no annotation is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the NoopProvider does not return any annotation even if it has been configured with a config.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>annotation should be of type LlmAnnotation</li>
                                        <li>annotation scenario should be an empty string</li>
                                        <li>annotation why needed should be an empty string</li>
                                        <li>annotation key assertions should be an empty list</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestNoopProvider::test_get_model_name_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_model_name` method of the `NoopProvider` class returns an empty string when no model is specified.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the `get_model_name` method would return a non-empty string for an empty input configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_model_name()` method should return an empty string.</li>
                                        <li>The `get_model_name()` method should not throw any exceptions.</li>
                                        <li>The `get_model_name()` method should be able to handle the case where no model is specified in the configuration.</li>
                                        <li>The `get_model_name()` method should have a clear and consistent naming convention.</li>
                                        <li>The `get_model_name()` method should not modify the input configuration in any way.</li>
                                        <li>The `get_model_name()` method should return a string that accurately reflects the absence of a model name.</li>
                                        <li>The `get_model_name()` method should be able to handle different types of configurations (e.g. None, dict, etc.)</li>
                                        <li>The `get_model_name()` method should not throw any errors when given invalid input</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 66)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestNoopProvider::test_is_available</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The NoopProvider should always be available in the tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the NoopProvider might not be available due to an issue with the configuration or dependencies.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider.is_available() must return True</li>
                                        <li>config is not None</li>
                                        <li>provider is an instance of NoopProvider</li>
                                        <li>assert provider.is_available() is True</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 107, 110-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 58)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_emits_summary</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the annotation summary is printed when annotations run.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the annotation summary is not printed, potentially causing confusion or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `test_annotate_tests_emits_summary` in `tests/test_llm_annotator.py` should print 'Annotated 1 test(s) via litellm' to the console.</li>
                                        <li>The function `test_annotate_tests_emits_summary` in `tests/test_llm_annotator.py` should capture and verify that this message is printed through the `capsys.readouterr()` method.</li>
                                        <li>The function `test_annotate_tests_emits_summary` in `tests/test_llm_annotator.py` should not fail or raise an exception if the annotation summary is not printed, ensuring test reliability.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_reports_progress</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the progress reporting callback is called for each test.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the progress reporting callback might not be called for all tests, potentially causing issues with the overall test suite.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The expected message should contain the correct information about the starting LLM annotation process.</li>
                                        <li>The expected message should include the name of the test being annotated.</li>
                                        <li>The expected message should display the progress report for a single test.</li>
                                        <li>The progress report should indicate that all tests have been successfully annotated.</li>
                                        <li>The progress report should not be empty after annotating all tests.</li>
                                        <li>The progress report should contain the correct number of annotations (1 in this case).</li>
                                        <li>The progress report should include the name of the test being annotated.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_respects_opt_out_and_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM annotations respect opt-out and limit settings.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring LLM annotations do not skip opt-out tests or exceed the maximum allowed number of tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that only 'tests/test_a.py::test_a' is called when opt-out is enabled.</li>
                                        <li>Verify that no annotation is made for 'tests/test_b.py::test_b' with LLM_opt_out=True.</li>
                                        <li>Verify that the LLM annotator does not make any annotations for 'tests/test_c.py::test_c'.</li>
                                        <li>Verify that the number of tests annotated does not exceed 1 (LLM_max_tests=1).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 45, 48-49, 56-57, 59, 61-62, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_respects_rate_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM annotations respect the requests-per-minute rate limit.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the LLM annotator does not respect the requests-per-minute rate limit, leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider's calls should match the expected list of node IDs.</li>
                                        <li>The sleep function should be called twice with times equal to 2.0 seconds.</li>
                                        <li>The sleep function should not be called more than once per minute.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-173, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_skips_unavailable_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that annotation skips unavailable providers with a clear message.</p>
                                <p><strong>Why Needed:</strong> To prevent the annotation of tests from failing when an unavailable provider is detected.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test verifies that the annotation function skips the annotation process for unavailable providers.</li>
                                        <li>The test verifies that the annotation function returns a message indicating that the provider is not available.</li>
                                        <li>The test verifies that the captured output includes the message 'is not available' when the annotation process fails due to an unavailable provider.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_uses_cache</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the use of cache in annotations.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the annotation process relies on a cached provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` attribute of the `TestCaseResult` object is set to `tests/test_sample.py::test_case` after calling `annotate_tests`.</li>
                                        <li>The `llm_annotation` attribute of the `TestCaseResult` object is not `None` after calling `annotate_tests`.</li>
                                        <li>The scenario of the annotation process is still 'cached' even after calling `annotate_tests`.</li>
                                        <li>The provider is called when it should not be, indicating a regression in the cache behavior.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">30 lines (ranges: 39-41, 53, 55-56, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-81, 87-92, 97-98, 100, 102, 104, 115-122, 127, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the schema requires both 'scenario' and 'why_needed' fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the schema is not enforced, allowing for invalid data to be accepted.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert 'scenario' in required</li>
                                        <li>assert 'why_needed' in required</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_from_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that AnnotationSchema.from_dict parses from a dictionary correctly.</p>
                                <p><strong>Why Needed:</strong> Prevents data tampering by ensuring the correct parsing of schema fields from a dictionary.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>checks password</li>
                                        <li>checks username</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the AnnotationSchema handles an empty input correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the AnnotationSchema may not be able to parse or validate an empty input.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>schema.scenario = ""</li>
                                        <li>schema.why_needed = ""</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_partial</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the AnnotationSchema.from_dict method correctly handles a partial input scenario.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the AnnotationSchema.from_dict method does not handle partial inputs correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>schema.scenario == 'Partial only'</li>
                                        <li>schema.why_needed == ''</li>
                                        <li>assert schema.scenario == 'Partial only' (to verify correct handling of partial input)</li>
                                        <li>assert schema.why_needed == '' (to verify absence of regression)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_has_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the schema has required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the schema is not properly defined with required fields, potentially leading to invalid or missing data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert 'scenario' in ANNOTATION_JSON_SCHEMA['properties'],</li>
                                        <li>assert 'why_needed' in ANNOTATION_JSON_SCHEMA['properties'],</li>
                                        <li>assert 'key_assertions' in ANNOTATION_JSON_SCHEMA['properties']</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> TestAnnotationSchema::test_schema_to_dict verifies that the AnnotationSchema instance is correctly serialized to a dictionary.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the AnnotationSchema instance can be converted into a dictionary without losing any critical information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assertion 1: The 'scenario' key in the dictionary matches the expected value.</li>
                                        <li>assertion 2: The 'why_needed' key in the dictionary matches the expected value.</li>
                                        <li>assertion 3: The 'key_assertions' list is present in the dictionary and contains all the expected values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 90-92, 94-96, 98)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_from_factory</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the factory method to return a NoopProvider when the provider is set to 'none'.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where a NoopProvider is returned for an invalid provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` attribute of the `NoopProvider` instance should be `None`.</li>
                                        <li>The `get_provider` function should return an instance of `Config` with the correct provider set to `'none'`.</li>
                                        <li>The `isinstance(provider, NoopProvider)` assertion should pass for a `Config` object with the specified provider.</li>
                                        <li>The `provider` attribute of the returned `NoopProvider` instance should be `None`, not an instance of `NoopProvider`.</li>
                                        <li>The `get_provider` function should return an instance of `Config` with the correct provider set to `'none'` and a valid configuration.</li>
                                        <li>The `isinstance(get_provider(config), NoopProvider)` assertion should pass for a valid `Config` object with the specified provider.</li>
                                        <li>The `provider` attribute of the returned `NoopProvider` instance should be `None`, not an instance of `NoopProvider`.</li>
                                        <li>The `get_provider` function should return an instance of `Config` with the correct provider set to `'none'` and a valid configuration.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_is_llm_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `NoopProvider` class is correctly instantiated as an instance of `LlmProvider`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `NoopProvider` class might be incorrectly instantiated or not properly configured to implement the `LlmProvider` interface.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` variable is assigned an instance of `Config`, which is expected to be used in conjunction with the `NoopProvider` class to create a valid LLM provider.</li>
                                        <li>The `provider` variable is assigned an instance of `LlmProvider`, which is the correct implementation of the interface.</li>
                                        <li>The `provider` variable has the correct type attribute set to `LLMProvider`.</li>
                                        <li>The `provider` variable does not have any additional attributes or methods that would prevent it from being used as a valid LLM provider.</li>
                                        <li>The `provider` variable does not have any invalid attributes or properties that could cause issues with its functionality.</li>
                                        <li>The `provider` variable is properly initialized and configured before use in the test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_returns_empty_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The NoopProvider returns an empty annotation when the test function does not contain any annotations.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the NoopProvider's default behavior of returning an empty annotation is not applied in cases where the test function itself does not contain any annotations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>- The `annotate` method returns an empty annotation for a test that does not have any annotations.</li>
                                        <li>- The `annotate` method should ideally return a meaningful annotation or error message when the test function contains no annotations.</li>
                                        <li>- The test verifies that the `annotate` method correctly handles cases where the test function is empty.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_annotate_returns_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `annotate` method of the `ProviderContract` class returns an instance of `TestCaseResult` with the expected attributes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the `annotate` method does not return a valid `TestCaseResult` object, potentially causing issues downstream in the testing pipeline.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `scenario` attribute is present and has the correct value.</li>
                                        <li>The `why_needed` attribute is present and has the correct value.</li>
                                        <li>The `key_assertions` list contains all expected assertions.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_handles_empty_code</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Provider handles empty code with an empty contract.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider does not handle cases with an empty contract.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider.annotate` method should return a non-None result for an empty contract.</li>
                                        <li>The `provider.annotate` method should set the `outcome` to 'passed' for an empty contract.</li>
                                        <li>An empty contract should be annotated correctly by the provider.</li>
                                        <li>A test with an empty code should pass without raising any errors or exceptions.</li>
                                        <li>No error message should be raised when annotating an empty contract.</li>
                                        <li>The provider's annotation process should not fail due to an empty contract.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_handles_none_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `provider` annotates a `TestCaseResult` with `None` when passed `None` as context.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider does not handle cases with `None` context correctly, leading to incorrect results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider.annotate` method should return `None` when passed `None` as context.</li>
                                        <li>The `TestCaseResult` nodeid and outcome should be unchanged in this case.</li>
                                        <li>No exception should be raised when passing `None` as context to the `provider.annotate` method.</li>
                                        <li>The provider's annotation of the `TestCaseResult` should not affect its internal state or behavior.</li>
                                        <li>The test result should still pass even if the `provider` annotates an empty string instead of `None`.</li>
                                        <li>No error message should be printed when passing `None` as context to the `provider.annotate` method.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_has_annotate_method</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> All providers should have an annotate method.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the LLM contract does not provide an annotate method for all providers.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider_name in ['none', 'ollama', 'litellm', 'gemini']</li>
                                        <li>hasattr(provider, 'annotate')</li>
                                        <li>callable(provider.annotate)</li>
                                        <li>provider_name is equal to 'none'</li>
                                        <li>provider_name is equal to 'ollama'</li>
                                        <li>provider_name is equal to 'litellm'</li>
                                        <li>provider_name is equal to 'gemini'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 52-53, 245, 247, 249-250, 252-253, 255, 257-258, 260, 262-263, 265)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_handles_context_too_large</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `annotate` method of the `GeminiProvider` class is called with a context that exceeds its capacity.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the `annotate` method might exceed its memory limit, leading to performance degradation or crashes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `context` attribute of the `GeminiProvider` instance should not be larger than the maximum allowed size.</li>
                                        <li>The `annotation` function should not be called with an argument that is too large to fit in memory.</li>
                                        <li>The `annotate` method should raise a `MemoryError` exception when attempting to annotate a context that exceeds its capacity.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52-53, 72, 75-76, 80, 165, 167, 175)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">155 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-221, 233, 245-248, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417-418, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_dependency</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The LiteLLM provider should report a missing dependency when the 'litellm' package is not installed.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the provider incorrectly reports dependencies as installed when they are actually missing.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation error message should include the name of the missing dependency, which in this case is 'litellm'.</li>
                                        <li>The annotation error message should be informative and provide instructions on how to install the required package.</li>
                                        <li>The test case should fail when the 'litellm' package is not installed, but pass otherwise.</li>
                                        <li>The provider's behavior should change if the 'litellm' package is installed correctly, reporting an error instead of a success message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 134, 136-139, 141-142, 160-164)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_token</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `annotate` method throws an error when a missing API token is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the `GeminiProvider` class does not raise an error when an API token is missing, potentially leading to unexpected behavior or errors in downstream code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `error` attribute of the annotation object should be equal to 'GEMINI_API_TOKEN is not set'.</li>
                                        <li>The `error` attribute of the annotation object should contain the string 'GEMINI_API_TOKEN is not set'.</li>
                                        <li>The `error` attribute of the annotation object should be a string.</li>
                                        <li>The `error` attribute of the annotation object should have the value 'GEMINI_API_TOKEN is not set'.</li>
                                        <li>The `error` attribute of the annotation object should contain the exact phrase 'GEMINI_API_TOKEN is not set'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 134, 136-139, 141-142, 160-161, 167-169)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_records_tokens</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that tokens recorded on the limiter are correctly annotated with usage metadata.</p>
                                <p><strong>Why Needed:</strong> Prevents regressions where token usage is not properly reported.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'totalTokenCount' in the response data should be equal to 123.</li>
                                        <li>The 'candidates' list should contain a single object with 'text' as its value.</li>
                                        <li>The 'usageMetadata' dictionary should contain a key named 'totalTokenCount'.</li>
                                        <li>The 'usageMetadata' dictionary should not be empty.</li>
                                        <li>The 'tokenUsage' list within the 'usageMetadata' dictionary should have exactly one element containing an integer value of 123.</li>
                                        <li>The 'candidates' list within the 'usageMetadata' dictionary should contain a single object with 'text' as its value and an integer value of 123.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">183 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-223, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-343, 346, 348-350, 352-355, 360-366, 368, 370-371, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_retries_on_rate_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `annotate` method of the `GeminiProvider` class should retry when rate limiting occurs.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the `annotate` method fails due to rate limiting and does not retry, leading to unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `annotate` method should call `self._retry_on_rate_limit` before attempting to annotate.</li>
                                        <li>The `annotate` method should attempt to annotate again after a successful retry.</li>
                                        <li>The `_retry_on_rate_limit` method should be called with the correct arguments (e.g., `max_retries`, `rate_limit`, etc.)</li>
                                        <li>The `annotate` method should not raise an exception when rate limiting occurs, but instead retry the annotation operation</li>
                                        <li>The number of retries performed by the `annotate` method should increase accordingly as it attempts to annotate again after a successful retry</li>
                                        <li>The `_retry_on_rate_limit` method should be able to handle different types of rate limits (e.g., fixed, exponential)</li>
                                        <li>The `annotate` method should not fail unexpectedly when rate limiting occurs and retries are attempted</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">181 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 233-234, 238-240, 242-243, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330-333, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-369, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_rotates_models_on_daily_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `annotate` method of the `GeminiProvider` class rotates models on the daily limit when called with a large number of annotations.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the model rotation may not occur correctly if the number of annotations is too high, leading to inconsistent results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `annotate` method should rotate all models on the daily limit when called with a large number of annotations.</li>
                                        <li>All models should be rotated after the specified number of annotations.</li>
                                        <li>No exceptions should be raised if the number of annotations is too high.</li>
                                        <li>The rotation of models should occur immediately after calling the `annotate` method.</li>
                                        <li>The rotation of models should not affect the performance of other methods in the class.</li>
                                        <li>The rotation of models should be consistent across different test runs with the same input data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">177 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419-420, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_skips_on_daily_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `annotate` method skips daily limits when used with a Gemini provider.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the `annotate` method would incorrectly skip daily limits due to an incorrect implementation of the `GeminiProvider` class.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `annotate` method should not be called when the daily limit has been reached.</li>
                                        <li>The `annotate` method should return a specific error message indicating that daily limits have been exceeded.</li>
                                        <li>The `annotate` method should skip any subsequent calls to `annotate` for the same provider instance within a short period (e.g., 1 hour).</li>
                                        <li>The `annotate` method should not be called on providers with a 'daily_limit' key set to 0.</li>
                                        <li>The `annotate` method should raise an exception when called on providers with a 'daily_limit' key set to a negative value or zero.</li>
                                        <li>The `annotate` method should only skip calls for the same provider instance within a short period (e.g., 1 hour).</li>
                                        <li>The `annotate` method should not be called on providers that are being throttled or have their daily limit exceeded due to other factors.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">184 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-208, 210-211, 213-214, 217-222, 225-227, 252-254, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_success_with_mock_response</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the annotate method correctly annotates a successful response from LiteLLMProvider with mock response data.</p>
                                <p><strong>Why Needed:</strong> Prevents regressions by ensuring that LiteLLMProvider returns a valid annotation even when it encounters a mock response.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>status ok</li>
                                        <li>redirect</li>
                                        <li>model gpt-4o</li>
                                        <li>content tests/test_auth.py::test_login</li>
                                        <li>def test_login()</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">177 lines (ranges: 39-42, 45-46, 48-49, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_exhausted_model_recovers_after_24h</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the exhausted model recovers after 24 hours.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the model does not recover from exhaustion within 24 hours.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The model's performance should improve over time, and it should be able to handle a large number of queries without significant degradation.</li>
                                        <li>The model's memory usage should decrease after each query, indicating that it is recovering from exhaustion.</li>
                                        <li>The model's latency should decrease as the number of queries increases, further confirming recovery.</li>
                                        <li>The model's error rate should remain low or even decrease over time, suggesting a healthy recovery process.</li>
                                        <li>The model's ability to handle a large number of queries without significant degradation should be demonstrated through various metrics such as throughput and memory usage.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">190 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-188, 190-191, 193-194, 196, 200-208, 210-211, 213-214, 217-222, 225-227, 252-254, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_fetch_available_models_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `fetch_available_models` method of the `GeminiProvider` class raises an error when no models are available.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `fetch_available_models` method returns an error instead of raising one when there are no available models.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assertRaisesError: The `fetch_available_models` method should raise an error when no models are available.</li>
                                        <li>assertRaisesError: The `fetch_available_models` method should not return any value (i.e., it should raise an exception).</li>
                                        <li>assertRaisesValueError: The `fetch_available_models` method should raise a ValueError with the message 'No models available'.</li>
                                        <li>assertRaisesValueError: The `fetch_available_models` method should raise a ValueError with the message 'No models available' when no models are available.</li>
                                        <li>assertRaisesValueError: The `fetch_available_models` method should not return any value (i.e., it should raise an exception).</li>
                                        <li>assertRaisesValueError: The `fetch_available_models` method should raise a ValueError with the message 'No models available'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 134, 136-139, 141-142, 280, 282-283, 286-290, 292-295, 297-298, 300-301, 346, 348-350, 352-355, 360-363, 374-377, 385, 387, 391-392, 396-402, 405, 408-410, 412-414, 417-418, 428, 430-432, 435-436)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_model_list_refreshes_after_interval</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The model list should refresh after an interval of time (e.g. seconds) when the LLM provider is updated.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the model list does not update correctly after the LLM provider has been updated.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `refresh_model_list` method should be called with an interval of time (e.g. seconds) as its argument.</li>
                                        <li>The `refresh_model_list` method should return a new list of models without modifying the existing one.</li>
                                        <li>The number of models in the list should decrease by 1 after each update.</li>
                                        <li>The model list should contain only the updated models.</li>
                                        <li>The model list should not contain any deleted models.</li>
                                        <li>The LLM provider's `refresh_interval` attribute should be set to a valid value (e.g. 60 seconds).</li>
                                        <li>The `refresh_model_list` method should call the `update_models` method of the LLM provider with the correct arguments.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">169 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-369, 374-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_handles_completion_error</span>
                        <div class="test-meta">
                            <span>90.00s</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the LiteLLMProvider annotates completion errors with a specific error message.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the LiteLLM provider surfaces completion errors correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation should contain the 'boom' error message.</li>
                                        <li>The annotation should indicate that the function `test_case` was annotated with an error.</li>
                                        <li>The annotation should include the specific error message 'boom'.</li>
                                        <li>The annotation should not be empty or None.</li>
                                        <li>The annotation should have a non-empty string value for the 'error' key.</li>
                                        <li>The annotation should contain the correct type hint for the function `test_case`.</li>
                                        <li>The annotation should indicate that the function `test_case` was annotated with an error.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 78-79, 81-82, 84-85)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_invalid_key_assertions</span>
                        <div class="test-meta">
                            <span>90.00s</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LiteLLMProvider rejects invalid key_assertions payloads.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the provider incorrectly accepts non-list responses for key_assertions.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>response_data must be a list</li>
                                        <li>response_data must contain at least one 'key_assertion' item</li>
                                        <li>response_data must not contain any 'key_assertion' items with invalid types (e.g., string, dictionary)</li>
                                        <li>response_data must not contain any 'key_assertion' items with missing keys (e.g., no 'key' or 'assertion' key)</li>
                                        <li>response_data must not contain any 'key_assertion' items with duplicate keys</li>
                                        <li>response_data must not contain any 'key_assertion' items with invalid values (e.g., non-string, non-integer)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207-209)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69, 73, 76, 81-82, 84-85)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_missing_dependency</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the LiteLLMProvider annotates a missing dependency correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the provider does not report an error for missing dependencies.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation includes the correct error message indicating that 'litellm' is missing and how to install it.</li>
                                        <li>The annotation includes the correct path to the installation command.</li>
                                        <li>The annotation includes the correct dependency name ('litellm').</li>
                                        <li>The annotation does not report an error for a non-existent dependency.</li>
                                        <li>The annotation reports the correct version of 'litellm' if available.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 37-41)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_success_with_mock_response</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the LiteLLM provider annotates a successful response with mock data.</p>
                                <p><strong>Why Needed:</strong> Prevents regressions by verifying that the provider correctly handles successful responses.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>status ok</li>
                                        <li>redirect</li>
                                        <li>model gpt-4o</li>
                                        <li>tests/test_auth.py::test_login in messages</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69-70)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_is_available_with_module</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the LiteLLM provider correctly detects the installed 'litellm' module.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider does not detect the installed module.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `provider.is_available()` should return True when the 'litellm' module is detected.</li>
                                        <li>The function `provider.is_available()` should raise an exception if the 'litellm' module is not detected.</li>
                                        <li>The function `provider.is_available()` should correctly handle cases where the 'litellm' module is installed but its path is incorrect.</li>
                                        <li>The function `provider.is_available()` should correctly handle cases where the 'litellm' module is installed but it does not have a valid import statement.</li>
                                        <li>The function `provider.is_available()` should raise an exception if the 'litellm' module has been removed from the system.</li>
                                        <li>The function `provider.is_available()` should correctly detect the 'litellm' module in Python 3.8 and later versions.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 107, 110-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 94-95, 97)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_fallbacks_on_context_length_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `test_annotate_fallbacks_on_context_length_error` test verifies that the LLM provider annotates fallbacks correctly when there is a context length error.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the LLM provider's behavior when encountering a context length error, ensuring consistent output for similar inputs.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `annotate_fallback` method returns an empty list when the input has no annotations.</li>
                                        <li>The `annotate` method returns an empty list when the input has no annotations and is not empty.</li>
                                        <li>The `annotate_fallbacks_on_context_length_error` test verifies that the LLM provider correctly annotates fallbacks in this scenario.</li>
                                        <li>The output of the `annotate` method for a context length error should be consistent across different inputs.</li>
                                        <li>The output of the `annotate_fallbacks_on_context_length_error` test should not contain any unexpected annotations.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">33 lines (ranges: 52-53, 72, 75-76, 78, 165, 167-173, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-60, 62-63, 66-67)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_handles_call_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test OllamaProvider::test_annotate_handles_call_error verifies that the annotate method returns an error message when a call to _call_ollama raises a RuntimeError.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the annotate method fails with an unexpected error message when a call to _call_ollama raises a RuntimeError.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation should return 'Failed after 10 retries. Last error: boom' as the error message.</li>
                                        <li>The annotation should not return any other error message or exception.</li>
                                        <li>The annotation should be able to handle multiple retries without changing the error message.</li>
                                        <li>The annotation should ignore the original call and only report the last error.</li>
                                        <li>The annotation should not raise an AssertionError when a call to _call_ollama raises a RuntimeError.</li>
                                        <li>The annotation should preserve the original system prompt.</li>
                                        <li>The annotation should be able to handle different types of errors (e.g., ConnectionError, TimeoutError).</li>
                                        <li>The annotation should not change the outcome of the test (i.e., it should still pass or fail based on the original call to _call_ollama)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-59, 71-72, 74-75, 77-78)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_missing_httpx</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The Ollama provider should report an error when annotating a test where the httpx dependency is missing.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider incorrectly reports that httpx is installed when it's not, potentially leading to incorrect configuration or errors in downstream tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation should include an error message indicating that httpx is not installed.</li>
                                        <li>The error message should be specific to the missing dependency (httpx) and not generic.</li>
                                        <li>The provider should correctly report the missing dependency, rather than a misleading installation instruction.</li>
                                        <li>The test case should pass with this corrected annotation.</li>
                                        <li>The configuration of the OllamaProvider should be updated accordingly to reflect the correct state of dependencies.</li>
                                        <li>The error message should be logged by the provider and propagated to the user through the CaseResult object.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 40-44)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_success_full_flow</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the Ollama provider's full annotation flow with mocked HTTP.</p>
                                <p><strong>Why Needed:</strong> Prevents authentication bugs by ensuring correct response from the API.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Check status of the response</li>
                                        <li>Validate token in the response</li>
                                        <li>Verify that the response is not empty or None</li>
                                        <li>Assert that the response contains a specific JSON structure</li>
                                        <li>Check for any errors in the response</li>
                                        <li>Ensure that the response matches the expected model structure</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-60, 62-63, 114, 116-123, 127-130, 132, 134-135)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_success</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Ollama provider makes correct API call when calling OLLAMA successfully.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the OLLAMA provider fails to make a successful API call.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'url' captured is set to the correct URL for the OLLAMA API call.</li>
                                        <li>The 'json' captured contains the expected response from the OLLAMA API.</li>
                                        <li>The 'timeout' captured matches the specified timeout value.</li>
                                        <li>The provider correctly calls the OLLAMA model with the provided parameters.</li>
                                        <li>The generated text is not empty and does not contain any errors.</li>
                                        <li>The stream parameter is set to False, indicating that no output will be produced.</li>
                                        <li>The API call has a successful status code (200).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 114, 116-123, 127-130, 132, 134-135)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_uses_default_model</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the default model is used when not specified for Ollama provider.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the default model is not used, potentially causing unexpected behavior in downstream applications.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `model` attribute of the captured response is set to 'llama3.2' (the default model).</li>
                                        <li>The `json()` method of the captured response returns a dictionary with a single key-value pair: `{'response': 'ok'}`.</li>
                                        <li>The `httpx.post()` function from the `FakeResponse` class returns an instance of `FakeResponse` instead of a valid HTTP request object.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 114, 116-123, 127-130, 132, 134-135)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the Ollama provider returns False when the server is unavailable.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the provider incorrectly assumes the server is available even if it's not.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The method _check_availability() of the OllamaProvider class should return False when the get() function raises a ConnectionError.</li>
                                        <li>The method _check_availability() of the OllamaProvider class should raise an AssertionError when the get() function raises a ConnectionError.</li>
                                        <li>The provider should not attempt to make requests to the server even if it's unavailable, as this would cause unexpected behavior.</li>
                                        <li>The provider should correctly handle the case where the server is down and return False without attempting to make any requests.</li>
                                        <li>The provider should raise an exception when the get() function raises a ConnectionError, rather than simply returning False.</li>
                                        <li>The provider should not silently fail or crash if it encounters a ConnectionError, but instead raise an exception that can be caught and handled by the test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 87-88, 90-91, 93-94)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_non_200</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the Ollama provider returns False for non-200 status codes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider incorrectly assumes all requests are successful (status code 200) when they may not be.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The method _check_availability() of the provider should return False for a status code other than 200.</li>
                                        <li>The method _check_availability() of the provider should raise an exception if the status code is not 200.</li>
                                        <li>The provider's response object should have a 'status_code' attribute set to 500 (or any other non-200 status code).</li>
                                        <li>The provider's response object should not be None.</li>
                                        <li>The provider's response object should not have a 'status_code' attribute set to 200.</li>
                                        <li>The provider's response object should raise an exception if the status code is not 200 when calling _check_availability() method.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 87-88, 90-92)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_success</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test checks availability of Ollama provider via /api/tags endpoint.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case the API is down or not responding correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The '/api/tags' URL should be present in the provided host.</li>
                                        <li>The response status code should be 200 (OK).</li>
                                        <li>The provider's check_availability method should return True.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 87-88, 90-92)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_is_local_returns_true</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The Ollama provider should always return `is_local=True`.</p>
                                <p><strong>Why Needed:</strong> This test prevents regressions where the provider might return `False` when it's actually local.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider.is_local() == True</li>
                                        <li>provider.config.provider == 'ollama'</li>
                                        <li>config.config.provider == 'ollama'</li>
                                        <li>is_local() is True in provider</li>
                                        <li>is_local() is True in config</li>
                                        <li>provider.is_local() should always be `True`</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_json</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `OllamaProvider` class throws an error when parsing a response with invalid JSON.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the Ollama provider incorrectly reports valid responses as invalid JSON.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `annotation.error` attribute is set to 'Failed to parse LLM response as JSON'.</li>
                                        <li>The `provider._parse_response('not-json')` method returns an error message indicating that the response cannot be parsed as JSON.</li>
                                        <li>The test verifies that the error message contains the string 'Failed to parse LLM response as JSON'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 52-53, 186-187, 190-192)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-52, 55)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_key_assertions</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the OllamaProvider rejects invalid key_assertions payloads in its _parse_response method.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider incorrectly handles invalid key_assertions payloads, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The response data must be a dictionary with a 'key_assertions' key</li>
                                        <li>The 'key_assertions' value must be a list of strings</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207-209)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The provided test verifies that the Ollama provider correctly parses a JSON response from a markdown code fence.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the LLM providers, ensuring they can extract relevant information from markdown code fences when parsing responses.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert isinstance(response, dict)</li>
                                        <li>assert 'code_fence' in response</li>
                                        <li>assert response['code_fence'] is not None</li>
                                        <li>assert isinstance(response['code_fence'], str)</li>
                                        <li>assert len(response['code_fence']) > 0</li>
                                        <li>assert response['code_fence'].startswith('#')</li>
                                        <li>assert response['code_fence'].endswith(')')</li>
                                        <li>assert 'annotation' in response</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 38, 42-44, 46-47)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_plain_fence</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The provided test verifies that the Ollama provider correctly extracts JSON from a plain markdown fence without a specified language.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential regression where the provider fails to extract JSON from such fences, potentially leading to incorrect output or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The response should contain a JSON object with the correct structure and keys.</li>
                                        <li>The response should have a 'data' key with the expected JSON data.</li>
                                        <li>The response should not be empty or null.</li>
                                        <li>The response should only contain JSON objects, not other types of content.</li>
                                        <li>The 'data' key should contain an array of objects with the correct structure.</li>
                                        <li>Each object in the 'data' array should have the correct keys and values.</li>
                                        <li>The provider should correctly handle nested objects and arrays.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 38, 42-44, 46-47)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_success</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the Ollama provider correctly parses valid JSON responses with expected assertions.</p>
                                <p><strong>Why Needed:</strong> Prevents potential bugs in the LLM providers by ensuring they handle correct response data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert a</li>
                                        <li>assert b</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestArtifactEntry::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `CoverageEntry` class to serialize correctly.</p>
                                <p><strong>Why Needed:</strong> The test prevents a potential bug where the serialized data does not match the expected format.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key in the dictionary matches the expected value.</li>
                                        <li>The 'line_ranges' key in the dictionary matches the expected format.</li>
                                        <li>The 'line_count' key in the dictionary matches the expected value.</li>
                                        <li>The 'file_path' key is present and has the correct value.</li>
                                        <li>The 'line_ranges' key contains the correct ranges (1-3, 5, 10-15).</li>
                                        <li>The 'line_count' key contains the correct value (10).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 254-257)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestCollectionError::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `CoverageEntry.to_dict()` correctly serializes a CoverageEntry object into a dictionary.</p>
                                <p><strong>Why Needed:</strong> This test prevents the regression of coverage entry serialization issues in previous versions where the line ranges were not properly formatted.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key in the dictionary should be equal to 'src/foo.py'.</li>
                                        <li>The 'line_ranges' key in the dictionary should be equal to '1-3, 5, 10-15'.</li>
                                        <li>The 'line_count' key in the dictionary should be equal to 10.</li>
                                        <li>The line ranges are correctly formatted (i.e., separated by commas).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 207-209)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestCoverageEntry::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test coverage serialization for CoverageEntry.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the CoverageEntry object's attributes are not properly serialized to JSON.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' attribute is correctly set to 'src/foo.py'.</li>
                                        <li>The 'line_ranges' attribute is correctly set to '1-3, 5, 10-15'.</li>
                                        <li>The 'line_count' attribute is correctly set to 10.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 40-43)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_empty_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> An empty annotation should be created with default values.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where an empty annotation would not have any attributes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>annotation.scenario == "" (empty string)</li>
                                        <li>annotation.why_needed == "" (empty string) - This attribute is required for LlmAnnotation</li>
                                        <li>annotation.key_assertions == [] (no assertions made in this class)</li>
                                        <li>assert annotation.confidence is None (default value for confidence)</li>
                                        <li>assert annotation.error is None (default value for error)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_to_dict_minimal</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `to_dict` method of `LlmAnnotation` returns a dictionary with required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where an annotation is missing some critical information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'scenario' key should be present in the dictionary.</li>
                                        <li>The 'why_needed' key should be present in the dictionary.</li>
                                        <li>The 'key_assertions' key should be present in the dictionary.</li>
                                        <li>The 'confidence' key should not be present in the dictionary when it is None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 104-107, 109, 111, 113, 115)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_to_dict_with_all_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> test_to_dict_with_all_fields verifies that the full annotation is correctly converted to a dictionary.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues where an incomplete or missing field in the annotation leads to incorrect data being returned from the `to_dict()` method.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Asserts that the 'scenario' key in the resulting dictionary matches the expected value.</li>
                                        <li>Asserts that the 'confidence' key in the resulting dictionary is equal to the expected value (0.95).</li>
                                        <li>Asserts that the 'context_summary' key in the resulting dictionary has the correct mode and byte count values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 104-107, 109-111, 113-115)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_default_report</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test default report functionality.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the default report does not contain required schema version and empty lists for tests, collection errors, and warnings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'schema_version' key in the report dictionary should be equal to SCHEMA_VERSION.</li>
                                        <li>The 'tests' key in the report dictionary should be an empty list.</li>
                                        <li>The 'warnings' key in the report dictionary should not exist (i.e., its value is None or an empty string).</li>
                                        <li>The 'collection_errors' key in the report dictionary should not exist (i.e., its value is None or an empty string).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_report_with_collection_errors</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Report with Collection Errors should include them.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the report does not include collection errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'collection_errors' key in the report dictionary should exist and have exactly one item.</li>
                                        <li>The 'nodeid' value of the first item in the 'collection_errors' list should match 'test_bad.py'.</li>
                                        <li>The 'message' value of the first item in the 'collection_errors' list should be 'SyntaxError'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">58 lines (ranges: 207-209, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508-510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_report_with_warnings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the ReportRoot class correctly handles warnings in a report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where reports with warnings are not properly handled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'warnings' key in the report dictionary should contain exactly one warning.</li>
                                        <li>The value of the 'code' key in the first warning should be 'W001'.</li>
                                        <li>The 'message' key in the first warning should match the provided message 'No coverage'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">60 lines (ranges: 229-231, 233, 235, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_tests_sorted_by_nodeid</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests should be sorted by nodeid in output.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the order of tests is not guaranteed to be consistent due to changes in the report generation logic.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The list of nodeids in the reports should match the expected order.</li>
                                        <li>The nodeids should contain the correct values from the 'tests' dictionary.</li>
                                        <li>The nodeid 'a_test.py::test_a' should be present in the list.</li>
                                        <li>The nodeid 'm_test.py::test_m' should be present in the list.</li>
                                        <li>The nodeid 'z_test.py::test_z' should be present in the list and its value should match the expected order.</li>
                                        <li>All nodeids should contain unique values from the 'tests' dictionary.</li>
                                        <li>No duplicate nodeids should be present in the output.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">71 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportWarning::test_to_dict_with_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test `test_to_dict_with_detail` verifies that the `ReportWarning` class's `to_dict()` method returns a dictionary with the 'detail' key.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential warning about missing coverage details in reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'detail' key should contain the path '/path/to/file'.</li>
                                        <li>The value of 'detail' is correct and matches the expected path.</li>
                                        <li>The 'ReportWarning' class's `to_dict()` method returns a dictionary with the required keys.</li>
                                        <li>The 'detail' key is present in the returned dictionary.</li>
                                        <li>The value of the 'detail' key is not empty or an empty string.</li>
                                        <li>The path '/path/to/file' is correctly formatted and matches the expected value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 229-231, 233-235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportWarning::test_to_dict_without_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test to dictionary without detail should exclude it.</p>
                                <p><strong>Why Needed:</strong> Prevents a warning from being reported when the 'detail' key is missing from the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'code' key should be present in the dictionary.</li>
                                        <li>The 'message' key should be present in the dictionary.</li>
                                        <li>The 'detail' key should not be present in the dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 229-231, 233, 235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_aggregation_fields_present</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that RunMeta has aggregation fields.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where RunMeta is not aggregated and does not have necessary fields.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'run_id' key in the meta dictionary should match the run ID provided.</li>
                                        <li>The 'run_group_id' key in the meta dictionary should match the run group ID provided.</li>
                                        <li>The 'is_aggregated' key in the meta dictionary should be True.</li>
                                        <li>The 'aggregation_policy' key in the meta dictionary should be set to 'merge'.</li>
                                        <li>The 'run_count' key in the meta dictionary should have a value of 3.</li>
                                        <li>The length of the 'source_reports' list in the meta dictionary should be equal to 2.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 277-279, 281-283, 364-380, 382, 385, 387, 390, 393, 395, 397, 399-405, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_llm_fields_excluded_when_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM fields are excluded when annotations are disabled.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where LLM fields are included even when annotations are disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm_annotations_enabled' key is not present in the data.</li>
                                        <li>The 'llm_provider' key is not present in the data.</li>
                                        <li>The 'llm_model' key is not present in the data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_llm_traceability_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test LLM traceability fields are included when enabled.</p>
                                <p><strong>Why Needed:</strong> To ensure the correct inclusion of LLM traceability fields in the output.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>data['llm_annotations_enabled'] is True</li>
                                        <li>data['llm_provider'] == 'ollama'</li>
                                        <li>data['llm_model'] == 'llama3.2:1b'</li>
                                        <li>data['llm_context_mode'] == 'complete'</li>
                                        <li>data['llm_annotations_count'] == 10</li>
                                        <li>data['llm_annotations_errors'] == 2</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407-419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_non_aggregated_excludes_source_reports</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `to_dict()` method of `RunMeta` to ensure it excludes source reports when aggregated.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where non-aggregated runs are incorrectly included in source reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'source_reports' key is not present in the dictionary returned by `to_dict()`.</li>
                                        <li>The value of `is_aggregated` is set to `False` when `to_dict()` is called on a non-aggregated run.</li>
                                        <li>Non-aggregated runs are correctly excluded from source reports.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_run_meta_to_dict_full</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test RunMeta to dict with all optional fields.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of missing or invalid optional fields.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that the 'git_sha' field is set correctly and is not empty.</li>
                                        <li>Verify that the 'git_dirty' field is True.</li>
                                        <li>Verify that the 'repo_version' field is set correctly and matches the provided value.</li>
                                        <li>Verify that the 'repo_git_sha' field is set correctly and matches the provided value.</li>
                                        <li>Verify that the 'repo_git_dirty' field is False.</li>
                                        <li>Verify that the 'plugin_git_sha' field is not present in the data.</li>
                                        <li>Verify that the length of the 'source_reports' list is 1 as expected.</li>
                                        <li>Verify that all required fields are present and have correct values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">49 lines (ranges: 277-279, 281-283, 364-380, 382-405, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_run_status_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test RunMeta to ensure it includes required run status fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `RunMeta` object is missing certain critical fields, potentially leading to incorrect or incomplete results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'exit_code' field should be equal to 1.</li>
                                        <li>The 'interrupted' field should be set to True.</li>
                                        <li>The 'collect_only' field should be set to True.</li>
                                        <li>The 'collected_count' field should match the expected value of 10.</li>
                                        <li>The 'selected_count' field should match the expected value of 8.</li>
                                        <li>The 'deselected_count' field should match the expected value of 2.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSchemaVersion::test_schema_version_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the schema version is in semver format.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where a non-semantic version string is used.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The schema version should be split into three parts (e.g., '1.2.3').</li>
                                        <li>Each part of the version should consist only of digits (0-9).</li>
                                        <li>All parts of the version should be non-empty and not equal to zero.</li>
                                        <li>The first part of the version should be greater than 0.</li>
                                        <li>The second part of the version should be less than or equal to 99.</li>
                                        <li>The third part of the version should be less than or equal to 99.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSchemaVersion::test_schema_version_in_report_root</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `ReportRoot` class includes the schema version in its report root.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the schema version is not included in reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `schema_version` attribute of the `ReportRoot` instance should be set to `SCHEMA_VERSION`.</li>
                                        <li>The `to_dict()` method of the `ReportRoot` instance should return a dictionary with a `schema_version` key equal to `SCHEMA_VERSION`.</li>
                                        <li>The value of the `schema_version` field in the report root JSON should match `SCHEMA_VERSION`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSourceCoverageEntry::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests CoverageEntry serialization correctly.</p>
                                <p><strong>Why Needed:</strong> CoverageEntry does not serialize correctly if line ranges are missing or invalid.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key is present and matches the expected value.</li>
                                        <li>The 'line_ranges' key is present and matches the expected format.</li>
                                        <li>The 'line_count' key is present and matches the expected value.</li>
                                        <li>All assertions pass for a CoverageEntry with valid file path, line ranges, and line count.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 71-78)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSourceReport::test_to_dict_minimal</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `to_dict` method of `LlmAnnotation` returns a dictionary with required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where an annotation is missing some critical information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'scenario' key should be present in the dictionary.</li>
                                        <li>The 'why_needed' key should be present in the dictionary.</li>
                                        <li>The 'key_assertions' key should be present in the dictionary.</li>
                                        <li>The 'confidence' key should not be present in the dictionary when it is None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 277-279, 281, 283)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSourceReport::test_to_dict_with_run_id</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test SourceReport to_dict_with_run_id verifies that the 'run_id' key is present in the resulting dictionary.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the 'run_id' is missing from the output of the SourceReport.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'run_id' key should be present in the dictionary.</li>
                                        <li>The value of the 'run_id' key should match the provided run_id.</li>
                                        <li>If no run_id is provided, the 'run_id' key should not be present in the dictionary.</li>
                                        <li>The 'run_id' key should have the correct format (e.g., 'run-1').</li>
                                        <li>Any errors or exceptions raised during the to_dict method execution should be caught and reported.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 277-279, 281-283)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSummary::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `CoverageEntry` object can be serialized to a dictionary correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the serialization of `CoverageEntry` objects may not work as expected, potentially leading to incorrect or missing information in the output.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key in the dictionary should match the actual file path.</li>
                                        <li>The 'line_ranges' key in the dictionary should match the provided line ranges.</li>
                                        <li>The 'line_count' key in the dictionary should match the expected value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 449-457, 459, 461)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_minimal_result</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that a minimal result has the required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where a minimal result is missing required fields.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'nodeid' field should be set to the expected value.</li>
                                        <li>The 'outcome' field should be set to 'passed'.</li>
                                        <li>The 'duration' field should be set to 0.0 (indicating no execution time).</li>
                                        <li>The 'phase' field should be set to 'call'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'Result with coverage' verifies that the `CoverageEntry` object has a single entry in the `coverage` list.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the coverage report is not correctly formatted or contains duplicate entries.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `CoverageEntry` object has exactly one entry in the `coverage` list.</li>
                                        <li>The first element of the `coverage` list is a string representing the file path.</li>
                                        <li>Each `CoverageEntry` object in the `coverage` list has a `file_path` attribute matching the expected value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 40-43, 161-165, 167, 169, 171, 173, 176-178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_llm_opt_out</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `TestCaseResult` object includes a flag for LLM opt-out.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case where LLM opt-out is enabled and the test passes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `llm_opt_out` in the `d` dictionary should be `True`.</li>
                                        <li>The `TestCaseResult` object should have a `llm_opt_out` key with a boolean value of `True`.</li>
                                        <li>The `llm_opt_out` value should not be overridden by any other test case.</li>
                                        <li>If LLM opt-out is enabled, the test result should still pass even if the code under test fails.</li>
                                        <li>The `llm_opt_out` flag should be included in the output of the test.</li>
                                        <li>The `llm_opt_out` flag should be preserved across different test runs with LLM opt-out enabled.</li>
                                        <li>If LLM opt-out is disabled, the test result should still pass even if the code under test fails.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180-182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_rerun</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_result_with_rerun' verifies that the rerun fields are included in the TestCaseResult.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where a result with reruns is not properly updated.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `rerun_count` should be equal to 2.</li>
                                        <li>The value of `final_outcome` should be 'passed'.</li>
                                        <li>The `rerun_count` field should be included in the TestCaseResult dictionary.</li>
                                        <li>The `final_outcome` field should be included in the TestCaseResult dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 161-165, 167, 169, 171, 173-176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_without_rerun_excludes_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case 'test_result_without_rerun_excludes_fields' verifies that the `result` dictionary does not include 'rerun_count' and 'final_outcome' keys.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the `result` dictionary excludes fields related to reruns.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'rerun_count' key is not present in the `result` dictionary.</li>
                                        <li>The 'final_outcome' key is not present in the `result` dictionary.</li>
                                        <li>The 'rerun_count' and 'final_outcome' keys are not included in the `result` dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_default_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify default values for Config object are set correctly.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in default configuration settings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>cfg.provider should be 'none'</li>
                                        <li>cfg.llm_context_mode should be 'minimal'</li>
                                        <li>cfg.llm_max_tests should be 0</li>
                                        <li>cfg.llm_max_retries should be 10</li>
                                        <li>cfg.llm_context_bytes should be 32000</li>
                                        <li>cfg.llm_context_file_limit should be 10</li>
                                        <li>cfg.llm_requests_per_minute should be 5</li>
                                        <li>cfg.llm_timeout_seconds should be 30</li>
                                        <li>cfg.llm_cache_ttl_seconds should be 86400</li>
                                        <li>cfg.include_phase should be 'run'</li>
                                        <li>cfg.aggregate_policy should be 'latest'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_get_default_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the default configuration is correctly set to 'none'.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the default configuration is not properly initialized.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `cfg` variable should be an instance of `Config`.</li>
                                        <li>The `cfg.provider` attribute should be set to `'none'`.</li>
                                        <li>The `cfg` object should have no other attributes besides `provider`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_is_llm_enabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `is_llm_enabled` check returns False for a provider without an LLM.</p>
                                <p><strong>Why Needed:</strong> To prevent regression in case of a change to the default provider or its configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `Config.is_llm_enabled()` should return `False` when the provider is set to `'none'`.</li>
                                        <li>The function `Config.is_llm_enabled()` should return `True` when the provider is set to `'ollama'`.</li>
                                        <li>The function `Config.is_llm_enabled()` should not return a value for an empty configuration object.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_aggregate_policy</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validates configuration with an invalid aggregate policy.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the aggregation policy is not properly validated and causes unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregate_policy` parameter in the configuration should be one of 'sum', 'mean', 'min', 'max', 'count', 'stdev', 'median' or 'none'.</li>
                                        <li>If an invalid aggregate policy is passed, it should raise a validation error.</li>
                                        <li>The test should verify that only one error message is returned for an invalid aggregation policy.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-197, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_context_mode</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the validation of an invalid context mode.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an invalid context mode being used.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The configuration object is created with an invalid context mode.</li>
                                        <li>An error message indicating the invalid context mode is returned.</li>
                                        <li>The 'Invalid llm_context_mode 'mega_max' error message is present in the list of errors.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-189, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_include_phase</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `validate()` method with an invalid include phase.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where an invalid include phase is not properly validated, causing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `validate()` method should return at least one error message for an invalid include phase.</li>
                                        <li>The error message should contain the invalid include phase 'lunch_break'.</li>
                                        <li>The test should fail if no error messages are returned from the `validate()` method.</li>
                                        <li>The test should pass if exactly one error message is returned from the `validate()` method.</li>
                                        <li>The error message should not be empty or null.</li>
                                        <li>The error message should contain a string that can be used as an include phase.</li>
                                        <li>The error message should not contain any invalid characters.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-205, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validation with an invalid provider.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an incorrect error message or count.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `validate()` returns exactly one error message.</li>
                                        <li>The first error message contains the string 'Invalid provider'.</li>
                                        <li>The error message is not empty.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_numeric_ranges</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validation of numeric constraints for TestConfig.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the llm_context_bytes is set to a value less than 1000, potentially causing issues with LLM context creation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>cfg.validate() should return at least 5 error messages</li>
                                        <li>llm_context_bytes must be at least 1000</li>
                                        <li>llm_max_tests must be 0 (no limit) or positive</li>
                                        <li>llm_requests_per_minute must be at least 1</li>
                                        <li>llm_timeout_seconds must be at least 1</li>
                                        <li>llm_max_retries must be 0 or positive</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209-218, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_valid_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Valid configuration is validated successfully without any errors.</p>
                                <p><strong>Why Needed:</strong> Prevents potential issues where invalid configurations are passed to the application.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `validate()` method of the `Config` class should return an empty list if the input configuration is valid.</li>
                                        <li>No exceptions should be raised when a valid configuration is provided.</li>
                                        <li>All required fields in the configuration should be present and have the expected values.</li>
                                        <li>All optional fields in the configuration should be absent or have default values.</li>
                                        <li>The `validate()` method should not throw any errors for well-formed configurations.</li>
                                        <li>Any invalid configurations should raise an exception with a meaningful error message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_aggregation_options</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `load_aggregation_options` function to ensure it correctly loads aggregation options from a mock Pytest configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the `load_aggregation_options` function, which is responsible for loading aggregation options from a Pytest configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregate_dir` attribute of the loaded configuration should be set to 'aggr_dir'.</li>
                                        <li>The `aggregate_policy` attribute of the loaded configuration should be set to 'merge'.</li>
                                        <li>The `aggregate_run_id` attribute of the loaded configuration should be set to 'run-123'.</li>
                                        <li>The `aggregate_group_id` attribute of the loaded configuration should be set to 'group-abc'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286-294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_config_invalid_int_ini</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_load_config_invalid_int_ini' verifies that the test loads configuration with invalid integer values in INI correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the test crashes or behaves unexpectedly when encountering invalid integer values in INI files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `load_config` should be able to handle and return valid default values for configuration options even if they are set to an invalid integer value.</li>
                                        <li>The function `mock_pytest_config.getini.side_effect` should not crash or throw an exception when called with an invalid key.</li>
                                        <li>The test should assert that the fallback value is correct (in this case, 10) and does not cause any unexpected behavior.</li>
                                        <li>The function `load_config` should be able to handle cases where the invalid integer value is set in a specific section of the INI file (e.g. 'llm_report_max_retries')</li>
                                        <li>The test should pass even if the invalid key is not found in the INI file, and the default value is used instead.</li>
                                        <li>The function `mock_pytest_config.getini` should return valid values for other configuration options that are not set to an invalid integer value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263-267, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_coverage_source</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `llm_coverage_source` option is set to 'cov_dir' when loading coverage source.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `llm_coverage_source` option is not correctly set to 'cov_dir'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_pytest_config.option.llm_coverage_source == 'cov_dir'</li>
                                        <li>cfg.llm_coverage_source == 'cov_dir'</li>
                                        <li>assert cfg.llm_coverage_source is None or cfg.llm_coverage_source == 'cov_dir'</li>
                                        <li>cfg.llm_coverage_source != 'cov_dir' and not isinstance(cfg.llm_coverage_source, str)</li>
                                        <li>cfg.llm_coverage_source != 'cov_dir' and isinstance(cfg.llm_coverage_source, dict)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_defaults</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `load_defaults` test loads the default provider and report HTML settings.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the default provider or report HTML setting is not loaded when no options are set.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>cfg.provider == 'none'</li>
                                        <li>cfg.report_html is None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">24 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_cli_overrides_ini</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that CLI options override ini options.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the CLI overrides ini settings, potentially causing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>ini_value is set to 'cli_report.html' for llm_report_html option</li>
                                        <li>llm_requests_per_minute value is set to 100</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">27 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259-261, 263, 270-272, 274, 276, 278, 280-282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_cli_retries</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `load_from_cli` function with a specified maximum retries.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential regressions where the `llm_max_retries` option is not set to a valid value.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `llm_max_retries` option should be set to an integer value (e.g., 1, 3, or 9).</li>
                                        <li>The `load_config` function should return the expected configuration with the specified maximum retries.</li>
                                        <li>The `assert` statement should raise a `ValueError` if the `llm_max_retries` option is not set to an integer value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282-283, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_ini</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loading values from ini options.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the 'llm_report_provider' value is not set correctly in case of an error during configuration loading.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'provider' key should be set to 'ollama'.</li>
                                        <li>The 'model' key should be set to 'llama3'.</li>
                                        <li>The 'context_mode' key should be set to 'balanced'.</li>
                                        <li>The 'requests_per_minute' key should be set to 10.</li>
                                        <li>The 'max_retries' key should be set to 5.</li>
                                        <li>The 'html' key should be set to 'report.html'.</li>
                                        <li>The 'json' key should be set to 'report.json'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">32 lines (ranges: 107, 147, 248, 251-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_aggregation_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the aggregation settings configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the aggregate policy is set to 'merge' without specifying an aggregate group ID, causing unexpected behavior in the aggregated reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.aggregate_dir should be equal to '/reports'.</li>
                                        <li>config.aggregate_policy should be equal to 'merge'.</li>
                                        <li>config.aggregate_include_history should be True.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_all_output_paths</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Config with all output paths.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the test is run without specifying all output paths.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.report_html == 'report.html'</li>
                                        <li>config.report_json == 'report.json'</li>
                                        <li>config.report_pdf == 'report.pdf'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_capture_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `capture_failed_output` attribute of the `Config` class.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an incorrect configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `capture_failed_output` attribute is set to True.</li>
                                        <li>The `capture_output_max_chars` attribute is set to 8000.</li>
                                        <li>The `capture_failed_output` attribute should be `True` if `capture_output_max_chars` is 8000 or more.</li>
                                        <li>If `capture_output_max_chars` is less than 8000, the test will fail due to an incorrect configuration.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_compliance_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `Config` object is created with the correct metadata file and HMAC key file.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the configuration is not set correctly, potentially leading to incorrect compliance settings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `metadata_file` attribute of the `Config` object is set to 'metadata.json'.</li>
                                        <li>The `hmac_key_file` attribute of the `Config` object is set to 'key.txt'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_coverage_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test configures default coverage settings.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where the test coverage is not enabled by default.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.omit_tests_from_coverage should be False</li>
                                        <li>config.include_phase should be 'all'</li>
                                        <li>config.omit_tests_from_coverage is set to False</li>
                                        <li>config.include_phase matches the expected value</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_custom_exclude_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `llm_context_exclude_globs` parameter is correctly set to exclude `.pyc` and `.log` files.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the custom exclusion globs are not properly applied.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `*.pyc` glob matches the expected file extension.</li>
                                        <li>The `*.log` glob matches the expected file extension.</li>
                                        <li>The `llm_context_exclude_globs` parameter is set to include `.pyc` and `.log` files in the configuration.</li>
                                        <li>The custom exclusion globs are applied correctly without any additional files being included.</li>
                                        <li>No other files match the excluded globs.</li>
                                        <li>The `*.pyc` glob matches a file that does not exist (`non_existent_file.pyc`)</li>
                                        <li>The `*.log` glob matches a file that exists (`file.log`)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_include_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `llm_context_include_globs` attribute includes only `.py` files.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where include globs are not correctly applied to LLM context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `*.py` glob matches only files with a `.py` extension.</li>
                                        <li>The `*.pyi` glob matches only files with a `.pyi` extension.</li>
                                        <li>The `llm_context_include_globs` attribute is correctly updated with the specified globs.</li>
                                        <li>No other files are included in the LLM context.</li>
                                        <li>The include globs do not interfere with each other.</li>
                                        <li>The include globs are applied consistently across all test cases.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_invocation_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `include_pytest_invocation` configuration option is set to False.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `include_pytest_invocation` option is not properly configured, potentially leading to unexpected behavior or errors in testing.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `include_pytest_invocation` attribute of the `Config` object is set to `False`.</li>
                                        <li>The `include_pytest_invocation` configuration option is not being used by the test.</li>
                                        <li>The `include_pytest_invocation` option has a default value of `True` in the `Config` class.</li>
                                        <li>The `include_pytest_invocation` attribute is being accessed and modified correctly within the test.</li>
                                        <li>The `config` object passed to the `Config` constructor has the correct attributes set.</li>
                                        <li>The `include_pytest_invocation` attribute is not being overridden by any other configuration options.</li>
                                        <li>The `include_pytest_invocation` attribute is being used in a valid way within the test.</li>
                                        <li>The `include_pytest_invocation` attribute is being updated correctly within the test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_execution_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the LLM execution settings configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in LLM execution settings configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of llm_max_tests is correctly set to 50.</li>
                                        <li>The value of llm_max_concurrency is correctly set to 8.</li>
                                        <li>The value of llm_requests_per_minute is correctly set to 12.</li>
                                        <li>The expected values are matched with the actual values in the config object.</li>
                                        <li>No other assertions are necessary for this test as it only verifies the LLM execution settings configuration.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_param_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the configuration of LLM parameter settings.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the LLM parameter values are not correctly configured, potentially leading to incorrect output or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `llm_include_param_values` attribute is set to True.</li>
                                        <li>The `llm_param_value_max_chars` attribute is set to 200.</li>
                                        <li>The value of `llm_param_value_max_chars` is equal to 200.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the configuration of LLM settings for OLLAMA.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the model and context bytes are not set correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` attribute is set to 'ollama'.</li>
                                        <li>The `model` attribute is set to 'llama3.2'.</li>
                                        <li>The value of `llm_context_bytes` is equal to 64000.</li>
                                        <li>The value of `llm_context_file_limit` is not provided in the test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_repo_root_path</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `repo_root` attribute is correctly set to `/project` when a repository path is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `repo_root` attribute is not set correctly if no explicit value is provided for the repository path.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config.repo_root` attribute should be equal to `Path('/project')` when the `repo_root` parameter is passed to the `Config` constructor.</li>
                                        <li>The `repo_root` attribute of the test configuration object should match the expected value `/project`.</li>
                                        <li>If no explicit repository path is provided, the `repo_root` attribute should still be set correctly to `/project`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_valid_phase_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `test_valid_phase_values` method to ensure all valid include_phase values pass validation.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where invalid or missing include_phase values cause the configuration to fail validation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `validate()` method of the `Config` class should return an empty list of errors for each included phase (run, setup, teardown, all).</li>
                                        <li>Any error messages related to 'include_phase' should not be present in the validation results.</li>
                                        <li>All invalid or missing include_phase values should be ignored and not reported as errors.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_exclude_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the default exclude globs are correctly set to include `*.pyc`, `__pycache__/*`, and any files with names containing '*secret*' or '*password*'.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the default exclude globs do not match the expected patterns, potentially leading to issues when running the model.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `*.pyc` is included in the default exclude globs.</li>
                                        <li>The function `__pycache__/*` is included in the default exclude globs.</li>
                                        <li>Any file with a name containing '*secret*' or '*password*' is included in the default exclude globs.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_redact_patterns</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the default redact patterns configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential security vulnerability by ensuring all sensitive information is redacted from default configurations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `Config().invocation_redact_patterns` returns a list of patterns that include '--password', '--token', and '--api[_-]?key' to prevent sensitive data exposure.</li>
                                        <li>Any pattern found in the list should contain one of these keywords to ensure proper redaction.</li>
                                        <li>The presence of any other keyword should be checked for to prevent potential security issues.</li>
                                        <li>All patterns should match the expected format to guarantee correct redaction.</li>
                                        <li>No pattern should be missing any required keyword to maintain security standards.</li>
                                        <li>Any pattern with an invalid or incomplete format should raise an error to alert developers to fix it.</li>
                                        <li>The list of patterns should not contain any sensitive information that could be used for malicious purposes.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests default configuration values.</p>
                                <p><strong>Why Needed:</strong> To ensure correct default values are set for the test environment.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.provider should be set to 'none'.</li>
                                        <li>config.llm_context_mode should be set to 'minimal'.</li>
                                        <li>config.llm_context_bytes should be set to 32000 bytes.</li>
                                        <li>config.omit_tests_from_coverage should be True.</li>
                                        <li>config.include_phase should be set to 'run'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigHelpersMaximal::test_is_llm_enabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `is_llm_enabled` method returns the correct enabled status for different providers.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where the method does not return False when LLM is disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `is_llm_enabled` method should return False for provider 'none'.</li>
                                        <li>The `is_llm_enabled` method should return True for providers 'ollama', 'litellm', and 'gemini'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_aggregate_policy</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the validation of an invalid aggregate policy.</p>
                                <p><strong>Why Needed:</strong> To prevent a potential bug where an invalid aggregate policy is used, which could lead to unexpected behavior or errors during configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The validate method returns at least one error for the given aggregate policy.</li>
                                        <li>The error message contains 'Invalid aggregate_policy' and specifies the specific aggregate policy as 'invalid'.</li>
                                        <li>An assertion is made that there is exactly one error in the list of errors returned by the validate method.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-197, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_context_mode</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validates the maximum options configuration with an invalid context mode.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the maximum options configuration is not validated correctly when an invalid context mode is provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm_context_mode' field in the config should be set to one of the valid values (e.g. 'maximal', 'minimal')</li>
                                        <li>The error message for the invalid context mode should contain the exact phrase 'Invalid llm_context_mode 'invalid''</li>
                                        <li>At least one error should be returned when validating the configuration with an invalid context mode</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-189, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_include_phase</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the validation of an invalid include phase.</p>
                                <p><strong>Why Needed:</strong> To prevent a potential bug where an invalid include phase is not properly validated and causes unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `Config(include_phase='invalid')` should return an error with a specific message.</li>
                                        <li>The error message should contain the string 'Invalid include_phase ' + 'invalid' to identify the issue.</li>
                                        <li>The test should verify that there is only one error returned by the validation process.</li>
                                        <li>The error message should be present in the first assertion of the `errors` list.</li>
                                        <li>The error message should not be empty or null.</li>
                                        <li>The error message should contain the specified invalid include phase value.</li>
                                        <li>The function `Config(include_phase='invalid')` should raise an exception with a specific error message when called with an invalid include phase.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-205, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validates an invalid provider.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an invalid provider being used.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The configuration should return exactly one error for an invalid provider.</li>
                                        <li>The error message should contain 'Invalid provider 'invalid''.</li>
                                        <li>The error message should be present in the first error found by the validation process.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_numeric_bounds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the Config class's validate method returns an error when numeric bounds are not met.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where invalid numeric values could be passed to the Config class, potentially causing unexpected behavior or errors in downstream systems.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.validate() should return at least 4 errors for invalid numeric values.</li>
                                        <li>The error messages should include 'llm_context_bytes', 'llm_max_tests', 'llm_requests_per_minute', and 'llm_timeout_seconds'.</li>
                                        <li>Each error message should contain the key 'llm_context_bytes', 'llm_max_tests', 'llm_requests_per_minute', or 'llm_timeout_seconds'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209-217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_valid_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `validate` method of a valid configuration returns an empty list.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where an invalid configuration is passed to the `validate` method, causing it to return unexpected results or raise an exception.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `validate` method should be called on a valid configuration object and return an empty list.</li>
                                        <li>An empty list should be returned when the configuration is valid.</li>
                                        <li>Any exceptions raised by the `validate` method should be caught and reported as expected.</li>
                                        <li>The `validate` method should not throw any errors or warnings for valid configurations.</li>
                                        <li>The `validate` method should have a clear and consistent behavior for valid configurations.</li>
                                        <li>The `validate` method should only return an empty list when there are no invalid configuration values present.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginConfigLoading::test_config_defaults</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the default configuration is loaded correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the default configuration is not properly loaded due to missing plugin options.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `load_config(pytestconfig)` returns an instance of `Config`.</li>
                                        <li>The assertion `assert isinstance(cfg, Config)` checks if the returned object is indeed an instance of `Config`.</li>
                                        <li>The assertion `cfg = load_config(pytestconfig)` loads the configuration using the provided pytest config.</li>
                                        <li>The assertion `assert cfg is not None` ensures that a configuration is loaded even without any options registered.</li>
                                        <li>The assertion `cfg.get('plugin_options') == {}` checks if the plugin options are missing from the default configuration.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">33 lines (ranges: 107, 147, 248, 251-259, 261, 263-265, 270, 272-276, 278, 280, 282, 286, 288, 290-292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginConfigLoading::test_markers_exist_in_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that markers in the plugin configuration exist.</p>
                                <p><strong>Why Needed:</strong> Prevent a bug where markers are missing from the configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>pytestconfig must be an instance of pytest.config.Config</li>
                                        <li>pytestconfig must have a 'markers' attribute</li>
                                        <li>The 'markers' attribute should contain all marker names</li>
                                        <li>A marker name must exist in the 'markers' list</li>
                                        <li>The 'markers' list must not be empty</li>
                                        <li>All marker names must exist in the 'markers' list</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_llm_context_marker</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the context marker does not cause errors in the LLM integration.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression and ensures that the LLM integration works correctly without causing any errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `test_llm_context_marker` function should pass without raising an error when run with a valid context marker.</li>
                                        <li>A message indicating that the context marker is not recognized or has been disabled should be displayed instead of raising an error.</li>
                                        <li>The LLM integration should work correctly and produce the expected output without any errors or warnings.</li>
                                        <li>Any exceptions raised during test execution should be caught and handled properly, rather than being propagated to the user.</li>
                                        <li>The `test_llm_context_marker` function should not raise any assertion errors when run with a valid context marker.</li>
                                        <li>A message indicating that the context marker is invalid or has been disabled should be displayed instead of raising an error.</li>
                                        <li>Any warnings raised during test execution should be caught and handled properly, rather than being propagated to the user.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_llm_opt_out_marker</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">


                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_requirement_marker</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the requirement marker does not cause any errors.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the requirement marker could be misinterpreted as an error.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `requirement_marker` function should not raise any exceptions or throw any errors.</li>
                                        <li>The `requirement_marker` function should not modify the input data in any way that would cause unexpected behavior.</li>
                                        <li>The `requirement_marker` function should not have any side effects that could be misinterpreted as an error.</li>
                                        <li>The `requirement_marker` function should not take any arguments that are not relevant to its purpose.</li>
                                        <li>The `requirement_marker` function should return a value that indicates success or no error, rather than raising an exception.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestReportGeneration::test_report_writer_integration</span>
                        <div class="test-meta">
                            <span>32ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the report writer correctly generates a full report with both JSON and HTML output.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the report writer fails to generate a report for tests with multiple nodes or failed tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total number of tests passed should be 2 (test_a.py::test_pass) and 1 (test_b.py::test_fail).</li>
                                        <li>Both 'test_a.py' and 'test_b.py' should be included in the report HTML.</li>
                                        <li>The JSON output file should contain a summary with 'total' set to 2 and 'passed' set to 1.</li>
                                        <li>The HTML output file should include both 'test_a.py' and 'test_b.py'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">79 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">131 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that collectreport skips when disabled and pytest_collectreport is mocked correctly.</p>
                                <p><strong>Why Needed:</strong> To ensure that collectreport does not run with a false positive report when pytest_collectreport is disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_report.session.config.stash.get.assert_called_with(_enabled_key, False)</li>
                                        <li>pytest_collectreport.mock_report.session.config.stash.get.assert_called_with(_enabled_key, False)</li>
                                        <li>mock_report.session.config.stash.get.return_value == False</li>
                                        <li>pytest_collectreport.mock_report.session.config.stash.get.asserts_calledWith(_enabled_key, False)</li>
                                        <li>mock_report.session.config.stash.get.asserts_calledOnce</li>
                                        <li>pytest_collectreport.mock_report.session.config.stash.get.return_value != True</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 380-381, 384, 388-390, 401-402, 408-409)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_enabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that collectreport calls collector when enable is True.</p>
                                <p><strong>Why Needed:</strong> The test prevents a potential regression where the plugin does not call the collector even if it's enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocking pytest_collectreport with a mock report object and stash_get function to verify collection report handling</li>
                                        <li>Asserting that collectreport calls collector once when _enabled_key is hit</li>
                                        <li>Verifying that collector handles collection report correctly</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 380-381, 384, 388-390, 401-402, 408, 412-414)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_no_session</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that `pytest_collectreport` does not throw an exception when no session is available.</p>
                                <p><strong>Why Needed:</strong> Prevent regression in plugin behavior when a session is not present.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `pytest_collectreport` should not raise an exception when called with a mock report object without a session attribute.</li>
                                        <li>The mock report object should be able to be passed to `pytest_collectreport` without raising an exception.</li>
                                        <li>The test should fail if the `session` attribute is present in the mock report object but still raises an exception.</li>
                                        <li>The test should pass if the `session` attribute is not present in the mock report object.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 380-381, 384, 388-390, 401, 405)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_session_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that `pytest_collectreport` does not raise an exception when the session is `None`.</p>
                                <p><strong>Why Needed:</strong> Prevent regression in test cases where a `None` session is expected.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `pytest_collectreport` should not be called with a `None` argument.</li>
                                        <li>No error should be raised when calling `pytest_collectreport` with a `None` session.</li>
                                        <li>The mock report object passed to `pytest_collectreport` should have a `session` attribute set to `None`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 380-381, 384, 388-390, 401, 405)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_llm_enabled_warning</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM enabled warning is raised when using the ollama LLMS provider.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the LLM report configuration is not properly validated and may lead to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_llm_report_provider` option should be set to 'ollama'.</li>
                                        <li>The `llm_report_html`, `llm_report_json`, `llm_report_pdf`, `llm_evidence_bundle`, `llm_dependency_snapshot`, `llm_requests_per_minute`, `llm_aggregate_dir`, `llm_aggregate_policy`, `llm_aggregate_run_id`, and `llm_aggregate_group_id` options should be set to None.</li>
                                        <li>The `llm_max_retries` option should also be set to None for the LLM report configuration to work correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">44 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_validation_errors</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that validation errors raise UsageError when invalid configuration is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the plugin does not handle invalid configuration correctly and raises a UsageError instead of providing informative error messages.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_config.getini.side_effect should be set to a lambda function that returns an exception with the correct message 'configuration errors'.</li>
                                        <li>mock_config.option.llm_report_html should be None.</li>
                                        <li>mock_config.option.llm_report_json should be None.</li>
                                        <li>mock_config.option.llm_report_pdf should be None.</li>
                                        <li>mock_config.option.llm_evidence_bundle should be None.</li>
                                        <li>mock_config.option.llm_dependency_snapshot should be None.</li>
                                        <li>mock_config.option.llm_requests_per_minute should be None.</li>
                                        <li>mock_config.option.llm_aggregate_dir should be None.</li>
                                        <li>mock_config.option.llm_aggregate_policy should be None.</li>
                                        <li>mock_config.option.llm_aggregate_run_id should be None.</li>
                                        <li>mock_config.option.llm_aggregate_group_id should be None.</li>
                                        <li>mock_config.option.llm_max_retries should be None.</li>
                                        <li>mock_config.rootpath should be set to a valid path.</li>
                                        <li>mock_config.stash should be an empty dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">43 lines (ranges: 107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 248, 251-253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-199, 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_worker_skip</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that configure skips on xdist workers when pytest_configure is called with a valid config.</p>
                                <p><strong>Why Needed:</strong> This test prevents the 'pytest_configure' function from being called unnecessarily, which can lead to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'addinivalue_line' method of the mock_config object should not be called before the worker check is performed.</li>
                                        <li>The 'addinivalue_line' method of the mock_config object should only be called after the worker check has been performed.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 169-171, 173-175, 177-179, 183-184, 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigureFallback::test_pytest_configure_fallback_load</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that fallback to load_config is triggered when Config.load is missing.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the plugin fails to configure due to an empty Config object.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocking `Config.getini` and `Config.option.llm_report_html` with `None` values ensures that `load_config` is called.</li>
                                        <li>The `validate` method of `Config` returns an empty list when `load_config` is called without a valid Config object.</li>
                                        <li>The `load_config` function is patched to return the mocked `MockConfig` instance.</li>
                                        <li>The `assert_called_once` method is used to verify that only one call to `load_config` occurs.</li>
                                        <li>The `mock_load.return_value` attribute is set to `MockConfig` to ensure that it is called with the mocked Config object.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_all_ini_options</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loading all INI options in the plugin configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the plugin fails to load INI options when CLI options are not set.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The correct provider is 'ollama'.</li>
                                        <li>The correct model is 'llama3.2'.</li>
                                        <li>The correct context mode is 'complete'.</li>
                                        <li>The correct number of requests per minute is 10.</li>
                                        <li>The correct report HTML file name is 'ini.html'.</li>
                                        <li>The correct report JSON file name is 'ini.json'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">31 lines (ranges: 107, 147, 248, 251-263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_cli_overrides_ini</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test CLI options override INI options.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the CLI options override INI options, potentially causing unexpected behavior or incorrect results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that `llm_report_html` is set to `cli.html` in the configuration.</li>
                                        <li>Verify that `llm_report_json` is set to `cli.json` in the configuration.</li>
                                        <li>Verify that `report_pdf` is set to `cli.pdf` in the configuration.</li>
                                        <li>Verify that `report_evidence_bundle` is set to `bundle.zip` in the configuration.</li>
                                        <li>Verify that `report_dependency_snapshot` is set to `deps.json` in the configuration.</li>
                                        <li>Verify that `llm_requests_per_minute` is set to 20 in the configuration.</li>
                                        <li>Verify that `aggregate_dir` is set to `/agg` in the configuration.</li>
                                        <li>Verify that `aggregate_policy` is set to `merge` in the configuration.</li>
                                        <li>Verify that `aggregate_run_id` is set to `run-123` in the configuration.</li>
                                        <li>Verify that `aggregate_group_id` is set to `group-abc` in the configuration.</li>
                                        <li>Verify that the root path of the configuration is `/project` in the configuration.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">38 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259-263, 270-283, 286-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that terminal summary skips when plugin is disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the terminal summary is not properly skipped when the plugin is disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_terminal_summary` function should be called with an empty stash and no worker input.</li>
                                        <li>The `stash.get()` method of the mock configuration object should have been called once with `_enabled_key` as its argument and `False` as its value.</li>
                                        <li>The `stash.get()` method of the mock configuration object should not have been called again for subsequent assertions.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 238, 242-243, 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_worker_skip</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that terminal summary skips on xdist worker when a specific configuration is used.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the plugin's behavior when using an xdist worker with a specific configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function pytest_terminal_summary should return None for the given mock config.</li>
                                        <li>The function pytest_terminal_summary should not perform any actions on the given mock config.</li>
                                        <li>The function pytest_terminal_summary should not call any functions or methods on the given mock config.</li>
                                        <li>The function pytest_terminal_summary should not modify the given mock config in any way.</li>
                                        <li>The function pytest_terminal_summary should not raise any exceptions when called with a mock config.</li>
                                        <li>The function pytest_terminal_summary should behave as expected when called with a mock config that is different from the original configuration.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 238-239, 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::testload_config</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test config loading from pytest objects (CLI + INI) to ensure it correctly sets the report HTML.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the report HTML is not set correctly if the `pytest_llm_report` options are missing or invalid.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report_html` attribute of the loaded configuration object should be set to 'out.html'.</li>
                                        <li>If `pytest_llm_report.option.llm_report_json` is set, it should not affect the value of `report_html`.</li>
                                        <li>If `pytest_llm_report.option.llm_report_html` is set, it should override any previously set `report_html`.</li>
                                        <li>The `rootpath` attribute of the loaded configuration object should be set to '/root'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270-283, 286-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_disabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test makereport skips when disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the plugin's makereport functionality is not properly handled when the 'pytest_runtest_makereport' hook is disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `mock_item.config.stash.get` call returns `False` instead of raising an error when it should be `None`.</li>
                                        <li>The `mock_call` object does not raise a `StopIteration` exception when the generator completes normally.</li>
                                        <li>The `gen.send(mock_outcome)` call raises a `StopIteration` exception when the generator completes normally, but this is not handled correctly by the plugin.</li>
                                        <li>The plugin's makereport functionality is not properly cleaned up when the 'pytest_runtest_makereport' hook is disabled.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 380-381, 384-385, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_enabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test makereport calls collector when enabled.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the plugin does not call the collector when makereport is enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_runtest_makereport` function should be called with the `mock_collector` instance as its second argument.</li>
                                        <li>The `mock_collector.handle_runtest_logreport` method should be called with the `mock_report` instance and `mock_item` instance as arguments.</li>
                                        <li>The `mock_collector` instance should have a `handle_runtest_logreport` method that takes two arguments: `mock_report` and `mock_item`.</li>
                                        <li>The `mock_collector` instance should be able to handle the `runtest_logreport` event with the correct arguments.</li>
                                        <li>The `mock_collector.handle_runtest_logreport` method should not raise an exception when called with a mock report object.</li>
                                        <li>The `pytest_runtest_makereport` function should yield control back to the test suite after calling the collector.</li>
                                        <li>The `mock_collector` instance should be able to handle multiple calls to `handle_runtest_logreport` without raising any exceptions.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that collection_finish is skipped when disabled.</p>
                                <p><strong>Why Needed:</strong> To prevent a regression where the plugin's hooks are not executed correctly when collection finish is disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The pytest_collection_finish function should be called with False as the stash.get argument.</li>
                                        <li>The pytest_collection_finish function should have been called once with _enabled_key and False as its argument.</li>
                                        <li>The mock_session.config.stash.get method should have returned False when it was called with _enabled_key and False as its argument.</li>
                                        <li>The pytest_collection_finish function should not have executed any hooks in this test case.</li>
                                        <li>The pytest_collection_finish function should not have been called multiple times in this test case.</li>
                                        <li>The pytest_collection_finish function should only be called once in this test case.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 380-381, 384, 388-390, 424-425)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_enabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that collection_finish is called when collection finish is enabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the collector is not called when collection finish is enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The stash_get function should return True for _enabled_key and False for _collector_key.</li>
                                        <li>The mock_collector handle_collection_finish method should be called once with mock_session.items as argument.</li>
                                        <li>MockSession items should have exactly two elements.</li>
                                        <li>MockCollector should not call stash_get or handle_collection_finish on other keys.</li>
                                        <li>pytest_collection_finish should be called with mock_session.items as argument when collection finish is enabled.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 380-381, 384, 388-390, 424, 428-430)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that sessionstart skips when disabled and checks enabled status.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where pytest_sessionstart fails to check the plugin's enabled status when the session is started in disabled mode.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mocked config.stash.get was called with _enabled_key and False</li>
                                        <li>mocked config.stash.get was not called with _enabled_key or False</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 380-381, 384, 388-390, 441-442)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_enabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that sessionstart initializes collector when enabled.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the collector is not initialized even though pytest_sessionstart is enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The _collector_key should be present in mock_stash.</li>
                                        <li>The _start_time_key should be present in mock_stash.</li>
                                        <li>The stash dictionary should contain _enabled_key set to True.</li>
                                        <li>The stash dictionary should not have _config_key set to None.</li>
                                        <li>The stash dictionary should support get() and [] operations without raising an error.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 380-381, 384, 388-390, 441, 445, 448, 450-451)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test pytest_addoption adds expected arguments and verifies specific options.</p>
                                <p><strong>Why Needed:</strong> pytest_addoption may not be correctly handling the 'llm-report' option or other related arguments.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>parser.getgroup.assert_called_with('llm-report', 'LLM-enhanced test reports')</li>
                                        <li>calls.any('--llm-report' in args[0])</li>
                                        <li>calls.any('--llm-coverage-source' in args[0])</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">99 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption_ini</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test pytest_addoption adds INI options (lines 13-34) to ensure it correctly handles ini file additions.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where pytest_addoption does not add INI options to the plugin's configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `pytest_addoption(parser)` is called with a `MagicMock` instance as its argument.</li>
                                        <li>The `addini.call_args_list` attribute of the `parser` object is checked for the expected ini file additions.</li>
                                        <li>The INI options 'llm_report_html', 'llm_report_json', and 'llm_report_max_retries' are found in the ini calls.</li>
                                        <li>The expected values are verified to be present in the ini calls.</li>
                                        <li>The `MagicMock` instance is used instead of a real parser object, ensuring the test can run independently.</li>
                                        <li>The test does not rely on any specific plugin configuration or setup.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">99 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_coverage_calculation</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test coverage percentage calculation logic for terminal summary.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in coverage reporting when terminal summaries are generated.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_terminal_summary` function correctly calculates the coverage percentage.</li>
                                        <li>The `CoverageMapper` class is properly loaded and used to report coverage.</li>
                                        <li>The `ReportWriter` class is called with the correct arguments.</li>
                                        <li>Mocking the existence of a coverage file does not prevent the test from running.</li>
                                        <li>The `report` method of the Coverage object is called correctly.</li>
                                        <li>The `load` method of the CoverageMapper class is called correctly.</li>
                                        <li>The `report_html` parameter is set to 'out.html' as expected.</li>
                                        <li>The mock coverage report matches the expected value (85.5%) when run with a valid configuration.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">53 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 307, 309-312, 324-325, 330-331, 358-368, 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_llm_enabled</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test terminal summary with LLM enabled runs annotations.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the case where LLM is enabled and the plugin provider is not properly configured.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that `pytest_terminal_summary_llm_enabled` is called with the correct configuration.</li>
                                        <li>Check if the config passed to `pytest_terminal_summary_llm_enabled` is correctly set.</li>
                                        <li>Assert that the annotation is called only once, even when multiple tests are run in parallel.</li>
                                        <li>Verify that the correct model name is used for LLM-based providers.</li>
                                        <li>Ensure that the provider is properly configured before running the test.</li>
                                        <li>Test that the `pytest_terminal_summary_llm_enabled` function handles different scenarios correctly.</li>
                                        <li>Verify that the mock stash is created and populated correctly with the provided configuration.</li>
                                        <li>Check if the coverage map is not affected by the patching of dependencies.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">59 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 324-325, 330-333, 336, 338, 341-343, 350-355, 358-368, 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_no_collector</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test terminal summary creates collector if missing.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the plugin does not create a collector even when it is supposed to be present in the configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_terminalreporter.call_args_list[0][1].get(_enabled_key) == False</li>
                                        <li>mock_terminalreporter.call_args_list[0][1].get(_config_key).report_html == 'out.html'</li>
                                        <li>mock_stash._enabled_key == True</li>
                                        <li>mock_stash._config_key == cfg</li>
                                        <li>mock_writer_cls.return_value.__call__.return_value.report_html == 'out.html'</li>
                                        <li>mock_mapper.map_coverage.return_value == {}</li>
                                        <li>mock_mapper_cls.return_value.__call__.return_value.map_coverage.return_value == {}</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 324, 330-331, 358-368, 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_with_aggregation</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test terminal summary with aggregation enabled.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where the aggregation feature is not properly handled in the terminal summary.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregate_dir` parameter should be set to `/agg` when using the `terminal_summary` function with aggregation.</li>
                                        <li>The `aggregate_report` method of the Aggregator instance should be called once with a report object.</li>
                                        <li>The `write_json` and `write_html` methods of the ReportWriter instance should be called once with the correct data.</li>
                                        <li>The `aggregate_dir` parameter should not be set to `/agg` when using the `terminal_summary` function without aggregation.</li>
                                        <li>The `aggregate_report` method of the Aggregator instance should not be called if no report is provided.</li>
                                        <li>The `write_json` and `write_html` methods of the ReportWriter instance should not be called if no data is provided.</li>
                                        <li>The `aggregate_dir` parameter should be set to `/agg` when using the `terminal_summary` function with aggregation, regardless of the number of runs.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 238, 242, 246, 249-250, 252-253, 256-257, 259, 261-265, 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummaryErrors::test_terminal_summary_coverage_error</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test coverage calculation error when loading coverage map.</p>
                                <p><strong>Why Needed:</strong> To prevent a potential bug where the coverage calculation fails due to an OSError during load of the coverage map.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `load` method of the `CoverageMapper` class should not raise an exception when the coverage map is loaded successfully.</li>
                                        <li>The `coverage.Coverage` object returned by `CoverageMapper.load()` should be a valid instance.</li>
                                        <li>The `report_writer.ReportWriter` object created with `ReportWriter` should not raise an exception when writing to it.</li>
                                        <li>The `pytest_terminal_summary` function should not raise a UserWarning when called with the mock configuration and coverage map.</li>
                                        <li>The `coverage.Coverage` object returned by `CoverageMapper.load()` has a valid `report_html` attribute.</li>
                                        <li>The `coverage.Coverage` object returned by `CoverageMapper.load()` has a valid `stash` attribute.</li>
                                        <li>The `pytest_terminal_summary` function does not raise an exception when called with the mock configuration and coverage map.</li>
                                        <li>The `pytest_terminal_summary` function writes to the report writer without raising an exception.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">52 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 315-318, 324-325, 330-331, 358-368, 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_balanced_context</span>
                        <div class="test-meta">
                            <span>7ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the ContextAssembler to assemble a balanced context for test_a.py with a utility function.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression that may occur when the llm_context_mode is set to 'balanced' and the assembly of the context fails due to missing dependencies.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test verifies that the assembled context includes the required file utils.py.</li>
                                        <li>The test verifies that the assembled context includes the required function def util() from the required file utils.py.</li>
                                        <li>The test ensures that the required file utils.py is present in the assembled context.</li>
                                        <li>The test verifies that the required function def util() is included in the assembly of the context.</li>
                                        <li>The test checks for coverage of the required file utils.py and function def util() within the assembly of the context.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">51 lines (ranges: 33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-155, 158-159, 163, 191-192, 194)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_complete_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `ContextAssembler` should assemble the complete context for a test file with no imports.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when a test file has no imports, as it ensures that the context is correctly assembled even without external dependencies.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The source code of the test function `test_1` should be present in the assembled context.</li>
                                        <li>The `test_1` function should be found in the assembled context.</li>
                                        <li>The `test_a.py::test_1` nodeid should match the actual file path of the test function.</li>
                                        <li>The `ContextAssembler` should correctly assemble a test file with no imports.</li>
                                        <li>The `TestCaseResult` nodeid should contain the correct information about the test result.</li>
                                        <li>The context should not be empty or None after assembly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">34 lines (ranges: 33, 49, 52, 55, 58, 60, 63, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 132-133, 180)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_minimal_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the ContextAssembler to assemble a minimal context for a test file.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the ContextAssembler is unable to assemble a minimal context for a test file without any additional configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'test_1' function should be found in the source code of the test file.</li>
                                        <li>The 'ContextAssembler' object should have an empty dictionary as its context.</li>
                                        <li>The 'source' variable should contain the modified source code with the 'test_1' function.</li>
                                        <li>The 'context' variable should be an empty dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">30 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_balanced_context_limits</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the ContextAssembler with balanced context limits to ensure it correctly truncates long content and reports coverage.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the ContextAssembler does not truncate long content and instead leaves it in the output.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'f1.py' file is present in the assembled context.</li>
                                        <li>The 'f1.py' file contains the expected truncation message.</li>
                                        <li>... The length of the 'f1.py' file is within the allowed limit (20 bytes + truncation message).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">34 lines (ranges: 33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-84, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-156, 158-159, 163, 191-192, 194)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_get_test_source_edge_cases</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the ContextAssembler to handle non-existent file and nested test names with parameters.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the ContextAssembler incorrectly handles files that do not exist or have nested test names with parameters.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `_get_test_source` returns an empty string when given a non-existent file path.</li>
                                        <li>The function `_get_test_source` correctly extracts the nested test name and parameter from the provided source code.</li>
                                        <li>The function `_get_test_source` handles nested test names with parameters by including them in the extracted source code.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 33, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_should_exclude</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the ContextAssembler should exclude certain Python files and a secret file from being processed.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the ContextAssembler incorrectly excludes certain files, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config._should_exclude('*.pyc') is True</li>
                                        <li>config._should_exclude('secret/*') is True</li>
                                        <li>assembler._should_exclude('public/readme.md') is False</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 33, 191-194)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_consecutive_lines</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'test_consecutive_lines' test verifies that consecutive lines in a list are compressed using range notation.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when consecutive lines are not compressed correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert compress_ranges([1, 2, 3]) == '1-3'</li>
                                        <li>assert compress_ranges([4, 5, 6]) == '4-6'</li>
                                        <li>assert compress_ranges([]) == ''</li>
                                        <li>assert compress_ranges([1]) == '1'</li>
                                        <li>assert compress_ranges([-1]) == '-1'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_duplicates</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the function correctly handles duplicate ranges.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function incorrectly identifies distinct ranges as duplicates.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return '1-3' for the input range [1, 2, 2, 3, 3, 3].</li>
                                        <li>The function should not return '1-4' for the input range [1, 2, 2, 3, 3, 4].</li>
                                        <li>The function should correctly handle ranges with duplicate values in them.</li>
                                        <li>The function should not incorrectly identify ranges as duplicates when there are no duplicates.</li>
                                        <li>The function should return '1-5' for the input range [1, 2, 2, 3, 3, 4].</li>
                                        <li>The function should correctly handle ranges with duplicate values in them (e.g. [1, 2, 2, 3, 3, 3]).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_empty_list</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `compress_ranges` function with an empty input list.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where an empty list is not correctly compressed to a single string.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return an empty string for an empty input list.</li>
                                        <li>The function should handle the case where `compress_ranges` is called with no arguments (i.e., an empty list) without raising any errors or exceptions.</li>
                                        <li>The function should produce a correct and meaningful output when given an empty list as input, rather than producing an incorrect or misleading result.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 29-30)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_mixed_ranges</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_mixed_ranges' verifies that the `compress_ranges` function handles mixed ranges correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the function incorrectly combines ranges with single values.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output should be in the format '1-3, 5, 10-12, 15'.</li>
                                        <li>The range '1' should be included as is.</li>
                                        <li>The range '2' to '4' should be combined into a single range '2-4'.</li>
                                        <li>The range '5' to '7' should be combined into a single range '5-7'.</li>
                                        <li>The range '8' to '10' should be combined into a single range '8-10'.</li>
                                        <li>The range '11' to '15' should be combined into a single range '11-15'.</li>
                                        <li>All ranges should have a unique start and end value.</li>
                                        <li>No single values should be included in the output without a corresponding range.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_non_consecutive_lines</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that non-consecutive lines are correctly compressed into a single string.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in cases where consecutive line numbers are not separated by commas.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function compresses the input list of integers into a comma-separated string.</li>
                                        <li>The resulting string contains only the specified line numbers (1, 3, and 5).</li>
                                        <li>No other line numbers are included in the output string.</li>
                                        <li>Non-consecutive line numbers are not separated by commas.</li>
                                        <li>Consecutive line numbers are separated correctly by commas.</li>
                                        <li>The function handles edge cases where the input list contains only one or two elements.</li>
                                        <li>It also works correctly when the input list is empty.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_single_line</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_ranges.py::TestCompressRanges::test_single_line</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the single-line function does not correctly handle ranges.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input list should be empty or contain only one element.</li>
                                        <li>The compressed string should match the original input.</li>
                                        <li>No range notation should be used in the output.</li>
                                        <li>The function should raise an error for invalid inputs (e.g., non-numeric values).</li>
                                        <li>The function should correctly handle ranges with multiple elements.</li>
                                        <li>The function should not use any range notation when given a single element.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 29, 33, 35-37, 39, 50, 52, 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_two_consecutive</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_ranges.py::TestCompressRanges::test_two_consecutive</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where two consecutive numbers are compressed to a single range.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return the correct range notation for two consecutive numbers.</li>
                                        <li>The function should handle cases where the input list contains only one number.</li>
                                        <li>The function should not compress consecutive ranges of zeros.</li>
                                        <li>The function should preserve the original order of non-zero numbers in the input list.</li>
                                        <li>The function should handle edge cases such as an empty input list or a list with only one element.</li>
                                        <li>The function should return the correct range notation even if the two consecutive numbers are equal.</li>
                                        <li>The function should not compress ranges that span multiple lines.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_unsorted_input</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the function with an unsorted list of ranges.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not handle unsorted input correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output should be in the format '1-3, 5'.</li>
                                        <li>The comma-separated values should contain both range keys and values.</li>
                                        <li>The ranges should be sorted alphabetically by key.</li>
                                        <li>The function should return an empty string if the input list is empty.</li>
                                        <li>The function should handle duplicate keys correctly (e.g., '1-2, 3').</li>
                                        <li>The function should not raise any exceptions when given unsorted input.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_empty_string</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `expand_ranges` function with an empty string.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function returns incorrect results for empty strings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `expand_ranges` function should return an empty list when given an empty string as input.</li>
                                        <li>The expected output of the `expand_ranges` function for an empty string is indeed an empty list.</li>
                                        <li>The test case verifies that the function handles empty strings correctly and produces no results.</li>
                                        <li>Any additional tests or assertions should be added to ensure this specific scenario works as expected.</li>
                                        <li>The test should also verify that the function raises a `ValueError` when given invalid input, such as non-string values.</li>
                                        <li>To further improve robustness, consider adding error handling for edge cases like empty strings with multiple ranges.</li>
                                        <li>A more comprehensive test case could involve checking the function's behavior with different types of inputs, such as lists or tuples.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 81-82)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_mixed</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_mixed' verifies the expansion of mixed ranges and singles.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the expand_ranges function does not correctly handle mixed range values (e.g., '1-3, 5, 10-12')</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The expanded list should contain all specified numbers from both ranges and singles.</li>
                                        <li>Numbers in the first range should be included before those in the second range.</li>
                                        <li>Single numbers should not be split across multiple ranges.</li>
                                        <li>Negative numbers should still be treated as single values.</li>
                                        <li>Zero is a valid number for this test.</li>
                                        <li>The function should handle cases where the input string contains commas correctly (e.g., '1-3, 5, 10-12').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 81, 84-91, 93, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_range</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'expand_ranges' function is expected to expand a range of numbers.</p>
                                <p><strong>Why Needed:</strong> This test prevents the function from expanding ranges that do not contain all specified numbers.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input string should be in the format 'start-end'</li>
                                        <li>The start value should be less than or equal to the end value</li>
                                        <li>All values in the range should be integers</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 81, 84-91, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_roundtrip</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `expand_ranges` function correctly reverses the compression of a list.</p>
                                <p><strong>Why Needed:</strong> This test prevents bugs where the inverse operation of `compress_ranges` is not implemented correctly, potentially causing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input list should be unchanged after calling `expand_ranges(compressed)`.</li>
                                        <li>All elements in the output list should be present in the original list.</li>
                                        <li>If an element is missing from the original list, it should also be missing from the output list.</li>
                                        <li>If two or more elements are removed from the original list, they should not be present in the output list.</li>
                                        <li>The order of elements in the output list should be preserved.</li>
                                        <li>If all elements are present in both lists (original and expanded), then the compressed list should also be present in the expanded list.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">27 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67, 81, 84-91, 93, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_single_number</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'expand_ranges' function should be able to handle a single input, producing the same output as a range of numbers.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in cases where only one number is provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Input: '5'</li>
                                        <li>Expected Output: [5]</li>
                                        <li>No error or exception should be raised</li>
                                        <li>The function should handle the input correctly and produce a list with one element</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 81, 84-87, 93, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestFormatDuration::test_milliseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the function formats duration as milliseconds for less than 1 second.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the function fails to format durations correctly for values greater than or equal to 1 second.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return '500ms' when given a duration of 0.5 seconds.</li>
                                        <li>The function should return '1ms' when given a duration of 0.001 seconds.</li>
                                        <li>The function should return '0ms' when given a duration of 0.0 seconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestFormatDuration::test_seconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the function formats seconds correctly.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where seconds are not formatted as expected for values less than 1 second.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return the correct string representation of seconds (e.g. '1.23s') for input values between 0 and 1.</li>
                                        <li>The function should return the correct string representation of seconds (e.g. '60.00s') for input values greater than or equal to 1 second.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestOutcomeToCssClass::test_all_outcomes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that all outcomes map to CSS classes correctly.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where 'xfailed' or 'xpassed' are incorrectly mapped to the wrong CSS class.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function outcome_to_css_class() maps 'passed', 'failed', and 'skipped' outcomes to the correct CSS classes ('outcome-passed', 'outcome-failed', 'outcome-skipped')</li>
                                        <li>The function outcome_to_css_class('xfailed') should map to 'outcome-xfailed'</li>
                                        <li>The function outcome_to_css_class('xpassed') should map to 'outcome-xpassed'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 79-85, 87)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where unknown outcomes are not properly handled and instead default to the 'outcome-unknown' class.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>outcome_to_css_class('unknown') == 'outcome-unknown'</li>
                                        <li>outcome_to_css_class('invalid') != 'outcome-unknown'</li>
                                        <li>outcome_to_css_class('default') == 'outcome-default'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 79-85, 87)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_basic_report</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that a basic report is rendered with the expected HTML structure.</p>
                                <p><strong>Why Needed:</strong> This test prevents a rendering issue where the report does not display correctly due to missing or incorrect HTML tags.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The presence of '<!DOCTYPE html>' in the rendered HTML.</li>
                                        <li>The presence of 'Test Report' in the rendered HTML.</li>
                                        <li>The presence of 'test::passed' and 'test::failed' in the rendered HTML.</li>
                                        <li>The presence of 'PASSED' and 'FAILED' in the rendered HTML.</li>
                                        <li>The presence of 'Plugin:</strong> v0.1.0' and 'Repo:</strong> v1.2.3' in the rendered HTML.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">52 lines (ranges: 65-67, 79-85, 87, 121-124, 126-127, 131-132, 141-143, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test renders coverage for fallback HTML.</p>
                                <p><strong>Why Needed:</strong> Prevents regression and ensures accurate coverage reporting.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test verifies that the `render_fallback_html` function includes the source file 'src/foo.py' in its rendered HTML.</li>
                                        <li>The test checks that there are exactly 5 lines of code in the rendered HTML.</li>
                                        <li>The test verifies that the line ranges for each line range from 1 to 5, indicating coverage.</li>
                                        <li>The test ensures that the `CoverageEntry` object is created with the correct file path and line count.</li>
                                        <li>The test checks that the `CoverageEntry` object has a `file_path` attribute matching 'src/foo.py' and a `line_count` attribute equal to 5.</li>
                                        <li>The test verifies that the rendered HTML includes all lines of code from the source file, as indicated by the line ranges (1-5).</li>
                                        <li>The test checks that the rendered HTML does not include any lines outside the specified range (1-5).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">52 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-129, 131-132, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_llm_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the report includes LLM annotations for LLMs.</p>
                                <p><strong>Why Needed:</strong> This test prevents authentication bypass by ensuring LLM annotations are included in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The report contains "Tests login flow" as a key assertion.</li>
                                        <li>The report contains "Prevents auth bypass" as a key assertion.</li>
                                        <li>The report includes LLM annotations for LLMs.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-134, 136-137, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_source_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test renders source coverage for fallback HTML.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where missing source code is not properly reported.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'Source Coverage' section should be present in the rendered HTML.</li>
                                        <li>The file path 'src/foo.py' should be included in the 'Source Coverage' section.</li>
                                        <li>The coverage percentage (80.0%) should be displayed correctly in the HTML.</li>
                                        <li>The ranges '1-4, 6-8' and '5, 9-10' should be accurately reported as covered or missed.</li>
                                        <li>The number of statements (10) should be included in the coverage report.</li>
                                        <li>The number of missed statements (2) should be correctly identified as missing.</li>
                                        <li>The percentage of covered statements (80.0%) should be calculated and displayed correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">63 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-132, 141-142, 145-153, 158-164, 166-172, 177, 192, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_xpass_summary</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'Should include xfailed/xpassed summary entries' verifies that the rendered report includes XFailed and XPassed summaries.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the summary section is missing or incorrectly formatted.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The string 'XFailed' should be present in the HTML output.</li>
                                        <li>The string 'XPassed' should be present in the HTML output.</li>
                                        <li>Both 'XFailed' and 'XPassed' should be included in the rendered report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">50 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-132, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestComputeSha256::test_different_content</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'different_content' verifies that the same input produces different hashes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where two inputs with the same content but different origins produce the same hash.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `compute_sha256` should return different values for different inputs.</li>
                                        <li>The output of `compute_sha256(b'hello')` and `compute_sha256(b'world')` should be different.</li>
                                        <li>The output of `compute_sha256(b'hello') != compute_sha256(b'world')` should be True.</li>
                                        <li>The hash of `compute_sha256(b'hello')` should not match the hash of `compute_sha256(b'world')`.</li>
                                        <li>The hash of `compute_sha256(b'hello')` should not be equal to `compute_sha256(b'hello')` (case sensitivity)</li>
                                        <li>The hash of `compute_sha256(b'hello')` should not be equal to `compute_sha256(b'HELLO')` (case sensitivity)</li>
                                        <li>The hash of `compute_sha256(b'hello')` should not be equal to `compute_sha256(b'world')` (case insensitivity)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 55)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestComputeSha256::test_empty_bytes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'Empty bytes should produce consistent hash' verifies that an empty byte string produces the same hash as a non-empty byte string.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the hash of an empty byte string is different from the hash of a non-empty byte string, potentially leading to incorrect reporting or analysis.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The hash of an empty byte string should be equal to the hash of a non-empty byte string (i.e., `hash1 == hash2`).</li>
                                        <li>The length of the resulting hash should be consistent for both cases (i.e., `len(hash1) == 64`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 55)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_run_meta</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test builds run metadata with correct version info.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the report writer does not include the pytest version in the build run metadata.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The duration of the test should be 60 seconds.</li>
                                        <li>The pytest version should have a value.</li>
                                        <li>The plugin version should be '0.1.0'.</li>
                                        <li>The python version should also be present and correct.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_summary_all_outcomes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the `build_summary` method correctly counts all outcome types and their corresponding values.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the count of each outcome type is not accurate due to missing or incorrect data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total number of outcomes should be equal to 6 (passed, failed, skipped, xfailed, xpassed, error).</li>
                                        <li>Each outcome type should have its corresponding value in the `summary` dictionary: passed = 1, failed = 1, skipped = 1, xfailed = 1, xpassed = 1, error = 1.</li>
                                        <li>The values of each outcome type are correctly assigned to their respective keys in the `summary` dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 156-158, 312, 314-315, 317-328, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_summary_counts</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_build_summary_counts' verifies that the summary counts outcomes correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the summary incorrectly counts failed tests as passed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>asserts that the total count of all tests is equal to 4</li>
                                        <li>asserts that the number of passed tests is equal to 2</li>
                                        <li>asserts that the number of failed tests is equal to 1</li>
                                        <li>asserts that the number of skipped tests is equal to 1</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">13 lines (ranges: 156-158, 312, 314-315, 317-322, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_create_writer</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the Writer initializes correctly with a given configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the Writer does not properly initialize with the provided configuration, potentially leading to incorrect or missing data in reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config` attribute of the `writer` object is set to the specified `Config` instance.</li>
                                        <li>The `warnings` list of the `writer` object is empty.</li>
                                        <li>The `artifacts` list of the `writer` object is empty.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 156-158)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_assembles_tests</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that ReportWriter writes a report with all tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the report does not include all tests, potentially causing confusion or missing important information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the report.tests list should be equal to 2.</li>
                                        <li>The value of report.summary.total should be equal to 2.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">93 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_includes_coverage_percent</span>
                        <div class="test-meta">
                            <span>6ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `ReportWriter` class writes a report with a total coverage percentage.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the coverage percentage is not included in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report.summary.coverage_total_percent` attribute should be equal to the provided `coverage_percent` value.</li>
                                        <li>The `report.summary.coverage_total_percent` attribute should contain only numeric values (e.g., integers or floats).</li>
                                        <li>The `report.summary.coverage_total_percent` attribute should not exceed 100% if coverage is above 100%</li>
                                        <li>The `report.summary.coverage_total_percent` attribute should be calculated correctly based on the provided `coverage_percent` value.</li>
                                        <li>The `report.summary.coverage_total_percent` attribute should be updated after writing a new report with different `coverage_percent` values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">93 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-199, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_includes_source_coverage</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test ReportWriter::test_write_report_includes_source_coverage verifies that the test writes a report with includes source coverage.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the report does not include source coverage, potentially misleading users about the code's quality.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the `source_coverage` list in the report should be exactly 1.</li>
                                        <li>The file path of the first `SourceCoverageEntry` in the `source_coverage` list should match 'src/foo.py'.</li>
                                        <li>All statements covered by the source code should be included in the coverage summary.</li>
                                        <li>At least one statement from the missed code should be included in the coverage summary.</li>
                                        <li>All covered lines should have a percentage greater than or equal to 87.5%.</li>
                                        <li>The `covered_ranges` attribute of each `SourceCoverageEntry` should match '1-4, 6-7'.</li>
                                        <li>At least one line from the missed code should be included in the coverage summary.</li>
                                        <li>All covered lines should have a percentage greater than or equal to 87.5% and less than 100%.</li>
                                        <li>The `missed_ranges` attribute of each `SourceCoverageEntry` should match '5'.</li>
                                        <li>The total number of statements, missed, and covered lines in the coverage summary should be consistent across all test runs.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">92 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_merges_coverage</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Report should merge coverage into tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the report does not correctly merge coverage from multiple tests, leading to incorrect reporting of test coverage.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert len(report.tests[0].coverage) == 1</li>
                                        <li>assert report.tests[0].coverage[0].file_path == 'src/foo.py'</li>
                                        <li>assert all(isinstance(c, CoverageEntry) for c in report.tests[0].coverage)</li>
                                        <li>assert all('file_path' in c.__dict__ for c in report.tests[0].coverage)</li>
                                        <li>assert len(report.tests[0].coverage['test1']) == 1</li>
                                        <li>assert report.tests[0].coverage['test1'][0] is not None</li>
                                        <li>assert isinstance(report.tests[0].coverage['test1'][0], CoverageEntry)</li>
                                        <li>assert 'file_path' in report.tests[0].coverage['test1'][0].__dict__</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">94 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186-189, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_atomic_write_fallback</span>
                        <div class="test-meta">
                            <span>6ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the fallback to direct write when atomic write fails.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where an atomic write operation fails and the direct write is used instead, potentially leading to unexpected behavior or data loss.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that the report.json file exists at the expected path.</li>
                                        <li>Verify that any warnings have code 'W203' when the direct write fails.</li>
                                        <li>Verify that the direct write has been performed successfully by checking for the existence of the report.json file.</li>
                                        <li>Verify that all warnings in the direct write are of type 'W203'.</li>
                                        <li>Verify that there are no other warnings or errors in the writer's output.</li>
                                        <li>Verify that the direct write does not fail and return an error code.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">125 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506-507, 509-512, 515-516)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_creates_directory_if_missing</span>
                        <div class="test-meta">
                            <span>6ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `ReportWriter` creates an output directory if it doesn't exist.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the report writer fails to create a directory when the input JSON file does not exist.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>If `tmp_path / 'subdir' / 'report.json'` does not exist, then `tmp_path / 'subdir' / 'report.json'.exists()` should return True.</li>
                                        <li>The output directory created by the report writer should have the correct name (`'subdir'`) and be located in the specified path (`'tmp_path.subdir'`).</li>
                                        <li>If a test case is passed, then `tmp_path / 'subdir' / 'report.json'.exists()` should return False.</li>
                                        <li>The output directory created by the report writer should have the correct permissions (i.e., read and write access for the current user).</li>
                                        <li>The output directory created by the report writer should be a valid JSON file with the expected structure.</li>
                                        <li>If an error occurs while writing the report, then `tmp_path / 'subdir' / 'report.json'.exists()` should return False.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">84 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">123 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-477, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_ensure_dir_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `ReportWriter` class raises a warning when attempting to create a directory that already exists.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `ReportWriter` class does not raise an error when trying to write a report in a non-existent directory.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `writer._ensure_dir(json_path)` should raise a `FileExistsError` with code 'W201' when creating a directory that already exists.</li>
                                        <li>The `writer.warnings` list should contain at least one warning object with code 'W201' when attempting to create the non-existent directory.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 156-158, 470-473, 480-484)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_git_info_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the report writer to handle git command failures gracefully.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the report writer fails to retrieve git information when it's not found.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_git_info()` should return `None` for both SHA and dirty flag if git is not found.</li>
                                        <li>The function `get_git_info()` should raise an exception with message 'Git not found' if git command fails.</li>
                                        <li>The test should pass even when the subprocess call to check_output raises an exception.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 67-73, 85-86)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_creates_file</span>
                        <div class="test-meta">
                            <span>31ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the `write_report` method creates an HTML file with expected content.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the report writer does not create an HTML file, potentially leading to missing or incorrect report data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file should exist at the specified path.</li>
                                        <li>The HTML file should contain the expected content (test1, test2, PASSED, FAILED, Skipped, XFailed, XPassed, Errors).</li>
                                        <li>All nodes in the report should be present in the HTML file.</li>
                                        <li>Each node type (PASSED, FAILED, Skipped, XFailed, XPassed) should be included in the HTML content.</li>
                                        <li>The 'Errors' and 'XPassed/XFailed' node types should be present in the HTML file.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">115 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_includes_xfail_summary</span>
                        <div class="test-meta">
                            <span>32ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_write_html_includes_xfail_summary' verifies that the report writer includes xfail outcomes in the HTML summary.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the report does not include xfail outcomes in the HTML summary, potentially misleading users about the status of tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'XFAILED' and 'XPASSED' keywords are present in the HTML summary.</li>
                                        <li>The 'xfailed' and 'xpassed' keywords are present in the HTML summary.</li>
                                        <li>All xfail outcomes are included in the HTML summary.</li>
                                        <li>No xpass outcomes are included in the HTML summary.</li>
                                        <li>The report does not include any unknown or unreported test results.</li>
                                        <li>The report includes all expected test results, including xfail and xpassed outcomes.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-326, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_json_creates_file</span>
                        <div class="test-meta">
                            <span>6ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that a JSON file is created with the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the report writer does not create a JSON file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report.json` file should exist at the specified path.</li>
                                        <li>At least one artifact should be tracked in the JSON file.</li>
                                        <li>The length of the artifacts list should be greater than zero.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">78 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">117 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_creates_file</span>
                        <div class="test-meta">
                            <span>33ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Should create PDF file when Playwright is available.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression that would occur if the playwright module was not available or could not be imported correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `write_pdf` function from the `ReportWriter` class should successfully write a PDF file to the specified path.</li>
                                        <li>The `report.pdf` attribute of the `writer` object should contain the expected file path.</li>
                                        <li>Any artifacts created by the report writer should have the correct path relative to the `report.pdf` file.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">125 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 401, 410, 412, 414-423, 434-435, 437-443, 448, 453, 455, 458-462, 470-471)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_missing_playwright_warns</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that a warning is raised when PDF output is requested without Playwright.</p>
                                <p><strong>Why Needed:</strong> To prevent the test from failing due to a missing required module (Playwright).</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file 'report.pdf' should not exist.</li>
                                        <li>Any warnings with code WarningCode.W204_PDF_PLAYWRIGHT_MISSING value should be present.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">98 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 401-405, 408)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer_coverage_v2.py::test_report_writer_ensure_dir_creation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Ensures directory creation of report writer output files.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where the report writer does not create the required directory structure for HTML files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `tmp_dir / 'r.html'` path exists before any warnings are printed.</li>
                                        <li>Any warning messages (code 'W202') are present in the output file.</li>
                                        <li>The `tmp_dir / 'r.html'` path is deleted after writing the report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 156-158, 470-477)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer_coverage_v2.py::test_report_writer_metadata_skips</span>
                        <div class="test-meta">
                            <span>10ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the scenario where report_writer_metadata_skips verifies that metadata skips when reports are disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that metadata is skipped when reports are disabled, which is a critical check for accurate reporting.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'start_time' key should be present in the metadata.</li>
                                        <li>Metadata should not contain an 'llm_model' key.</li>
                                        <li>The 'llm_model' value should be None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_schemas.py::TestAnnotationSchema::test_from_dict_full</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `AnnotationSchema.from_dict` can create a valid annotation from a dictionary with all required fields.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the `from_dict` method is used incorrectly, potentially causing invalid annotations to be created.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert schema.scenario == 'Verify login'</li>
                                        <li>assert schema.why_needed == 'Catch auth bugs'</li>
                                        <li>assert schema.key_assertions == ['assert 200', 'assert token']</li>
                                        <li>assert schema.confidence == 0.95</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_schemas.py::TestAnnotationSchema::test_to_dict_full</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test converting AnnotationSchema to dictionary with all fields.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in schema conversion logic, ensuring accurate representation of annotation data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert 'scenario' in data and data['scenario'] == 'Verify login'</li>
                                        <li>assert 'why_needed' in data and data['why_needed'] == 'Catch auth bugs'</li>
                                        <li>assert 'key_assertions' in data and data['key_assertions'].all()</li>
                                        <li>assert 200 in data['key_assertions'] and data['key_assertions'][0] == 'assert 200'</li>
                                        <li>assert token in data['key_assertions'] and data['key_assertions'][1] == 'assert token'</li>
                                        <li>assert confidence in data and data['confidence'] == 0.95</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 90-92, 94-98)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_report_created</span>
                        <div class="test-meta">
                            <span>82ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The HTML report is generated correctly and can be accessed.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the report might not be created or accessible due to changes in the pytester's environment.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file path of the report should exist after running the test.</li>
                                        <li>The content of the report should contain '<html>' and 'test_simple' as expected.</li>
                                        <li>The function name 'test_simple' should be present in the report content.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 324, 330-331, 358-368, 380-381, 384, 388-390, 401, 405, 424, 428-430, 441, 445, 448, 450-451)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">101 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_summary_counts_all_statuses</span>
                        <div class="test-meta">
                            <span>117ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> test_html_summary_counts_all_statuses verifies that all statuses are included in the HTML summary.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the count of all statuses is missing from the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>asserts that 'Total Tests' and 'Errors' labels appear in the report</li>
                                        <li>asserts that 'Passed', 'Failed', 'Skipped', 'XFailed', and 'XPassed' labels appear in the report</li>
                                        <li>asserts that 'Errors' label appears only once in the report</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 78-79, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212-214, 216, 227-228, 230-236, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 324, 330-331, 358-368, 380-381, 384, 388-390, 401, 405, 424, 428-430, 441, 445, 448, 450-451)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">111 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-328, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_json_report_created</span>
                        <div class="test-meta">
                            <span>72ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The JSON report is created successfully.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the report generation process fails to create the expected JSON file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report_path` exists after running the test.</li>
                                        <li>The `data` dictionary in the `report_path` contains the correct schema version and summary statistics.</li>
                                        <li>The total number of tests passed is equal to the total number of tests failed.</li>
                                        <li>At least one test passed and at least one test failed.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">51 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-118, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 324, 330-331, 358-368, 380-381, 384, 388-390, 401, 405, 424, 428-430, 441, 445, 448, 450-451)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">107 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_annotations_in_report</span>
                        <div class="test-meta">
                            <span>75ms</span>
                            <span title="Covered file count">üõ°Ô∏è 13</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that LLM annotations are included in the report generated by pytester for a provider enabled.</p>
                                <p><strong>Why Needed:</strong> Prevents regressions and ensures that LLM annotations are properly included in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The scenario 'Checks the happy path' is present in the report.</li>
                                        <li>The reason 'Prevents regressions' is present in the report.</li>
                                        <li>The key assertions 'asserts True' are present in the report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 52-53, 72, 75, 80, 107, 110-111, 128, 136, 147, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">23 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69-70, 94-95, 97)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">94 lines (ranges: 104-107, 109-111, 113, 115, 161-165, 167, 169, 171, 173, 176, 178-180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407-419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">186 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 324, 330-333, 336, 338, 341-345, 348, 350-355, 358-368, 380-381, 384, 388-390, 401, 405, 424, 428-430, 441, 445, 448, 450-451)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_error_is_reported</span>
                        <div class="test-meta">
                            <span>90.08s</span>
                            <span title="Covered file count">üõ°Ô∏è 12</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that LLM errors are surfaced in HTML output.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where LLM errors are not reported correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test verifies the presence of 'LLM error' and 'boom' in the report content.</li>
                                        <li>The test asserts that both 'LLM error' and 'boom' are found in the report content.</li>
                                        <li>The test checks for the correct spelling of 'LLM error' and 'boom'.</li>
                                        <li>The test verifies that the LLM error is reported correctly, not just raised.</li>
                                        <li>The test ensures that the error message contains both 'LLM error' and 'boom'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 39-41, 53, 55-56, 86, 88, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">73 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137-139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198-201, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 52-53, 72, 75, 80, 107, 110-111, 128, 136, 147, 165, 167, 175, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 78-79, 81-82, 84-85, 94-95, 97)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">186 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 324, 330-333, 336, 338, 341-346, 350-355, 358-368, 380-381, 384, 388-390, 401, 405, 424, 428-430, 441, 445, 448, 450-451)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">101 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestMarkers::test_llm_opt_out_marker</span>
                        <div class="test-meta">
                            <span>56ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the LLM opt-out marker functionality.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in LLM opt-out marker detection, ensuring that all test cases are correctly marked as 'opted out'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `test_opt_out()` is called with the correct arguments.</li>
                                        <li>The `llm_opt_out` attribute of each test case is set to `True` after running the LLM opt-out marker.</li>
                                        <li>The number of tests that pass (i.e., are marked as 'opted out') remains unchanged across different runs.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181-182, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180-182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 324, 330-331, 358-368, 380-381, 384, 388-390, 401, 405, 424, 428-430, 441, 445, 448, 450-451)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestMarkers::test_requirement_marker</span>
                        <div class="test-meta">
                            <span>55ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that a requirement marker is correctly recorded and verified.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the requirement marker might not be recorded or verified correctly, potentially leading to false positives or negatives in the test results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest.mark.requirement` decorator is applied to the `test_with_req` function with two requirements: 'REQ-001' and 'REQ-002'.</li>
                                        <li>The `pytester.runpytest` command is used to run the tests with a custom report file, which includes the requirement markers.</li>
                                        <li>The test data is loaded from the report file using `json.loads`, and the length of the tests list is verified to be 1.</li>
                                        <li>The requirements for each test are extracted from the test data and verified to contain 'REQ-001' and 'REQ-002'.</li>
                                        <li>The `pytester.path` attribute is used to get the path to the report file, which is a required argument for the `runpytest` command.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-200, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188-190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 324, 330-331, 358-368, 380-381, 384, 388-390, 401, 405, 424, 428-430, 441, 445, 448, 450-451)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_multiple_xfail_outcomes</span>
                        <div class="test-meta">
                            <span>62ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'Multiple xfailed tests are recorded in the report' verifies that multiple xfailed tests are correctly reported.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that multiple xfailed tests are not missed or incorrectly counted.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total number of xfailed tests is equal to the sum of individual test outcomes.</li>
                                        <li>Each xfailed test has a corresponding outcome in the report (xfailed and xpassed).</li>
                                        <li>Multiple xfailed tests are correctly recorded in the report, without any duplicates or omissions.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 324, 330-331, 358-368, 380-381, 384, 388-390, 401, 405, 424, 428-430, 441, 445, 448, 450-451)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">108 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-324, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_skip_outcome</span>
                        <div class="test-meta">
                            <span>55ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that skipped tests are recorded and their count is verified.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the number of skipped tests is not correctly reported.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'summary' key in the report.json file should contain the correct number of skipped tests.</li>
                                        <li>The value of the 'skipped' key in the 'summary' dictionary should be equal to 1.</li>
                                        <li>The total count of skipped tests should match the actual number of skipped tests in the test suite.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">43 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 106-107, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 324, 330-331, 358-368, 380-381, 384, 388-390, 401, 405, 424, 428-430, 441, 445, 448, 450-451)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">107 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321-322, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_xfail_outcome</span>
                        <div class="test-meta">
                            <span>57ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the test 'test_xfail' is marked as Xfailed and its outcome is recorded in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the 'test_xfail' function is not properly marked as Xfailed, causing it to be counted towards the total number of failed tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'test_xfail' function is marked with the @pytest.mark.xfail marker.</li>
                                        <li>The outcome of the test 'test_xfail' is recorded in the report.</li>
                                        <li>The value of 'xfailed' in the report matches the number of times the 'test_xfail' function was run.</li>
                                        <li>The total count of failed tests is updated correctly after running the test.</li>
                                        <li>The 'summary' section of the report includes the correct key-value pair for 'xfailed'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 324, 330-331, 358-368, 380-381, 384, 388-390, 401, 405, 424, 428-430, 441, 445, 448, 450-451)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">108 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-324, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestParametrization::test_parametrized_tests</span>
                        <div class="test-meta">
                            <span>58ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the parameterized tests feature to ensure it records and runs correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by verifying that the parameterized tests are recorded separately and run with the correct report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total number of tests in the report is 3 (1 passed, 2 failed).</li>
                                        <li>Each test has a unique name (test_param) and a valid assertion (assert x > 0).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167, 169-171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 324, 330-331, 358-368, 380-381, 384, 388-390, 401, 405, 424, 428-430, 441, 445, 448, 450-451)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_help_contains_examples</span>
                        <div class="test-meta">
                            <span>50ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The CLI help text should include usage examples.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the help message does not contain any usage examples, making it difficult for users to understand how to use the plugin.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert result.stdout.fnmatch_lines(['*Example:*--llm-report*'])</li>
                                        <li>assert 'Example:' in result.stdout</li>
                                        <li>assert '--llm-report' in result.stdout</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_markers_registered</span>
                        <div class="test-meta">
                            <span>45ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM markers are registered and correctly displayed in the pytest output.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where LLM markers are not properly registered or displayed, potentially causing issues with the test suite.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm_opt_out' marker should be found in the stdout of the pytest run.</li>
                                        <li>The 'llm_context' marker should be found in the stdout of the pytest run.</li>
                                        <li>The 'requirement' marker should be found in the stdout of the pytest run.</li>
                                        <li>The 'llm_opt_out', 'llm_context', and 'requirement' markers should match the expected lines in the stdout.</li>
                                        <li>The 'llm_opt_out' marker should not be matched with any other marker in the stdout.</li>
                                        <li>The 'llm_context' marker should not be matched with any other marker in the stdout.</li>
                                        <li>The 'requirement' marker should not be matched with any other marker in the stdout.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_plugin_registered</span>
                        <div class="test-meta">
                            <span>51ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the plugin is correctly registered via pytest11 and displays the LLM report in stdout.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the plugin might not be properly registered or configured, potentially leading to incorrect results or errors during testing.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytester.runpytest` command is executed with the `--help` option.</li>
                                        <li>The output of the `stdout` stream contains the string 'LLM report'.</li>
                                        <li>The plugin is registered correctly via pytest11.</li>
                                        <li>The LLM report is displayed in stdout.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 380-381, 384, 388-390)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestSpecialCharacters::test_special_chars_in_nodeid</span>
                        <div class="test-meta">
                            <span>82ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that special characters in nodeid are handled correctly by pytester.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential crash and ensures the HTML generated is valid.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report.html` file should exist after running the test.</li>
                                        <li>The `report.html` file should contain '<html>' in its content.</li>
                                        <li>The nodeid parameter in pytester.makepyfile does not cause a crash but instead generates an HTML report with valid content.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 324, 330-331, 358-368, 380-381, 384, 388-390, 401, 405, 424, 428-430, 441, 445, 448, 450-451)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">101 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_boundary_one_minute</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the 'format_duration' function with a boundary of exactly one minute.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly formats minutes as seconds instead of seconds.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result should be in the format '1m 0.0s'.</li>
                                        <li>The result should contain only one unit (either 's' for seconds or 'm' for minutes).</li>
                                        <li>The value of '0.0s' should not exceed the maximum allowed length.</li>
                                        <li>The function should handle cases where the input is less than or equal to zero.</li>
                                        <li>The function should handle cases where the input is exactly one minute (60 seconds).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_microseconds_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function to ensure it correctly formats sub-millisecond durations as microseconds.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not format durations with microseconds when they are less than one millisecond.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of calling `format_duration(0.0005)` should contain the string 'Œºs' (microseconds).</li>
                                        <li>The result of calling `format_duration(0.001)` should be equal to '1Œºs'.</li>
                                        <li>The function should correctly format durations with microseconds, even when they are less than one millisecond.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 39-40)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_milliseconds_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `format_duration` function correctly formats sub-second durations as milliseconds.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the format string does not include 'ms' for all sub-second durations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of calling `format_duration(0.5)` should contain the string 'ms'.</li>
                                        <li>The value of `result` should be equal to '500.0ms'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 39, 41-42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_minutes_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `format_duration` function to verify it correctly formats durations over a minute.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the function incorrectly returns 'm' instead of 'mm' for minutes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return '1m 30.5s' after formatting a duration of 90.5 seconds.</li>
                                        <li>The assertion should check if the returned string contains the character 'm'.</li>
                                        <li>The assertion should compare the result with the expected string '1m 30.5s'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_multiple_minutes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function with a scenario that formats multiple minutes.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in cases where the input duration is in minutes, as it may be incorrectly formatted to seconds.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of calling `format_duration(185.0)` should be '3m 5.0s'.</li>
                                        <li>The format string should include a unit of 'm' for minutes and an optional unit of 's' for seconds.</li>
                                        <li>The function should correctly handle cases where the input duration is in minutes but not in seconds (e.g., 185.25).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_one_second</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function with a duration of exactly one second.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in time-related functionality where a single-second duration is expected.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function correctly formats the duration as '1.00s'.</li>
                                        <li>The function returns an error if the input duration is not exactly one second.</li>
                                        <li>The function does not silently fail for non-numeric input durations.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 39, 41, 43-44)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_seconds_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the 'seconds' format for durations under one minute.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where seconds are not correctly formatted as 'xx.s' (e.g., 10.0s becomes 10.00s).</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result contains the string 's', indicating it is in seconds format.</li>
                                        <li>The result equals '5.50s' to ensure correct conversion.</li>
                                        <li>The result does not contain any non-numeric characters, ensuring it is a valid time string.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 39, 41, 43-44)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_small_milliseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function with a duration of 1 millisecond.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in formatting small durations to milliseconds.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output should be '1.0ms' when the input is 1 millisecond.</li>
                                        <li>The unit should be 'ms' (milli-seconds).</li>
                                        <li>No decimal places should be displayed for the duration value.</li>
                                        <li>The function should correctly handle durations in the range of milliseconds.</li>
                                        <li>No exceptions should be raised if the input is not a non-negative number.</li>
                                        <li>The output should match the expected string representation when converted to a float.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 39, 41-42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_very_small_microseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function with a very small duration (1 microsecond).</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly formats durations less than 1 millisecond.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output of `format_duration(0.000001)` should be '1Œºs'.</li>
                                        <li>The value of `result` is equal to '1Œºs' when passed as an argument to `format_duration`.</li>
                                        <li>The function correctly formats durations less than 1 millisecond.</li>
                                        <li>The function handles negative values and zero correctly.</li>
                                        <li>The function does not throw any exceptions for invalid input (e.g., non-numeric values).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 39-40)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_datetime_with_utc</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the functionality of datetime with UTC timezone in ISO format.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where datetime objects from other timezones are not correctly formatted into UTC timezone.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The datetime object is created with the correct timezone (UTC).</li>
                                        <li>The iso_format function returns the expected result ('2024-01-15T10:30:45+00:00').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_naive_datetime</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that naive datetime formats are correctly converted to ISO format without timezone.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where naive datetime formats may not be correctly converted to ISO format, potentially leading to incorrect results or errors in downstream applications.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `iso_format(dt)` correctly converts the input `dt` (2024-06-20T14:00:00) to an ISO formatted string "2024-06-20T14:00:00".</li>
                                        <li>The function does not add any timezone information to the output.</li>
                                        <li>The function handles edge cases where the input datetime is in a format that is not supported by `iso_format` (e.g. invalid date or time values).</li>
                                        <li>The function raises an error if the input datetime is not a valid ISO formatted string.</li>
                                        <li>The function preserves the original timezone information of the input datetime.</li>
                                        <li>The function correctly handles different cases where the input datetime is in UTC, EST, etc.</li>
                                        <li>The function does not silently ignore or suppress errors that occur during conversion (e.g. invalid date/time values).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_with_microseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `iso_format` function with datetime objects that include microseconds.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the `iso_format` function may not correctly format dates with microseconds if the input datetime object does not have enough time components.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of calling `iso_format(dt)` should contain the string '123456'.</li>
                                        <li>The result of calling `iso_format(dt)` should include the substring '123456' in its value.</li>
                                        <li>The format string passed to `iso_format(dt)` should be able to correctly handle microseconds by including them in the output.</li>
                                        <li>The `datetime` object passed to `iso_format(dt)` should have a time component that includes microseconds.</li>
                                        <li>The `tzinfo` parameter of the `datetime` object passed to `iso_format(dt)` is set to UTC.</li>
                                        <li>The `result` variable should contain the string '123456' after calling `iso_format(dt).'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestUtcNow::test_has_utc_timezone</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `utc_now()` function returns a datetime object with an associated UTC timezone.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in tests where the test environment does not have a valid UTC timezone.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>result.tzinfo is not None</li>
                                        <li>result.tzinfo == UTC</li>
                                        <li>assert result.tzinfo is not None and result.tzinfo == UTC</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestUtcNow::test_is_current_time</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `utc_now()` function returns a time within UTC.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the current time is not correctly identified as being in UTC.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `before` variable should be less than or equal to the `result` and greater than or equal to the `after` variables.</li>
                                        <li>The `result` should be within a tolerance of `before` and `after`.</li>
                                        <li>The `utc_now()` function correctly identifies the current time as being in UTC.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestUtcNow::test_returns_datetime</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `utc_now()` function should return a datetime object.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function returns an incorrect type of value (datetime vs. datetime.datetime).</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>result is an instance of datetime or datetime.datetime</li>
                                        <li>result is not None</li>
                                        <li>result has a valid timezone</li>
                                        <li>result is not a string</li>
                                        <li>result is not a timedelta</li>
                                        <li>result is not a date</li>
                                        <li>result is not a time</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 380-381, 384, 388-390)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
        </div>

        <section class="source-coverage">
            <h2>Source Coverage</h2>
            <div class="source-coverage-table">
                <div class="source-coverage-header">
                    <span>File</span>
                    <span>Stmts</span>
                    <span>Miss</span>
                    <span>Cover</span>
                    <span>%</span>
                    <span>Covered Lines</span>
                    <span>Missed Lines</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/_git_info.py</span>
                    <span>2</span>
                    <span>0</span>
                    <span>2</span>
                    <span>100.0%</span>
                    <span class="source-lines">2-3</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/aggregation.py</span>
                    <span>116</span>
                    <span>5</span>
                    <span>111</span>
                    <span>95.69%</span>
                    <span class="source-lines">13, 15-19, 21, 35, 38, 44, 46, 52-53, 55-57, 59, 61-64, 69, 73-74, 77-80, 84, 87-89, 93, 103, 109-111, 113-117, 119-120, 125, 127-128, 130-131, 134-135, 141-144, 146, 148, 162, 164, 168, 170, 172, 182, 184-188, 190-191, 194, 196, 205, 217, 219-233, 235, 237, 245-246, 248-249, 251, 253-255, 259, 262-263, 265-266, 269, 271-272, 274, 276-277, 281</span>
                    <span class="source-lines">66, 90-91, 192, 203</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/cache.py</span>
                    <span>47</span>
                    <span>3</span>
                    <span>44</span>
                    <span>93.62%</span>
                    <span class="source-lines">13, 15-19, 21, 27, 33, 39-41, 43, 53, 55-56, 58, 60-62, 68-69, 78, 86, 88, 90, 92, 94, 97, 103, 107, 118-119, 121, 123, 129, 132-136, 141, 144, 153</span>
                    <span class="source-lines">64-65, 130</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/collector.py</span>
                    <span>111</span>
                    <span>2</span>
                    <span>109</span>
                    <span>98.2%</span>
                    <span class="source-lines">19, 21-22, 24, 26-27, 33-34, 45-50, 52, 58, 60-62, 69, 78-79, 81, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127-128, 130, 132-133, 135-137, 140, 143, 155, 163-164, 167-169, 171, 173, 181-182, 185-189, 191, 198-200, 202, 209-210, 212-214, 216, 218, 227-228, 230-236, 238, 241, 250-252, 254, 261, 264-265, 268-269, 271, 277, 279, 285</span>
                    <span class="source-lines">141, 239</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/coverage_map.py</span>
                    <span>135</span>
                    <span>10</span>
                    <span>125</span>
                    <span>92.59%</span>
                    <span class="source-lines">13, 15-17, 19-22, 30, 38, 44-45, 47, 58-60, 64, 72-73, 83, 86, 88-90, 92, 94-96, 98, 101-104, 106-108, 114, 116, 118, 121-122, 127, 131-135, 137-140, 144-146, 148, 150, 152-153, 156, 160-162, 165, 167-168, 173, 176, 178-184, 187-189, 191, 196, 199-200, 202, 204, 216-217, 220, 224-225, 228-234, 236, 239, 241, 243-244, 246-248, 250, 252-254, 259-260, 263-264, 271, 273, 276-279, 281-283, 285, 299-300, 302, 308</span>
                    <span class="source-lines">62, 123, 125, 128, 157, 221, 249, 251, 257, 274</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/errors.py</span>
                    <span>35</span>
                    <span>0</span>
                    <span>35</span>
                    <span>100.0%</span>
                    <span class="source-lines">8-9, 12, 25-28, 31-36, 39-42, 45-46, 49-51, 54-55, 64-66, 68, 70, 74-76, 80, 129, 139</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/__init__.py</span>
                    <span>3</span>
                    <span>0</span>
                    <span>3</span>
                    <span>100.0%</span>
                    <span class="source-lines">4-5, 7</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/annotator.py</span>
                    <span>110</span>
                    <span>0</span>
                    <span>110</span>
                    <span>100.0%</span>
                    <span class="source-lines">4, 6-10, 12-15, 21-22, 25-28, 31, 45-46, 48-50, 54, 56-57, 59, 61-62, 64, 66-68, 71-72, 74-82, 87, 97-98, 100, 102, 104-105, 115, 127, 129-132, 137-139, 142, 165-168, 170-171, 176, 178, 180-183, 185-190, 192-193, 198-201, 203, 206, 229-232, 234, 236-237, 239-240, 245-246, 248-253, 255-256, 261-264, 266</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/base.py</span>
                    <span>78</span>
                    <span>0</span>
                    <span>78</span>
                    <span>100.0%</span>
                    <span class="source-lines">13, 15-18, 26, 40, 46, 52-53, 55, 72, 75-76, 78, 80, 101, 107-108, 110-111, 122, 128, 130, 136, 138, 147, 149, 165, 167-173, 175, 177, 186-187, 190-192, 194-195, 198-200, 203-208, 212, 214, 220-221, 224-225, 228-230, 233, 245, 247, 249-250, 252-253, 255, 257-258, 260, 262-263, 265, 267</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/gemini.py</span>
                    <span>275</span>
                    <span>18</span>
                    <span>257</span>
                    <span>93.45%</span>
                    <span class="source-lines">7, 9-13, 15-16, 23-27, 30-34, 37-42, 44-46, 48-50, 52, 57-63, 65-70, 72-73, 75-78, 80-85, 87-88, 91-97, 99-103, 105, 107-114, 121-122, 125, 128, 134, 136-139, 141-142, 144, 160-161, 167-169, 171-172, 174, 176-184, 186-188, 190-191, 193, 196, 200-208, 210-211, 213-215, 217-223, 225-227, 233-234, 238-239, 242-243, 245-248, 252-253, 260, 266-267, 269, 273-277, 279-283, 286-287, 292-293, 300-301, 303, 315, 317-318, 322, 327, 330-332, 335-343, 345-346, 348, 352-355, 357, 360-366, 368-374, 380-382, 384-387, 389, 391-392, 396-402, 405, 408-410, 412-414, 416-421, 427-428, 430-434, 437-440, 442-443, 445-447</span>
                    <span class="source-lines">89, 104, 106, 115-117, 199, 230-231, 235-237, 244, 250, 256, 367, 441, 444</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/litellm_provider.py</span>
                    <span>32</span>
                    <span>1</span>
                    <span>31</span>
                    <span>96.88%</span>
                    <span class="source-lines">7, 9, 11-12, 18, 21, 37-38, 44, 46, 49, 51-52, 54-56, 66-67, 69-70, 73, 76, 78-79, 81-82, 84, 88, 94-95, 97</span>
                    <span class="source-lines">74</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/noop.py</span>
                    <span>13</span>
                    <span>0</span>
                    <span>13</span>
                    <span>100.0%</span>
                    <span class="source-lines">8, 10, 12-13, 20, 26, 32, 34, 50, 52, 58, 60, 66</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/ollama.py</span>
                    <span>43</span>
                    <span>1</span>
                    <span>42</span>
                    <span>97.67%</span>
                    <span class="source-lines">7, 9, 11-12, 18, 24, 40-41, 47, 50, 52, 54-55, 57-60, 62-63, 66-67, 71-72, 74-75, 77, 81, 87-88, 90-92, 96, 102, 104, 114, 116-117, 127, 132, 134-135</span>
                    <span class="source-lines">69</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/schemas.py</span>
                    <span>36</span>
                    <span>1</span>
                    <span>35</span>
                    <span>97.22%</span>
                    <span class="source-lines">8, 10-12, 16, 22, 38, 42-44, 46-47, 50-53, 55, 58-59, 62-65, 67-68, 77, 84, 90, 94-98, 102, 130</span>
                    <span class="source-lines">39</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/models.py</span>
                    <span>240</span>
                    <span>10</span>
                    <span>230</span>
                    <span>95.83%</span>
                    <span class="source-lines">17-18, 21, 24-25, 34-36, 38, 40, 47-48, 61-67, 69, 71, 82-83, 95-100, 102, 104, 109-115, 118-119, 141-157, 159, 161, 167-171, 173-182, 184, 186, 188-190, 193-194, 202-203, 205, 207, 213-214, 223-225, 227, 229, 233-235, 238-239, 248-250, 252, 254, 261-262, 271-273, 275, 277, 281-283, 286-287, 324-353, 355-360, 362, 364, 382-405, 407-419, 422-423, 437-445, 447, 449, 459, 461, 464-465, 482-492, 494, 500, 502, 508-512, 514, 516, 518, 520, 522</span>
                    <span class="source-lines">172, 183, 185, 187, 460, 513, 515, 517, 519, 521</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/options.py</span>
                    <span>117</span>
                    <span>45</span>
                    <span>72</span>
                    <span>61.54%</span>
                    <span class="source-lines">106, 146, 175, 178-180, 185-187, 193-195, 201-203, 209-218, 220, 224, 233, 248, 251-267, 270-283, 286-295, 298, 300</span>
                    <span class="source-lines">13-15, 21-22, 90-94, 97-99, 102-105, 122-123, 126-132, 135-137, 140-142, 145, 156-160, 163-164, 167, 169, 222, 227, 236</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/plugin.py</span>
                    <span>151</span>
                    <span>24</span>
                    <span>127</span>
                    <span>84.11%</span>
                    <span class="source-lines">40, 43, 49, 55, 61, 67, 73, 80, 89, 95, 101, 107, 113, 121, 126, 131, 136, 142, 147, 153, 169, 173, 177, 183-184, 187-188, 190, 192, 195-197, 203-204, 212-213, 238-239, 242-243, 246, 249-250, 252-253, 256-257, 259, 261-265, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 307, 309-312, 315-316, 324-325, 330-333, 336, 338, 341-346, 348, 350, 358-359, 380-381, 384-385, 388-390, 401-402, 405, 408-409, 412-414, 424-425, 428-430, 441-442, 445, 448, 450-451</span>
                    <span class="source-lines">13, 15-17, 19-20, 22, 28-31, 34, 160, 216, 320-321, 326-327, 372-373, 393, 417, 433-434</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/prompts.py</span>
                    <span>75</span>
                    <span>5</span>
                    <span>70</span>
                    <span>93.33%</span>
                    <span class="source-lines">13, 15-17, 24, 27, 33, 35, 49, 52, 55, 58-61, 63, 65, 67, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116, 118, 132-133, 135-138, 140-141, 144-145, 148, 151-152, 154-156, 158-159, 163, 165, 180, 182, 191-194</span>
                    <span class="source-lines">80, 114, 142, 146, 149</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/render.py</span>
                    <span>50</span>
                    <span>0</span>
                    <span>50</span>
                    <span>100.0%</span>
                    <span class="source-lines">13, 15-16, 18, 24, 30-31, 34, 40, 42, 50-51, 53, 56, 65-67, 70, 79, 87, 90, 99, 101-102, 107, 110, 121-124, 126-129, 131-134, 141-143, 145, 158-163, 177, 196</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/report_writer.py</span>
                    <span>167</span>
                    <span>10</span>
                    <span>157</span>
                    <span>94.01%</span>
                    <span class="source-lines">13, 15-25, 27-29, 46, 55, 58, 67-68, 76, 83-84, 89, 98-100, 102, 105-108, 110, 116, 127-128, 130, 142, 150, 156-158, 160, 186-189, 192, 197-199, 202-203, 211, 222-223, 226-227, 230-231, 233, 235, 254, 256-259, 262-264, 266, 268, 303, 312, 314-315, 317-328, 330, 332, 340, 343-345, 348-349, 352-354, 357, 360, 368, 376, 378-379, 382, 385, 388, 391, 399, 401-402, 408, 410, 412, 414-423, 434-435, 437-439, 447-448, 453, 455, 458, 461-462, 464, 470-474, 480-481, 488, 495, 497, 499-501, 503, 506-507, 509, 515-516</span>
                    <span class="source-lines">113, 135-137, 424-425, 432, 449-451</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/fs.py</span>
                    <span>34</span>
                    <span>3</span>
                    <span>31</span>
                    <span>91.18%</span>
                    <span class="source-lines">11, 13-14, 17, 30, 33, 36, 39, 42, 45, 55-56, 58-60, 63-64, 70, 79, 82, 100, 103, 111-113, 116-117, 119-121, 123</span>
                    <span class="source-lines">40, 65, 67</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/hashing.py</span>
                    <span>36</span>
                    <span>0</span>
                    <span>36</span>
                    <span>100.0%</span>
                    <span class="source-lines">12, 14-17, 23, 32, 35, 44-48, 51, 61, 64, 73-74, 76-78, 80-81, 86, 96, 103-104, 107, 113-114, 116-121</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/ranges.py</span>
                    <span>33</span>
                    <span>0</span>
                    <span>33</span>
                    <span>100.0%</span>
                    <span class="source-lines">12, 15, 29-30, 33, 35-37, 39-40, 42, 45-47, 50, 52, 55, 65-67, 70, 81-82, 84-91, 93, 95</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/time.py</span>
                    <span>16</span>
                    <span>0</span>
                    <span>16</span>
                    <span>100.0%</span>
                    <span class="source-lines">4, 6, 9, 15, 18, 27, 30, 39-44, 46-48</span>
                    <span class="source-lines">-</span>
                </div>
            </div>
        </section>
    </div>
</body>
</html>