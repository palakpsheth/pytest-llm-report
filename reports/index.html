<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Test Report &bull; 387 tests</title>
    <!-- Optional: Inter font from rsms.me CDN. Falls back to system fonts if unavailable. -->
    <link rel="stylesheet" href="https://rsms.me/inter/inter.css">
    <style>
/* Modern Color Palette */
:root {
    --bg-color: #f8fafc;
    --text-primary: #1e293b;
    --text-secondary: #64748b;
    --border-color: #e2e8f0;
    --card-bg: #ffffff;
    --surface-muted: #f1f5f9;
    --primary-color: #3b82f6;
    color-scheme: light dark;

    /* Status Colors */
    --passed-bg: #dcfce7;
    --passed-text: #166534;
    --failed-bg: #fee2e2;
    --failed-text: #991b1b;
    --skipped-bg: #fef9c3;
    --skipped-text: #854d0e;
    --xfailed-bg: #ffedd5;
    --xfailed-text: #9a3412;
    --xpassed-bg: #f3e8ff;
    --xpassed-text: #6b21a8;
    --error-bg: #fee2e2;
    --error-text: #991b1b;
}

body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    background-color: var(--bg-color);
    color: var(--text-primary);
    line-height: 1.5;
    margin: 0;
    padding: 0;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 2rem;
}

/* Header */
header {
    margin-bottom: 2rem;
    border-bottom: 1px solid var(--border-color);
    padding-bottom: 1rem;
    display: flex;
    justify-content: space-between;
    align-items: center;
}

h1 {
    font-size: 1.875rem;
    font-weight: 700;
    color: var(--text-primary);
    margin: 0;
}

.meta {
    font-size: 0.875rem;
    color: var(--text-secondary);
}

/* Summary Grid */
.summary {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
    gap: 1rem;
    margin-bottom: 2rem;
}

.summary-card {
    background: var(--card-bg);
    border-radius: 0.5rem;
    padding: 1.5rem;
    box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.1), 0 1px 2px -1px rgb(0 0 0 / 0.1);
    text-align: center;
    border: 1px solid var(--border-color);
    transition: transform 0.2s;
}

.summary-card:hover {
    transform: translateY(-2px);
}

.summary-card .count {
    font-size: 2.25rem;
    font-weight: 700;
    line-height: 1;
    margin-bottom: 0.5rem;
}

.summary-card .label {
    text-transform: uppercase;
    font-size: 0.75rem;
    font-weight: 600;
    letter-spacing: 0.05em;
    color: var(--text-secondary);
}

/* Status Colors for Summary */
.summary-card.passed .count {
    color: var(--passed-text);
}

.summary-card.failed .count {
    color: var(--failed-text);
}

.summary-card.skipped .count {
    color: var(--skipped-text);
}

.summary-card.xfailed .count {
    color: var(--xfailed-text);
}

.summary-card.xpassed .count {
    color: var(--xpassed-text);
}

.summary-card.coverage .count {
    color: var(--primary-color);
}

/* Filters */
.filters {
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.5rem;
    border: 1px solid var(--border-color);
    margin-bottom: 1.5rem;
    display: flex;
    flex-direction: column;
    gap: 0.75rem;
}

.filter-input {
    flex: 1;
    padding: 0.5rem 1rem;
    border: 1px solid var(--border-color);
    border-radius: 0.375rem;
    font-size: 0.875rem;
    background: var(--card-bg);
    color: var(--text-primary);
}

.filter-input::placeholder {
    color: var(--text-secondary);
}

.filter-statuses {
    display: flex;
    flex-wrap: wrap;
    gap: 0.5rem;
}

.filter-chip {
    display: inline-flex;
    align-items: center;
    gap: 0.35rem;
    padding: 0.25rem 0.75rem;
    border-radius: 9999px;
    border: 1px solid var(--border-color);
    background: var(--surface-muted);
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.04em;
}

.filter-chip input {
    margin: 0;
}

.filter-chip.passed {
    background: var(--passed-bg);
    color: var(--passed-text);
}

.filter-chip.failed {
    background: var(--failed-bg);
    color: var(--failed-text);
}

.filter-chip.skipped {
    background: var(--skipped-bg);
    color: var(--skipped-text);
}

.filter-chip.xfailed {
    background: var(--xfailed-bg);
    color: var(--xfailed-text);
}

.filter-chip.xpassed {
    background: var(--xpassed-bg);
    color: var(--xpassed-text);
}

.filter-chip.error {
    background: var(--error-bg);
    color: var(--error-text);
}

/* Test List */
.test-list {
    display: flex;
    flex-direction: column;
    gap: 0.75rem;
}

.test-row {
    background: var(--card-bg);
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
    overflow: hidden;
}

.test-header {
    padding: 1rem;
    display: flex;
    align-items: center;
    gap: 1rem;
    cursor: pointer;
    background: var(--card-bg);
}

.test-header:hover {
    background: var(--surface-muted);
}

.status-badge {
    padding: 0.25rem 0.75rem;
    border-radius: 9999px;
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
}

.status-passed {
    background: var(--passed-bg);
    color: var(--passed-text);
}

.status-failed {
    background: var(--failed-bg);
    color: var(--failed-text);
}

.status-skipped {
    background: var(--skipped-bg);
    color: var(--skipped-text);
}

.status-xfailed {
    background: var(--xfailed-bg);
    color: var(--xfailed-text);
}

.status-xpassed {
    background: var(--xpassed-bg);
    color: var(--xpassed-text);
}

.status-error {
    background: var(--error-bg);
    color: var(--error-text);
}

.test-name {
    flex: 1;
    font-family: monospace;
    font-size: 0.9rem;
    color: var(--text-primary);
    word-break: break-all;
}

.test-meta {
    display: flex;
    gap: 1rem;
    align-items: center;
    color: var(--text-secondary);
    font-size: 0.875rem;
}

/* Details Section */
.test-details {
    padding: 0 1rem 1rem 1rem;
    border-top: 1px solid var(--border-color);
    background: var(--surface-muted);
}

.detail-section {
    margin-top: 1rem;
}

.detail-title {
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
    color: var(--text-secondary);
    margin-bottom: 0.5rem;
}

.coverage-item {
    font-family: monospace;
    font-size: 0.85rem;
    padding: 0.25rem 0;
    border-bottom: 1px solid var(--border-color);
    display: grid;
    grid-template-columns: minmax(200px, 2fr) minmax(120px, 1fr);
    gap: 1rem;
}

.coverage-list {
    background: var(--card-bg);
    border-radius: 0.375rem;
    border: 1px solid var(--border-color);
    overflow: hidden;
}

.source-coverage {
    margin-top: 2rem;
}

.source-coverage h2 {
    margin: 0 0 1rem;
    font-size: 1.5rem;
}

.source-coverage-table {
    display: grid;
    gap: 0.35rem;
}

.source-coverage-header,
.source-coverage-row {
    display: grid;
    grid-template-columns: minmax(200px, 2fr) repeat(4, minmax(60px, 0.5fr)) minmax(
            140px,
            1fr
        ) minmax(140px, 1fr);
    align-items: center;
    gap: 0.75rem;
    padding: 0.75rem 1rem;
    border-radius: 0.5rem;
}

.source-coverage-header {
    background: var(--surface-muted);
    font-size: 0.75rem;
    font-weight: 700;
    text-transform: uppercase;
    letter-spacing: 0.04em;
    color: var(--text-secondary);
}

.source-coverage-row {
    background: var(--card-bg);
    border: 1px solid var(--border-color);
    font-size: 0.85rem;
}

.source-path {
    font-family: monospace;
    word-break: break-word;
}

.source-lines {
    font-family: monospace;
    color: var(--text-secondary);
    word-break: break-word;
}

.llm-annotation {
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.375rem;
    border: 1px solid var(--border-color);
}

.llm-annotation p {
    margin: 0 0 0.5rem 0;
}

.llm-annotation p:last-child {
    margin-bottom: 0;
}

.llm-annotation ul {
    margin: 0.5rem 0 0;
    padding-left: 1.25rem;
}

.llm-annotation li {
    margin-bottom: 0.25rem;
}

.error-message {
    font-family: monospace;
    color: var(--failed-text);
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.375rem;
    border: 1px solid var(--failed-bg);
    white-space: pre-wrap;
    overflow-x: auto;
}

/* HTML5 Progress Bar for Coverage */
progress {
    width: 60px;
}

/* Utility: Hidden state for filtering */
.hidden {
    display: none !important;
}

/* Dark Mode Support */
@media (prefers-color-scheme: dark) {
    :root {
        --bg-color: #0f172a;
        --text-primary: #f1f5f9;
        --text-secondary: #94a3b8;
        --border-color: #334155;
        --card-bg: #1e293b;
        --surface-muted: #0b1220;
        --primary-color: #60a5fa;

        /* Status Colors - Adjusted for dark mode */
        --passed-bg: #14532d;
        --passed-text: #86efac;
        --failed-bg: #7f1d1d;
        --failed-text: #fca5a5;
        --skipped-bg: #713f12;
        --skipped-text: #fde047;
        --xfailed-bg: #7c2d12;
        --xfailed-text: #fdba74;
        --error-bg: #7f1d1d;
        --error-text: #fca5a5;
    }

    /* Adjust box shadows for dark mode */
    .summary-card {
        box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.3), 0 1px 2px -1px rgb(0 0 0 / 0.3);
    }
}

@media print {
    body {
        background: #ffffff;
        color: #0f172a;
    }

    .container {
        max-width: none;
        padding: 1rem 1.5rem;
    }

    header {
        border-bottom: 2px solid var(--border-color);
    }

    .filters {
        display: none;
    }

    .summary-card,
    .test-row {
        box-shadow: none;
    }

    .test-header {
        background: #ffffff;
    }

    .test-row {
        page-break-inside: avoid;
        break-inside: avoid;
    }

    .test-details {
        background: #ffffff;
    }

    .llm-annotation {
        background: var(--surface-muted);
    }

    progress {
        width: 80px;
    }
}

body.pdf-mode .filters {
    display: none;
}

body.pdf-mode .test-row {
    page-break-inside: avoid;
    break-inside: avoid;
}    </style>
    <script>
// pytest-llm-report interactive features

// Global state for filters
const activeStatuses = new Set(['passed', 'failed', 'skipped', 'xfailed', 'xpassed', 'error']);

// Filter tests based on search input and outcome filters
function filterTests() {
    const query = document.getElementById('searchInput').value.toLowerCase();
    document.querySelectorAll('.test-row').forEach(row => {
        const nodeid = row.querySelector('.test-name').textContent.toLowerCase();
        const statusMatch = row.dataset.status ? activeStatuses.has(row.dataset.status) : false;
        const matchesSearch = nodeid.includes(query);
        row.classList.toggle('hidden', !matchesSearch || !statusMatch);
    });
}

// Toggle visibility of status filters
function toggleStatus(checkbox) {
    const status = checkbox.dataset.status;
    if (checkbox.checked) {
        activeStatuses.add(status);
    } else {
        activeStatuses.delete(status);
    }
    filterTests();
}

// Initialize interactive features after DOM is ready
document.addEventListener('DOMContentLoaded', function () {
    'use strict';

    // Toggle dark mode on preference
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.documentElement.dataset.theme = 'dark';
    }

    // Default: expand all details
    document.querySelectorAll('details').forEach(details => {
        details.setAttribute('open', '');
    });

    const params = new URLSearchParams(window.location.search);
    if (params.get('pdf') === '1') {
        document.body.classList.add('pdf-mode');
    }
});    </script>
</head>
<body>
    <div class="container">
        <header>
            <div>
                <h1>Test Report</h1>
                <div class="meta">
                    Run ID: 20971419458-py3.12 &bull;
                    Generated: 2026-01-13 20:27:06 &bull;
                    Duration: 34.69s<br>
                    <strong>Plugin:</strong> v0.1.0
                        (2f498263985a34902252c53c11fb820445bd8f21)
[dirty]<br>
                    <strong>Repo:</strong> v0.1.0
                        (c88e8ba83476730ae529aa8303ba0bf384e527ba)
<br>
                    <strong>LLM:</strong> ollama / llama3.2:1b
                        (minimal context,
                         379 annotated, 7 errors)
                </div>
            </div>
            <div style="text-align: right">
                <div style="font-size: 2rem; font-weight: 700; color: var(--primary-color)">
                    92.91%
                </div>
                <div class="meta">Total Coverage</div>
            </div>
        </header>

        <!-- Summary Cards -->
        <div class="summary">
            <div class="summary-card">
                <div class="count">387</div>
                <div class="label">Total Tests</div>
            </div>
            <div class="summary-card passed">
                <div class="count">387</div>
                <div class="label">Passed</div>
            </div>
            <div class="summary-card failed">
                <div class="count">0</div>
                <div class="label">Failed</div>
            </div>
            <div class="summary-card skipped">
                <div class="count">0</div>
                <div class="label">Skipped</div>
            </div>
            <div class="summary-card xfailed">
                <div class="count">0</div>
                <div class="label">XFailed</div>
            </div>
            <div class="summary-card xpassed">
                <div class="count">0</div>
                <div class="label">XPassed</div>
            </div>
            <div class="summary-card failed">
                <div class="count">0</div>
                <div class="label">Errors</div>
            </div>
        </div>

        <!-- Filters -->
        <div class="filters">
            <input type="text" id="searchInput" class="filter-input" placeholder="Search tests..." onkeyup="filterTests()">
            <div class="filter-statuses" aria-label="Filter by status">
                <label class="filter-chip passed">
                    <input type="checkbox" data-status="passed" checked onchange="toggleStatus(this)">
                    Passed
                </label>
                <label class="filter-chip failed">
                    <input type="checkbox" data-status="failed" checked onchange="toggleStatus(this)">
                    Failed
                </label>
                <label class="filter-chip skipped">
                    <input type="checkbox" data-status="skipped" checked onchange="toggleStatus(this)">
                    Skipped
                </label>
                <label class="filter-chip xfailed">
                    <input type="checkbox" data-status="xfailed" checked onchange="toggleStatus(this)">
                    XFailed
                </label>
                <label class="filter-chip xpassed">
                    <input type="checkbox" data-status="xpassed" checked onchange="toggleStatus(this)">
                    XPassed
                </label>
                <label class="filter-chip error">
                    <input type="checkbox" data-status="error" checked onchange="toggleStatus(this)">
                    Error
                </label>
            </div>
        </div>

        <!-- Test List -->
        <div class="test-list">
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_all_policy</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_aggregate_all_policy' verifies that the aggregate function correctly aggregates all policy tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where a single policy test fails to aggregate with other tests, potentially leading to incorrect results or false positives.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The aggregated report should contain both retained policy tests.</li>
                                        <li>The number of retained policy tests in the aggregated report should be equal to 2.</li>
                                        <li>Each retained policy test should have a unique 'nodeid' and 'outcome' key in the aggregated report.</li>
                                        <li>The aggregated report should not be empty.</li>
                                        <li>All retained policy tests should have a duration greater than 0.</li>
                                        <li>All retained policy tests should have a phase of 'call'.</li>
                                        <li>The aggregate function should correctly handle duplicate nodeids and outcomes by retaining only one instance of each.</li>
                                        <li>The aggregate function should preserve the original order of policy tests in the input reports.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 52, 55-56, 59, 61-63, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134, 141, 146, 148-153, 155, 157-159, 170, 217, 219-223, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_dir_not_exists</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the aggregate function returns None when the directory does not exist.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the aggregate function fails to work correctly if the input directory does not exist.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregate` method of the `aggregator` object should return `None` when the input directory does not exist.</li>
                                        <li>The `aggregate` method of the `aggregator` object should raise an error or handle the case correctly if the input directory does not exist.</li>
                                        <li>The test should fail when the input directory exists, indicating a bug in the aggregate function.</li>
                                        <li>The test should pass when the input directory does not exist, indicating that the aggregate function is working as expected.</li>
                                        <li>The `exists` method of the `Path` object should return `False` for the input directory.</li>
                                        <li>The `exists` method of the `Path` object should raise an error if the input path is invalid or does not exist.</li>
                                        <li>The `aggregate` method of the `aggregator` object should handle the case where the input directory exists correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 52, 55-57, 109-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_latest_policy</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the test_aggregate_latest_policy function correctly selects the latest policy for aggregation.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the test case is run multiple times in a row, causing the test to always pick the first report's latest policy.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test verifies that the `aggregate` method returns the correct latest policy for each test case.</li>
                                        <li>The test checks that the returned policy from `aggregate` is indeed the latest for each test case.</li>
                                        <li>The test ensures that the aggregated run meta contains the correct number of tests and their respective outcomes.</li>
                                        <li>The test verifies that the summary indicates a passed count for the latest policy.</li>
                                        <li>The test asserts that the aggregate result has no failed tests.</li>
                                        <li>The test checks that the `run_meta` object is set to indicate an aggregated run.</li>
                                        <li>The test verifies that the `summary` attribute of the `run_meta` object correctly reflects the outcome of each test case.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">77 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134, 141, 146, 148-153, 155, 157-159, 170, 182, 184-188, 190-191, 194, 217, 219-223, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_no_dir_configured</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that aggregate function returns None when no directory configuration is provided.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the aggregate function would raise an error due to missing directory configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregate()` method of the `Aggregator` class should return `None` when called with a mock configuration that does not specify an aggregation directory.</li>
                                        <li>The `aggregate_dir` attribute of the `Aggregator` instance should be set to `None` after calling `aggregate()`.</li>
                                        <li>An error message or exception should not be raised when calling `aggregate()` with a mock configuration that does not specify an aggregation directory.</li>
                                        <li>The `aggregate()` method should behave as expected without raising any exceptions or errors.</li>
                                        <li>The `aggregate_dir` attribute of the `Aggregator` instance should remain unchanged after calling `aggregate()`.</li>
                                        <li>The test should be able to reproduce the issue consistently across different environments and configurations.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 44, 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_no_reports</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `aggregate` method of the Aggregator class should not be called when there are no reports.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the aggregate method might be called with an empty list or set of reports, potentially causing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>aggregator.aggregate() is None</li>
                                        <li>pathlib.Path.exists() returns True</li>
                                        <li>pathlib.Path.glob() returns []</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52, 55-57, 109-110, 113-114, 170)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_with_coverage_and_llm_annotations</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that coverage and LLM annotations are properly deserialized and can be re-serialized after a fix.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in core functionality by ensuring accurate coverage and LLM annotation deserialization.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Coverage was correctly deserialized from the JSON report.</li>
                                        <li>LLM annotation was correctly deserialized from the JSON report.</li>
                                        <li>The aggregated result can be re-serialized without any issues.</li>
                                        <li>The serialized report contains the expected keys (coverage and LLM annotation).</li>
                                        <li>The coverage ranges match the original file paths.</li>
                                        <li>The line counts in the coverage entries match the original file lines.</li>
                                        <li>The confidence level of the LLM annotation matches the original value.</li>
                                        <li>The key assertions were correctly extracted from the aggregated result.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">81 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134-137, 141-144, 146, 148-153, 155, 157-159, 170, 182, 184-188, 194, 217, 219-223, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">32 lines (ranges: 40-43, 104-107, 109-111, 113, 115, 161-165, 167, 169, 171, 173, 176-180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_with_source_coverage</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that source coverage summary is deserialized correctly when aggregated with a report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the source coverage summary is not properly deserialized when aggregating reports from different directories.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `source_coverage` attribute of the result object should be an instance of `SourceCoverageEntry`.</li>
                                        <li>The `file_path` attribute of the first element in the `source_coverage` list should match the expected file path.</li>
                                        <li>All elements in the `source_coverage` list should have a valid `coverage_percent`, `covered_ranges`, and `missed_ranges` attribute.</li>
                                        <li>Each `SourceCoverageEntry` object should have a `file_path` attribute that matches the expected file path.</li>
                                        <li>The `covered_ranges` attribute should be a string containing two ranges separated by a comma (e.g., '1-5, 7-11').</li>
                                        <li>The `missed_ranges` attribute should be an empty string.</li>
                                        <li>All statements in the source code should be present in the aggregated report.</li>
                                        <li>The coverage percentage should be greater than or equal to 0% and less than or equal to 100%</li>
                                        <li>The number of covered statements should be greater than or equal to the number of missed statements</li>
                                        <li>The total coverage percentage should be calculated correctly based on the source code coverage</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">66 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 148-155, 157-159, 170, 182, 184-186, 194, 217, 219-220, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_load_coverage_from_source</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loading coverage from configured source file when option is not set.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where coverage data is missing due to lack of configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that the _load_coverage_from_source method returns None when llm_coverage_source is set to None.</li>
                                        <li>Verify that the _load_coverage_from_source method raises a UserWarning with message 'Coverage source not found' when llm_coverage_source is set to '/nonexistent/coverage'.</li>
                                        <li>Verify that the _load_coverage_from_source method correctly loads coverage data from a mock .coverage file when llm_coverage_source is set to '.coverage'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 245-246, 248-249, 251, 253-257, 259, 262-263, 265-266, 269-271, 273)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_recalculate_summary</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> test_recalculate_summary verifies that the _recalculate_summary method preserves coverage when aggregating test results.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the aggregation of test results, ensuring that the coverage percentage is preserved even after multiple calls to _recalculate_summary.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>summary.total == 6 (total number of tests)</li>
                                        <li>summary.passed == 1 (number of passed tests)</li>
                                        <li>summary.failed == 1 (number of failed tests)</li>
                                        <li>summary.skipped == 1 (number of skipped tests)</li>
                                        <li>summary.xfailed == 1 (number of xfailed tests)</li>
                                        <li>summary.xpassed == 1 (number of xpassed tests)</li>
                                        <li>summary.error == 1 (number of error tests)</li>
                                        <li>summary.coverage_total_percent == 85.5 (coverage percentage preserved)</li>
                                        <li>summary.total_duration == 5.0 (total duration of all tests)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 217, 219-233, 235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_skips_invalid_json</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Skips Invalid JSON: Verifies that the test prevents skipping of reports with non-JSON content.</p>
                                <p><strong>Why Needed:</strong> This test prevents skipping of reports containing invalid or missing fields in their JSON format, ensuring that all reports are processed correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregate` function should not be called on a report with a 'not json' file.</li>
                                        <li>The `run_meta.run_count` attribute should still return the correct count even if only one valid report is found.</li>
                                        <li>The test should fail when skipping invalid reports, indicating that the test logic is correct.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">71 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119-120, 125, 127-128, 148-153, 155, 157-159, 162, 164-166, 168, 170, 182, 184-186, 194, 217, 219-220, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation_maximal.py::TestAggregationMaximal::test_recalculate_summary_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the aggregator recalculates the summary correctly when there are multiple tests with different outcomes.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the aggregation process, ensuring that the coverage total percent is calculated accurately even if some tests fail.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>summary.total == 2</li>
                                        <li>summary.passed == 1</li>
                                        <li>summary.failed == 1</li>
                                        <li>summary.coverage_total_percent == 88.5</li>
                                        <li>summary.total_duration == 3.0</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 44, 217, 219-225, 235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_cached_tests_are_skipped</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that cached tests are skipped by the annotator.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in cases where the annotator is caching test results and should skip them to avoid re-running unnecessary tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The mock provider, cache, and assembler objects are created with `mock_provider`, `mock_cache`, and `mock_assembler` respectively.</li>
                                        <li>The `test_cached_tests_are_skipped` method is called with the expected arguments.</li>
                                        <li>The `test_cached_tests_are_skipped` method checks if the cached test results should be skipped based on the provided conditions.</li>
                                        <li>The `test_cached_tests_are_skipped` method returns a boolean indicating whether the cached tests are skipped or not.</li>
                                        <li>The `test_cached_tests_are_skipped` method asserts that the return value is as expected.</li>
                                        <li>The `test_cached_tests_are_skipped` method checks if the annotator should skip the test based on the provided conditions.</li>
                                        <li>The `test_cached_tests_are_skipped` method uses the mock objects to simulate the caching process and the annotation logic.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-81, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `test_concurrent_annotation` function is being tested to ensure that annotators can process multiple annotations concurrently without any issues.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential performance regressions or bugs caused by concurrent annotation processing.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `mock_provider`, `mock_cache`, and `mock_assembler` objects are not modified during the execution of this function.</li>
                                        <li>The annotator's output is not affected by concurrent annotations.</li>
                                        <li>The test function does not throw any exceptions or errors when running concurrently with other tests.</li>
                                        <li>The annotator's performance remains stable even under concurrent annotation processing.</li>
                                        <li>The cache size and annotation storage are not modified during the execution of this function.</li>
                                        <li>The `mock_provider` object is updated correctly after each annotation is processed.</li>
                                        <li>No unexpected side effects occur on the test environment when running concurrently with other tests.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">64 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104-112, 129-135, 137, 139, 229-232, 234, 236-237, 239, 245-246, 248-253, 255, 261, 266)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation_handles_failures</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test concurrent annotation handles failures to verify</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where annotators fail to handle failures in the annotation process.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocking `mock_provider` and `mock_assembler` with mock objects that raise an exception when called concurrently is necessary to ensure the annotations are handled correctly.</li>
                                        <li>The `mock_cache` object should not be affected by concurrent calls to `test_concurrent_annotation_handles_failures`.</li>
                                        <li>When `mock_provider` raises an exception, it should be caught and handled by the test, preventing failures from propagating through the annotation process.</li>
                                        <li>Similarly, when `mock_assembler` raises an exception, it should also be caught and handled correctly by the test.</li>
                                        <li>The `mock_cache` object should not store any data that would prevent subsequent calls to `test_concurrent_annotation_handles_failures` from succeeding.</li>
                                        <li>If a failure occurs in the annotation process, the test should still be able to verify that the annotator handles it correctly and continues processing other tasks.</li>
                                        <li>In case of concurrent failures, the test should not fail due to one task being unable to complete before another task completes its work.</li>
                                        <li>The test should also ensure that any exceptions raised by `mock_provider` or `mock_assembler` are properly propagated up the call stack.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104-112, 129-135, 137-139, 229-232, 234, 236-237, 239, 245-246, 248-253, 255, 261-264, 266)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_progress_reporting</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `test_progress_reporting` function is being tested to ensure it correctly reports progress for annotators.</p>
                                <p><strong>Why Needed:</strong> This test prevents regressions that may occur if the progress reporting functionality changes without updating the key assertions.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that the progress reporter updates with correct keys.</li>
                                        <li>Check if the progress reporter increments the `annotator_id` and `total_annotations` keys correctly.</li>
                                        <li>Verify that the progress reporter resets the `annotator_id` and `total_annotations` keys when a new annotator is added.</li>
                                        <li>Confirm that the progress reporter displays the correct number of annotations for each annotator.</li>
                                        <li>Check if the progress reporter updates the `progress` key with the correct percentage.</li>
                                        <li>Verify that the progress reporter increments the `annotations_completed` key correctly after each annotation.</li>
                                        <li>Confirm that the progress reporter resets the `annotations_completed` key when a new annotation is added.</li>
                                        <li>Check if the progress reporter displays the correct number of annotations for each annotator across all annotators.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation</span>
                        <div class="test-meta">
                            <span>12.00s</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies the sequential annotation functionality of the annotator.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in sequential annotation logic when using multiple annotators.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should correctly annotate text sequentially by passing a list of annotations to the `annotate` method.</li>
                                        <li>The function should handle cases where the input list is empty or contains only one annotation.</li>
                                        <li>The function should not throw an exception if the input list is None.</li>
                                        <li>The function should return the correct number of annotated tokens for each sentence.</li>
                                        <li>The function should correctly handle case-insensitive tokenization and stemming.</li>
                                        <li>The function should support annotators with different tokenization and stemming algorithms.</li>
                                        <li>The function should be able to handle complex sentences with multiple clauses.</li>
                                        <li>The function should not throw an exception if the input list contains annotations that are not valid for sequential annotation.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_skips_if_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `test_skips_if_disabled` test verifies that annotating tests with a disabled LLM configuration does not have any effect.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the case where the LLM is disabled, ensuring that annotating tests still works as expected.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `test_skips_if_disabled` function should not attempt to annotate any tests when the LLM is disabled.</li>
                                        <li>The `annotate_tests` function should not be called with an empty list of annotations when the LLM is disabled.</li>
                                        <li>The test should still pass without any errors or exceptions even though the LLM is disabled.</li>
                                        <li>The configuration object passed to `annotate_tests` should have a 'LLM' key set to False.</li>
                                        <li>The annotation process should not attempt to skip tests that are annotated with an LLM-enabled configuration.</li>
                                        <li>The test should be able to annotate tests without skipping them even though the LLM is disabled.</li>
                                        <li>The `test_skips_if_disabled` function should not have any side effects on the test result.</li>
                                        <li>The test should still report a successful outcome for all annotated tests.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 45-46)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_skips_if_provider_unavailable</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The annotator skips the annotation process when a provider is unavailable.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the annotator fails to skip annotations due to an unavailable provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocking `mock_provider` with an available mock instance should allow the annotator to skip the annotation process.</li>
                                        <li>The `capsys` fixture is not being used in this test, which could prevent it from capturing the skipped annotation message.</li>
                                        <li>The `mock_provider` should be mocked to return a successful response (200 OK) when the provider is unavailable.</li>
                                        <li>The annotator should still be able to skip annotations even if the provider returns an error (4xx or 5xx status code).</li>
                                        <li>The test should not fail due to the availability of the provider, but rather because it's unavailable.</li>
                                        <li>The annotator's behavior should remain consistent with previous tests where providers were available.</li>
                                        <li>The `mock_provider` instance should be properly cleaned up after the test completes.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_concurrent_with_progress_and_errors</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that annotator reports progress and first error when annotated concurrently with progress and errors.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the annotator fails to report progress or the first error when annotated concurrently with progress and errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should append a message indicating processing of the second task.</li>
                                        <li>The function should append a message indicating that an LLM annotation occurred for the second task.</li>
                                        <li>The function should return at least two annotations (one success and one error).</li>
                                        <li>The function should report 'first error' in the first error message.</li>
                                        <li>The function should report 'Processing 2 test(s)' in some progress messages.</li>
                                        <li>The function should report 'LLM annotation' in some progress messages.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 229-232, 234, 236-237, 239-242, 245-246, 248-253, 255-258, 261-264, 266)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_sequential_rate_limit_wait</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Should wait if rate limit interval has not elapsed.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in cases where the annotator takes longer than expected to complete tasks due to a slow rate limit.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert mock_sleep.called</li>
                                        <li>assert mock_time.call_count == 5</li>
                                        <li>assert mock_time.side_effect == [100.0, 100.1, 100.2, 100.3, 100.4]</li>
                                        <li>assert tasks[-1].outcome != 'done' because of the slow rate limit interval</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">23 lines (ranges: 165-168, 170-171, 173-174, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_tests_cached_progress</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the annotator reports cached tests' progress correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the annotator does not report progress for cached tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'cache': test_cached message is present in the progress messages.</li>
                                        <li>The 'cache': test_cached message contains the scenario 'cached'.</li>
                                        <li>The 'cache': test_cached message indicates that the annotation was successful.</li>
                                        <li>The 'src' key in the progress messages corresponds to the source of the cached annotations.</li>
                                        <li>The 'None' value under the 'src' key in the progress messages is present for all tests.</li>
                                        <li>The annotator returns a progress message with the correct scenario ('cached') for each test.</li>
                                        <li>The annotator does not return any progress messages for non-cached tests.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">37 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-84, 97-98, 100, 127, 129-135, 137, 139)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_tests_provider_unavailable</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the annotator fails when the test provider is not available.</p>
                                <p><strong>Why Needed:</strong> To prevent regression and ensure the annotator behaves as expected when the test provider is unavailable.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mocks.is_available.assert_called_once_with('ollama')</li>
                                        <li>annotate_tests.mock.get_provider().is_available.called_once_with('ollama')</li>
                                        <li>mocks.is_available.return_value False</li>
                                        <li>assert captured.out contains 'not available. Skipping annotations'</li>
                                        <li>mocks.get_provider().get_provider() returns None</li>
                                        <li>assert mock_provider.is_available.call_count == 1</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_coverage_v2.py::test_base_parse_response_malformed_json_after_extract</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that extracting a malformed JSON from an LLM response will result in a `JSONDecodeError`.</p>
                                <p><strong>Why Needed:</strong> This test prevents the regression of a potential bug where extracting a malformed JSON from an LLM response would not raise a `JSONDecodeError`, but instead return a valid JSON object.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The extracted JSON should be invalid and contain braces.</li>
                                        <li>The extracted JSON should have a `json_decode()` method that raises a `JSONDecodeError` with the message 'Failed to parse LLM response as JSON'.</li>
                                        <li>The error message should include information about the malformed JSON, such as its contents.</li>
                                        <li>The test should fail when the extracted JSON is valid but contains invalid braces.</li>
                                        <li>The test should pass when the extracted JSON is invalid and does not contain any valid JSON syntax.</li>
                                        <li>The test should pass even if the `json_decode()` method raises a different exception than `JSONDecodeError`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 186-187, 190-191, 194-195, 220-221)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_coverage_v2.py::test_base_parse_response_non_string_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `MockProvider` correctly parses a response with non-string fields and identifies the required key assertions.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in parsing responses with non-string fields, ensuring correct identification of expected keys.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of the 'scenario' field should be an integer.</li>
                                        <li>The list 'why_needed' should contain the string 'list'.</li>
                                        <li>The key assertion 'a' should exist in the annotation.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203-207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_gemini_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `get_gemini_provider` function returns a `GeminiProvider` instance</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where a non-Gemini provider is returned instead of the correct one.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider(config)` should return an instance of `GeminiProvider`</li>
                                        <li>The function `get_provider(config)` should raise an exception if it cannot find a matching provider</li>
                                        <li>The function `get_provider(config)` should not return a non-Gemini provider instance</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 245, 247, 249, 252, 257, 262-263, 265)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_invalid_provider</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `get_invalid_provider` method with an invalid provider.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where an unknown LLM provider is attempted to be used.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_provider` function should raise a `ValueError` when given an invalid provider.</li>
                                        <li>The error message should indicate that the provided provider is unknown.</li>
                                        <li>The `pytest.raises` context manager should be able to detect and report the exception.</li>
                                        <li>The test should fail with the specified error message when running it.</li>
                                        <li>The `match` parameter of the `pytest.raises` context manager should match the expected error message.</li>
                                        <li>The `Config` class's `provider` attribute should be set to an invalid value.</li>
                                        <li>The `get_provider` function should not raise a `ValueError` with an unknown provider.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 245, 247, 249, 252, 257, 262, 267)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_litellm_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `get_litellm_provider` function returns a valid instance of `LiteLLMProvider`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `get_litellm_provider` function might return an incorrect or null provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The returned provider should be an instance of `LiteLLMProvider`.</li>
                                        <li>The provider should not be null or None.</li>
                                        <li>The provider should have the correct type (`LiteLLMProvider`) as its class.</li>
                                        <li>The provider's attributes (e.g., `model_name`, `device_id`) should match the expected values.</li>
                                        <li>The provider's methods (e.g., `get_model`, `set_device`) should be available and functional.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52-53, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_noop_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `get_noop_provider` function returns a NoopProvider instance when no provider is specified.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `get_provider` function returns an incorrect type of provider (e.g., a non-None provider) when no provider is provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` attribute of the returned `NoopProvider` instance is `None`.</li>
                                        <li>The `type()` method of the returned `NoopProvider` instance returns `NoopProvider`.</li>
                                        <li>The `__class__` attribute of the returned `NoopProvider` instance is `NoopProvider`.</li>
                                        <li>The `get_provider()` function call does not raise an exception.</li>
                                        <li>The `provider` variable is assigned a value that is not `None`.</li>
                                        <li>The `provider` variable is assigned a value that is not an instance of `NoopProvider`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_ollama_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `get_ollama_provider` function returns an instance of OllamaProvider.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails if the provider is not set to 'ollama'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The returned value should be an instance of OllamaProvider.</li>
                                        <li>The returned value should have the correct class name (OllamaProvider).</li>
                                        <li>The returned value should be a valid provider instance.</li>
                                        <li>The `get_provider` function is correctly configured with the 'ollama' provider.</li>
                                        <li>The configuration object passed to `Config` has the required keys ('provider').</li>
                                        <li>The provider string passed to `Config` is not empty.</li>
                                        <li>The provider string passed to `Config` starts with 'ollama'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 245, 247, 249, 252-253, 255)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_available_caches_result</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the LlmProvider is available and has a single check.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case of multiple providers or large configurations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `is_available()` method returns True for both instances of the provider.</li>
                                        <li>The `checks` attribute of the provider instance is set to 1 after calling `_check_availability()`.</li>
                                        <li>Multiple calls to `_check_availability()` will not increase the `checks` counter.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 107-108, 110-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_model_name_defaults_to_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Ensures that the `get_model_name` method returns the default model name from the configuration when no custom model is specified.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the `get_model_name` method does not return the expected default model name when no custom model is provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider.get_model_name()` call should return 'test-model'.</li>
                                        <li>The `provider.get_model_name()` call should be able to determine the default model name from the configuration without any custom model being specified.</li>
                                        <li>The `provider.get_model_name()` call should not throw an exception when no custom model is provided.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 136)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_rate_limits_defaults_to_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the default rate limits for LLM providers are set to None when no configuration is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the default rate limits are not properly initialized with None, potentially causing unexpected behavior in downstream applications.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_rate_limits()` method of the `ConcreteProvider` class returns `None` when no configuration is provided.</li>
                                        <li>The `rate_limits` attribute of the provider instance does not have a valid default value (i.e., it's not set to None).</li>
                                        <li>The `rate_limits` attribute of the provider instance has a valid default value (i.e., it's set to a list or dictionary with at least one element), but this value is not correctly initialized.</li>
                                        <li>The `get_rate_limits()` method does not raise an exception when no configuration is provided, which could lead to unexpected behavior in downstream applications.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 128)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_is_local_defaults_to_false</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `is_local()` method returns False for a non-local configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of incorrect or outdated configuration settings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider.is_local()` method is called with an instance of `Config`.</li>
                                        <li>The `provider.is_local()` method checks if the `local` setting is set to True.</li>
                                        <li>The `is_local()` method returns a boolean value indicating whether the provider is local or not.</li>
                                        <li>The test asserts that the returned value is False for non-local configurations.</li>
                                        <li>The test verifies that the assertion passes when the configuration is correct but outdated.</li>
                                        <li>The test ensures that the assertion fails when the configuration is incorrect or outdated.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestHashSource::test_consistent_hash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the hash of a function with the same source code is equal to its own hash.</p>
                                <p><strong>Why Needed:</strong> This ensures consistency in cache behavior and prevents unexpected collisions between different versions of the same source code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>source == source</li>
                                        <li>hash_source(source) == hash_source(source)</li>
                                        <li>source.__code__.co_filename == source.__code__.co_filename</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestHashSource::test_different_source_different_hash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the behavior of different sources with the same function.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where two functions with the same name but different source code produce the same hash value.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `hash_source` should return a different hash value for two different source strings.</li>
                                        <li>The function `hash_source` should not raise an error when given the same input.</li>
                                        <li>The function `hash_source` should correctly handle the case where the input is a string containing multiple words or phrases.</li>
                                        <li>The function `hash_source` should be able to distinguish between functions with similar names but different source code.</li>
                                        <li>The function `hash_source` should not produce the same hash value for two different functions with the same name and source code.</li>
                                        <li>The function `hash_source` should raise an error when given invalid input, such as a non-string or non-function value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestHashSource::test_hash_length</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies the length of the hash generated by the `hash_source` function.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where the hash is not exactly 16 characters long, which could lead to unexpected behavior in certain applications.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the hash should be exactly 16 characters.</li>
                                        <li>The hash should have no leading zeros (e.g., `0x12345678`).</li>
                                        <li>No whitespace characters should be present in the hash.</li>
                                        <li>No special characters or non-ASCII characters should be present in the hash.</li>
                                        <li>The hash should not be empty.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_clear</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that clearing the cache removes all entries.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where adding multiple annotations to the cache and then clearing it would leave some entries behind.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The cache should be cleared with a count of 2.</li>
                                        <li>The annotation 'test::a' should be removed from the cache with a value of None.</li>
                                        <li>The annotation 'test::b' should be removed from the cache with a value of None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 129, 132-136, 141)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_does_not_cache_errors</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that annotations with errors do not get cached.</p>
                                <p><strong>Why Needed:</strong> To prevent caching of error-related annotations, which could lead to incorrect results or data loss in production environments.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation 'error' is set for the key 'test::foo'.</li>
                                        <li>The value of the annotation 'error' is 'API timeout'.</li>
                                        <li>The cache does not store the annotation with the given key and error message.</li>
                                        <li>If the annotation is retrieved from the cache, it should return None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 39-41, 53, 55-56, 86, 88, 118-119, 121)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_get_missing</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case 'test_get_missing' verifies that the `get` method returns `None` for missing entries in the cache.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `get` method returns `None` when an entry is not found in the cache, causing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `cache.get()` method should return `None` for a non-existent key.</li>
                                        <li>The `result` variable should be set to `None` after calling `cache.get()`.</li>
                                        <li>The test should fail when an entry is not found in the cache, indicating that the bug is present.</li>
                                        <li>The `assert` statement should raise an AssertionError with a meaningful message if the expected behavior is not met.</li>
                                        <li>The `result` variable should be set to `None` before calling `assert result is None`.</li>
                                        <li>The test should fail when an entry is not found in the cache, indicating that the bug is present.</li>
                                        <li>The `cache.get()` method should raise a KeyError with a meaningful message if the key does not exist in the cache.</li>
                                        <li>The `config.cache_dir` attribute should be set to a valid path for the cache directory.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 39-41, 53, 55-56, 118-119, 121)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_set_and_get</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the functionality of setting and retrieving annotations in the LLMCache.</p>
                                <p><strong>Why Needed:</strong> Prevents bypass attacks by ensuring that cache contents are not tampered with.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that the annotation is correctly stored in the cache.</li>
                                        <li>Check if the retrieved annotation matches the expected scenario and confidence level.</li>
                                        <li>Ensure that the cache does not return an empty result when a valid annotation is found.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 39-41, 53, 55, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorCollectionErrors::test_collection_error_structure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the structure of collection errors to ensure they match their expected format.</p>
                                <p><strong>Why Needed:</strong> To prevent a potential bug where incorrect or missing information is reported in collection errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The error nodeid should be set to the name of the file that caused the error.</li>
                                        <li>The error message should contain the actual error message.</li>
                                        <li>The error message should not be empty.</li>
                                        <li>The error message should only contain the syntax-related information (e.g., 'SyntaxError').</li>
                                        <li>The nodeid should match the expected value provided in the test.</li>
                                        <li>The message should not contain any additional information that is not relevant to the error.</li>
                                        <li>The message should not be longer than 50 characters.</li>
                                        <li>The message should only contain alphanumeric characters and underscores.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorCollectionErrors::test_get_collection_errors_initially_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that an empty collection is returned when the `get_collection_errors` method is called on a newly created `TestCollector` instance with an empty configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where an empty collection might be returned unexpectedly without any errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_collection_errors()` method should return an empty list when the input collection is empty.</li>
                                        <li>An error message or exception should not be raised if the input collection is empty.</li>
                                        <li>The test should fail with a meaningful error message when the input collection is empty, indicating that it's expected to be empty.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_context_override_default_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the default llm_context_override is set to None for a TestCaseResult.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the default llm_context_override is not correctly set to None for certain cases.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The llm_context_override attribute of TestCaseResult is indeed None.</li>
                                        <li>If llm_context_override is not None, it should be set to None.</li>
                                        <li>The default value of llm_context_override is None as expected.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_opt_out_default_false</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the default LLM opt-out value is correctly set to False.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where the default LLM opt-out value might be incorrectly set to True.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The llm_opt_out attribute of TestCaseResult nodeid='test.py::test_foo' should be False.</li>
                                        <li>The llm_opt_out attribute of TestCaseResult nodeid='test.py::test_foo' is not equal to 'True'.</li>
                                        <li>The llm_opt_out attribute of TestCaseResult nodeid='test.py::test_foo' is a boolean value (False or True).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorOutputCapture::test_capture_disabled_by_default</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the output capture feature is disabled by default.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the default behavior of capturing failed outputs might not be as expected.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.capture_failed_output should be set to False</li>
                                        <li>the captured output should not contain any error messages</li>
                                        <li>no exception should be raised when calling capture() with no arguments</li>
                                        <li>the captured output should not have a 'capture' attribute</li>
                                        <li>the captured output should not have an 'error' attribute</li>
                                        <li>the captured output should not have a 'message' attribute</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorOutputCapture::test_capture_max_chars_default</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the default value of `capture_output_max_chars` in the `Config` class is 4000.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the maximum characters to capture is not set to a reasonable default value (in this case, 4000).</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert config.capture_output_max_chars == 4000</li>
                                        <li>assert isinstance(config.capture_output_max_chars, int)</li>
                                        <li>config.capture_output_max_chars should be greater than or equal to 1</li>
                                        <li>config.capture_output_max_chars should not exceed 10000</li>
                                        <li>config.capture_output_max_chars is a positive integer</li>
                                        <li>config.capture_output_max_chars is an integer value</li>
                                        <li>config.capture_output_max_chars is not None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorXfailHandling::test_xfail_failed_is_xfailed</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that xfail failures are correctly recorded as xfailed in the TestCollector.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where xfail failures are not properly recorded as expected failure.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `wasxfail` key in the report should be set to 'expected failure'.</li>
                                        <li>The `outcome` field of the result object should be set to 'xfailed'.</li>
                                        <li>The `nodeid` field of the report should match the expected node id.</li>
                                        <li>The `when` field of the report should match the expected when condition.</li>
                                        <li>The `passed` field of the report should be False, indicating that the test failed.</li>
                                        <li>The `skipped` field of the report should be False, indicating that the test was not skipped.</li>
                                        <li>The `duration` field of the report should be a small value (e.g. 0.01 seconds).</li>
                                        <li>The `longrepr` field of the report should be an AssertionError message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212, 216, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorXfailHandling::test_xfail_passed_is_xpassed</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that xfail passes are recorded as xpassed.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where xfail passes are not correctly recorded as xpassed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `results` dictionary of the collector contains a key-value pair with 'outcome' set to 'xpassed'.</li>
                                        <li>The value of `result.outcome` is equal to 'xpassed'.</li>
                                        <li>The `nodeid` in the `results` dictionary matches the expected node id 'test_xfail.py::test_unexpected_pass'.</li>
                                        <li>The `when` field in the `results` dictionary is set to 'call', which indicates a test run.</li>
                                        <li>The `duration` and `longrepr` fields are both empty strings, indicating no issues with these metrics.</li>
                                        <li>The `wasxfail` field matches the expected value 'expected failure'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212-214)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestTestCollector::test_create_collector</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `create_collector` method of `TestCollector` class.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the collector does not initialize with empty results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `results` attribute of the `collector` object is set to an empty dictionary.</li>
                                        <li>The `collection_errors` list is empty.</li>
                                        <li>The `collected_count` attribute of the `collector` object is set to 0.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestTestCollector::test_get_results_sorted</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_results` method returns a list of node IDs sorted by their values.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the order of results is not preserved due to manual sorting.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output list should contain the same nodes as the input list, but in ascending order.</li>
                                        <li>The `nodeid` attribute of each result object should be present and match the expected values.</li>
                                        <li>No duplicate node IDs should be included in the output list.</li>
                                        <li>All node IDs should be sorted alphabetically (case-insensitive).</li>
                                        <li>The sorting is done correctly even if there are multiple results with the same outcome.</li>
                                        <li>No unexpected nodes or keys are added to the `results` dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestTestCollector::test_handle_collection_finish</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `handle_collection_finish` method to ensure it correctly tracks collected and deselected items.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `handle_collection_finish` method does not accurately count the number of collected and deselected items.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `collected_count` attribute should be set to 3 after calling `handle_collection_finish` with 3 collected items and 1 deselected item.</li>
                                        <li>The `deselected_count` attribute should be set to 1 after calling `handle_collection_finish` with 3 collected items and 1 deselected item.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 78-79, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_disabled_via_handle_report</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the collector does not capture output when config is disabled and handle_report integration is used.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the collector captures output even though the `capture_failed_output` configuration is set to False.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `results` dictionary of the node 't' should be empty (i.e., no captured stdout).</li>
                                        <li>The `captured_stdout` attribute of the `collector.results['t']` object should be `None`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stderr</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `capture_output` method captures stderr and reports it correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the captured stderr is not reported as expected.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `captured_stderr` attribute of the `result` object should be set to 'Some error'.</li>
                                        <li>The `report.capstderr` method should have been called with the correct value ('Some error').</li>
                                        <li>The `report.capstdout` attribute should not have been called (it was set to an empty string).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264, 268-269)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stdout</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `TestCollector` captures stdout correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the collector does not capture stdout as expected.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The captured stdout is set to 'Some output'.</li>
                                        <li>The captured stderr is set to an empty string.</li>
                                        <li>The `TestCollector` instance has been updated with the correct configuration (capture_failed_output=True).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_truncated</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `test_capture_output_truncated` function truncates output exceeding the max chars setting.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the collector does not truncate output exceeding the max chars setting, potentially causing unexpected behavior or errors in downstream processing.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The captured stdout length should be less than or equal to 10 characters.</li>
                                        <li>The captured stderr length should be zero (i.e., no error message was written).</li>
                                        <li>The `captured_stdout` attribute of the `TestCaseResult` object should contain only the truncated output string.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_create_result_with_item_markers</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test creates a result with item markers and verifies the expected behavior.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case an item marker is not extracted correctly, which could lead to incorrect reporting of requirements.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>item.get_closest_marker('llm_opt_out') returns MagicMock() when 'llm_opt_out' is not present in the item's spec.</li>
                                        <li>item.get_closest_marker('llm_context') returns MagicMock() when 'llm_context' is not present in the item's spec.</li>
                                        <li>item.get_closest_marker('requirement') returns MagicMock() when 'requirement' is not present in the item's spec.</li>
                                        <li>result.param_id is set to 'param1' after extracting the closest marker.</li>
                                        <li>result.llm_opt_out is set to True after extracting the closest marker.</li>
                                        <li>result.llm_context_override is set to 'complete' after extracting the closest marker.</li>
                                        <li>result.requirements contains ['REQ-1', 'REQ-2'] after extracting the closest marker.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">35 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163-164, 167-169, 171, 181-182, 185-189, 198-200, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_repr_crash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `test_extract_error_repr_crash` function to verify it handles ReprFileLocation causing crash reports correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential crash in the `test_extract_error_repr_crash` function due to an incorrect assumption about how `str()` might be used with `Report.longrepr.__str__.return_value = 'Crash report'`.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_error` method of `TestCollector` should not crash when given a `Report` object that contains a `longrepr` attribute set to `'Crash report'`.</li>
                                        <li>The `__str__` method of `Report.longrepr` should return the expected string value 'Crash report'.</li>
                                        <li>The `_extract_error` method of `TestCollector` should not raise an exception when given a `Report` object that contains a `longrepr` attribute set to `'Crash report'`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_string</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the _extract_error method returns the correct string when an error occurs.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential regression where the test fails due to incorrect handling of errors in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `report.longrepr` is set to 'Some error occurred' before calling `_extract_error(report).'</li>
                                        <li>_extract_error(report) returns 'Some error occurred'.</li>
                                        <li>The extracted string matches the expected value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_fallback</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `_extract_skip_reason` method returns `None` when no longrepr is provided.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function does not handle cases without a longrepr correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `collector._extract_skip_reason(report)` call should return `None` if `report.longrepr` is `None`.</li>
                                        <li>The `report.longrepr` attribute should be `None` when called on an instance with no `longrepr` set.</li>
                                        <li>The `_extract_skip_reason` method should not raise any exceptions when given a report without a longrepr.</li>
                                        <li>The return value of `_extract_skip_reason` should be `None` in this case.</li>
                                        <li>The function name and docstring should indicate that it handles cases without a longrepr correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250, 252)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_string</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `_extract_skip_reason` method of `TestCollector` with a mock report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the skip reason is not correctly extracted from the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report.longrepr` attribute should be set to 'Just skipped' when calling `_extract_skip_reason(report)`.</li>
                                        <li>The `report.skip_reason` attribute should be `None` when calling `_extract_skip_reason(report)`.</li>
                                        <li>The `report.longrepr` attribute should contain the string 'Just skipped'.</li>
                                        <li>The `report.skip_reason` attribute should not contain any other information.</li>
                                        <li>The `report.skip_reason` attribute should only contain the string 'Just skipped'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_tuple</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that extract skip reason tuple is used correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the skip message from the tuple longrepr is not extracted properly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The report.longrepr tuple contains the file, line and message.</li>
                                        <li>The reported reason is 'Skipped for reason'.</li>
                                        <li>The extracted reason matches the one in the report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_collection_report_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `handle_collection_report` method correctly records a collection error in the `TestCollector` instance.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where a collection report is not recorded when an error occurs during collection, potentially leading to missing errors in reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `collection_errors` attribute of the `collector` instance should contain exactly one record with `nodeid='test_broken.py'` and `message='SyntaxError'`.</li>
                                        <li>The `nodeid` field of the first record in `collector.collection_errors` should be 'test_broken.py'.</li>
                                        <li>The `message` field of the first record in `collector.collection_errors` should be 'SyntaxError'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 58, 60-65, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_rerun</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'handle_runtest_rerun' verifies that the TestCollector handles rerun attribute correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the TestCollector does not handle reruns correctly, potentially leading to incorrect results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>res.rerun_count should be equal to 1 (the expected number of reruns for this test)</li>
                                        <li>res.final_outcome should be 'failed' (indicating that the test failed due to a rerun)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_setup_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'handle_runtest_setup_failure' verifies that the TestCollector reports a setup error when runtest log report fails.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the TestCollector correctly handles setup failures and records them in its logs.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>res.outcome == 'error'</li>
                                        <li>res.phase == 'setup'</li>
                                        <li>res.error_message == 'Setup failed'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99-103, 109-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_teardown_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test should record error if teardown fails after pass.</p>
                                <p><strong>Why Needed:</strong> Prevents regression by ensuring that the test catches and reports teardown failures, preventing potential errors from being silently ignored.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert res.outcome == 'error'</li>
                                        <li>assert res.phase == 'teardown'</li>
                                        <li>assert res.error_message == 'Cleanup failed'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">38 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-115, 124, 127-128, 130, 132-133, 135-137, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_model_parsing_edge_cases</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify Gemini model parsing edge cases for coverage boosters test.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in coverage analysis when encountering edge cases with 'None' or empty lists of models.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function _parse_preferred_models() should return a list containing 'm1' and 'm2'.</li>
                                        <li>The function _parse_preferred_models() should return an empty list when the model is set to None.</li>
                                        <li>The function _parse_preferred_models() should return all models when the model is set to 'All'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 134, 136-139, 141-142, 385, 387, 417-424)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_rate_limiter_edge_math</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the edge case where a rate limiter is triggered when there are no tokens available.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression that could occur if the rate limiter was not properly reset when there were no tokens available.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert limiter.next_available_in(60) > 0</li>
                                        <li>assert limiter.next_available_in(10) == 0</li>
                                        <li>assert limiter.tokens_left() == 50</li>
                                        <li>assert limiter.tokens_left() + limiter.tokens_used() == 100</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">35 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_models_to_dict_variants</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `to_dict()` method of `SourceCoverageEntry` and `LlmAnnotation` classes returns expected values for coverage percent, error message, and duration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the coverage percentage is not correctly calculated when there are no covered lines in the source code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `d['coverage_percent']` should be equal to 50.0.</li>
                                        <li>The value of `ann.to_dict()['error']` should be 'timeout'.</li>
                                        <li>The value of `meta.to_dict()['duration']` should be equal to 1.0.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 71-78, 104-107, 109, 111-113, 115, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_create_mapper</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the creation of a CoverageMapper instance.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where a new CoverageMapper instance is created with an incorrect or missing configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config` attribute of the CoverageMapper instance should be set to the provided `Config` object.</li>
                                        <li>The `warnings` attribute of the CoverageMapper instance should be initialized with an empty list.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 44-45)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_get_warnings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `get_warnings` method in the `CoverageMapper` class should be able to retrieve a list of warnings from the coverage data.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `get_warnings` method returns an empty list when there are no warnings available.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `warnings` variable is expected to be a list.</li>
                                        <li>The `warnings` variable is expected to contain at least one warning.</li>
                                        <li>The `warnings` variable is not empty.</li>
                                        <li>The `warnings` variable does not contain any warnings.</li>
                                        <li>The `warnings` variable contains only warnings and no other data points.</li>
                                        <li>The `warnings` variable has a length greater than 0.</li>
                                        <li>The `warnings` variable has a length less than 1.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 44-45, 308)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_map_coverage_no_coverage_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the test_map_coverage_no_coverage_file function returns an empty dictionary when no coverage file is present.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the test_map_coverage function incorrectly returns a non-empty dictionary when there are no coverage files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The mapper.map_coverage() method should return an empty dictionary.</li>
                                        <li>The result of mapper.map_coverage() should not contain any warnings.</li>
                                        <li>At least one warning should be present in the mapper.warnings list.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_all_phases</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `CoverageMapper` correctly extracts node IDs for all phases when including all phases.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the coverage map might not include all phases if only 'run' phase is included.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>When the `include_phase` parameter is set to `'all'`, the `_extract_nodeid` method should return the full node ID for each phase.</li>
                                        <li>The `_extract_nodeid` method should correctly extract node IDs from all phases, including 'setup' and 'teardown'.</li>
                                        <li>If only the 'run' phase is included in the configuration, the `_extract_nodeid` method should still return the correct node ID for the 'test_foo' function.</li>
                                        <li>The `CoverageMapper` instance should not throw any errors when called with an invalid include_phase value (e.g., `'foo'`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_empty_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 44-45, 216-217)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_filters_setup</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `nodeid` extraction filter does not match nodes in the setup phase of a test.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the `nodeid` extraction filter might incorrectly identify nodes in the setup phase as part of the test code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The node id is extracted from the string 'test.py::test_foo|setup'.</li>
                                        <li>The node id is None because it does not match any nodes in the setup phase.</li>
                                        <li>The `nodeid` extraction filter should exclude nodes in the setup phase.</li>
                                        <li>The test code should be able to correctly identify and exclude nodes in the setup phase from coverage analysis.</li>
                                        <li>The `nodeid` extraction filter should handle cases where the node id contains special characters or spaces.</li>
                                        <li>The test should pass without any errors when running with the `include_phase=run` configuration.</li>
                                        <li>The `nodeid` extraction filter should be able to handle complex node ids that contain multiple words or phrases.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 44-45, 216, 220, 224-225, 228-230)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_with_run_phase</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the coverage mapper extracts nodeid from run phase context correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the nodeid is not extracted from the run phase context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function _extract_nodeid in CoverageMapper configures to extract nodeid from the run phase context.</li>
                                        <li>The input string 'test.py::test_foo|run' is correctly split into nodes.</li>
                                        <li>The extracted nodeid matches the expected value 'test.py::test_foo'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_full_logic</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `extract_contexts` method of the `CoverageMapper` class correctly extracts all paths in `_extract_contexts` when given mock data.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the coverage map might not include all contexts due to missing or incorrect context definitions.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The method should return at least one context for `test_app.py::test_one` and `test_app.py::test_two`.</li>
                                        <li>Each returned context should have exactly two lines (lines 1 and 2).</li>
                                        <li>The line count of each context should match the expected number of lines in `app.py`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">57 lines (ranges: 44-45, 118, 121-122, 127, 131-135, 137-140, 144, 148, 150, 152-153, 156, 160-163, 165, 167-168, 173, 176, 178-184, 187-189, 191-194, 196, 199-200, 202, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">13 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65-67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_no_contexts</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `extract_contexts` method returns an empty dictionary when there are no test contexts.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in coverage analysis where data has no test contexts.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_data.contexts_by_lineno.return_value == {}</li>
                                        <li>mock_data.measured_files.return_value == ['app.py']</li>
                                        <li>result == {}</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 44-45, 118, 121-122, 127, 131-135, 144-146)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_nodeid_variants</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Extract Node ID Variants</p>
                                <p><strong>Why Needed:</strong> This test verifies that the `CoverageMapper` correctly extracts node IDs for different phases and contexts.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_nodeid()` method returns the expected node ID for each context.</li>
                                        <li>The `_extract_nodeid()` method filters out lines with missing phase information.</li>
                                        <li>The `_extract_nodeid()` method handles cases where there are no nodes in a given phase.</li>
                                        <li>The `_extract_nodeid()` method correctly extracts node IDs from non-pipe contexts.</li>
                                        <li>The `_extract_nodeid()` method returns the expected node ID for each context, even when there are no nodes with that specific name.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231-234, 236, 239)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_no_files</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the test_load_coverage_data_no_files function raises an assertion error when no coverage files exist.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the function does not raise an assertion error when there are no coverage files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return None and have exactly one warning message.</li>
                                        <li>The first warning message should be 'W001'.</li>
                                        <li>All warnings should be related to file paths that do not exist.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 44-45, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_read_error</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the CoverageMapperMaximal class can handle errors reading coverage files.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the CoverageMapperMaximal class does not correctly handle errors when loading coverage data from corrupted or invalid files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function _load_coverage_data() should return None if an error occurs while reading the coverage file.</li>
                                        <li>Any warnings generated by the mapper should contain the message 'Failed to read coverage data'.</li>
                                        <li>The warnings should not be empty.</li>
                                        <li>The function _load_coverage_data() should raise an exception when an error occurs while reading the coverage file.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 44-45, 72-73, 83, 86, 88, 92, 94-96, 107-111, 114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_with_parallel_files</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test should handle parallel coverage files from xdist and verify that the CoverageMapper correctly updates its internal data structures.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in handling parallel coverage files, ensuring that the CoverageMapper's update mechanism is called for these cases.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `update` method of the mock `CoverageData` class should be called at least twice when loading coverage data with parallel files.</li>
                                        <li>The `update` method of the mock `CoverageData` class should not be called more than twice when loading coverage data with parallel files.</li>
                                        <li>The number of times the `update` method is called for each mock instance of `CoverageData` should be at least 2.</li>
                                        <li>The `update` method should only be called once for each mock instance of `CoverageData` that has a different value than its initial state.</li>
                                        <li>The `update` method should not be called more than once for any given mock instance of `CoverageData` when loading coverage data with parallel files.</li>
                                        <li>The number of times the `update` method is called for each mock instance of `CoverageData` should be at most 2.</li>
                                        <li>If no instances of `CoverageData` are provided, the `update` method should not be called at all.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 44-45, 72-73, 83, 86, 88, 92, 94, 98, 101-104, 106)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_coverage_no_data</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `map_coverage` method when it is called without any coverage data.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the `map_coverage` method returns an empty dictionary when there is no coverage data to process.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_load_coverage_data` method of the `CoverageMapper` instance should return `None` when called without any coverage data.</li>
                                        <li>The `map_coverage` method should return an empty dictionary when there is no coverage data.</li>
                                        <li>The result of calling `map_coverage` should be a dictionary with all key-value pairs that were covered by the test code.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 44-45, 58-60)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_analysis_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `map_source_coverage` method to handle errors during analysis.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in coverage map generation when an error occurs during analysis.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocking `analysis2` with an exception is sufficient to test this scenario.</li>
                                        <li>The `get_data` call should return mock data without raising an exception.</li>
                                        <li>The `map_source_coverage` method should skip files with errors and not generate a coverage map.</li>
                                        <li>The number of entries in the coverage map should be 0 when an error occurs during analysis.</li>
                                        <li>No exceptions should be raised within the `map_source_coverage` method itself.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 44-45, 243-244, 246-248, 250, 252-254, 259, 261, 263-268, 271, 299-300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_comprehensive</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the test maps all paths in map_source_coverage to comprehensive coverage.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that all source files are covered under the maximal coverage analysis.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function mapper.map_source_coverage returns a list of entries where each entry contains information about a file's coverage.</li>
                                        <li>Each entry in the returned list has the following properties: `file_path`, `statements`, `covered`, `missed`, and `coverage_percent`.</li>
                                        <li>All covered files should have a percentage of 100% coverage.</li>
                                        <li>The function mapper.map_source_coverage should return exactly one entry for each file in map_source_coverage.</li>
                                        <li>Each entry in the returned list should contain the following properties: `file_path`, `statements`, `covered`, `missed`, and `coverage_percent`.</li>
                                        <li>All covered files should have a percentage of 100% coverage.</li>
                                        <li>The function mapper.map_source_coverage should return exactly one entry for each file in map_source_coverage.</li>
                                        <li>Each entry in the returned list should contain the following properties: `file_path`, `statements`, `covered`, `missed`, and `coverage_percent`.</li>
                                        <li>All covered files should have a percentage of 100% coverage.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">32 lines (ranges: 44-45, 243-244, 246-248, 250, 252, 259-261, 273, 276-279, 281-283, 285-293, 295, 299-300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors.py::test_make_warning</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `make_warning` factory function to verify it returns a WarningCode.W001_NO_COVERAGE instance with the correct message and detail.</p>
                                <p><strong>Why Needed:</strong> The test prevents a potential bug where the `make_warning` function does not return an error when a non-existent warning code is provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>w.code == WarningCode.W001_NO_COVERAGE</li>
                                        <li>assert 'No .coverage file found' in w.message</li>
                                        <li>assert w.detail == 'test-detail'</li>
                                        <li>w_unknown.message == 'Unknown warning.'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors.py::test_warning_code_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that warning codes have correct values.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the warning code values are incorrect, potentially leading to unexpected behavior or errors in the application.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>{'message': 'assert WarningCode.W001_NO_COVERAGE.value == "W001"', 'description': 'Verify that WarningCode.W001_NO_COVERAGE has correct value'}</li>
                                        <li>{'message': 'assert WarningCode.W101_LLM_ENABLED.value == "W101"', 'description': 'Verify that WarningCode.W101_LLM_ENABLED has correct value'}</li>
                                        <li>{'message': 'assert WarningCode.W201_OUTPUT_PATH_INVALID.value == "W201"', 'description': 'Verify that WarningCode.W201_OUTPUT_PATH_INVALID has correct value'}</li>
                                        <li>{'message': 'assert WarningCode.W301_INVALID_CONFIG.value == "W301"', 'description': 'Verify that WarningCode.W301_INVALID_CONFIG has correct value'}</li>
                                        <li>{'message': 'assert WarningCode.W401_AGGREGATE_DIR_MISSING.value == "W401"', 'description': 'Verify that WarningCode.W401_AGGREGATE_DIR_MISSING has correct value'}</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors.py::test_warning_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `to_dict()` method of the `Warning` class.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the warning message is not properly formatted when converted to a dictionary.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `code` attribute of the `Warning` instance should be set to 'W001'.</li>
                                        <li>The `message` attribute of the `Warning` instance should be set to 'No coverage'.</li>
                                        <li>The `detail` attribute of the `Warning` instance should be set to 'some/path'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 70-72, 74-76)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_known_code</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that a warning with the correct code and message is created when known code is used.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where unknown or unexpected code may cause warnings to be generated.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `make_warning` returns an instance of `WarningCode.W101_LLM_ENABLED` with the correct code.</li>
                                        <li>The warning message is set to `WARNING_MESSAGES[WarningCode.W101_LLM_ENABLED]` as expected.</li>
                                        <li>The detail attribute is not provided, which is expected for warnings related to unknown or unexpected code.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_unknown_code</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test MakeWarning::test_make_warning_unknown_code verifies that the test uses a fallback message for unknown code.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where an unknown warning code is not handled correctly and causes unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The method make_warning() returns a WarningMessage object with the correct message 'Unknown warning.'</li>
                                        <li>The WARNING_MESSAGES dictionary is updated to store the old message for the missing code</li>
                                        <li>The WARNING_MESSAGES dictionary is restored after the test finishes</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_with_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_make_warning_with_detail' verifies that a warning is created with the correct code and detail.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where a warning is not correctly set with the required detail.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>w.code == WarningCode.W301_INVALID_CONFIG</li>
                                        <li>w.detail == 'Bad value'</li>
                                        <li>assert w.detail in ['Bad value', 'Invalid configuration']</li>
                                        <li>assert isinstance(w.detail, str)</li>
                                        <li>assert len(w.detail) > 0</li>
                                        <li>assert not isinstance(w.detail, int)</li>
                                        <li>assert not isinstance(w.detail, bool)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestWarningCodes::test_codes_are_strings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Ensures that enum values are properly initialized as strings.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where enum values might not be strings, potentially leading to unexpected behavior or errors in the application.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert isinstance(code.value, str)</li>
                                        <li>assert code.value.startswith('W')</li>
                                        <li>code.value is not None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_no_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the warning to dictionary conversion without detail.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where warnings are not properly serialized to dictionaries.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'to_dict()' method of Warning class should return a dictionary with 'code' and 'message' keys.</li>
                                        <li>The 'to_dict()' method should preserve the original warning code and message values.</li>
                                        <li>The 'to_dict()' method should not include any additional detail in the returned dictionary.</li>
                                        <li>The 'to_dict()' method should raise an exception when no detail is available (e.g., for warnings with no coverage)</li>
                                        <li>The 'to_dict()' method should handle warnings with different severity levels (e.g., WarningCode.W002)</li>
                                        <li>The 'to_dict()' method should preserve the original warning message even if it's a single line string</li>
                                        <li>The 'to_dict()' method should not add any additional whitespace or formatting to the returned dictionary</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 70-72, 74, 76)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_with_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the warning to dict conversion with detailed message.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where warnings are not properly serialized in dictionaries.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'code' key should be set to 'W001'.</li>
                                        <li>The 'message' key should be set to 'No coverage'.</li>
                                        <li>The 'detail' key should be set to 'Check setup'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 70-72, 74-76)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestIsPythonFile::test_non_python_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `is_python_file` function returns False for non-.py files.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function incorrectly identifies Python files as non-Python files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file 'foo/bar.txt' is not a Python file because it does not contain Python code.</li>
                                        <li>The file 'foo/bar.pyc' is not a Python file because it contains compiled Python code.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 79)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestIsPythonFile::test_python_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `is_python_file` function correctly identifies .py files.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function incorrectly identifies non-py files as Python files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return True for files with the `.py` extension.</li>
                                        <li>The function should raise an error or return False for files without the `.py` extension.</li>
                                        <li>The function should correctly handle files with multiple extensions (e.g., `.pyc`, `.pyo`).</li>
                                        <li>The function should not incorrectly identify non-Python file types (e.g., `foo.txt`),</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 79)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestMakeRelative::test_makes_path_relative</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_makes_path_relative' verifies that making an absolute path relative to a temporary directory results in the expected output.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the `make_relative` function incorrectly returns the original file path when the input is an absolute path.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'result' variable should be equal to 'subdir/file.py'.</li>
                                        <li>The parent directory of 'file_path' should have been created with parents=True and exist_ok=True.</li>
                                        <li>The 'make_relative' function should not return the original file path when the input is an absolute path.</li>
                                        <li>The relative path from the temporary directory to the expected output should be correct.</li>
                                        <li>The 'tmp_path' object passed as the second argument to 'make_relative' should have been modified correctly.</li>
                                        <li>The 'file_path.parent' attribute should have been created with parents=True and exist_ok=True.</li>
                                        <li>The 'touch' method should have been called on 'file_path' without raising an exception.</li>
                                        <li>The 'result' variable should be equal to the expected output string after modification.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `make_relative` function returns a normalized path when there is no base.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where a relative path without a base would not be normalized correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of `make_relative('foo/bar')` should be 'foo/bar'.</li>
                                        <li>The directory separator (./) should be replaced with the root directory (..).</li>
                                        <li>Any leading or trailing slashes in the input path should be removed.</li>
                                        <li>Any backslashes in the input path should be converted to forward slashes.</li>
                                        <li>The resulting normalized path should not contain any redundant separators (e.g., ./foo/../bar).</li>
                                        <li>The resulting normalized path should start with a single separator (either ./ or ..).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 30, 33, 36, 39, 42, 55-56)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestNormalizePath::test_already_normalized</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `normalize_path` function should be able to handle and return the original path for already normalized paths.</p>
                                <p><strong>Why Needed:</strong> To prevent a potential bug where the function incorrectly returns an empty string or raises an exception when given already-normalized paths.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert normalize_path('foo/bar') == 'foo/bar'</li>
                                        <li>assert normalize_path('/foo/bar') == '/foo/bar'</li>
                                        <li>assert normalize_path('//foo/bar') == '//foo/bar'</li>
                                        <li>assert normalize_path('/foo//bar') == '/foo//bar'</li>
                                        <li>assert normalize_path('/foo/./bar') == '/foo/./bar'</li>
                                        <li>assert normalize_path('/foo/../bar') == '/foo/bar'</li>
                                        <li>assert normalize_path('foo/../bar') == 'foo/bar'</li>
                                        <li>assert normalize_path('../bar') == '../bar'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestNormalizePath::test_forward_slashes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `normalize_path` function correctly converts backslashes to forward slashes when the path contains a single forward slash.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not handle paths with multiple consecutive forward slashes correctly, potentially leading to incorrect output or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert normalize_path('foo\bar') == 'foo/bar'</li>
                                        <li>normalize_path('foo/\bar') should return 'foo/bar'</li>
                                        <li>normalize_path('foo//bar') should return 'foo/bar'</li>
                                        <li>normalize_path('foo/./bar') should return 'foo/bar'</li>
                                        <li>normalize_path('foo/../bar') should return 'bar'</li>
                                        <li>normalize_path('/foo/\bar') should return '/foo/bar'</li>
                                        <li>normalize_path('/foo/../\bar') should return '/bar'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `normalize_path` function correctly strips trailing slashes from file paths.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where a file path with a trailing slash is not properly normalized, potentially causing issues in certain applications.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input string should be modified to remove any trailing slashes before normalization.</li>
                                        <li>The normalized output should be the same as the original input without any trailing slashes.</li>
                                        <li>Any leading or trailing whitespace characters should be preserved during normalization.</li>
                                        <li>The function should handle paths with multiple consecutive slashes correctly.</li>
                                        <li>Paths with a single slash (e.g., 'foo/') should not have their trailing slash removed.</li>
                                        <li>Paths with a double slash (e.g., 'foo://') should be treated as if they had only one slash.</li>
                                        <li>Any file system path that starts with a forward slash should be normalized to start with the last occurrence of a forward slash.</li>
                                        <li>The function should handle paths with relative references correctly, such as '../file.txt'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_custom_exclude_patterns</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies whether a path matches custom exclusion patterns.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `should_skip_path` function incorrectly excludes paths that match custom patterns.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The path 'tests/conftest.py' should be excluded from being skipped due to its matching custom pattern.</li>
                                        <li>The path 'src/module.py' should not be excluded from being skipped due to its non-matching custom pattern.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116-117, 119-121, 123)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_normal_path</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `should_skip_path` function does not return True for normal paths.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where the test would incorrectly flag normal paths as skipped.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `should_skip_path` function should return False for the path 'src/module.py'.</li>
                                        <li>The `should_skip_path` function should not raise an exception when given a valid file name like 'src/module.py'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_git</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `should_skip_path` function correctly identifies `.git` directories.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where the test incorrectly skips non-`.git` directories.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return True for files within `.git/objects/` (e.g., `.git/objects/foo`)</li>
                                        <li>The function should not return True for other `.git` subdirectories or directories containing `objects`</li>
                                        <li>The function should raise an exception when encountering a non-`.git` directory</li>
                                        <li>The function should handle nested `.git` directories correctly</li>
                                        <li>The function should skip files within the same level as the `.git` directory (e.g., `.git/objects/foo/bar`)</li>
                                        <li>The function should not skip files within subdirectories of the `.git` directory (e.g., `objects/foo/bar/baz`)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_pycache</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `should_skip_path` function correctly identifies and skips `__pycache__` directories.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the `should_skip_path` function incorrectly includes `__pycache__` directories in the list of paths to skip.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The path should be skipped because it is located within `__pycache__`.</li>
                                        <li>The path should not be included in the list of paths to skip.</li>
                                        <li>The `should_skip_path` function should correctly identify and exclude `__pycache__` directories from the list of paths to check for skipping.</li>
                                        <li>The test should fail when a `__pycache__` directory is present in the input path.</li>
                                        <li>The test should pass when no `__pycache__` directory is present in the input path.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_venv</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `should_skip_path` function correctly identifies venv directories.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the function incorrectly identifies venv directories as Python site packages, potentially leading to incorrect skipping of these directories.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>venv/lib/python/site.py</li>
                                        <li>.venv/lib/python/site.py</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_pruning</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that pruning clears request times and token usage after a past request.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the rate limiter does not clear request times and token usage for requests made in the past, leading to unexpected behavior or performance issues.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of `limiter._request_times` should be 0 after pruning.</li>
                                        <li>The length of `limiter._token_usage` should be 0 after pruning.</li>
                                        <li>No request times and token usage should exist in the limiter after pruning.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 39-42, 81-85, 87-88)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_rpm_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter prevents a request from being recorded after exceeding the limit.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where a user exceeds the RPM limit and their requests are not recorded.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `next_available_in` method should return an available time slot within the next minute.</li>
                                        <li>The `wait` assertion should be greater than 0, indicating that the request is unavailable.</li>
                                        <li>The `wait` assertion should be less than or equal to 60.0 seconds, representing the maximum allowed wait time before recording a new request.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95, 97, 100-102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_tpm_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter prevents a regression when tokens are not yet available.</p>
                                <p><strong>Why Needed:</strong> The test prevents a potential regression in the rate limiter's behavior when tokens are not yet available, ensuring consistency with the expected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>limiter._token_usage should be equal to 2 after record_tokens(10)</li>
                                        <li>wait > 0 after record_tokens(90) and record_tokens(20)</li>
                                        <li>_token_usage is updated correctly when tokens are not yet available</li>
                                        <li>len(limiter._token_usage) == 2 after record_tokens(10)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">33 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-94, 100-101, 103, 105, 107-108, 110-114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_wait_for_slot</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `wait_for_slot` method sleeps after a request is recorded.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the rate limiter does not sleep when a new request is made, potentially leading to performance issues or unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `time.sleep` function was called with the correct argument (1).</li>
                                        <li>The `time.sleep` function was called after the `record_request` method.</li>
                                        <li>The `wait_for_slot` method does not sleep when a new request is made.</li>
                                        <li>The `wait_for_slot` method sleeps for at least 1 second.</li>
                                        <li>The `wait_for_slot` method sleeps for no more than 1 second.</li>
                                        <li>The `time.sleep` function was called with the correct argument (1) and the correct time (1 seconds).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">31 lines (ranges: 39-42, 45-46, 48, 52-54, 58-59, 61-63, 73, 76-78, 81-82, 84, 87-88, 92-93, 95, 97, 100-102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_record_zero_tokens</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the limiter records zero tokens when no tokens are available.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the limiter does not record tokens for an empty rate limit configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of `_token_usage` should be 0 after calling `record_tokens(0)`.</li>
                                        <li>The `_token_usage` list should contain no elements.</li>
                                        <li>The number of tokens in `_token_usage` should remain unchanged.</li>
                                        <li>The limiter's internal state should not change unexpectedly.</li>
                                        <li>_token_usage is a list, it should have at least one element.</li>
                                        <li>The length of `_token_usage` should be 0 after calling `record_tokens(0)`.</li>
                                        <li>The number of tokens in `_token_usage` should remain unchanged.</li>
                                        <li>The limiter's internal state should not change unexpectedly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39-42, 66-67)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_requests_per_day_exhaustion</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the test raises an error when exceeding daily rate limit.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the application exceeds its daily rate limit and attempts to do so again, causing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The limiter is configured with `requests_per_day=1` which means it will only allow one request per day.</li>
                                        <li>When trying to exceed this limit, a new error `_GeminiRateLimitExceeded` is raised with the message 'requests_per_day'.</li>
                                        <li>The test checks that this error is not raised when waiting for a slot of 10 requests.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">24 lines (ranges: 32-34, 39-42, 45-46, 48-50, 58-60, 73, 76-78, 81-82, 84, 87-88)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_tpm_fallback_wait</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter waits for TPM availability when tokens are exceeded.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the rate limiter does not wait for TPM availability when tokens are exceeded, leading to unexpected behavior or performance issues.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The current rate limit is sufficient to fill up the TPM before waiting for it to become available.</li>
                                        <li>Tokens used plus request tokens exceed the rate limit AND token usage is not empty.</li>
                                        <li>The rate limiter waits for TPM availability when tokens are exceeded, as expected.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">24 lines (ranges: 39-42, 66, 68-70, 81-82, 84, 87-88, 100-101, 103, 105, 107-108, 110-114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_provider_rpm_cooldown</span>
                        <div class="test-meta">
                            <span>616ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that RPM rate limit cooldown handling is correctly implemented when a call fails with an error.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the RPM rate limit cooldown might not be properly set on first calls to the provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'models/gemini-pro' model should be in the _cooldowns dictionary with a value greater than 1000.0.</li>
                                        <li>The cooldown time for 'models/gemini-pro' should be greater than 1000.0 seconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">23 lines (ranges: 52-53, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">117 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-215, 217-223, 225-227, 233-234, 238-240, 242-243, 274-277, 280, 282-290, 292-295, 297-298, 300-301, 346, 348-350, 352-353, 381-382, 385-386)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_rate_limit_retry</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the GeminiProvider annotates a rate limit retry scenario correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the GeminiProvider's ability to handle rate limits and retries.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation should contain the correct scenario 'Recovered Scenario'.</li>
                                        <li>The mock_post call count should be equal to 2, indicating two attempts to annotate with a 429 status code.</li>
                                        <li>The annotation should not have an error message.</li>
                                        <li>The annotation's scenario should match the one provided in the test result.</li>
                                        <li>The annotation's model list should contain only 'models/m1'.</li>
                                        <li>The annotation's supported generation methods should be 'generateContent'.</li>
                                        <li>The annotation's content should contain a single part with text 'Recovered Scenario'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">181 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-215, 217-222, 225-227, 233-234, 238-240, 242-243, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330-333, 335-341, 343, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_success</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that _annotate_internal returns the correct annotation for a successful response from _call_gemini.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case of an error when calling _call_gemini with a failed response.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The scenario 'Success Scenario' is correctly extracted from the annotation.</li>
                                        <li>There are no errors in the annotation.</li>
                                        <li>The annotation does not contain any invalid or unexpected information.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">173 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-223, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-343, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_availability</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the GeminiProvider class checks availability correctly when environment variables are not set.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider does not check for availability in a non-existent environment.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider should be able to determine if it is available by checking its configuration.</li>
                                        <li>If no environment variable is set, the provider should return False indicating that it is unavailable.</li>
                                        <li>If an environment variable is set with a valid API token, the provider should return True indicating that it is available.</li>
                                        <li>If an environment variable is set with an invalid or missing API token, the provider should raise an exception or return an error message.</li>
                                        <li>The provider's configuration should be able to override the default availability check.</li>
                                        <li>The provider's availability check should not rely on external factors such as network connectivity.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 134, 136-139, 141-142, 266-267, 269)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpd_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter prevents exceeding the daily limit of 1 request per day.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the rate limiter allows more than one request to be processed within a single day, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>limiter.next_available_in() should return None when called with an argument of 100 (representing the daily limit).</li>
                                        <li>The limiter's next_available_in() method should not raise any exceptions if it cannot find an available slot within the specified number of requests.</li>
                                        <li>The limiter's next_available_in() method should update the limiter's internal state correctly after each request, ensuring that no more than one request is processed within a single day.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 39-42, 45-46, 48-50, 73, 76-78, 81-82, 84, 87-88)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpm_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the rate limiter does not block requests for a short period after the first two requests.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the rate limiter blocks subsequent requests from being processed immediately after the initial two requests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The next available time slot should be greater than 0.</li>
                                        <li>The next available time slot should not exceed 60 seconds.</li>
                                        <li>The wait time should not be exactly equal to 1 second (which would indicate a bug).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">27 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-97, 100-102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeConfigHash::test_different_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that different configurations produce different hashes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where two different configurations are hashed to the same value, potentially leading to incorrect results or errors in downstream applications.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Two different Config instances should have different hashes.</li>
                                        <li>The hash of config1 should not be equal to the hash of config2.</li>
                                        <li>compute_config_hash(config1) != compute_config_hash(config2)</li>
                                        <li>compute_config_hash(config1) is not a string</li>
                                        <li>compute_config_hash(config1).startswith('none')</li>
                                        <li>compute_config_hash(config1).endswith('ollama')</li>
                                        <li>compute_config_hash(config1).lower() == 'none'</li>
                                        <li>compute_config_hash(config1).upper() == 'NONE'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 96-101, 103-104)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeConfigHash::test_returns_short_hash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies the length of the computed hash is 16 characters.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where the hash may be too long, potentially causing performance issues or leading to incorrect results in certain scenarios.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the computed hash should be exactly 16 characters.</li>
                                        <li>The computed hash should not exceed 15 characters.</li>
                                        <li>The computed hash should contain only hexadecimal digits (0-9, A-F, a-f).</li>
                                        <li>The computed hash should start with '00000000'.</li>
                                        <li>The computed hash should end with 'ffffffff'.</li>
                                        <li>No leading zeros are allowed in the computed hash.</li>
                                        <li>No trailing zeros are allowed in the computed hash.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 96-101, 103-104)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeFileSha256::test_consistent_with_bytes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the computed SHA-256 hash of a file matches its content hash when the same file is hashed with different input.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the SHA-256 hash of a file changes even if the content remains the same, due to differences in the way Python's `hashlib` library handles file hashes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The computed SHA-256 hash of the file should be equal to its content hash.</li>
                                        <li>The content hash of the file should match the computed SHA-256 hash.</li>
                                        <li>The difference between the computed SHA-256 hash and the content hash should be zero (i.e., they should be equal).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 32, 44-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeFileSha256::test_hashes_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the correctness of computing a SHA-256 hash for a file.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues where the hash computation is incorrect or incomplete, potentially leading to data corruption or security vulnerabilities.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the computed hash should be 64 bytes.</li>
                                        <li>The hash should contain all necessary information about the original file contents (e.g., byte order, padding).</li>
                                        <li>The hash should not be empty (i.e., it should have at least one non-zero byte).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 44-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeHmac::test_different_key</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_different_key' verifies that different keys produce different signatures.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where two different keys might generate the same HMAC signature, potentially leading to security vulnerabilities or unexpected behavior in cryptographic applications.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The computed HMAC values for different input data ('content') and different keys ('key1' and 'key2') are distinct.</li>
                                        <li>The computed HMAC value for key1 is different from the computed HMAC value for key2.</li>
                                        <li>The computed HMAC value for key1 is not equal to the expected output of a random key.</li>
                                        <li>The computed HMAC value for key2 is different from the computed HMAC value for key1.</li>
                                        <li>The computed HMAC value for key2 is not equal to the expected output of a random key.</li>
                                        <li>A different input data ('content') and a different key ('key1') produce a distinct HMAC signature.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 61)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeHmac::test_with_key</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies the computation of HMAC for a given content and secret key.</p>
                                <p><strong>Why Needed:</strong> Prevents potential security vulnerabilities such as weak or reused keys, which could compromise the integrity of sensitive data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the generated signature should be 64 bytes.</li>
                                        <li>The HMAC algorithm used is SHA-256 (or a similar secure hash function).</li>
                                        <li>The secret key used for computation is not reused across different test runs. This ensures that each test case has its own unique key.</li>
                                        <li>The content being hashed is not empty or null, as this could lead to incorrect signature generation.</li>
                                        <li>The secret key provided does not contain any whitespace characters (spaces, tabs, etc.), ensuring it can be properly encoded in the HMAC algorithm.</li>
                                        <li>The content being hashed contains only ASCII characters, which are supported by the SHA-256 algorithm. Non-ASCII characters may cause issues with signature generation.</li>
                                        <li>No exceptions are raised during the computation of the HMAC signature, indicating a successful operation.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 61)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeSha256::test_consistent</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The function `compute_sha256` generates the same hash for two identical input strings.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where different inputs to the function could produce different hashes, potentially leading to inconsistencies in the system's behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Input string is identical (same bytes)</li>
                                        <li>Hash values are equal</li>
                                        <li>No changes were made to the input string</li>
                                        <li>Function produces consistent hash for given input</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeSha256::test_length</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The hash function should produce a SHA-256 hash of the input string 'test' and return its length.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues where the hash length is not as expected due to incorrect implementation or configuration of the hashing algorithm.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output of the `len(h)` assertion should be equal to 64, indicating that the hash function correctly produces a SHA-256 hash.</li>
                                        <li>The output of the `h` variable should contain exactly 64 hexadecimal characters.</li>
                                        <li>The length of the `h` string should not exceed 64 bytes (the maximum allowed length for a SHA-256 hash).</li>
                                        <li>The `h` variable should be a string containing exactly 64 hexadecimal digits.</li>
                                        <li>The `len(h)` assertion should fail if the actual output is less than or equal to 63, indicating an issue with the hashing algorithm.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestGetDependencySnapshot::test_includes_pytest</span>
                        <div class="test-meta">
                            <span>82ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_dependency_snapshot` function returns a snapshot containing the 'pytest' package.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the dependency snapshot does not include all required packages, potentially causing issues with downstream code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Snapshot contains 'pytest'</li>
                                        <li>Includes pytest package in dependency snapshot</li>
                                        <li>Includes pytest package in dependency snapshot</li>
                                        <li>Includes pytest package in dependency snapshot</li>
                                        <li>Includes pytest package in dependency snapshot</li>
                                        <li>Includes pytest package in dependency snapshot</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 113-114, 116-121)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestGetDependencySnapshot::test_returns_dict</span>
                        <div class="test-meta">
                            <span>83ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The function `get_dependency_snapshot()` returns a dictionary when called.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function might return an incorrect data type (e.g., list instead of dict) or throw an error if the snapshot is empty.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>snapshot should be an instance of dict</li>
                                        <li>snapshot should not be None</li>
                                        <li>snapshot should contain only package names and their dependencies</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 113-114, 116-121)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_loads_key</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `load_hmac_key` function correctly loads a key from a file.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the loaded key is not correct due to incorrect file encoding or formatting.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file is written in UTF-8 encoding and contains only one line of text.</li>
                                        <li>The file does not contain any newline characters (\n) after the secret key.</li>
                                        <li>The `load_hmac_key` function correctly reads the entire file into memory.</li>
                                        <li>The loaded key is a bytes object with the correct length (16 bytes).</li>
                                        <li>The loaded key is equal to the expected value (`b'my-secret-key'`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 73, 76-77, 80-81)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_missing_key_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case: TestLoadHmacKey::test_missing_key_file verifies that the function returns None when a non-existent key file is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function would return an incorrect result or crash if a non-existent key file is passed in.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return `None` when a non-existent key file is provided.</li>
                                        <li>The function should not attempt to load the HMAC key from the non-existent file.</li>
                                        <li>The test should fail when a non-existent key file is used, indicating an error or unexpected behavior.</li>
                                        <li>The function's internal state should remain unchanged after a non-existent key file is passed in.</li>
                                        <li>Other tests that rely on this specific functionality should be updated to handle the case where a non-existent key file is provided.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 73, 76-78)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_no_key_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `load_hmac_key` function returns `None` when no key file is specified.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function does not handle cases without a key file configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `load_hmac_key` function should return `None` if no key file is provided.</li>
                                        <li>No exception should be raised when no key file is specified.</li>
                                        <li>The function should correctly identify that no key file was found.</li>
                                        <li>The function's behavior should not depend on the presence or absence of a key file configuration.</li>
                                        <li>The function's error handling should prioritize returning `None` over raising an exception.</li>
                                        <li>The function's return type should be consistent with its expected behavior.</li>
                                        <li>No unexpected side effects should occur when calling `load_hmac_key` without a key file.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 73-74)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_aggregation_defaults</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test aggregation defaults with an empty aggregate directory.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the aggregation policy is not set correctly when no aggregate directory is provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.aggregate_dir is None</li>
                                        <li>config.aggregate_policy == 'latest'</li>
                                        <li>config.aggregate_include_history is False</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_capture_failed_output_default_false</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the default capture failed output setting is set to False.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the default capture failed output setting is incorrectly set to True.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `capture_failed_output` configuration option is not set to `False` by default.</li>
                                        <li>The `capture_failed_output` value is not `None` when the default configuration is used.</li>
                                        <li>The `capture_failed_output` value is not `True` when the default configuration is used.</li>
                                        <li>The `capture_failed_output` value does not match the expected default setting (False) for this test scenario.</li>
                                        <li>The `capture_failed_output` value does not change when the test is run with a different set of inputs.</li>
                                        <li>The `capture_failed_output` value does not match the expected behavior in all test scenarios.</li>
                                        <li>The `capture_failed_output` configuration option is not properly updated when the default configuration is changed.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_context_mode_default_minimal</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the default context mode of LLM integration gate.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where the context mode is set to 'minimal' by default.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function get_default_config() returns an instance of Config with llm_context_mode set to 'minimal'.</li>
                                        <li>The value of llm_context_mode in the returned config is 'minimal'.</li>
                                        <li>The configuration object passed to test_context_mode_default_minimal has a llm_context_mode attribute equal to 'minimal'.</li>
                                        <li>If context mode defaults to minimal, then it should be possible to set it to another value.</li>
                                        <li>Setting context mode to 'minimal' in the default config should not have any side effects on other parts of the system.</li>
                                        <li>The test should fail if context mode is set to 'minimal' by default and no explicit configuration is provided.</li>
                                        <li>If context mode defaults to minimal, then it should be possible to set it to another value without affecting the behavior of other tests.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_llm_not_enabled_by_default</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that LLM is not enabled by default in the configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where LLM is enabled by default, which could lead to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `is_llm_enabled()` method returns False when called on an empty config object.</li>
                                        <li>The `is_llm_enabled()` method should return True for the default configuration.</li>
                                        <li>The `get_default_config()` function is used to retrieve the default configuration.</li>
                                        <li>The default configuration should not have any LLM enabled settings.</li>
                                        <li>The `is_llm_enabled()` method should be able to distinguish between a non-empty config object and an empty one.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 107, 147, 224, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_omit_tests_default_true</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `TestConfigDefaults` class's `test_omit_tests_default_true` method verifies that omitting tests from coverage by default is enabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the default behavior of omitting tests from coverage was not correctly implemented.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.omit_tests_from_coverage is set to True</li>
                                        <li>assert config.omit_tests_from_coverage == True</li>
                                        <li>config.omit_tests_from_coverage should be True by default according to the TestConfigDefaults class</li>
                                        <li>The `TestConfigDefaults` class's default behavior should correctly omit tests from coverage</li>
                                        <li>The configuration of the TestConfigDefaults instance should reflect its default behavior</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_provider_default_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the default provider setting when it is set to None.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the provider is not set to 'none' when it should be.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config.provider` attribute of the test configuration object is set to 'none'.</li>
                                        <li>The `provider` attribute of the default configuration object is set to 'none'.</li>
                                        <li>The `get_default_config()` function returns a configuration object with a `provider` attribute set to 'none'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_secret_exclude_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that secret files are excluded by default from the LLM context.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where sensitive information like secret files might be inadvertently included in the LLM context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `llm_context_exclude_globs` configuration setting is set to exclude 'secret' files.</li>
                                        <li>The `llm_context_exclude_globs` configuration setting is set to exclude '.env' files.</li>
                                        <li>Any secret files found in the excluded list are not included in the LLM context.</li>
                                        <li>No sensitive information like 'secret' or '.env' is present in the excluded list.</li>
                                        <li>The test ensures that only non-sensitive files are used for the LLM context.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_deterministic_output</span>
                        <div class="test-meta">
                            <span>7ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `test_deterministic_output` test reports are deterministic (sorted by nodeid).</p>
                                <p><strong>Why Needed:</strong> This test prevents regression and ensures that the output of the pipeline is always in a predictable order.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `nodeids` list should be sorted in ascending order.</li>
                                        <li>The `nodeids` list should contain only unique values.</li>
                                        <li>All node IDs should be present in the sorted list.</li>
                                        <li>No duplicates should be found in the sorted list.</li>
                                        <li>The sorting is stable (i.e., if two nodes have the same ID, their original order is preserved).</li>
                                        <li>The sorting is consistent across different runs of the test.</li>
                                        <li>The `nodeids` list should not contain any empty strings.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">78 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">117 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_empty_test_suite</span>
                        <div class="test-meta">
                            <span>6ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that an empty test suite produces a valid report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where an empty test suite is expected to produce a valid report, but the current implementation does not handle this case correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total count of tests in the report should be zero.</li>
                                        <li>The summary section of the report should have a 'total' key with a value of zero.</li>
                                        <li>All test data loaded from the report.json file should be empty.</li>
                                        <li>The report writer does not throw an error when writing to an empty report.</li>
                                        <li>The report writer correctly handles an empty test suite by producing a valid report.</li>
                                        <li>The report summary is correct and does not contain any invalid values.</li>
                                        <li>The report data is correct and does not contain any missing or invalid values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_html_report_generation</span>
                        <div class="test-meta">
                            <span>32ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the full pipeline generates an HTML report.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential regression where the HTML report is not generated correctly or does not contain expected information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test passes and creates an HTML file at the specified path.</li>
                                        <li>The HTML file contains the string '<html>' in its content.</li>
                                        <li>The HTML file contains the string 'test_pass' in its content.</li>
                                        <li>The report is created with a valid configuration that specifies the report HTML file.</li>
                                        <li>The report HTML file exists at the expected location.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">113 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_json_report_generation</span>
                        <div class="test-meta">
                            <span>55ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that a full pipeline generates a valid JSON report with the correct schema version, summary statistics, and number of tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the integration gate where the JSON report is generated. Without this test, the pipeline may produce incorrect or incomplete reports, leading to issues downstream.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>data['schema_version'] == SCHEMA_VERSION</li>
                                        <li>data['summary']['total'] == 3</li>
                                        <li>data['summary']['passed'] == 1</li>
                                        <li>data['summary']['failed'] == 1</li>
                                        <li>data['summary']['skipped'] == 1</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/_git_info.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 2-3)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">78 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">133 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-322, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_report_root_has_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the ReportRoot object has all required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the report root is missing required fields.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'schema_version' field should be present in the data.</li>
                                        <li>The 'run_meta' field should be present in the data.</li>
                                        <li>The 'summary' field should be present in the data.</li>
                                        <li>The 'tests' field should be present in the data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_aggregation_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'RunMeta has aggregation fields' verifies that the RunMeta object has 'is_aggregated', 'run_count', and possibly other aggregation policy fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where a RunMeta object is created without any aggregation policies, potentially leading to incorrect results or errors in downstream processing.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>is_aggregated</li>
                                        <li>run_count</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_status_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'RunMeta has run status fields' verifies that the RunMeta object contains the required status fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the RunMeta object is not populated with the necessary status fields, potentially leading to incorrect analysis results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'exit_code' field should be present in the data.</li>
                                        <li>The 'interrupted' field should be present in the data.</li>
                                        <li>The 'collect_only' field should be present in the data.</li>
                                        <li>The 'collected_count' field should be present in the data.</li>
                                        <li>The 'selected_count' field should be present in the data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_schema_version_defined</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the schema version is defined and matches a semantic version.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the schema version is not defined or does not match a semantic version, potentially causing integration issues.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>SCHEMA_VERSION is set to a valid string.</li>
                                        <li>SCHEMA_VERSION contains at least one dot (.) character.</li>
                                        <li>SCHEMA_VERSION matches a semantic version format (e.g., '1.2.3'),</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_test_case_has_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `TestSchemaCompatibility` function verifies that the `TestCaseResult` object contains all required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `TestCaseResult` object is missing one or more required fields, potentially leading to incorrect results or errors during schema compatibility checks.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'nodeid' key should be present in the `data` dictionary.</li>
                                        <li>The 'outcome' key should be present in the `data` dictionary.</li>
                                        <li>The 'duration' key should be present in the `data` dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_gemini_returns_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_provider` function returns a `GeminiProvider` instance when the `provider` parameter is set to 'gemini',</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the provider returned is not correctly identified as 'GeminiProvider' due to incorrect configuration or implementation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_provider` function should return an instance of `GeminiProvider` when `provider='gemini'`</li>
                                        <li>The `provider` parameter should be set to `'gemini'` for the test to pass</li>
                                        <li>The returned provider instance should have a class name matching 'GeminiProvider'</li>
                                        <li>The `get_provider` function should correctly identify the provider as 'GeminiProvider' in this case</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 245, 247, 249, 252, 257, 262-263, 265)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_litellm_returns_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_provider` function returns an instance of LiteLLMProvider when a provider with name 'litellm' is specified.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `get_provider` function does not return an instance of LiteLLMProvider if the provided provider is not recognized ('litellm').</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` attribute of the returned `LiteLLMProvider` instance should be set to 'liteLLM'.</li>
                                        <li>The `__class__.__name__` attribute of the `LiteLLMProvider` instance should match 'LiteLLMProvider'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52-53, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_none_returns_noop</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> test_get_provider_with_none_as_provider returns NoopProvider.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the LLM is not properly initialized with a non-existent provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_provider` function should return an instance of `NoopProvider` when provided with a 'none' configuration.</li>
                                        <li>The `provider` attribute of the returned `NoopProvider` instance should be set to `'none'`.</li>
                                        <li>The `isinstance(provider, NoopProvider)` assertion should pass for the `NoopProvider` instance returned by `get_provider`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_ollama_returns_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that OllamaProvider is returned when the `provider` parameter is set to 'ollama' in the provided configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the correct provider type is not detected if the `httpx` library is missing or not properly installed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_provider()` function should return an instance of OllamaProvider.</li>
                                        <li>The `provider` parameter in the configuration object should be set to 'ollama'.</li>
                                        <li>The `__class__.__name__` attribute of the returned provider instance should match 'OllamaProvider'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 245, 247, 249, 252-253, 255)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_unknown_raises</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that an unknown provider raises a ValueError when trying to get a provider.</p>
                                <p><strong>Why Needed:</strong> This test prevents the regression of not raising a ValueError for unknown providers.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider` should raise a `ValueError` with message 'unknown' when called with an unknown provider.</li>
                                        <li>The error message should contain the string 'unknown'.</li>
                                        <li>When an unknown provider is passed to `get_provider`, it should not be able to return any value.</li>
                                        <li>The function call should fail and raise an exception instead of returning a result.</li>
                                        <li>The test should pass if the function raises a ValueError with the specified error message.</li>
                                        <li>The test should fail if the function does not raise a ValueError with the specified error message.</li>
                                        <li>The test should only pass if the `Config` class is correctly configured to handle unknown providers.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 245, 247, 249, 252, 257, 262, 267)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestLlmProviderContract::test_noop_implements_interface</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that NoopProvider implements LlmProvider interface.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the NoopProvider class does not implement all required methods of the LlmProvider contract.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider should have annotate method</li>
                                        <li>provider should have is_available method</li>
                                        <li>provider should have get_model_name method</li>
                                        <li>provider should have config attribute</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestNoopProvider::test_annotate_returns_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the annotate method of NoopProvider returns an empty LlmAnnotation object when given a TestCaseResult.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the annotate method does not return any annotation when given a TestCaseResult with no scenario or why.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>annotation is an instance of LlmAnnotation</li>
                                        <li>annotation has an empty scenario</li>
                                        <li>annotation has an empty why_needed</li>
                                        <li>annotation has an empty key_assertions</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestNoopProvider::test_get_model_name_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The NoopProvider returns an empty string when the model name is not specified.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the model name is not provided and the provider returns an empty string.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert provider.get_model_name() == ''</li>
                                        <li>assert provider.get_model_name() != 'noop' (to avoid false positives)</li>
                                        <li>assert config.model_name is None (to ensure no model name was specified)</li>
                                        <li>assert provider.get_model_name_from_config(config) == '' (to verify the default behavior)</li>
                                        <li>assert provider.get_model_name_from_str('noop') == '' (to test the edge case)</li>
                                        <li>assert provider.get_model_name_from_str('model_name') is None (to ensure no model name was provided)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 66)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestNoopProvider::test_is_available</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The NoopProvider instance should always be available.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider might not be available due to some internal issue.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider.is_available() == True</li>
                                        <li>provider._noop_provider is None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 107, 110-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 58)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_emits_summary</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that annotation summary is printed when annotations run.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where annotation summary is not printed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider` from `pytest_llm_report.llm.annotator` returns a `FakeProvider` instance.</li>
                                        <li>The `test_case` in the `TestCaseResult` object has an outcome of 'passed'.</li>
                                        <li>The captured output contains the string 'Annotated 1 test(s) via litellm'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_reports_progress</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the progress report is generated for all test cases in the annotate_tests function.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the progress report is not generated for some test cases, potentially leading to incorrect results or missed tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test case should start with a message indicating that LLM annotations are being started.</li>
                                        <li>The test case should have a unique identifier and outcome.</li>
                                        <li>The progress messages should include the provider name (in this case, 'litellm') and the test case scenario.</li>
                                        <li>Each test case should be annotated with one or more messages that describe the annotation process.</li>
                                        <li>The number of annotations for each test case should match the expected number based on the test case outcome.</li>
                                        <li>The progress report should include a message indicating whether the LLM annotation is complete or not.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_respects_opt_out_and_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests for LLM annotations respecting opt-out and limit settings.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that LLM annotations do not skip opt-out tests or exceed the maximum number of tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` function is called with a valid configuration.</li>
                                        <li>The `llm_annotation` attribute is accessed on the first test result.</li>
                                        <li>The second and third test results have their `llm_annotation` attributes set to `None`.</li>
                                        <li>The `provider.calls` list contains only one call to `get_provider` with the correct configuration.</li>
                                        <li>All three test results have valid `outcome` values (passed).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 45, 48-49, 56-57, 59, 61-62, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_respects_rate_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM annotations respect the requests-per-minute rate limit.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the annotator does not respect the rate limit and makes excessive calls to the provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The correct list of nodes should be ['tests/test_a.py::test_a', 'tests/test_b.py::test_b']</li>
                                        <li>The sleep function call should have been made at minute 2.0, not immediately.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-173, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_skips_unavailable_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that annotation with unavailable providers skips the test.</p>
                                <p><strong>Why Needed:</strong> To prevent regression when an unavailable LLM provider is used for annotation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `is_available` method of the `UnavailableProvider` class returns False.</li>
                                        <li>The `get_provider` function from `pytest_llm_report.llm.annotator` calls the `UnavailableProvider` instance with a valid configuration.</li>
                                        <li>The `llm.annotator.get_provider` call is made before checking if the provider is available.</li>
                                        <li>The annotation process skips the test when an unavailable provider is used.</li>
                                        <li>The message 'is not available' is printed to the console during the annotation process.</li>
                                        <li>The `tmp_path` and `config` variables are preserved between test runs.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_uses_cache</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests that annotations are cached between runs and that the annotation function is called when it should be.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential regression where the annotation function is not called even though the tests have passed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider.calls` assertion checks if the `get_provider` method of `pytest_llm_report.llm.annotator` was called with the correct configuration.</li>
                                        <li>The `test.llm_annotation` assertion checks if `test.llm_annotation` is not `None` after calling `annotate_tests` with the same configuration.</li>
                                        <li>The `test.llm_annotation.scenario` assertion checks if `test.llm_annotation`'s scenario matches 'cached'.</li>
                                        <li>The `provider_next.annotate` assertion checks if the annotation function was called when it should be, and raises an AssertionError otherwise.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">30 lines (ranges: 39-41, 53, 55-56, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-81, 87-92, 97-98, 100, 102, 104, 115-122, 127, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `test_required_fields` test verifies the presence of 'scenario' and 'why_needed' fields in the annotation.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the schema is not correctly defined or used, potentially leading to incorrect validation or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function checks if 'scenario' and 'why_needed' are present in the required list of annotations.</li>
                                        <li>If either 'scenario' or 'why_needed' is missing from the required list, it raises an AssertionError with a descriptive message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_from_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the AnnotationSchema.from_dict method to ensure it correctly parses a dictionary.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential bugs in the AnnotationSchema class where incorrect or missing data is passed to its methods.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation schema's scenario attribute should be set to the provided value.</li>
                                        <li>The annotation schema's why_needed attribute should match the expected value.</li>
                                        <li>The number of key assertions in the annotation schema should be equal to the specified count.</li>
                                        <li>The confidence level of the annotation schema should be within the expected range (0.95).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the AnnotationSchema class correctly handles an empty input.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the AnnotationSchema class does not validate or handle empty inputs correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The schema should be able to parse and validate an empty dictionary.</li>
                                        <li>The schema should report an error when encountering an empty input.</li>
                                        <li>The schema should not silently ignore or skip invalid inputs.</li>
                                        <li>The schema should provide a clear indication of the expected output for an empty input.</li>
                                        <li>The schema should handle nested structures correctly when dealing with empty inputs.</li>
                                        <li>The schema should validate and return the correct expected output for an empty input.</li>
                                        <li>The schema's validation logic should be robust and handle edge cases correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_partial</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the AnnotationSchema from_dict method correctly sets the scenario attribute to 'Partial only' when a partial input is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the AnnotationSchema's behavior changes unexpectedly when encountering partial inputs.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>schema.scenario should be set to 'Partial only'</li>
                                        <li>schema.why_needed should not be empty</li>
                                        <li>schema.scenario should match the expected value</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_has_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the annotation schema has required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the annotation schema is missing necessary field definitions.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert 'scenario' in ANNOTATION_JSON_SCHEMA['properties']</li>
                                        <li>assert 'why_needed' in ANNOTATION_JSON_SCHEMA['properties']</li>
                                        <li>assert 'key_assertions' in ANNOTATION_JSON_SCHEMA['properties']</li>
                                        <li>assert isinstance(ANNOTATION_JSON_SCHEMA['properties']['key_assertions'], list)</li>
                                        <li>assert len(ANNOTATION_JSON_SCHEMA['properties']['key_assertions']) > 0</li>
                                        <li>assert all(isinstance(assertion, str) for assertion in ANTONALOGY_ASSERTIONS)</li>
                                        <li>assert all(isinstance(assertion, dict) for assertion in ANNOTATION_JSON_SCHEMA['properties']['key_assertions'])</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the serialization of AnnotationSchema to dict.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the schema is not properly serialized to a dictionary.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>asserts that the 'scenario' key matches the expected value.</li>
                                        <li>asserts that the 'why_needed' key matches the expected value.</li>
                                        <li>asserts that the 'key_assertions' key exists in the data and its value is a list of strings.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 90-92, 94-96, 98)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_from_factory</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the Factory to return a NoopProvider when the provider is 'none'.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case the factory returns an incorrect provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider` should return a NoopProvider for the provider='none' configuration.</li>
                                        <li>The `isinstance(provider, NoopProvider)` assertion should pass when `provider` is indeed a NoopProvider.</li>
                                        <li>The `assert` statement with `self` as the first argument should not raise an AssertionError.</li>
                                        <li>The function `get_provider` should be able to handle different provider configurations correctly.</li>
                                        <li>The `Config` class should be able to create a valid configuration object for 'none' provider.</li>
                                        <li>The `provider` variable should hold the correct value after calling `get_provider`.</li>
                                        <li>The `assert isinstance(provider, NoopProvider)` assertion should pass without any errors.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_is_llm_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `NoopProvider` class should be a subclass of `LlmProvider`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the `NoopProvider` is not properly inherited from `LlmProvider`.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider is an instance of LlmProvider</li>
                                        <li>provider is a subclass of NoopProvider</li>
                                        <li>provider has a type hint of LlmProvider</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_returns_empty_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The NoopProvider should return an empty annotation when the test function does not modify any variables.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the NoopProvider returns incorrect annotations for tests that do not modify any variables.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert result.scenario == "" (empty string)</li>
                                        <li>assert result.why_needed == "" (empty string)</li>
                                        <li>assert result.key_assertions == [] (no key assertions performed)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_annotate_returns_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `annotate` method returns a `TestCaseResult` object with the required attributes.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the `annotate` method does not return a valid `TestCaseResult` object.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `scenario` attribute is present and has the expected value.</li>
                                        <li>The `why_needed` attribute is present and has the expected value.</li>
                                        <li>The `key_assertions` list contains all critical checks performed by the test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_handles_empty_code</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `provider` correctly handles an empty code block.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where an empty code block would cause the contract to fail.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `test_result` object is not None after calling `provider.annotate()`.</li>
                                        <li>The `nodeid` attribute of the `test_result` object matches the expected value.</li>
                                        <li>The `outcome` attribute of the `test_result` object is set to 'passed'.</li>
                                        <li>The `code` attribute of the `test_result` object contains an empty string.</li>
                                        <li>The `annotations` dictionary returned by `provider.annotate()` does not contain any errors or warnings.</li>
                                        <li>The `result` object passed to `provider.annotate()` is a valid `TestCaseResult` instance.</li>
                                        <li>The `config` object passed to `provider` has the correct `nodeid` and `outcome` values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_handles_none_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `provider` handles a `None` context gracefully by annotating a `TestCaseResult` with `code`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the annotation of a `TestCaseResult` with `code` would fail due to the absence of a value for the `code` field when provided with a `None` context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>...</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_has_annotate_method</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that all providers have an 'annotate' method.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in the contract where a provider might not have this method.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider has an attribute named 'annotate'.</li>
                                        <li>The provider is callable.</li>
                                        <li>The provider has an 'annotate' method with no arguments.</li>
                                        <li>The provider does not raise any exceptions when calling its 'annotate' method.</li>
                                        <li>The provider's 'annotate' method returns a value (e.g., string, list).</li>
                                        <li>The provider's 'annotate' method calls itself recursively without terminating.</li>
                                        <li>The provider's 'annotate' method has the correct signature (method name and parameters).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 52-53, 245, 247, 249-250, 252-253, 255, 257-258, 260, 262-263, 265)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_handles_context_too_large</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `annotate` method of the `GeminiProvider` class is called with a context that is too large, causing an error.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `annotate` method throws an exception when handling contexts larger than expected.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `context` parameter passed to `annotate` should not be longer than 1024 bytes.</li>
                                        <li>The `context` parameter passed to `annotate` should contain only strings and dictionaries.</li>
                                        <li>The `annotate` method should raise an exception when the context is too large.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52-53, 72, 75-76, 80, 165, 167, 175)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">155 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-221, 233, 245-248, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417-418, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_dependency</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The LiteLLMProvider should correctly report a missing dependency when the required package is not installed.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider incorrectly reports a missing dependency, causing users to install the wrong package instead of using LiteLLM.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>annotation.error == 'litellm not installed. Install with: pip install litellm'</li>
                                        <li>provider.annotate(...) returns an annotation object with error message</li>
                                        <li>assert True is asserted in test_case()</li>
                                        <li>test_case() contains a comment indicating that the required package is missing</li>
                                        <li>mock_import_error('litellm') is called to simulate a missing dependency</li>
                                        <li>config.provider == 'litellm' is set correctly</li>
                                        <li>LiteLLMProvider(...) is instantiated with correct configuration</li>
                                        <li>CaseResult(...) is created with correct nodeid and outcome</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 134, 136-139, 141-142, 160-164)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_token</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that annotating a missing API token prevents the error GEMINI_API_TOKEN is not set.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the LLM provider throws an error when it cannot find the required API token.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation should fail with the message 'GEMINI_API_TOKEN is not set'.</li>
                                        <li>The annotation should have the correct provider and nodeid.</li>
                                        <li>The annotation should have a valid error message.</li>
                                        <li>The annotation should be able to identify the missing API token as the cause of the failure.</li>
                                        <li>The annotation should provide a meaningful error message that indicates the root cause of the issue.</li>
                                        <li>The annotation should not throw an error when the API token is present, but the provider still requires it.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 134, 136-139, 141-142, 160-161, 167-169)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_records_tokens</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that tokens are recorded correctly on the limiter.</p>
                                <p><strong>Why Needed:</strong> Prevents regressions where token usage is not properly recorded.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'status' of the response should be ok.</li>
                                        <li>The 'totalTokenCount' in the response payload should match the expected value.</li>
                                        <li>The 'candidates' list in the response payload should contain a single dictionary with a 'text' key.</li>
                                        <li>The 'usageMetadata' dictionary in the response payload should have a 'totalTokenCount' key and its value should be 123.</li>
                                        <li>The rate limits logic should run without error.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">183 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-223, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-343, 346, 348-350, 352-355, 360-366, 368, 370-371, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_retries_on_rate_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `test_annotate_retries_on_rate_limit` test verifies that the LLM provider annotates retries on rate limits.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the LLM provider does not annotate retries on rate limits, potentially causing incorrect results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `rate_limit` attribute of the LLM provider is accessed before retrying.</li>
                                        <li>The `retry_on_rate_limit` method is called with the correct arguments (e.g., `max_retries`, `retries_delay`).</li>
                                        <li>The `annotate` method is called on the LLM provider with the correct arguments (e.g., `rate_limit`, `retry_on_rate_limit`).</li>
                                        <li>The `retry` method is called on the LLM provider with the correct arguments (e.g., `max_retries`, `retries_delay`).</li>
                                        <li>The `time.sleep` function is used to introduce a delay between retries.</li>
                                        <li>The `LLMProvider` instance has an attribute indicating that it supports rate limiting.</li>
                                        <li>The `rate_limit` and `retry_on_rate_limit` attributes are set correctly on the LLM provider instance before retrying.</li>
                                        <li>The `annotate` method returns a boolean value indicating whether the annotation was successful.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">181 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 233-234, 238-240, 242-243, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330-333, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-369, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_rotates_models_on_daily_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `annotate` method rotates models on the daily limit.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the model rotation is not applied correctly when the daily limit is exceeded.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `rotate_models_on_daily_limit` method should rotate all models to the next day if the current date exceeds the daily limit.</li>
                                        <li>Each model should have its `id` attribute updated to reflect the new date.</li>
                                        <li>The `annotate` method should update the `metadata` dictionary with the rotated model information.</li>
                                        <li>All models should be marked as 'rotated' in the `models` list.</li>
                                        <li>The `rotate_models_on_daily_limit` method should not rotate models that are already on the next day (i.e., the current date is greater than or equal to the daily limit).</li>
                                        <li>If a model is rotated, its `metadata` dictionary should contain the correct information about the rotation (e.g., the new date and any relevant metadata).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">177 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419-420, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_skips_on_daily_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the annotate method skips daily limit checks when it is called.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the annotation process might skip daily limits due to performance or resource constraints.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>monkeypatch.assert_called_once_with(monkeypatch)</li>
                                        <li>self.annotate_skips_on_daily_limit.called_once_with(self, 'daily_limit')</li>
                                        <li>self.annotate_skips_on_daily_limit.return_value == False</li>
                                        <li>self.annotate_skips_on_daily_limit.return_value is not None</li>
                                        <li>self.annotate_skips_on_daily_limit.__name__ == 'skips_on_daily_limit'</li>
                                        <li>self.annotate_skips_on_daily_limit.__doc__ == 'Skips daily limit checks when annotating'</li>
                                        <li>self.annotate_skips_on_daily_limit.__annotations__ == {'skips_on_daily_limit': bool}</li>
                                        <li>self.annotate_skips_on_daily_limit.__defaults__ == (None,)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">184 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-208, 210-211, 213-214, 217-222, 225-227, 252-254, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_success_with_mock_response</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LiteLLM provider annotates a successful response correctly with mock response.</p>
                                <p><strong>Why Needed:</strong> Prevents regressions by ensuring the correct annotation of a successful response from the LiteLLM provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation contains the correct scenario 'Checks login'.</li>
                                        <li>The annotation contains the correct why needed 'Stops regressions'.</li>
                                        <li>The annotation contains the correct key assertions ['status ok', 'redirect'].</li>
                                        <li>The annotation has a high confidence level of 0.8.</li>
                                        <li>The captured model is set to 'gpt-4o'.</li>
                                        <li>The test login function is found in the captured messages.</li>
                                        <li>The def test_login() function is also found in the captured messages.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">177 lines (ranges: 39-42, 45-46, 48-49, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_exhausted_model_recovers_after_24h</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The LLM model recovers after 24 hours of being exhausted.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the model does not recover from exhaustion and returns incorrect results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The model's output should be similar to its original value within 24 hours.</li>
                                        <li>The model's output should decrease in magnitude over time, indicating recovery.</li>
                                        <li>The model's output should remain constant or increase slowly after 24 hours.</li>
                                        <li>The model's error rate should not increase significantly over the first 24 hours.</li>
                                        <li>The model's latency should be similar to its original value within 24 hours.</li>
                                        <li>The model's memory usage should decrease over time, indicating recovery.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">190 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-188, 190-191, 193-194, 196, 200-208, 210-211, 213-214, 217-222, 225-227, 252-254, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_fetch_available_models_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `fetch_available_models` method of the `GeminiProvider` class raises an error when there are no available models.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the `fetch_available_models` method returns an error due to insufficient model availability.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assertRaisesError with type 'RuntimeWarning' or 'ValueError'</li>
                                        <li>the `GeminiProvider.fetch_available_models()` method raises an error</li>
                                        <li>the error message is not a standard warning or ValueError</li>
                                        <li>the error message includes the string 'no available models'</li>
                                        <li>the error message includes the string 'insufficient data'</li>
                                        <li>the error message does not include any specific model names</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 134, 136-139, 141-142, 280, 282-283, 286-290, 292-295, 297-298, 300-301, 346, 348-350, 352-355, 360-363, 374-377, 385, 387, 391-392, 396-402, 405, 408-410, 412-414, 417-418, 428, 430-432, 435-436)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_model_list_refreshes_after_interval</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The model list is refreshed after an interval of time.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the model list does not refresh after an interval.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `refresh_interval` attribute of the provider is set to a valid number.</li>
                                        <li>The `model_list` attribute is updated with the latest models within the specified time frame.</li>
                                        <li>The `refreshed_at` timestamp for each model in the `model_list` is accurate and up-to-date.</li>
                                        <li>No stale or outdated models are present in the `model_list` after the interval has passed.</li>
                                        <li>The provider's behavior remains consistent across different test runs.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">169 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-369, 374-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_handles_completion_error</span>
                        <div class="test-meta">
                            <span>6.00s</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the LiteLLMProvider annotates completion errors correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the LLM provider does not surface completion errors in annotations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'boom' error is present in the annotation.</li>
                                        <li>The 'boom' error is reported as an error.</li>
                                        <li>The 'boom' error is included in the annotation's error message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 78-79, 81-82, 84-85)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_invalid_key_assertions</span>
                        <div class="test-meta">
                            <span>6.00s</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LiteLLMProvider rejects invalid key_assertions payloads.</p>
                                <p><strong>Why Needed:</strong> To prevent the LiteLLMProvider from incorrectly handling invalid key_assertions payloads, which could lead to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Invalid response: key_assertions must be a list</li>
                                        <li>Key assertion error message should be informative and specific to the test case</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207-209)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69, 73, 76, 81-82, 84-85)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_missing_dependency</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The LiteLLMProvider should report an error when a missing dependency is encountered.</p>
                                <p><strong>Why Needed:</strong> This test prevents the LiteLLMProvider from silently failing when a required library is not installed, instead reporting the issue publicly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation returned by `provider.annotate(test, 'def test_case(): assert True')` contains an error message that includes the name of the missing dependency (litellm).</li>
                                        <li>The error message should include a clear indication of what was installed and how to install it.</li>
                                        <li>The error message should be concise and easy to understand for users who may not be familiar with pip or package management.</li>
                                        <li>The test should fail if the mock import error is not raised when a missing dependency is encountered.</li>
                                        <li>The test should pass if the mock import error is raised correctly, indicating that an error occurred.</li>
                                        <li>The annotation returned by `provider.annotate(test, 'def test_case(): assert True')` should include the name of the missing dependency (litellm) in its message.</li>
                                        <li>The message should not contain any misleading information about how to install the required library.</li>
                                        <li>The test should only fail if the mock import error is raised when a missing dependency is encountered, and pass otherwise.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 37-41)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_success_with_mock_response</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LiteLLMProvider annotates a successful response with the expected key assertions and confidence level.</p>
                                <p><strong>Why Needed:</strong> This test prevents regressions by ensuring that LiteLLMProvider correctly annotates responses from the liteellm module.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>status ok</li>
                                        <li>redirect</li>
                                        <li>confidence >= 0.8</li>
                                        <li>model = 'gpt-4o'</li>
                                        <li>messages[0]['role'] == 'system'</li>
                                        <li>tests/test_auth.py::test_login in messages[1]['content']</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69-70)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_is_available_with_module</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the LiteLLM provider detects installed modules correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider does not detect installed modules.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `is_available()` method of the `LiteLLMProvider` class returns True when the 'litellm' module is available in the system's modules.</li>
                                        <li>The `is_available()` method should return False if the 'litellm' module is not installed or not found in the system's modules.</li>
                                        <li>The test should fail if the 'litellm' module is not available in the system's modules, indicating a bug in the provider.</li>
                                        <li>The test should pass when the 'litellm' module is installed and present in the system's modules.</li>
                                        <li>The test should also pass when the 'litellm' module is not installed or not found in the system's modules, but the provider still detects it correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 107, 110-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 94-95, 97)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_fallbacks_on_context_length_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the functionality of annotating fallbacks when context length is an error.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where annotating fallbacks fails due to context length errors in certain scenarios.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>A fallback annotation should be added to the context when it exceeds the maximum allowed length.</li>
                                        <li>The fallback annotation should have the correct type (e.g., 'OllamaProvider') and key.</li>
                                        <li>The fallback annotation should not be removed or hidden after being annotated.</li>
                                        <li>The fallback annotation should only be added if the original annotation is missing or empty.</li>
                                        <li>The fallback annotation should not interfere with other annotations in the context.</li>
                                        <li>The fallback annotation should be preserved when switching between different LLM providers.</li>
                                        <li>The fallback annotation should be correctly propagated to downstream consumers of the annotated context.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">33 lines (ranges: 52-53, 72, 75-76, 78, 165, 167-173, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-60, 62-63, 66-67)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_handles_call_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the Ollama provider correctly annotates a call to `test_case` with an error message when it fails due to a system prompt.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in handling call errors, ensuring the annotation is accurate and informative even when the system prompt returns an unexpected value.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation should include the 'Failed after 3 retries. Last error: boom' message as expected.</li>
                                        <li>The annotation should not include any additional information other than the 'Failed after 3 retries. Last error: boom' message.</li>
                                        <li>The annotation should only include the 'Failed after 3 retries. Last error: boom' message, without any other details about the system prompt.</li>
                                        <li>The annotation should not raise an exception when it fails to annotate the call (e.g., due to a timeout or network issue).</li>
                                        <li>The annotation should be able to handle different types of system prompts (e.g., 'boom', 'error', etc.).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-59, 71-72, 74-75, 77-78)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_missing_httpx</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Ollama provider reports missing httpx dependency.</p>
                                <p><strong>Why Needed:</strong> The test prevents a bug where the Ollama provider incorrectly assumes that httpx is installed when it's not.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert annotation.error == 'httpx not installed. Install with: pip install httpx'</li>
                                        <li>assert annotation.nodeid == 'tests/test_sample.py::test_case'</li>
                                        <li>assert annotation.outcome == 'passed'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 40-44)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_success_full_flow</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the full annotation flow of Ollama provider with mocked HTTP.</p>
                                <p><strong>Why Needed:</strong> Prevents authentication bugs by ensuring correct response from API.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Check if the status code is 200 (OK) and the response contains a JSON object with expected keys</li>
                                        <li>Validate the presence of a valid token in the response</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-60, 62-63, 114, 116-123, 127-130, 132, 134-135)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_success</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the Ollama provider makes a correct API call to generate text.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of incorrect or missing timeout settings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The response from the Ollama model is 'test response'.</li>
                                        <li>The URL used for the API call is 'http://localhost:11434/api/generate'.</li>
                                        <li>The model used by the Ollama provider is 'llama3.2:1b'.</li>
                                        <li>The prompt used to generate text is 'test prompt'.</li>
                                        <li>The system prompt used to generate text is 'system prompt'.</li>
                                        <li>The stream flag for the generated text is False.</li>
                                        <li>The timeout setting for the API call is 60 seconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 114, 116-123, 127-130, 132, 134-135)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_uses_default_model</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Ollama provider uses default model when not specified.</p>
                                <p><strong>Why Needed:</strong> To prevent a regression where the Ollama provider fails to use the default model if it is not provided in the configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'model' key in the captured dictionary should be equal to 'llama3.2'.</li>
                                        <li>The 'json' value in the captured dictionary should contain a 'response' key with the value 'ok'.</li>
                                        <li>The 'model' key in the captured dictionary should not be empty.</li>
                                        <li>The 'json' value in the captured dictionary should not be None.</li>
                                        <li>The 'response' key in the captured dictionary should have a 'response' key with the value 'ok'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 114, 116-123, 127-130, 132, 134-135)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the Ollama provider returns False when the server is unavailable.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the provider incorrectly assumes the server is available even if it's not.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_check_availability()` method of the `OllamaProvider` instance should return `False` when the server is unavailable.</li>
                                        <li>The `config` object passed to the `OllamaProvider` constructor has a valid `provider` set to 'ollama'.</li>
                                        <li>When the `fake_get()` function raises a `ConnectionError`, it should be caught and propagated as an exception.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 87-88, 90-91, 93-94)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_non_200</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the Ollama provider returns False for non-200 status codes when checking availability.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the Ollama provider incorrectly reports availability for non-200 status codes, causing downstream services to rely on incorrect information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function _check_availability() returns False for any URL that has a status code other than 200.</li>
                                        <li>The function _check_availability() does not raise an exception when encountering a non-200 status code.</li>
                                        <li>The provider's method to check availability is called with the correct argument (status_code) even if it's not 200.</li>
                                        <li>The provider's method to check availability returns False for any URL that has a status code other than 200.</li>
                                        <li>The provider's method to check availability does not raise an exception when encountering a non-200 status code.</li>
                                        <li>The configuration provider is set correctly and the OllamaProvider instance is created with it.</li>
                                        <li>The config provider is set correctly and the provider instance is created with it.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 87-88, 90-92)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_success</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the Ollama provider checks availability via /api/tags endpoint successfully.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider does not return an error or message when it cannot find the tags, potentially leading to unexpected behavior in downstream applications.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The '/api/tags' URL is present in the provided URL.</li>
                                        <li>The response status code is 200 (OK).</li>
                                        <li>The 'tags' key is not present in the response. This indicates that the provider cannot find the tags.</li>
                                        <li>An error or message indicating failure to find the tags is returned by the provider.</li>
                                        <li>The provider raises an exception when it encounters a non-existent resource, such as a 404 Not Found error.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 87-88, 90-92)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_is_local_returns_true</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The Ollama provider should always return `is_local=True`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the provider might not correctly identify if it's local or not.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider.is_local() == True</li>
                                        <li>provider.is_local() != False</li>
                                        <li>provider.is_local() is not None</li>
                                        <li>config.provider == 'ollama'</li>
                                        <li>OllamaProvider(config).is_local() is True</li>
                                        <li>OllamaProvider(config) is not None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_json</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `OllamaProvider` class throws an error when attempting to parse a non-JSON response.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the Ollama provider incorrectly interprets responses without proper validation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>If the provided response is not JSON, the `_parse_response` method of `OllamaProvider` should throw an exception with the message 'Failed to parse LLM response as JSON'.</li>
                                        <li>The error message should include the string 'Failed to parse LLM response as JSON'.</li>
                                        <li>The test should fail if the provided response is not a valid JSON string.</li>
                                        <li>If the response contains any non-JSON characters, it should be considered invalid and trigger the exception.</li>
                                        <li>The `Config` class should be able to validate the input response against the expected format.</li>
                                        <li>The `_parse_response` method of `OllamaProvider` should raise an exception with the specified error message when encountering a non-JSON response.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 52-53, 186-187, 190-192)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-52, 55)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_key_assertions</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Ollama provider rejects invalid key_assertions payloads when parsing responses.</p>
                                <p><strong>Why Needed:</strong> This test prevents the Ollama provider from incorrectly handling invalid key_assertions in its responses.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The response must be a list</li>
                                        <li>Key assertions should not contain any keys</li>
                                        <li>Invalid or missing keys are not allowed</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207-209)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The provided test verifies that the Ollama provider correctly parses a JSON response from a markdown code fence.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential bugs where the provider incorrectly or incompletely extracts JSON from markdown code fences, potentially leading to incorrect or incomplete model training data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The extracted JSON is in the correct format and does not contain any invalid characters.</li>
                                        <li>The extracted JSON contains only valid JSON syntax and does not include any extraneous whitespace or formatting.</li>
                                        <li>The extracted JSON does not contain any malicious or unexpected data, such as unquoted strings or arrays with non-string values.</li>
                                        <li>The extracted JSON is a single object or array, rather than multiple objects or arrays.</li>
                                        <li>The extracted JSON contains only one top-level key-value pair, rather than multiple pairs.</li>
                                        <li>The extracted JSON does not contain any nested objects or arrays that exceed the maximum allowed depth.</li>
                                        <li>The extracted JSON does not contain any circular references or other self-referential data structures.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 38, 42-44, 46-47)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_plain_fence</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The provided test verifies that the Ollama provider correctly extracts JSON from a plain markdown fence without any language specification.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the case where the input JSON is not properly formatted or does not contain any valid keys.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The response should be a JSON object with no properties (i.e., an empty dictionary).</li>
                                        <li>The response should only contain string values (e.g., 'hello').</li>
                                        <li>There should be no nested objects or arrays in the response.</li>
                                        <li>All string values should have a length of 1 character.</li>
                                        <li>No keys should be present in the JSON object.</li>
                                        <li>The JSON object should not have any circular references.</li>
                                        <li>The response should only contain strings and numbers, without any other types (e.g., booleans).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 38, 42-44, 46-47)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_success</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the Ollama provider's ability to parse valid JSON responses.</p>
                                <p><strong>Why Needed:</strong> To prevent bugs in the Ollama provider that may occur when parsing invalid or malformed JSON responses.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert a is not None</li>
                                        <li>assert b is not None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestArtifactEntry::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `CoverageEntry.to_dict()` correctly serializes the object.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in coverage entry serialization.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key is set to the expected value.</li>
                                        <li>The 'line_ranges' key is set to the expected value.</li>
                                        <li>The 'line_count' key is set to the expected value.</li>
                                        <li>The 'coverage_data' object is not created with an empty dictionary.</li>
                                        <li>The 'coverage_data' object has a non-empty dictionary for 'file_path'.</li>
                                        <li>The 'coverage_data' object has a non-empty dictionary for 'line_ranges'.</li>
                                        <li>The 'coverage_data' object has a non-empty dictionary for 'line_count'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 254-257)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestCollectionError::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `to_dict` method of `CoverageEntry` class.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `to_dict` method does not correctly serialize the coverage entry data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The expected file path is 'src/foo.py'.</li>
                                        <li>The expected line ranges are '1-3, 5, 10-15'.</li>
                                        <li>The expected line count is 10.</li>
                                        <li>The `to_dict` method should return a dictionary with the specified keys and values.</li>
                                        <li>The `to_dict` method should not raise an exception when the coverage entry data is invalid or missing required information.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 207-209)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestCoverageEntry::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests CoverageEntry to_dict method.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the coverage entry serialization is incorrect.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key should be set to the actual file path.</li>
                                        <li>The 'line_ranges' key should contain valid range notation (e.g., '1-3, 5, 10-15').</li>
                                        <li>The 'line_count' key should match the expected value of 10.</li>
                                        <li>Any additional line ranges should be properly formatted and not exceed the maximum allowed length.</li>
                                        <li>If an invalid file path is provided, the test should fail with a clear error message.</li>
                                        <li>If any other unexpected issue occurs during serialization, the test should also fail with a meaningful error message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 40-43)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_empty_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> An empty annotation should be created with default values.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where an empty annotation does not have default values.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>annotation.scenario = ''</li>
                                        <li>annotation.why_needed = ''</li>
                                        <li>annotation.key_assertions = []</li>
                                        <li>assert annotation.confidence is None</li>
                                        <li>assert annotation.error is None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_to_dict_minimal</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `LlmAnnotation` object can be serialized to a dictionary with the required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the minimal annotation format includes all necessary keys for serialization.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'scenario' key should be present in the dictionary.</li>
                                        <li>The 'why_needed' key should be present in the dictionary.</li>
                                        <li>The 'key_assertions' key should be present in the dictionary.</li>
                                        <li>The 'confidence' key should not be present in the dictionary when it is None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 104-107, 109, 111, 113, 115)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_to_dict_with_all_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test to dictionary with all fields</p>
                                <p><strong>Why Needed:</strong> Prevents auth bypass by ensuring a full annotation is created.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Assert that the 'scenario' field matches the expected value.</li>
                                        <li>Assert that the 'confidence' field matches the expected value (0.95).</li>
                                        <li>Assert that the 'context_summary' field contains the expected values for mode and bytes.</li>
                                        <li>Verify that the dictionary is created with all required fields.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 104-107, 109-111, 113-115)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_default_report</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test default report schema version and empty lists.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the default report is missing required information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'schema_version' key should be present in the report dictionary with value equal to SCHEMA_VERSION.</li>
                                        <li>The 'tests' key should be an empty list.</li>
                                        <li>The 'warnings' key should not be included in the report dictionary.</li>
                                        <li>The 'collection_errors' key should not be included in the report dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_report_with_collection_errors</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Report with Collection Errors should include them.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the report does not include collection errors when they are present.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of `collection_errors` in the report is 1.</li>
                                        <li>The value of `nodeid` in the first element of `collection_errors` is 'test_bad.py'.</li>
                                        <li>Each error in `collection_errors` has a valid `nodeid` and `message`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">58 lines (ranges: 207-209, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508-510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_report_with_warnings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the ReportRoot class correctly handles warnings in a report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where reports with warnings are not properly handled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `ReportWarning` object is created with the correct warning code and message.</li>
                                        <li>The length of the `warnings` list in the report dictionary is 1.</li>
                                        <li>The first warning in the `warnings` list has the correct warning code.</li>
                                        <li>The warning code 'W001' is present in the first warning.</li>
                                        <li>The warning message 'No coverage' is present in the first warning.</li>
                                        <li>The `ReportWarning` object has a non-empty `code` attribute.</li>
                                        <li>The `ReportWarning` object has a non-empty `message` attribute.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">60 lines (ranges: 229-231, 233, 235, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_tests_sorted_by_nodeid</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests should be sorted by nodeid in output.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the order of tests is not guaranteed to match their original nodeid.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The list of nodeids returned matches the expected order.</li>
                                        <li>Each nodeid appears only once in the list.</li>
                                        <li>No duplicate nodeids are present in the list.</li>
                                        <li>All nodeids are present in the list.</li>
                                        <li>Nodeids without tests are not included in the list.</li>
                                        <li>Nodeids with multiple tests are sorted correctly.</li>
                                        <li>The test order is preserved across different runs of the same test suite.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">71 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportWarning::test_to_dict_with_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test `test_to_dict_with_detail` verifies that the `to_dict()` method of `ReportWarning` returns a dictionary with the 'detail' key.</p>
                                <p><strong>Why Needed:</strong> This test prevents a warning about missing coverage details in reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'detail' key should be present in the returned dictionary.</li>
                                        <li>The value of the 'detail' key should match the provided path '/path/to/file'.</li>
                                        <li>The 'message' and 'code' keys are not included in the returned dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 229-231, 233-235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportWarning::test_to_dict_without_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test to dictionary without detail should exclude it.</p>
                                <p><strong>Why Needed:</strong> This test prevents a warning that occurs when the 'to_dict' method is called on a ReportWarning object without providing any additional details.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'code' key in the dictionary should be equal to 'W001'.</li>
                                        <li>The 'message' key in the dictionary should be equal to 'No coverage'.</li>
                                        <li>The 'detail' key in the dictionary should not exist.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 229-231, 233, 235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_aggregation_fields_present</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that RunMeta has aggregation fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the aggregation policy is not correctly applied to multiple runs.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert d['run_id'] == 'run-123'</li>
                                        <li>assert d['run_group_id'] == 'group-456'</li>
                                        <li>assert d['is_aggregated'] is True</li>
                                        <li>assert d['aggregation_policy'] == 'merge'</li>
                                        <li>assert d['run_count'] == 3</li>
                                        <li>assert len(d['source_reports']) == 2</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 277-279, 281-283, 364-380, 382, 385, 387, 390, 393, 395, 397, 399-405, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_llm_fields_excluded_when_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test LLM fields are excluded when annotations not enabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the LLM fields (llm_annotations_enabled, llm_provider, and llm_model) are included in the metadata even when annotations are disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm_annotations_enabled' key is present in the data.</li>
                                        <li>The 'llm_provider' key is not present in the data.</li>
                                        <li>The 'llm_model' key is not present in the data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_llm_traceability_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test LLM traceability fields are included when enabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the case where llm_traceability_fields is disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of llm_annotations_enabled should be True.</li>
                                        <li>llm_provider should be set to 'ollama'.</li>
                                        <li>llm_model should be set to 'llama3.2:1b'.</li>
                                        <li>llm_context_mode should be set to 'complete'.</li>
                                        <li>llm_annotations_count should be 10.</li>
                                        <li>llm_annotations_errors should be 2.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407-419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_non_aggregated_excludes_source_reports</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that non-aggregated reports do not include source_reports.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where non-aggregated reports are incorrectly including source_reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'source_reports' key should be absent from the report dictionary.</li>
                                        <li>The 'is_aggregated' value should be set to False for non-aggregated reports.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_run_meta_to_dict_full</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test RunMeta to dict with all optional fields.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of missing plugin or repository git SHA.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert data['git_sha'] == 'abc1234',</li>
                                        <li>assert data['git_dirty'] is True,</li>
                                        <li>assert data['repo_version'] == '1.0.0',</li>
                                        <li>assert data['repo_git_sha'] == 'abc1234',</li>
                                        <li>assert data['repo_git_dirty'] is True,</li>
                                        <li>assert data['plugin_git_sha'] == 'def5678',</li>
                                        <li>assert data['plugin_git_dirty'] is False,</li>
                                        <li>assert len(data['source_reports']) == 1</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">49 lines (ranges: 277-279, 281-283, 364-380, 382-405, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_run_status_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the 'RunMeta' class to ensure it includes required run status fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the 'RunMeta' object is missing certain critical information about its execution status.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'exit_code' field of the 'RunMeta' object should be equal to 1.</li>
                                        <li>The 'interrupted' field of the 'RunMeta' object should be True.</li>
                                        <li>The 'collect_only' field of the 'RunMeta' object should be True.</li>
                                        <li>The 'collected_count' field of the 'RunMeta' object should be equal to 10.</li>
                                        <li>The 'selected_count' field of the 'RunMeta' object should be equal to 8.</li>
                                        <li>The 'deselected_count' field of the 'RunMeta' object should be equal to 2.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSchemaVersion::test_schema_version_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies the schema version is formatted as a semver string.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the schema version is not in semver format.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The schema version should be split into three parts (e.g., '1.2.3')</li>
                                        <li>Each part of the schema version should be a digit</li>
                                        <li>The length of each part should be exactly 3 characters</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSchemaVersion::test_schema_version_in_report_root</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `ReportRoot` class includes the schema version in its report root.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the schema version is not included in the report root.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `schema_version` attribute of `ReportRoot` should be equal to `SCHEMA_VERSION`.</li>
                                        <li>The value of `schema_version` in the JSON representation of `ReportRoot` should also be equal to `SCHEMA_VERSION`.</li>
                                        <li>The `to_dict()` method of `ReportRoot` returns a dictionary with a key-value pair where the key is `'schema_version'` and the value is `SCHEMA_VERSION`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSourceCoverageEntry::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test coverage entry serialization.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in coverage reporting when file paths or line ranges change.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key is correctly set to the expected value.</li>
                                        <li>The 'line_ranges' key is correctly set to the expected format.</li>
                                        <li>The 'line_count' key is correctly set to the expected value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 71-78)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSourceReport::test_to_dict_minimal</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `to_dict` method of `LlmAnnotation` returns a dictionary with required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the minimal annotation is missing some required fields.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The presence of 'scenario' in the dictionary is expected.</li>
                                        <li>The presence of 'why_needed' in the dictionary is expected.</li>
                                        <li>The absence of 'confidence' in the dictionary is expected (since it's optional and None by default).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 277-279, 281, 283)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSourceReport::test_to_dict_with_run_id</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'SourceReport with run_id should include it' verifies that the SourceReport object includes its 'run_id' attribute.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the 'run_id' is not included in the dictionary representation of the SourceReport object, potentially causing issues when serializing or comparing the object.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'run_id' key should be present in the dictionary representation of the SourceReport object.</li>
                                        <li>The value of the 'run_id' key should match the expected string value.</li>
                                        <li>The 'run_id' key should not be missing from the dictionary representation of the SourceReport object.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 277-279, 281-283)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSummary::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `CoverageEntry` class correctly serializes a coverage entry into a dictionary.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the serialization of coverage entries is incorrect, potentially leading to unexpected behavior or errors in downstream applications.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert d['file_path'] == 'src/foo.py'</li>
                                        <li>assert d['line_ranges'] == '1-3, 5, 10-15'</li>
                                        <li>assert d['line_count'] == 10</li>
                                        <li>The `CoverageEntry` class correctly handles line ranges with multiple occurrences.</li>
                                        <li>The `CoverageEntry` class correctly handles overlapping line ranges.</li>
                                        <li>The `CoverageEntry` class correctly handles missing line ranges.</li>
                                        <li>The `CoverageEntry` class correctly handles invalid line range formats (e.g., non-integer values).</li>
                                        <li>The `to_dict()` method returns a dictionary with the correct keys and values for a coverage entry.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 449-457, 459, 461)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_minimal_result</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_models.py::TestTestCaseResult::test_minimal_result</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the minimal result is missing required fields.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'nodeid' field should be present and match the expected value.</li>
                                        <li>The 'outcome' field should be present and match the expected value.</li>
                                        <li>The 'duration' field should be present and have a value of 0.0.</li>
                                        <li>The 'phase' field should be present and match the expected value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the `result` dictionary includes a single 'coverage' key with a list of coverage entries.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the coverage information is missing or incorrect.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'coverage' key should be present in the 'result' dictionary and contain a list of coverage entries.</li>
                                        <li>Each coverage entry should have a 'file_path' attribute set to the expected file path.</li>
                                        <li>Each coverage entry should have a 'line_ranges' attribute set to the expected line ranges (1-5).</li>
                                        <li>Each coverage entry should have a 'line_count' attribute set to the expected line count (5).</li>
                                        <li>The list of coverage entries should contain exactly one element.</li>
                                        <li>All file paths in the list should match the expected file path ('src/foo.py').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 40-43, 161-165, 167, 169, 171, 173, 176-178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_llm_opt_out</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the `TestCaseResult` object includes a flag indicating LLM opt-out.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the `TestCaseResult` object accurately reflects the presence of LLM opt-out.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `llm_opt_out` in the `d` dictionary is set to `True`.</li>
                                        <li>The `llm_opt_out` key exists in the `d` dictionary.</li>
                                        <li>The value of `llm_opt_out` is a boolean value (`True` or `False`).</li>
                                        <li>The presence of LLM opt-out is verified through the `to_dict()` method.</li>
                                        <li>The `TestCaseResult` object is converted to a dictionary before assertions are made.</li>
                                        <li>The dictionary contains all required keys (nodeid, outcome, and llm_opt_out).</li>
                                        <li>Assertions about the value of `llm_opt_out` are performed on the resulting dictionary.</li>
                                        <li>The test ensures that the `TestCaseResult` object accurately represents LLM opt-out.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180-182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_rerun</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_result_with_rerun' verifies that the rerun fields are included in the TestCaseResult.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where a result is not properly updated with rerun information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `rerun_count` should be equal to 2.</li>
                                        <li>The value of `final_outcome` should be 'passed'.</li>
                                        <li>The `result` object should have the `to_dict()` method called on it.</li>
                                        <li>The `rerun_count` and `final_outcome` keys should exist in the `d` dictionary.</li>
                                        <li>The `rerun_count` key should contain a value of 2.</li>
                                        <li>The `final_outcome` key should contain a value of 'passed'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 161-165, 167, 169, 171, 173-176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_without_rerun_excludes_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case 'test_result_without_rerun_excludes_fields' verifies that the 'result' dictionary does not contain 'rerun_count' and 'final_outcome' keys.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the 'rerun_count' and 'final_outcome' fields are included in the result dictionary when rerunning tests without re-running the same code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'result' dictionary should not contain 'rerun_count' key.</li>
                                        <li>The 'result' dictionary should not contain 'final_outcome' key.</li>
                                        <li>The 'result' dictionary should exclude 'rerun_count' and 'final_outcome' keys when passed without rerunning the test.</li>
                                        <li>The 'result' dictionary should be consistent with the expected output after excluding 'rerun_count' and 'final_outcome' fields.</li>
                                        <li>The 'result' dictionary should not contain any other fields that are specific to reruns or final outcomes.</li>
                                        <li>The 'result' dictionary should only contain keys relevant to the test case's functionality.</li>
                                        <li>The 'result' dictionary should be consistent with the expected output after excluding certain fields.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_default_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that default values are set correctly for the TestConfig class.</p>
                                <p><strong>Why Needed:</strong> Prevents potential configuration errors and inconsistencies when creating a new TestConfig instance.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider should be set to 'none'.</li>
                                        <li>The llm_context_mode should be set to 'minimal'.</li>
                                        <li>The llm_max_tests should be set to 0.</li>
                                        <li>The llm_max_retries should be set to 3.</li>
                                        <li>The llm_context_bytes should be set to 32000.</li>
                                        <li>The llm_context_file_limit should be set to 10.</li>
                                        <li>The llm_requests_per_minute should be set to 5.</li>
                                        <li>The llm_timeout_seconds should be set to 30.</li>
                                        <li>The include_phase should be set to 'run'.</li>
                                        <li>The aggregate_policy should be set to 'latest'.</li>
                                        <li>The is_llm_enabled() method should return False.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_get_default_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `get_default_config` function returns a valid default configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the default provider is not correctly set to 'none'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>cfg is an instance of `Config`.</li>
                                        <li>cfg.provider == "none".</li>
                                        <li>cfg.provider should be set to 'none' by the factory function.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_is_llm_enabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `is_llm_enabled` check for different providers.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of provider changes or updates to the LLM provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return False when the provider is set to 'none'.</li>
                                        <li>The function should return True when the provider is set to 'ollama'.</li>
                                        <li>The function should not return a value when the provider is set to an unknown or invalid configuration.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_aggregate_policy</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-197, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_context_mode</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-189, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_include_phase</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-205, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validates configuration with an invalid provider.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an invalid provider being used.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test verifies that there is exactly one error message related to an invalid provider.</li>
                                        <li>The test asserts that the error message contains the string 'Invalid provider '</li>
                                        <li>The test checks for the presence of the specific error message in the list of errors.</li>
                                        <li>The test verifies that the error message is not empty or null.</li>
                                        <li>The test ensures that the error message does not contain any other invalid providers.</li>
                                        <li>The test validates that the configuration is still valid after encountering an invalid provider.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_numeric_ranges</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validation of numeric constraints for Config object.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the configuration is not validated correctly, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm_context_bytes' value must be at least 1000.</li>
                                        <li>The 'llm_max_tests' value must be 0 (no limit) or positive.</li>
                                        <li>The 'llm_requests_per_minute' value must be at least 1.</li>
                                        <li>The 'llm_timeout_seconds' value must be at least 1.</li>
                                        <li>The 'llm_max_retries' value must be 0 or positive.</li>
                                        <li>All numeric constraints (context_bytes, max_tests, requests_per_minute, timeout_seconds, max_retries) must be validated correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209-218, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_valid_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Valid configuration is validated without any errors.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential bugs that could occur when validating an invalid configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>A valid configuration object is created and assigned to the cfg variable.</li>
                                        <li>The validate() method of the Config class returns an empty list (i.e., no errors).</li>
                                        <li>No exceptions are raised during the validation process.</li>
                                        <li>The cfg object has no attributes that would cause a validation error.</li>
                                        <li>All required fields in the configuration are present and have valid values.</li>
                                        <li>The configuration is not empty or None.</li>
                                        <li>The configuration does not contain any invalid data (e.g., missing, malformed, etc.)</li>
                                        <li>The configuration conforms to all expected schema definitions.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_aggregation_options</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loads aggregation options with default values.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the aggregation policy is not set correctly when loading configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The aggregate_dir attribute of the config object should be equal to 'aggr_dir'.</li>
                                        <li>The aggregate_policy attribute of the config object should be equal to 'merge'.</li>
                                        <li>The aggregate_run_id attribute of the config object should be equal to 'run-123'.</li>
                                        <li>The aggregate_group_id attribute of the config object should be equal to 'group-abc'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286-294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_config_invalid_int_ini</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Load Config: Handling Invalid Integer Values in INI</p>
                                <p><strong>Why Needed:</strong> Prevents test from crashing due to invalid integer values in INI files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `load_config` should not crash when encountering an invalid integer value in the INI file.</li>
                                        <li>The default value for `llm_max_retries` should be used instead of a garbage value.</li>
                                        <li>The test should pass even if the `getini` method raises an exception.</li>
                                        <li>The test should verify that the fallback value is correct (3 in this case).</li>
                                        <li>The function `load_config` should not raise an exception when encountering invalid integer values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263-267, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_coverage_source</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `llm_coverage_source` option is set to 'cov_dir' when loading coverage source.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the coverage source is not correctly loaded due to an incorrect value being passed to the `load_config` function.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `llm_coverage_source` option should be set to 'cov_dir' when loading the coverage source.</li>
                                        <li>The `load_config` function should be called with the correct `llm_coverage_source` option value.</li>
                                        <li>The `cfg.llm_coverage_source` attribute should have a value of 'cov_dir'.</li>
                                        <li>The `mock_pytest_config.option.llm_coverage_source` attribute should return 'cov_dir' when accessed.</li>
                                        <li>The `load_config` function should correctly load the coverage source from the provided directory.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_defaults</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the default provider and report HTML are correctly loaded when no options are provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the default provider ('none') or report HTML ('None') are not loaded correctly without any configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>cfg.provider == 'none'</li>
                                        <li>cfg.report_html is None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">24 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_cli_overrides_ini</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that CLI options override ini options.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where CLI overrides ini settings and report HTML is not generated correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>ini_value should be set to 'cli_report.html' for llm_report_html option</li>
                                        <li>llm_requests_per_minute should be set to 100 for llm_requests_per_minute option</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">27 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259-261, 263, 270-272, 274, 276, 278, 280-282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_cli_retries</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `llm_max_retries` option is correctly set to 9 when loading from CLI.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the `llm_max_retries` option is not properly set for retries in the command-line interface.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `llm_max_retries` should be equal to 9.</li>
                                        <li>The `llm_max_retries` option should be correctly set even if it's overridden by a CLI argument.</li>
                                        <li>The test should fail when `llm_max_retries` is not set or is less than 1.</li>
                                        <li>The value of `llm_max_retries` should remain unchanged after the test is run.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282-283, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_ini</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loading values from ini options for load_config function.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the correct values are not loaded from ini files due to incorrect configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'provider' key in the config dictionary should be set to 'ollama'.</li>
                                        <li>The 'model' key in the config dictionary should be set to 'llama3'.</li>
                                        <li>The 'context_mode' key in the config dictionary should be set to 'balanced'.</li>
                                        <li>The 'requests_per_minute' key in the config dictionary should be set to 10.</li>
                                        <li>The 'max_retries' key in the config dictionary should be set to 5.</li>
                                        <li>The 'html' key in the config dictionary should be set to 'report.html'.</li>
                                        <li>The 'json' key in the config dictionary should be set to 'report.json'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">32 lines (ranges: 107, 147, 248, 251-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_aggregation_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the aggregation settings configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the aggregation policy is set to 'merge' without specifying an aggregate group or include history.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregate_dir` attribute of the test configuration should be equal to '/reports'.</li>
                                        <li>The `aggregate_policy` attribute of the test configuration should be equal to 'merge'.</li>
                                        <li>The `aggregate_include_history` attribute of the test configuration should be set to True.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_all_output_paths</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Config with all output paths.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of multiple report formats being used.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report_html` attribute is set to 'report.html'.</li>
                                        <li>The `report_json` attribute is set to 'report.json'.</li>
                                        <li>The `report_pdf` attribute is set to 'report.pdf'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_capture_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `capture_failed_output` option is set to `True`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the captured output exceeds the maximum allowed characters (8000).</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.capture_failed_output is True</li>
                                        <li>config.capture_output_max_chars is 8000</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_compliance_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `metadata_file` and `hmac_key_file` are set correctly in the test configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the compliance settings are not properly configured, potentially leading to errors or unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `metadata_file` is set to 'metadata.json'.</li>
                                        <li>The `hmac_key_file` is set to 'key.txt'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_coverage_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the configuration of coverage settings.</p>
                                <p><strong>Why Needed:</strong> Prevents a bug where coverage settings are not applied to all tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.omit_tests_from_coverage is set to False</li>
                                        <li>config.include_phase is set to "all"</li>
                                        <li>asserts that omit_tests_from_coverage is False</li>
                                        <li>asserts that include_phase is set to "all"</li>
                                        <li>asserts that config.include_phase matches the expected value</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_custom_exclude_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `llm_context_exclude_globs` attribute is populated with custom exclude globs.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `llm_context_exclude_globs` attribute is not correctly set for custom exclude globs.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `*.pyc` glob should be included in the `llm_context_exclude_globs` list.</li>
                                        <li>The `*.log` glob should be included in the `llm_context_exclude_globs` list.</li>
                                        <li>The `*.class` glob (not shown) should also be included in the `llm_context_exclude_globs` list if it is a custom exclude glob.</li>
                                        <li>If no custom exclude globs are provided, the `llm_context_exclude_globs` attribute should still contain the default exclude globs (`*.pyc`, `*.log`), which are not included in this test.</li>
                                        <li>The `exclude_globs` parameter of the `Config` class should be able to accept a list of custom glob patterns.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_include_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `include_globs` configuration option includes only Python files.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `include_globs` option is not properly configured, potentially leading to incorrect or incomplete model training.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `*.py` glob matches any file with a `.py` extension.</li>
                                        <li>The `*.pyi` glob matches any file with an `.pyi` extension.</li>
                                        <li>The `include_globs` configuration option is applied to the LLM context.</li>
                                        <li>The `llm_context_include_globs` attribute contains the specified globs.</li>
                                        <li>The `include_globs` value does not include any files that do not have a `.py` or `.pyi` extension.</li>
                                        <li>The `include_globs` configuration option is applied to the LLM context with the correct glob patterns.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_invocation_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that `include_pytest_invocation` is set to `False` for the specified configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where `include_pytest_invocation` is incorrectly set to `True` by an external factor, causing unexpected behavior in tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `include_pytest_invocation` attribute of the `Config` object is set to `False`.</li>
                                        <li>The `include_pytest_invocation` attribute of the `Config` object does not match its expected value (`False`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_execution_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the LLM execution settings are correctly configured.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the maximum tests, concurrency, requests per minute, and timeout seconds are not properly set for the LLM.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of llm_max_tests is indeed 50.</li>
                                        <li>The value of llm_max_concurrency is indeed 8.</li>
                                        <li>The value of llm_requests_per_minute is indeed 12.</li>
                                        <li>The maximum tests configured in the test are not exceeded.</li>
                                        <li>The concurrency limit is respected for each test.</li>
                                        <li>The requests per minute threshold is respected within the allowed range.</li>
                                        <li>The timeout seconds are respected within the allowed range.</li>
                                        <li>A cache directory of the correct path (.cache) is used.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_param_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the configuration of LLM parameter settings.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the maximum character limit for LLM parameters is not respected.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.llm_include_param_values should be set to True</li>
                                        <li>config.llm_param_value_max_chars should equal 200</li>
                                        <li>config.llm_include_param_values should be a boolean value</li>
                                        <li>config.llm_param_value_max_chars should be an integer value</li>
                                        <li>config.llm_param_value_max_chars should not exceed 200</li>
                                        <li>config.llm_include_param_values should only include parameter values</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the configuration of LLM settings with OLLAMA provider.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the LLM context bytes are not set correctly for the OLLAMA provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `config.llm_context_bytes` is equal to 64000 (the expected default value).</li>
                                        <li>The value of `config.provider` is equal to 'ollama' (the expected default provider).</li>
                                        <li>The value of `config.model` is equal to 'llama3.2' (the expected default model).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_repo_root_path</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `repo_root` attribute is correctly set to `/project` when a `Config` object is created with this configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `repo_root` attribute is not set correctly, potentially leading to incorrect repository root detection.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `repo_root` attribute of the `Config` object should be equal to `/project`.</li>
                                        <li>The `repo_root` attribute of the `Config` object should be a `Path` object representing the absolute path `/project`.</li>
                                        <li>The `repo_root` attribute of the `Config` object should not be `None` or an empty string.</li>
                                        <li>The `repo_root` attribute of the `Config` object should be a valid directory path.</li>
                                        <li>The `repo_root` attribute of the `Config` object should not be a relative path.</li>
                                        <li>The `repo_root` attribute of the `Config` object should not be set to an absolute path that is already in use by another configuration.</li>
                                        <li>The `repo_root` attribute of the `Config` object should be correctly set when using a `Path` object as the repository root.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_valid_phase_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that all valid include_phase values pass validation.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where invalid or missing include_phase values cause the validation to fail.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The validate() method of the Config object should not return any errors for include_phase values set to 'run', 'setup', and 'teardown'.</li>
                                        <li>The validate() method of the Config object should return an empty list for include_phase values set to 'all'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_exclude_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the default exclude globs for llm context.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where default exclude globs are not correctly set.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `defaults` should contain the glob pattern `*.pyc`.</li>
                                        <li>The function `defaults` should contain the glob pattern `__pycache__/*`.</li>
                                        <li>The function `defaults` should contain the glob pattern `*secret*`.</li>
                                        <li>The function `defaults` should contain the glob pattern `*password*`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_redact_patterns</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test default redact patterns in Config DefaultsMaximal test.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential security vulnerability by ensuring that sensitive information is not exposed when defaulting to minimal configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `--password` pattern should be found in the default redact patterns.</li>
                                        <li>The `--token` pattern should be found in the default redact patterns.</li>
                                        <li>The `--api[_-]?key` pattern should be found in the default redact patterns.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test default values of the configuration.</p>
                                <p><strong>Why Needed:</strong> To ensure that the default values are correct and do not contain any errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider should be set to 'none'.</li>
                                        <li>The llm_context_mode should be set to 'minimal'.</li>
                                        <li>The llm_context_bytes should be set to 32000.</li>
                                        <li>The omit_tests_from_coverage flag should be True.</li>
                                        <li>The include_phase flag should be set to 'run'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigHelpersMaximal::test_is_llm_enabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `is_llm_enabled` method returns the correct enabled status for different providers.</p>
                                <p><strong>Why Needed:</strong> Prevents a bug where the test fails when using 'ollama' provider due to incorrect implementation of `is_llm_enabled` method.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `is_llm_enabled` method should return False for the 'none' provider.</li>
                                        <li>The `is_llm_enabled` method should return True for the 'ollama', 'litellm', and 'gemini' providers.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_aggregate_policy</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the validation of an invalid aggregate policy.</p>
                                <p><strong>Why Needed:</strong> To prevent a potential bug where an invalid aggregate policy is used, causing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `Config` should be instantiated with a valid aggregate policy.</li>
                                        <li>The error message for an invalid aggregate policy should include the specific policy being used.</li>
                                        <li>At least one error should be returned when validating an invalid aggregate policy.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-197, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_context_mode</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-189, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_include_phase</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-205, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validates an invalid provider.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails with an error message indicating an invalid provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `Config` is called with an invalid provider.</li>
                                        <li>An error message is returned when the provider is invalid.</li>
                                        <li>The error message contains the string 'Invalid provider'.</li>
                                        <li>The number of errors returned is correct (1).</li>
                                        <li>The first error message contains the string 'invalid'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_numeric_bounds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_numeric_bounds</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the llm_context_bytes value is set to a non-integer or negative number.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm_context_bytes' field should be an integer and not exceed 2^31-1.</li>
                                        <li>The 'llm_max_tests' field should be an integer greater than 0.</li>
                                        <li>The 'llm_requests_per_minute' field should be an integer greater than 0.</li>
                                        <li>The 'llm_timeout_seconds' field should be an integer greater than 0.</li>
                                        <li>All fields should not exceed their maximum allowed values (2^31-1, 10^9, etc.).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209-217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_valid_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `validate` method returns an empty list for a valid configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents potential infinite recursion in case of invalid configurations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `validate` method should return an empty list when given a valid configuration.</li>
                                        <li>No exceptions should be raised if the input is valid.</li>
                                        <li>The method should not throw any errors or warnings for well-formed configurations.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginConfigLoading::test_config_defaults</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `load_config` function returns a valid configuration object with default settings.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the plugin's configuration is missing or has unexpected defaults if no options are provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `cfg` variable should be an instance of `Config`.</li>
                                        <li>The `cfg` variable should have some default settings (e.g., `base_dir`, `output_dir`).</li>
                                        <li>If `pytestconfig` is not a valid pytest configuration, the test will fail with a meaningful error message.</li>
                                        <li>The `cfg` object's attributes (`base_dir`, `output_dir`) should be set to their default values.</li>
                                        <li>The `cfg` object's attributes (`log_level`, `verbose`, etc.) should have some default values (e.g., `INFO`, `DEBUG`).</li>
                                        <li>If the plugin has custom settings, they should override the defaults if provided in `pytestconfig`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">33 lines (ranges: 107, 147, 248, 251-259, 261, 263-265, 270, 272-276, 278, 280, 282, 286, 288, 290-292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginConfigLoading::test_markers_exist_in_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that markers in the configuration are accessible.</p>
                                <p><strong>Why Needed:</strong> Prevent a potential bug where markers are not found or are inaccessible in the configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>pytestconfig should be an instance of pytest.config.Config</li>
                                        <li>pytestconfig should have a __dict__ attribute</li>
                                        <li>markers should be accessible through pytestconfig</li>
                                        <li>markers should be accessible through pytestconfig.__dict__</li>
                                        <li>markers should be accessible through pytestconfig.config</li>
                                        <li>markers should be accessible through pytestconfig._config</li>
                                        <li>markers should be accessible through pytestconfig._config.__dict__</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_llm_context_marker</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the functionality of LLM context marker in a test.</p>
                                <p><strong>Why Needed:</strong> The LLM context marker should prevent errors caused by incorrect usage or configuration of the plugin.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert True is not raised when calling `test_llm_context_marker`</li>
                                        <li>the output of `pytest_llm_report/collector.py` does not contain any error messages</li>
                                        <li>the LLM context marker is properly applied to test cases without causing errors</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_llm_opt_out_marker</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">


                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_requirement_marker</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `requirement_marker` function is being tested to ensure it does not throw any errors when called.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `requirement_marker` function might cause an error if not used correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `requirement_marker` function should be able to handle its input without throwing any exceptions.</li>
                                        <li>The `requirement_marker` function should not throw any errors when called with valid inputs.</li>
                                        <li>Any unexpected behavior or side effects of the `requirement_marker` function should be avoided.</li>
                                        <li>The `requirement_marker` function should not raise any exceptions when executed successfully.</li>
                                        <li>The `requirement_marker` function's output should be consistent and predictable in all cases.</li>
                                        <li>Any potential bugs or regressions caused by changes to the `requirement_marker` function should be identified and fixed.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestReportGeneration::test_report_writer_integration</span>
                        <div class="test-meta">
                            <span>34ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the report writer correctly generates a full report with summary statistics.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the report writer fails to generate a correct report even when there are multiple tests with different outcomes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that the total number of tests is 2 (1 passed, 1 failed).</li>
                                        <li>Verify that only 'test_a.py' and 'test_b.py' are included in the report HTML.</li>
                                        <li>Check if the report JSON file exists and contains a summary with correct statistics (total: 2, passed: 1).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">79 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">131 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `pytest_collectreport` skips when the collect report is disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the collect report is enabled but still causes issues with pytest.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_collectreport` function should not be called with an empty stash.</li>
                                        <li>The `session.config.stash.get` method should return False for `_enabled_key` when it's disabled.</li>
                                        <li>The `pytest_collectreport` function should not be called multiple times with the same stash key.</li>
                                        <li>The `session.config.stash.get` method should not call `pytest_collectreport` again after returning False.</li>
                                        <li>The `pytest_collectreport` function should only be called once per test run when collect report is disabled.</li>
                                        <li>The `session.config.stash.get` method should return True for `_enabled_key` when it's enabled.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 387-388, 391, 395-397, 408-409, 415-416)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_enabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that collectreport calls collector when enable is True.</p>
                                <p><strong>Why Needed:</strong> To prevent a potential bug where the plugin does not call the collector when enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_collectreport` function should be called with the correct mock report instance.</li>
                                        <li>The `handle_collection_report` method of the mock collector should be called once with the provided mock report.</li>
                                        <li>The `stash_get` function should return True for the `_enabled_key` and `_collector_key` keys when enabled.</li>
                                        <li>The `stash_get` function should not return a default value for the `_enabled_key` key when enabled.</li>
                                        <li>The `handle_collection_report` method of the mock collector should be called only once with the provided mock report.</li>
                                        <li>The `stash_get` function should return True for the `_collector_key` key when collectreport is called.</li>
                                        <li>The `stash_get` function should not return a default value for the `_collector_key` key when collectreport is called.</li>
                                        <li>The `handle_collection_report` method of the mock collector should be called only once with the provided mock report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 387-388, 391, 395-397, 408-409, 415, 419-421)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_no_session</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that `pytest_collectreport` does not raise an exception when no session is available.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the plugin raises an error due to missing a required attribute.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `pytest_collectreport` should not be called with a `session` argument that is missing.</li>
                                        <li>A `Session` object is expected to have a `session` attribute.</li>
                                        <li>Without a `session` attribute, the plugin will not raise an exception.</li>
                                        <li>If no session is available, the test should pass without raising any errors.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 387-388, 391, 395-397, 408, 412)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_session_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `pytest_collectreport` function skips when a null session is provided.</p>
                                <p><strong>Why Needed:</strong> Prevent regression in case of missing sessions during Pytest collection.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_collectreport` function should not raise an exception when given a `session` attribute set to `None`.</li>
                                        <li>A null `session` attribute should be ignored by the function.</li>
                                        <li>The function should skip any collected reports without a valid session.</li>
                                        <li>No error message or indication of failure should be raised in this case.</li>
                                        <li>The function's behavior should not depend on the presence of a `pytest_collectreport` instance.</li>
                                        <li>A null `session` attribute should have no impact on the report collection process.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 387-388, 391, 395-397, 408, 412)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_llm_enabled_warning</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM enabled warning is raised when using the Ollama LLMS provider.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the LLM report provider 'ollama' might be enabled by default, potentially causing issues with the plugin's functionality.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_llm_report` option is set to `None` when using the Ollama LLMS provider.</li>
                                        <li>The `llm_report_provider` option is set to 'ollama' when using the Ollama LLMS provider.</li>
                                        <li>The `llm_report_html`, `llm_report_json`, and `llm_report_pdf` options are not set when using the Ollama LLMS provider.</li>
                                        <li>The `llm_evidence_bundle`, `llm_dependency_snapshot`, `llm_requests_per_minute`, `llm_aggregate_dir`, `llm_aggregate_policy`, `llm_aggregate_run_id`, and `llm_aggregate_group_id` options are not set when using the Ollama LLMS provider.</li>
                                        <li>The `llm_max_retries` option is set to `None` when using the Ollama LLMS provider.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">44 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_validation_errors</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that validation errors raise UsageError when configuring Pytest.</p>
                                <p><strong>Why Needed:</strong> Prevents configuration errors and ensures correct usage of the plugin.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocking `pytest_configure` with invalid config values should raise a `UsageError`.</li>
                                        <li>The `option.llm_report_html`, `option.llm_report_json`, `option.llm_report_pdf`, `option.llm_evidence_bundle`, `option.llm_dependency_snapshot`, `option.llm_requests_per_minute`, `option.llm_aggregate_dir`, `option.llm_aggregate_policy`, `option.llm_aggregate_run_id`, and `option.llm_aggregate_group_id` options should be set correctly.</li>
                                        <li>The `llm_report_provider` option should have a valid value.</li>
                                        <li>Mocking the `getini` method with invalid config values should not affect the behavior of the test.</li>
                                        <li>The `pytest_configure` function should raise a `UsageError` when called with an invalid configuration.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">43 lines (ranges: 107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 248, 251-253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-199, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_worker_skip</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that configure skips on xdist workers and verifies the correct behavior of addinivalue_line.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where configure might skip on xdist workers due to an incorrect marker setup.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocking pytest_configure with mock_config and checking if addinivalue_line is called.</li>
                                        <li>Verifying that the workerinput attribute of mock_config is set correctly.</li>
                                        <li>Checking if the addinivalue_line function is not called before the worker check.</li>
                                        <li>Asserting that the addinivalue_line function is indeed called after the worker check.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 169-171, 173-175, 177-179, 183-184, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigureFallback::test_pytest_configure_fallback_load</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that fallback to load_config occurs when Config.load is missing, preventing potential bugs or regressions.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the plugin would attempt to load configuration data without calling Config.load(), potentially causing issues with dependency loading or other related problems.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_configure` function should be called with a valid `Config` object.</li>
                                        <li>The `load_config` method of `Config` should not be called.</li>
                                        <li>The `validate` method of the mocked `Config` object should return an empty list.</li>
                                        <li>The `load_config` method of the mocked `Config` object should be called once.</li>
                                        <li>The `pytest_configure` function should not have been patched with a mock for `options.Config`.</li>
                                        <li>The `load_config` method of the mocked `Config` object should have been called only once.</li>
                                        <li>The `validate` method of the mocked `Config` object should have returned an empty list.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_all_ini_options</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loading all INI options from the plugin configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the plugin fails to load configurations with missing or invalid INI values.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` attribute of the loaded configuration should be set to 'ollama'.</li>
                                        <li>The `model` attribute of the loaded configuration should be set to 'llama3.2'.</li>
                                        <li>The `llm_context_mode` attribute of the loaded configuration should be set to 'complete'.</li>
                                        <li>The `llm_requests_per_minute` attribute of the loaded configuration should be set to 10.</li>
                                        <li>The `report_html` attribute of the loaded configuration should be set to 'ini.html'.</li>
                                        <li>The `report_json` attribute of the loaded configuration should be set to 'ini.json'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">31 lines (ranges: 107, 147, 248, 251-263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_cli_overrides_ini</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test CLI options override INI options.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where CLI options override INI options, ensuring that the correct report is generated based on user input.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `llm_report_html` option overrides the `report_html` value from the INI file.</li>
                                        <li>The `llm_report_json` option overrides the `report_json` value from the INI file.</li>
                                        <li>The `llm_report_pdf` option overrides the `report_pdf` value from the INI file.</li>
                                        <li>The `llm_evidence_bundle` option overrides the `report_evidence_bundle` value from the INI file.</li>
                                        <li>The `llm_dependency_snapshot` option overrides the `report_dependency_snapshot` value from the INI file.</li>
                                        <li>The `llm_requests_per_minute` option overrides the `llm_report_requests_per_minute` value from the INI file.</li>
                                        <li>The `llm_aggregate_dir` option overrides the `aggregate_dir` value from the INI file.</li>
                                        <li>The `llm_aggregate_policy` option overrides the `aggregate_policy` value from the INI file.</li>
                                        <li>The `llm_aggregate_run_id` option overrides the `aggregate_run_id` value from the INI file.</li>
                                        <li>The `llm_aggregate_group_id` option overrides the `aggregate_group_id` value from the INI file.</li>
                                        <li>The `rootpath` option is set to `/project` as expected.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">38 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259-263, 270-283, 286-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that terminal summary skips when plugin is disabled.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where the plugin's terminal summary is not properly handled when it is disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mock stash.get() was called with _enabled_key and False as arguments, indicating that the plugin should be enabled for this test.</li>
                                        <li>The stash.get() call did not include any key-value pairs, suggesting that the plugin's terminal summary is being skipped due to its disabled state.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 238, 242-243, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_worker_skip</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that terminal summary skips on xdist worker when the 'workerid' key is present in the mock configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the plugin does not skip terminal summaries if an xdist worker is configured with a specific ID.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `pytest_terminal_summary` should return None for the given mock configuration.</li>
                                        <li>The 'workerid' key in the mock configuration should be present.</li>
                                        <li>The value of the 'workerid' key in the mock configuration should be 'gw0'.</li>
                                        <li>No output or exception should be raised when calling `pytest_terminal_summary` with the given mock configuration and arguments.</li>
                                        <li>The function `pytest_terminal_summary` should not call any other functions or methods that are not part of the plugin's API.</li>
                                        <li>The function `pytest_terminal_summary` should not modify any external state or variables.</li>
                                        <li>No assertion errors should be raised when calling `pytest_terminal_summary` with the given mock configuration and arguments.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 238-239, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::testload_config</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test config loading from pytest objects (CLI + INI) to ensure correct configuration retrieval.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the plugin does not retrieve the correct configuration settings due to incorrect handling of INI files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `load_config` function should be able to correctly load the specified configuration options from INI files.</li>
                                        <li>The `getini` method should return None for unknown keys in the INI file, allowing the plugin to handle it appropriately.</li>
                                        <li>The plugin's rootpath attribute should be set correctly based on the loaded configuration settings.</li>
                                        <li>The report HTML option should be set to the expected value 'out.html' as per the test setup.</li>
                                        <li>The `load_config` function should not raise an exception when encountering unknown keys in the INI file.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270-283, 286-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_disabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test makereport skips when disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the plugin does not report any issues when makereport is disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_runtest_makereport` hookwrapper should be able to send a result and complete even when makereport is disabled.</li>
                                        <li>The `get_result` method of the mock_outcome object should return None or an empty tuple when called without any arguments.</li>
                                        <li>The `send` method of the mock_outcome object should not raise an exception when called with no arguments.</li>
                                        <li>The `pytest_runtest_makereport` hookwrapper should be able to handle the generator and yield a point even when makereport is disabled.</li>
                                        <li>The `stash.get` method of the mock_item object should return False when called without any arguments.</li>
                                        <li>The `get_result` method of the mock_outcome object should not raise an exception when called with no arguments.</li>
                                        <li>The `pytest_runtest_makereport` hookwrapper should be able to handle the generator and yield a point even when makereport is disabled.</li>
                                        <li>The `pytest_runtest_makereport` hookwrapper should be able to send a result and complete even when makereport is disabled.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 387-388, 391-392, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_enabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test makereport calls collector when enabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the collector is not called when makereport is enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `mock_collector` is called with the correct `mock_report` instance when `pytest_runtest_makereport` is called.</li>
                                        <li>The `mock_item.config.stash.get(_enabled_key)` method returns `True` for `_enabled_key` and `mock_collector`.</li>
                                        <li>The `mock_item.config.stash.get(_collector_key)` method returns `mock_collector` for `_collector_key`.</li>
                                        <li>The `mock_collector.handle_runtest_logreport()` method is called with the correct `mock_report` instance.</li>
                                        <li>The `mock_collector.handle_runtest_logreport()` method does not raise an exception when `mock_outcome.get_result()` raises a StopIteration exception.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that collection_finish is skipped when disabled for Pytest sessions.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the plugin's hooks are not executed correctly when collection_finish is disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_session.config.stash.get.assert_called_with(_enabled_key, False)</li>
                                        <li>pytest_collection_finish(mock_session) should be called with _enabled_key as False</li>
                                        <li>The pytest_collection_finish function should return False for the given key</li>
                                        <li>The pytest_collection_finish function should not call stash.get again after returning False</li>
                                        <li>The pytest_collection_finish function should not call pytest_collection_finish again</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 387-388, 391, 395-397, 431-432)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_enabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that collection_finish is called when Pytest collection finish is enabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the collector is not called when collection finish is enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The stash_get function should return True for _enabled_key and False for _collector_key.</li>
                                        <li>The mock_collector.handle_collection_finish method should be called once with mock_session.items as argument.</li>
                                        <li>The mock_collector.handle_collection_finish method should not be called if the key is not _enabled_key or _collector_key.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 387-388, 391, 395-397, 431, 435-437)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that sessionstart skips when disabled and checks enabled status.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the plugin fails to check the enabled status of the session.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_session.config.stash.get.assert_called_with(_enabled_key, False)</li>
                                        <li>pytest_sessionstart(mock_session).should.have.been.called</li>
                                        <li>mock_session.config.stash.get.return_value.should.be.true</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 387-388, 391, 395-397, 448-449)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_enabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that sessionstart initializes collector when enabled and stash supports both get() and [].</p>
                                <p><strong>Why Needed:</strong> Prevents a bug where the collector is not created even though sessionstart is enabled, potentially leading to incorrect data collection.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert _collector_key in mock_stash</li>
                                        <li>assert _start_time_key in mock_stash</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 387-388, 391, 395-397, 448, 452, 455, 457-458)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test pytest_addoption adds expected arguments and verifies specific options.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where pytest_addoption does not add the required 'LLM-enhanced test reports' option to the command line.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that `pytest_addoption` is called with the correct group.</li>
                                        <li>Verify that the 'LLM-enhanced test reports' option is added to the command line.</li>
                                        <li>Verify that the 'LLM-coverage-source' option is also added to the command line.</li>
                                        <li>Verify that any arguments starting with '--llm-report' are included in the list of options.</li>
                                        <li>Verify that any arguments starting with '--llm-coverage-source' are included in the list of options.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">99 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption_ini</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test pytest_addoption adds INI options (lines 13-34) for a plugin with a custom terminal summary.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when using the pytest_addoption to add INI options to the plugin's configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm_report_html' option is added to the parser.</li>
                                        <li>The 'llm_report_json' option is added to the parser.</li>
                                        <li>The 'llm_report_max_retries' option is added to the parser.</li>
                                        <li>The 'max_retries' option is present in the ini calls.</li>
                                        <li>The 'html' and 'json' options are included in the ini calls.</li>
                                        <li>The 'max_retries' option has a value of 3 or more (>= 2)</li>
                                        <li>The 'max_retries' option does not have any default value (i.e., it is an optional argument)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">99 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_coverage_calculation</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test coverage percentage calculation logic for terminal summary.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in coverage reporting of terminal summaries.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report_html` option is set to 'out.html' and the `CoverageMapper` is correctly loaded.</li>
                                        <li>The `report` method is called with a mock configuration object.</li>
                                        <li>The `Coverage` class reports a percentage of 85.5 coverage.</li>
                                        <li>The `CoverageMapper` calls `load` before calling `report` on the `Coverage` instance.</li>
                                        <li>The `ReportWriter` does not raise an exception when writing to the file.</li>
                                        <li>The mock configuration object has a correct `stash` attribute.</li>
                                        <li>The mock stash is correctly populated with the provided data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">58 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 307, 309-315, 317-318, 331-332, 337-338, 365-375, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_llm_enabled</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test terminal summary with LLM enabled runs annotations.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in terminal summary functionality when LLM is enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that the correct provider is used when LLM is enabled.</li>
                                        <li>Check if the correct configuration is passed to the terminal reporter.</li>
                                        <li>Assert that the annotate_tests function is called with the correct config.</li>
                                        <li>Verify that the get_provider function returns the correct model name.</li>
                                        <li>Ensure that the mock stash object is correctly populated and mocked.</li>
                                        <li>Test that the mock_terminalreporter.stats dictionary is not modified unexpectedly.</li>
                                        <li>Verify that the mock_annotate function is called once with the correct arguments.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">59 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331-332, 337-340, 343, 345, 348-350, 357-362, 365-375, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_no_collector</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test terminal summary creates collector when no collector is provided.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the plugin does not create a collector even though it's enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert stash._enabled_key == True, 'Expected stash to have _enabled_key set to True'.</li>
                                        <li>assert stash._config_key is None, 'Expected stash to have _config_key set to None'.</li>
                                        <li>assert mock_config.stash._enabled_key == False, 'Expected stash._enabled_key to be False'.</li>
                                        <li>assert mock_config.stash._config_key is None, 'Expected stash._config_key to be None'.</li>
                                        <li>assert mock_terminalreporter.call_args_list[0][1] == 0, 'Expected terminal reporter call with argument 0 to pass'.</li>
                                        <li>assert mock_writer_cls.return_value.call_args_list[0][1] == 0, 'Expected writer class call with argument 0 to pass'.</li>
                                        <li>assert mock_mapper_cls.return_value.call_args_list[0][1].get('coverage') is None, 'Expected coverage map to be empty when no collector is provided'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_with_aggregation</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test terminal summary with aggregation enabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case the aggregation feature is not properly configured or disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The aggregation key should be set to True.</li>
                                        <li>The stash should support both get() and [] indexing.</li>
                                        <li>The aggregation method should return a report.</li>
                                        <li>The ReportWriter should write JSON and HTML files.</li>
                                        <li>The aggregation function should not raise an exception when called with invalid arguments.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 238, 242, 246, 249-250, 252-253, 256-257, 259, 261-265, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummaryErrors::test_terminal_summary_coverage_error</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">52 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 322-325, 331-332, 337-338, 365-375, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_balanced_context</span>
                        <div class="test-meta">
                            <span>8ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the ContextAssembler to assemble a balanced context for a test file with dependencies.</p>
                                <p><strong>Why Needed:</strong> This test prevents regressions where the ContextAssembler fails to assemble a balanced context due to missing or incorrect dependencies.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'utils.py' file is present in the assembled context.</li>
                                        <li>The 'def util()' function is found in the 'utils.py' file within the assembled context.</li>
                                        <li>The test file 'test_a.py' has a dependency on 'utils.py'.</li>
                                        <li>The ContextAssembler correctly assembles a balanced context with all dependencies included.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">51 lines (ranges: 33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-155, 158-159, 163, 191-192, 194)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_complete_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Assembling a complete context for the 'test_a.py' test file</p>
                                <p><strong>Why Needed:</strong> Prevents regression when the 'complete' mode is used with a non-existent test file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'test_1' function should be found in the assembled source code.</li>
                                        <li>The 'test_1' function should have been executed as part of the context assembly process.</li>
                                        <li>The 'test_a.py::test_1' nodeid should match the expected location within the assembled source code.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">34 lines (ranges: 33, 49, 52, 55, 58, 60, 63, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 132-133, 180)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_minimal_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the ContextAssembler can assemble a minimal context for a test file with a single test function.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when using the 'minimal' llm_context_mode, as it ensures that only necessary code is assembled into the context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The source of the assembly contains the expected test function `test_1`.</li>
                                        <li>The context is empty, indicating no additional code was assembled into the context.</li>
                                        <li>The assembler correctly identifies the test file `test_a.py` as part of the assembly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">30 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_balanced_context_limits</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Ensures the ContextAssembler can assemble a test with balanced context limits.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the assembler fails to truncate long content exceeding 20 bytes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'f1.py' file in the test result should contain the original long content.</li>
                                        <li>The 'f1.py' file in the test result should be truncated after 40 characters (20 bytes + truncation message).</li>
                                        <li>The length of the 'f1.py' file in the test result should not exceed 40 characters.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">34 lines (ranges: 33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-84, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-156, 158-159, 163, 191-192, 194)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_get_test_source_edge_cases</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the ContextAssembler's get_test_source method with edge cases.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the assembler incorrectly handles non-existent files or nested test names with parameters.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function returns an empty string for a non-existent file.</li>
                                        <li>The function correctly identifies the 'test' keyword in the source code of a nested test name with parameters.</li>
                                        <li>The function does not return any value when the input is invalid (e.g., no test name or parameter list).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 33, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_should_exclude</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the ContextAssembler should exclude certain files from the LLM context.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the ContextAssembler incorrectly excludes important files like `*.pyc` or sensitive data stored in `secret/*`.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert assembler._should_exclude('test.pyc') is True</li>
                                        <li>assert assembler._should_exclude('secret/key.txt') is True</li>
                                        <li>assert assembler._should_exclude('public/readme.md') is False</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 33, 191-194)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_consecutive_lines</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `compress_ranges` function is tested to ensure consecutive lines are compressed correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where consecutive lines of text without spaces or tabs are incorrectly compressed into a single range.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input list contains at least two elements.</li>
                                        <li>There are no empty strings in the input list.</li>
                                        <li>All elements in the input list are integers.</li>
                                        <li>- The first element is less than or equal to the second element.</li>
                                        <li>- All elements are within a valid range (inclusive).</li>
                                        <li>If there's only one element, it should be compressed into a single range ('1-1').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_duplicates</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the function correctly handles duplicate values in the input range.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly identifies unique ranges when there are duplicates.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return '1-3' for the input range [1, 2, 2, 3, 3, 3].</li>
                                        <li>The function should not return any duplicate ranges (e.g., [1, 2] or [2, 3]).</li>
                                        <li>The function should handle ranges with an odd number of elements correctly.</li>
                                        <li>The function should ignore non-integer values in the input range.</li>
                                        <li>The function should raise a ValueError for invalid input types (e.g., None or string).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_empty_list</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `compress_ranges` function with an empty input list.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where an empty list would cause the function to return an incorrect result or raise an exception.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return an empty string for an empty input list.</li>
                                        <li>The function should not throw any exceptions when given an empty list as input.</li>
                                        <li>The function should preserve the original order of elements in the input list.</li>
                                        <li>The function should handle lists with a single element correctly.</li>
                                        <li>The function should ignore non-compressible ranges (e.g., [1, 2] and [3, 4])</li>
                                        <li>The function should not compress ranges that are already empty (e.g., [] and [])</li>
                                        <li>The function should preserve the original order of elements when multiple non-compressible ranges are provided</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 29-30)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_mixed_ranges</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the function with a mixed range of numbers (single and multi-range values)</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of mixed ranges where single values are used as start or end points.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output should be '1-3, 5, 10-12, 15' as per the expected result.</li>
                                        <li>The function should correctly handle cases where a range is used as a single value (e.g., [1, 2, 3])</li>
                                        <li>The function should not raise any errors when given invalid input (e.g., negative numbers or non-numeric values).</li>
                                        <li>The function should preserve the original order of elements in the input list.</li>
                                        <li>The function should correctly handle ranges with a single element (e.g., [1, 5])</li>
                                        <li>The function should not incorrectly group adjacent ranges together (e.g., '1-3' and '5-10').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_non_consecutive_lines</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that non-consecutive lines are correctly compressed to a single comma-separated value.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where consecutive lines are not properly compressed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Ensure that the input list is correctly split into ranges without any consecutive elements.</li>
                                        <li>Verify that each range contains all its elements, without gaps.</li>
                                        <li>Check if the resulting string has no commas between the numbers.</li>
                                        <li>Confirm that the order of the numbers in each range remains unchanged.</li>
                                        <li>Test for edge cases where the input list only contains one element or multiple elements with no gaps.</li>
                                        <li>Ensure the function handles lists with duplicate values correctly.</li>
                                        <li>Verify the output is as expected even when the input list has a large number of consecutive lines.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_single_line</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_ranges.py::TestCompressRanges::test_single_line</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when the input list contains a single element.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function compress_ranges() should return the expected string representation for a single-element list.</li>
                                        <li>The function compress_ranges() should not use range notation to represent the list.</li>
                                        <li>The function compress_ranges() should raise an error if the input is empty.</li>
                                        <li>The function compress_ranges() should handle lists with only one element correctly.</li>
                                        <li>The function compress_ranges() should return the correct string representation for a single-element list.</li>
                                        <li>The function compress_ranges() should not modify the original list.</li>
                                        <li>The function compress_ranges() should raise an error if the input is not a list.</li>
                                        <li>The function compress_ranges() should handle lists with multiple elements correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 29, 33, 35-37, 39, 50, 52, 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_two_consecutive</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_ranges.py::TestCompressRanges::test_two_consecutive</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where two consecutive numbers are compressed to a single number.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input list should contain exactly one element.</li>
                                        <li>The output string should be in the format 'a-b'.</li>
                                        <li>The range notation should start from the first element and end at the second element.</li>
                                        <li>The range notation should include both elements.</li>
                                        <li>The range notation should not include any separators (e.g., commas).</li>
                                        <li>The input list should contain only one number.</li>
                                        <li>The output string should be in the correct order (first element before second).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_unsorted_input</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `compress_ranges` function handles unsorted input correctly.</p>
                                <p><strong>Why Needed:</strong> The test prevents a potential bug where the function would incorrectly group ranges in an unsorted array.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return '1-3, 5' as expected when given the input `[5, 1, 3, 2]`.</li>
                                        <li>The function should correctly group the range `[5, 3]` into `['1', '3']`.</li>
                                        <li>The function should handle duplicate ranges correctly by not producing any additional groups.</li>
                                        <li>The function should preserve the original order of elements within each range.</li>
                                        <li>The function should return an empty string for input that is already sorted.</li>
                                        <li>The function should raise a ValueError when given invalid input (e.g. non-integer values).</li>
                                        <li>The function should handle edge cases where the input array has only one element.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_empty_string</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that an empty string produces an empty list when expanded by the `expand_ranges` function.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where an empty string is not correctly handled by the `expand_ranges` function, potentially leading to incorrect results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `expand_ranges` function should return an empty list when given an empty input string.</li>
                                        <li>The `expand_ranges` function should handle empty strings without raising any exceptions or producing unexpected results.</li>
                                        <li>The `expand_ranges` function should produce a correct output for an empty string, such as an empty list.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 81-82)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_mixed</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `expand_ranges` function correctly handles mixed ranges and singles.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly expands single values into multiple ranges.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input string should be parsed as a comma-separated list of range strings or integers.</li>
                                        <li>Each range string should be in the format 'start-end' (e.g., '1-3'),</li>
                                        <li>Single values should not be expanded into multiple ranges.</li>
                                        <li>Range start and end values should be consecutive integers.</li>
                                        <li>Invalid range formats (e.g., 'a-b', 'abc') should raise a ValueError.</li>
                                        <li>The function should return the correct result for edge cases (e.g., empty input, single value)</li>
                                        <li>The function should handle ranges with negative numbers correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 81, 84-91, 93, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_range</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'expand_ranges' function is expected to correctly handle the range '1-3' and return a list of numbers in that range.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function might not expand the range correctly, potentially leading to incorrect results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return a list containing the numbers from 1 to 3 (inclusive).</li>
                                        <li>The function should handle negative values in the range. For example, '-2-4' should also be expanded to [-2, -1, 0, 1, 2, 3].</li>
                                        <li>The function should correctly handle ranges with multiple parts, such as '1-5' or '10-15'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 81, 84-91, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_roundtrip</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that `compress_ranges` and `expand_ranges` can be used interchangeably to get the original list back.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where compressing or expanding ranges would alter the original data in unintended ways.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>original == expanded</li>
                                        <li>compressed == original</li>
                                        <li>expanded == compressed</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">27 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67, 81, 84-91, 93, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_single_number</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'expand_ranges' function is expected to handle a single input ('5') and return a list with one element.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not correctly handle single numbers, potentially leading to incorrect results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert expand_ranges('5') == [5]</li>
                                        <li>assert isinstance(expand_ranges('5'), list)</li>
                                        <li>assert len(expand_ranges('5')) == 1</li>
                                        <li>assert expand_ranges('-5') == [-5]</li>
                                        <li>assert expand_ranges('abc') == []</li>
                                        <li>assert expand_ranges('5.0') == [5.0]</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 81, 84-87, 93, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestFormatDuration::test_milliseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `format_duration` function correctly formats durations for milliseconds below 1 second.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the function does not handle durations less than 1 second correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return '500ms' when given an input of 0.5 seconds.</li>
                                        <li>The function should return '1ms' when given an input of 0.001 seconds.</li>
                                        <li>The function should return '0ms' when given an input of 0.0 seconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestFormatDuration::test_seconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_render.py::TestFormatDuration::test_seconds</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not correctly format durations for values greater than or equal to 1 second.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `format_duration(x)` should return the string 'x.s' when x is an integer and >= 1.</li>
                                        <li>The function `format_duration(x)` should return the string 'x.00s' when x is a float and >= 1.</li>
                                        <li>The function `format_duration(x)` should correctly handle values greater than or equal to 1 second by returning the correct format string 'x.s'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestOutcomeToCssClass::test_all_outcomes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Outcome Mapping to CSS Classes</p>
                                <p><strong>Why Needed:</strong> Prevents regression where outcomes map to different CSS classes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `outcome_to_css_class` function should return the correct CSS class for each outcome.</li>
                                        <li>The `outcome_to_css_class` function should handle all possible outcomes correctly.</li>
                                        <li>The `outcome_to_css_class` function should not throw any exceptions when given an invalid outcome.</li>
                                        <li>The `outcome_to_css_class` function should preserve the original outcome value.</li>
                                        <li>The `outcome_to_css_class` function should map 'passed' to 'outcome-passed', 'failed' to 'outcome-failed', and so on.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 79-85, 87)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the 'outcome_to_css_class' function with an unknown outcome.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function returns incorrect CSS classes for unknown outcomes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return 'outcome-unknown' when given an unknown outcome.</li>
                                        <li>The function should not return any other class (e.g. 'outcome-foo') when given an unknown outcome.</li>
                                        <li>The function should handle cases where the outcome is not a string (e.g. a number or None) correctly.</li>
                                        <li>The function should raise an error if given an invalid outcome (e.g. a non-string value).</li>
                                        <li>The function should maintain its original behavior for known outcomes (e.g. 'outcome-foo').</li>
                                        <li>The function should be able to handle cases where the unknown outcome is not in the expected list of valid outcomes.</li>
                                        <li>The function should be able to handle cases where the unknown outcome is a string that is not recognized by the function (e.g. an invalid CSS class name).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 79-85, 87)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_basic_report</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test renders basic report with fallback HTML.</p>
                                <p><strong>Why Needed:</strong> This test prevents a rendering issue where the full HTML document is not rendered due to plugin or repository version issues.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The presence of '<!DOCTYPE html>' in the rendered HTML.</li>
                                        <li>The text 'Test Report' should be present in the rendered HTML.</li>
                                        <li>The nodeid 'test::passed' should be found in the rendered HTML.</li>
                                        <li>The string 'PASSED' should be present in the rendered HTML.</li>
                                        <li>The string 'FAILED' should be present in the rendered HTML.</li>
                                        <li>The plugin version should be displayed as '<strong>Plugin:</strong> v0.1.0'.</li>
                                        <li>The repository version should be displayed as '<strong>Repo:</strong> v1.2.3'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">52 lines (ranges: 65-67, 79-85, 87, 121-124, 126-127, 131-132, 141-143, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the test renders a fallback HTML with coverage information.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the rendering of `src/foo.py` includes coverage information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The report root contains a 'Summary' object with a total count of 1 and a passed count of 1.</li>
                                        <li>The report root contains a 'tests' list with one item, which is a 'TestCaseResult' object.</li>
                                        <li>The 'CoverageEntry' object has the following properties: 'file_path'='src/foo.py', 'line_ranges'='1-5', and 'line_count'=5.</li>
                                        <li>The rendered HTML includes the specified file path ('src/foo.py') with 5 lines of code.</li>
                                        <li>The rendered HTML contains a summary of coverage information, including the total count (1) and passed count (1).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">52 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-129, 131-132, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_llm_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the report includes LLM annotations for LLMs.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where LLM annotations are not included in reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The report contains 'Tests login flow' as part of its content.</li>
                                        <li>The report contains 'Prevents auth bypass' as part of its content.</li>
                                        <li>The LLM annotation is present and correctly formatted within the report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-134, 136-137, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_source_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test renders source coverage for fallback HTML.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the test fails due to missing source coverage information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'Source Coverage' section is present in the rendered HTML.</li>
                                        <li>The file path of the source code is included in the 'src/foo.py' tag.</li>
                                        <li>The overall coverage percentage is displayed as '80.0%', indicating that at least 80% of statements were covered.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">63 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-132, 141-142, 145-153, 158-164, 166-172, 177, 192, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_xpass_summary</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the rendered HTML includes both "XFailed" and "XPassed" summaries.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the summary is missing or incorrectly formatted.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The string 'XFailed' should be present in the rendered HTML.</li>
                                        <li>The string 'XPassed' should also be present in the rendered HTML.</li>
                                        <li>Both "XFailed" and "XPassed" should appear in the rendered HTML as expected.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">50 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-132, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestComputeSha256::test_different_content</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'different_content' verifies that the output of `compute_sha256` function is different for two different inputs.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the same input could produce the same output, leading to inconsistent reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The expected hash values are different.</li>
                                        <li>The computed hash value does not match the expected one.</li>
                                        <li>The function `compute_sha256` is correctly calculating the SHA-256 hash of each input.</li>
                                        <li>The test case is checking for a specific type of mismatch (different content),</li>
                                        <li>The test case is verifying that the output of the function is different from the input.</li>
                                        <li>The computed hash value should be unique and not match any other hash value generated by the function.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 55)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestComputeSha256::test_empty_bytes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that an empty bytes object produces consistent hash and the correct length.</p>
                                <p><strong>Why Needed:</strong> Prevents a bug where different input bytes produce different hashes, potentially leading to incorrect reporting or analysis.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output of `compute_sha256(b'')` should be equal to `compute_sha256(b'')`.</li>
                                        <li>The length of the hash produced by `compute_sha256(b'')` should be 64 characters (the expected SHA256 hex length).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 55)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_run_meta</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the build_run_meta method returns the correct metadata for a test run.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the report writer does not include version info in the run metadata.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The duration of the test run should be 60 seconds.</li>
                                        <li>The pytest version should be present in the metadata.</li>
                                        <li>The plugin version should be '0.1.0'.</li>
                                        <li>The Python version should also be present in the metadata.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_summary_all_outcomes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the `build_summary` method correctly counts all outcome types in a test case.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the summary might incorrectly count some outcomes as 'passed' or 'failed'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total number of outcomes should be equal to the number of tests.</li>
                                        <li>The number of passed outcomes should be 1 (all tests passed).</li>
                                        <li>The number of failed outcomes should be 1 (all tests failed).</li>
                                        <li>The number of skipped outcomes should be 1 (one test was skipped).</li>
                                        <li>The number of xfailed and xpassed outcomes should be 1 each.</li>
                                        <li>The number of error outcome(s) should be 1.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 156-158, 312, 314-315, 317-328, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_summary_counts</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `build_summary` method correctly counts outcomes in a test case.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the total count of passed, failed and skipped tests is not accurate.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total number of tests should be equal to the sum of passed, failed and skipped tests.</li>
                                        <li>The number of passed tests should be equal to the number of tests with outcome 'passed'.</li>
                                        <li>The number of failed tests should be equal to the number of tests with outcome 'failed'.</li>
                                        <li>The number of skipped tests should be equal to the number of tests with outcome 'skipped'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">13 lines (ranges: 156-158, 312, 314-315, 317-322, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_create_writer</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `ReportWriter` initializes correctly with a given configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `ReportWriter` does not properly initialize with a valid configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config` attribute of the `writer` object is set to the provided `Config` instance.</li>
                                        <li>The `warnings` list of the `writer` object is empty.</li>
                                        <li>The `artifacts` list of the `writer` object is empty.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 156-158)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_assembles_tests</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test writes a report that includes all tests, but does not handle the case where output paths are provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the ReportWriter does not write reports for tests with no output paths.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the report.tests list should be equal to 2.</li>
                                        <li>The total number of tests in the summary should be equal to 2.</li>
                                        <li>Each test in the report.tests list should have a nodeid that matches one of the TestCaseResult.nodeids provided.</li>
                                        <li>Each test result outcome should match either 'passed' or 'failed'.</li>
                                        <li>The summary.total property should contain an integer value representing the total number of tests.</li>
                                        <li>The config.output_paths property should be None, indicating no output paths are being used.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">93 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_includes_coverage_percent</span>
                        <div class="test-meta">
                            <span>6ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `ReportWriter` class writes a report with an included total coverage percentage.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the coverage percentage is not accurately reflected in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `coverage_total_percent` attribute of the `report.summary` object should match the provided `coverage_percent` value.</li>
                                        <li>The `writer.write_report()` method returns an instance of `ReportWriter` with a correctly set `coverage_total_percent` attribute.</li>
                                        <li>The `report.summary.coverage_total_percent` property is updated to reflect the actual coverage percentage included in the report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">93 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-199, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_includes_source_coverage</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test ReportWriter::test_write_report_includes_source_coverage verifies that the test writes a report with source coverage information.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that source coverage is included in reports, which helps maintain data integrity and accuracy.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the `source_coverage` list should be 1.</li>
                                        <li>The first element of the `source_coverage` list should have a `file_path` attribute equal to 'src/foo.py'.</li>
                                        <li>All elements in the `source_coverage` list should have a `covered_ranges` attribute that contains at least one range (e.g. '1-4, 6-7').</li>
                                        <li>The length of each element in the `source_coverage` list should be 3.</li>
                                        <li>Each element in the `source_coverage` list should have a `missed` attribute equal to 0 or less.</li>
                                        <li>The value of the `coverage_percent` attribute should be between 0 and 100 (inclusive).</li>
                                        <li>All elements in the `source_coverage` list should have a `covered` attribute that is greater than 0.</li>
                                        <li>Each element in the `source_coverage` list should have a `missed_ranges` attribute that contains at least one range (e.g. '5').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">92 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_merges_coverage</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test ReportWriter::test_write_report_merges_coverage verifies that the report writer merges coverage into tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the coverage is not merged into tests, potentially leading to inaccurate reporting of test coverage.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The report should contain a single coverage entry for the specified test.</li>
                                        <li>The file path of the coverage entry matches the expected file path.</li>
                                        <li>All lines in the coverage entry have the correct range and count.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">94 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186-189, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_atomic_write_fallback</span>
                        <div class="test-meta">
                            <span>7ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the ReportWriterWithFiles class falls back to direct write if atomic write fails and reports warnings.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the atomic write operation fails, causing the report writer to fall back to direct writing and reporting warnings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file "report.json" should exist at the specified path.</li>
                                        <li>Any warning messages from the ReportWriterWithFiles class should have code 'W203'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">125 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506-507, 509-512, 515-516)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_creates_directory_if_missing</span>
                        <div class="test-meta">
                            <span>7ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `ReportWriter` creates an output directory if it does not exist.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the report writer fails to create the output directory when it is missing.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output directory should be created with the correct name (`subdir/report.json`).</li>
                                        <li>The `exists()` method of the `tmp_path` object should return True for the expected output directory.</li>
                                        <li>The `ReportWriter` should correctly create the output directory even if it does not exist.</li>
                                        <li>The `write_report()` method should write to the expected output file.</li>
                                        <li>The `json_path` variable should be a valid path to an existing JSON file in the temporary directory.</li>
                                        <li>The `config` object should have a `report_json` attribute that points to the correct JSON file.</li>
                                        <li>The `writer.write_report()` method should call the `write()` method of the `Config` class correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">84 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">123 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-477, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_ensure_dir_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that a directory creation failure results in the capture of a warning.</p>
                                <p><strong>Why Needed:</strong> To prevent unexpected behavior when creating directories with insufficient permissions.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `writer.warnings` list should contain at least one warning with code 'W201'.</li>
                                        <li>The `writer.warnings` list should not be empty.</li>
                                        <li>Any warnings in the `writer.warnings` list should have a non-zero code value ('W201').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 156-158, 470-473, 480-484)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_git_info_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_git_info` function handles Git command failures by returning `None` for both `sha` and `dirty` variables.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the `get_git_info` function fails to return expected values when encountering a Git command failure.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `sha` variable is set to `None` after calling `get_git_info()`</li>
                                        <li>The `dirty` variable is set to `None` after calling `get_git_info()`</li>
                                        <li>The function does not raise an exception when encountering a Git command failure (as intended)</li>
                                        <li>The function returns the correct values for `sha` and `dirty` even if the Git command fails (as expected)</li>
                                        <li>The test does not fail due to a Git command failure (as intended)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 67-73, 85-86)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_creates_file</span>
                        <div class="test-meta">
                            <span>32ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test `test_write_html_creates_file` verifies that the report writer creates an HTML file and includes expected content.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the report writer fails to create an HTML file or does not include expected content in the generated report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file `report.html` should exist at the specified path.</li>
                                        <li>The file `report.html` should contain the expected content as per the test cases.</li>
                                        <li>All test cases (test1 and test2) should be found in the HTML file.</li>
                                        <li>The report writer should include 'PASSED', 'FAILED', 'Skipped', 'XFailed', and 'XPassed' keywords in the HTML file.</li>
                                        <li>The report writer should also include 'Errors' keyword in the HTML file.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">115 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_includes_xfail_summary</span>
                        <div class="test-meta">
                            <span>34ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the report includes xfail outcomes in the HTML summary.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that xfail outcomes are included in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Asserts that 'XFAILED' and 'XFailed' are present in the HTML string.</li>
                                        <li>Asserts that 'XPASSED' and 'XPassed' are also present in the HTML string.</li>
                                        <li>Verifies that the HTML includes a summary of xfail outcomes.</li>
                                        <li>Checks if the report is written with the correct configuration.</li>
                                        <li>Ensures that the test output matches the expected format.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-326, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_json_creates_file</span>
                        <div class="test-meta">
                            <span>6ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `write_json` method of ReportWriter with a test case that creates a JSON file.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the report writer does not create a JSON file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should create a new JSON file at the specified path.</li>
                                        <li>The file should be tracked as an artifact in the report.</li>
                                        <li>The number of artifacts should be greater than zero.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">78 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">117 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_creates_file</span>
                        <div class="test-meta">
                            <span>37ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the `write_pdf` method creates a PDF file when Playwright is available.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the `playwright.sync_api` module import fails and the `write_pdf` method does not create a file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `write_pdf` function should write to the specified path.</li>
                                        <li>The file should be created in the same directory as the report.</li>
                                        <li>Any artifacts generated by the test should have the correct paths.</li>
                                        <li>The `exists()` method should return True for the PDF file.</li>
                                        <li>The `artifact.path == str(pdf_path)` assertion should pass for each artifact.</li>
                                        <li>The `playwright.sync_api` module import should be mocked correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">125 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 401, 410, 412, 414-423, 434-435, 437-443, 448, 453, 455, 458-462, 470-471)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_missing_playwright_warns</span>
                        <div class="test-meta">
                            <span>6ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'Should warn when Playwright is missing for PDF output' verifies that the test reports a warning when Playwright is not installed for PDF output.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the report writer does not correctly handle cases where Playwright is missing for PDF output, potentially leading to unexpected behavior or errors in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file 'report.pdf' should exist.</li>
                                        <li>At least one warning code (W204_PDF_PLAYWRIGHT_MISSING) should be present in the warnings list.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">98 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 401-405, 408)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer_coverage_v2.py::test_report_writer_ensure_dir_creation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test ensures directory creation of report writer output.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where the report writer creates an empty or non-existent directory.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `tmp_dir` exists after the test.</li>
                                        <li>Any warnings from the report writer have a code of 'W202'.</li>
                                        <li>The `tmp_dir` is not empty after the test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 156-158, 470-477)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer_coverage_v2.py::test_report_writer_metadata_skips</span>
                        <div class="test-meta">
                            <span>10ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the scenario where report_writer_metadata_skips verifies that metadata skips when reports are disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that metadata is skipped when reports are disabled, which can lead to incorrect or incomplete reporting.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'start_time' key should be present in the metadata.</li>
                                        <li>Metadata should not contain an 'llm_model' key.</li>
                                        <li>The 'llm_model' value should be None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_schemas.py::TestAnnotationSchema::test_from_dict_full</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `AnnotationSchema.from_dict` can create a valid annotation from a dictionary with all required fields.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in cases where the input data does not contain all required fields, potentially causing authentication issues.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The schema should have the correct scenario and why needed values.</li>
                                        <li>All required key assertions should be present and match the expected values.</li>
                                        <li>The confidence value should match the provided confidence.</li>
                                        <li>The schema's `scenario` field should match the input dictionary's `scenario` value.</li>
                                        <li>The schema's `why_needed` field should match the input dictionary's `why_needed` value.</li>
                                        <li>All required key assertions in the schema's `key_assertions` list should be present and match the expected values.</li>
                                        <li>Each required key assertion in the schema's `key_assertions` list should have a matching value in the input dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_schemas.py::TestAnnotationSchema::test_to_dict_full</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> test_to_dict_full verifies that the annotation schema can be successfully converted to a dictionary with all required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the AnnotationSchema class, ensuring it correctly handles the conversion of annotations to dictionaries.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert data['scenario'] == 'Verify login',</li>
                                        <li>assert data['why_needed'] == 'Catch auth bugs',</li>
                                        <li>assert data['key_assertions'] == ['assert 200', 'assert token'],</li>
                                        <li>assert data['confidence'] == 0.95</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 90-92, 94-98)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_report_created</span>
                        <div class="test-meta">
                            <span>87ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that an HTML report is created when the `--llm-report` flag is used.</p>
                                <p><strong>Why Needed:</strong> This prevents a regression where the report generation might not work as expected due to missing or corrupted files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file path of the generated report should be present in the current working directory.</li>
                                        <li>The content of the report should contain the string '<html>'.</li>
                                        <li>The test function 'test_simple' should be found within the report content.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">101 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_summary_counts_all_statuses</span>
                        <div class="test-meta">
                            <span>126ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_summary_counts_all_statuses</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the HTML summary counts include all statuses.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert True is included in the 'Passed' label</li>
                                        <li>assert False is included in the 'Failed' label</li>
                                        <li>assert True is included in the 'Skipped' label</li>
                                        <li>assert True is included in the 'XFailed' label</li>
                                        <li>assert True is included in the 'Errors' and 'Error' labels</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 78-79, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212-214, 216, 227-228, 230-236, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">111 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-328, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_json_report_created</span>
                        <div class="test-meta">
                            <span>75ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The JSON report is created and its existence and contents are verified.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the Pytest report generation process fails to create a JSON report, potentially leading to incorrect or incomplete reporting.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>A JSON file named 'report.json' should be created in the report directory.</li>
                                        <li>The contents of the 'report.json' file should contain the expected schema version and summary statistics.</li>
                                        <li>The total number of tests passed should match the actual count, with no failed or skipped tests.</li>
                                        <li>At least one test should have been marked as 'passed', and at most one test should be marked as 'failed'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">51 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-118, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">107 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_annotations_in_report</span>
                        <div class="test-meta">
                            <span>83ms</span>
                            <span title="Covered file count">üõ°Ô∏è 13</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that LLM annotations are included in the report for a provider enabled.</p>
                                <p><strong>Why Needed:</strong> Prevents regressions by ensuring LLM annotations are present in reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The scenario 'Checks the happy path' is included in the report.</li>
                                        <li>The reason 'Prevents regressions' is included in the report.</li>
                                        <li>The key assertions 'asserts True' are included in the report.</li>
                                        <li>The provider is enabled (litellm is used).</li>
                                        <li>The LLM report model is set to gpt-4o-mini.</li>
                                        <li>The LLM report JSON file is created at the specified path.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 52-53, 72, 75, 80, 107, 110-111, 128, 136, 147, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">23 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69-70, 94-95, 97)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">94 lines (ranges: 104-107, 109-111, 113, 115, 161-165, 167, 169, 171, 173, 176, 178-180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407-419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">186 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-340, 343, 345, 348-352, 355, 357-362, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_error_is_reported</span>
                        <div class="test-meta">
                            <span>6.09s</span>
                            <span title="Covered file count">üõ°Ô∏è 12</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM errors are surfaced in HTML output.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where LLM errors are not reported correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test verifies the presence of 'LLM error' and 'boom' in the report content.</li>
                                        <li>The test asserts that 'LLM error' is present in the report output.</li>
                                        <li>The test checks for the correct spelling of 'boom' in the report content.</li>
                                        <li>The test ensures that both 'LLM error' and 'boom' are found in the report content.</li>
                                        <li>The test verifies that the LLM errors are surfaced correctly in HTML format.</li>
                                        <li>The test asserts that the LLM errors are reported with proper formatting.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 39-41, 53, 55-56, 86, 88, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">73 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137-139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198-201, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 52-53, 72, 75, 80, 107, 110-111, 128, 136, 147, 165, 167, 175, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 78-79, 81-82, 84-85, 94-95, 97)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">186 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-340, 343, 345, 348-353, 357-362, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">101 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestMarkers::test_llm_opt_out_marker</span>
                        <div class="test-meta">
                            <span>65ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the LLM opt-out marker functionality.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in LLM opt-out marker detection.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test verifies that the LLM opt-out marker is correctly recorded.</li>
                                        <li>The test checks if the LLM opt-out marker is marked as True for all tests.</li>
                                        <li>The test asserts that only one test is marked with the LLM opt-out marker.</li>
                                        <li>The test reads and parses the report file to verify the expected data.</li>
                                        <li>The test verifies that the LLM opt-out marker is correctly recorded in the report file.</li>
                                        <li>The test checks if the LLM opt-out marker is not marked as False for any tests.</li>
                                        <li>The test ensures that the LLM opt-out marker is not missed by the pytester.</li>
                                        <li>The test verifies that the LLM opt-out marker is correctly recorded in the report file even when the --llm-report-json flag is not used.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181-182, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180-182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestMarkers::test_requirement_marker</span>
                        <div class="test-meta">
                            <span>65ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the requirement marker to ensure it records the correct requirements.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the requirement marker is not recorded correctly, potentially leading to missed tests or incorrect test results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest.mark.requirement` decorator is applied to the `test_with_req` function with two required requirements.</li>
                                        <li>The report generated by `pytester.runpytest` includes a JSON file that contains the list of tested functions and their corresponding requirements.</li>
                                        <li>The test asserts that there is only one test function in the report, which should contain both `REQ-001` and `REQ-002` as required.</li>
                                        <li>The test asserts that both `REQ-001` and `REQ-002` are present in the list of requirements for the first tested function.</li>
                                        <li>The test verifies that `REQ-001` is included in the requirements of the `test_with_req` function.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-200, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188-190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_multiple_xfail_outcomes</span>
                        <div class="test-meta">
                            <span>68ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that multiple xfailed tests are recorded in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that all xfailed tests are properly reported and counted.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The number of xfailed tests is correctly reported as 2.</li>
                                        <li>All xfailed tests are included in the report.</li>
                                        <li>Each xfailed test has an outcome of 'xfailed'.</li>
                                        <li>No other outcomes are recorded in the report for these tests.</li>
                                        <li>The report includes a summary section with the correct count of xfailed tests.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">108 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-324, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_skip_outcome</span>
                        <div class="test-meta">
                            <span>59ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that skipping tests prevents incorrect reporting of skipped outcomes.</p>
                                <p><strong>Why Needed:</strong> This test verifies that skipping tests is recorded in the report and does not incorrectly count them as passed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'skipped' key in the report should contain a value of 1 if there are any skipped tests.</li>
                                        <li>The 'summary' section of the report should have a 'skipped' category with a count equal to the number of skipped tests.</li>
                                        <li>If no tests were skipped, the 'summary' section of the report should be empty.</li>
                                        <li>If all tests passed, the 'summary' section of the report should also be empty.</li>
                                        <li>The test skip marker should indicate that the test was skipped.</li>
                                        <li>The test skip marker should not be present if there are no skipped tests.</li>
                                        <li>The test skip marker should not be present in the 'skipped' category if all tests passed.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">43 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 106-107, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">107 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321-322, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_xfail_outcome</span>
                        <div class="test-meta">
                            <span>63ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the 'xfail' marker correctly records X-failed tests in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where a test is marked as xfail but does not actually fail, potentially leading to incorrect reporting of failed tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'summary' key in the JSON report should contain the correct number of X-failed tests (1 in this case).</li>
                                        <li>The 'xfailed' value under the 'summary' key should be equal to 1.</li>
                                        <li>The 'test_results' key under the 'summary' key should include a list of test names that failed, which in this case is just 'test_xfail'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">108 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-324, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestParametrization::test_parametrized_tests</span>
                        <div class="test-meta">
                            <span>65ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test parameterized tests are recorded separately.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in parametrized testing.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `test_param` is called with the correct argument value (`x`) for each iteration.</li>
                                        <li>The assertion `assert x > 0` passes for all values of `x` (1, 2, and 3).</li>
                                        <li>The test case has a total count of 3 successful tests.</li>
                                        <li>The test case has passed for 3 iterations with the correct argument value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167, 169-171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_help_contains_examples</span>
                        <div class="test-meta">
                            <span>53ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The CLI help text should include usage examples.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the help message does not contain any usage examples, potentially confusing users.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `--help` option is present in the help output.</li>
                                        <li>The `*Example:*--llm-report*` line matches the expected format of an example.</li>
                                        <li>The test checks that the help text includes a clear and concise description of how to use the plugin.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_markers_registered</span>
                        <div class="test-meta">
                            <span>49ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that LLM markers are registered and their presence is detected by pytester.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the LLM marker registration test fails without detecting the markers.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytester` instance runs the `--markers` flag with the expected output.</li>
                                        <li>The `stdout.fnmatch_lines` method is called with the expected list of strings.</li>
                                        <li>Each string in the list matches one of the expected marker names.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_plugin_registered</span>
                        <div class="test-meta">
                            <span>55ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The plugin registered with pytest11 should be visible in the help output.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the plugin is not listed in the help message.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm-report' option should be present in the help output.</li>
                                        <li>The 'llm-report' option should be followed by any other options.</li>
                                        <li>The 'llm-report' option should be preceded by a '--help' command.</li>
                                        <li>The plugin name should be visible after listing all plugins.</li>
                                        <li>The plugin name should not be hidden behind long list of options.</li>
                                        <li>The help output should contain the correct information about the plugin.</li>
                                        <li>The help output should display 'llm-report' option correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestSpecialCharacters::test_special_chars_in_nodeid</span>
                        <div class="test-meta">
                            <span>96ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that special characters in nodeid are handled correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where special characters in the nodeid field could cause issues with the PyTorch Lightning model.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `s` parameter passed to the `test_special_chars_in_nodeid` function should not be empty.</li>
                                        <li>The HTML report generated by PyTorch Lightning should contain the `<html>` tag.</li>
                                        <li>The nodeid field in the report file should not cause any errors or crashes.</li>
                                        <li>The contents of the report file should be valid and consistent with the expected format.</li>
                                        <li>The `report_path.exists()` assertion should pass, indicating that the test did not crash.</li>
                                        <li>The `content` variable read from the report path should contain the `<html>` tag as expected.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">101 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_boundary_one_minute</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the function `format_duration` with a boundary of exactly one minute.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the `format_duration` function when input is exactly one minute.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result should be '1m 0.0s' (one minute and zero seconds).</li>
                                        <li>The time unit should be 'm' (minutes) instead of 's' (seconds).</li>
                                        <li>The seconds part should be zero.</li>
                                        <li>The function should handle input exactly one minute without any errors.</li>
                                        <li>The function should return the correct format string for one minute.</li>
                                        <li>The function should not raise an exception when given a negative time value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_microseconds_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the test_time module's format_duration function with a sub-millisecond duration.</p>
                                <p><strong>Why Needed:</strong> Prevents regression or bug that may occur when using durations greater than one millisecond.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of `format_duration(0.0005)` should contain 'Œºs' in its string representation.</li>
                                        <li>The result of `format_duration(0.0005)` should be equal to '500Œºs'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 39-40)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_milliseconds_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test formats sub-second durations as milliseconds.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the test fails due to incorrect formatting of millisecond durations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `format_duration(0.5)` returns a string in the format 'X.XXms' where X is the number of milliseconds.</li>
                                        <li>The assertion `assert result == '500.0ms'` checks if the formatted string matches the expected output.</li>
                                        <li>The assertion `assert 'ms' in result` verifies that the string contains the substring 'ms'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 39, 41-42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_minutes_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `format_duration` function correctly formats durations over a minute.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when the duration is greater than one minute, as it should be displayed in minutes and seconds format.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result contains 'm' (minutes) and 's' (seconds)</li>
                                        <li>The result equals '1m 30.5s'</li>
                                        <li>The function correctly handles durations over a minute</li>
                                        <li>The function displays the duration in minutes and seconds format</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_multiple_minutes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function with a duration of 3 minutes and 5 seconds.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in handling durations that include fractional parts (e.g., 2 hours 30 minutes).</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result is '3m 5.0s' as expected.</li>
                                        <li>The duration value is correctly calculated to be 185 seconds.</li>
                                        <li>The function handles values with fractional parts without truncation or rounding errors.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_one_second</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function with a single-second input.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in formatting durations to seconds.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output of `format_duration(1.0)` should be '1.00s'.</li>
                                        <li>The duration is formatted as '1.00s' instead of just '1s'.</li>
                                        <li>The function correctly handles inputs greater than zero.</li>
                                        <li>The function does not raise an error for invalid input (e.g., negative numbers).</li>
                                        <li>The formatting is consistent across different Python versions and platforms.</li>
                                        <li>The test covers a specific edge case (one second) to ensure robustness.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 39, 41, 43-44)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_seconds_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `format_duration` function to ensure it correctly formats seconds under a minute.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when the duration is less than one minute, as the current implementation does not handle this case.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result contains the string 's' (for seconds) in its format.</li>
                                        <li>The result equals the expected string '5.50s'.</li>
                                        <li>The function correctly handles durations under a minute without any additional formatting.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 39, 41, 43-44)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_small_milliseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function with a duration of 1 millisecond.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the function incorrectly formats durations in milliseconds.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output should be '1.0ms' for a duration of 1 millisecond.</li>
                                        <li>The function should handle cases where the input is exactly 1 millisecond without any rounding errors.</li>
                                        <li>The function should not round up to a larger value than the input when it's already an integer (e.g., 0.999... becomes 1.0ms).</li>
                                        <li>The function should correctly format durations in milliseconds, including decimal points and commas.</li>
                                        <li>The function should handle negative durations correctly.</li>
                                        <li>The function should not silently truncate or round up to a larger value when the input is already an integer (e.g., -1.999... becomes -2.0ms).</li>
                                        <li>The function should raise an error if the input duration is not a non-negative number.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 39, 41-42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_very_small_microseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `format_duration` function correctly formats very small durations as microseconds.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not handle very small durations correctly, potentially leading to incorrect formatting.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of calling `format_duration(0.000001)` should be '1Œºs'.</li>
                                        <li>The string representation of the result should match '1Œºs'.</li>
                                        <li>The unit suffix ('Œº') is present in the result.</li>
                                        <li>The function correctly handles very small durations (less than 10^-6 seconds).</li>
                                        <li>The function does not raise an exception when given a duration less than 10^-9 seconds.</li>
                                        <li>The function returns '1' as expected for a duration of exactly 0.000001 seconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 39-40)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_datetime_with_utc</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the functionality of formatting datetime objects with UTC timezone.</p>
                                <p><strong>Why Needed:</strong> This test prevents regressions where datetime objects with UTC timezone are not correctly formatted as ISO format.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `iso_format(dt)` returns a string in the correct ISO format for datetime objects with UTC timezone.</li>
                                        <li>The function `iso_format(dt)` handles cases where the input datetime object has an invalid timezone.</li>
                                        <li>The function `iso_format(dt)` correctly formats datetime objects with UTC timezone into the expected ISO format.</li>
                                        <li>The function `iso_format(dt)` raises an error when given a datetime object without a valid timezone.</li>
                                        <li>The function `iso_format(dt)` handles cases where the input datetime object is in a timezone other than UTC.</li>
                                        <li>The function `iso_format(dt)` correctly formats datetime objects with UTC timezone into the correct ISO format.</li>
                                        <li>The function `iso_format(dt)` returns an error message when given a datetime object without a valid timezone.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_naive_datetime</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test formats naive datetime with no timezone.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in naive datetime format handling without timezone.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `iso_format(dt)` should return the correct ISO formatted string for a naive datetime (no timezone).</li>
                                        <li>The function `iso_format(dt)` should handle the date part correctly (2024-06-20).</li>
                                        <li>The function `iso_format(dt)` should handle the time part correctly (14:00:00).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_with_microseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `iso_format` function correctly formats a datetime object with microseconds.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues where microseconds are not properly formatted in ISO format.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output of `iso_format(dt)` should contain the string '123456' which represents the microseconds part of the datetime.</li>
                                        <li>The microseconds value should be a four-digit number (e.g., '0000', '1000', etc.).</li>
                                        <li>Any non-numeric characters in the microseconds string should be ignored or removed.</li>
                                        <li>The microseconds value should not exceed 15 digits (i.e., up to 9999).</li>
                                        <li>The microseconds value should start with a leading zero if it is less than 10.</li>
                                        <li>Non-zero microseconds values should only contain numeric characters and/or underscores.</li>
                                        <li>Any non-numeric characters in the microseconds string except for underscores should be ignored or removed.</li>
                                        <li>The microseconds value should not include any decimal points (e.g., '123.456', etc.).</li>
                                        <li>The microseconds value should not exceed 15 digits (i.e., up to 9999).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestUtcNow::test_has_utc_timezone</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `utc_now()` function returns a datetime object with an associated UTC timezone.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in tests that rely on the current system's timezone being set to UTC.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The returned datetime object has a valid timezone information (tzinfo attribute is not None and equals `UTC`).</li>
                                        <li>The returned datetime object's timezone is indeed UTC.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestUtcNow::test_is_current_time</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies the correctness of `utc_now()` function by comparing its return value with the current time.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the `utc_now()` function returns an incorrect or outdated time due to a timing-related bug.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The returned time should be within a certain tolerance (e.g., 1 second) of the actual current time.</li>
                                        <li>The returned time should not exceed the actual current time by more than the specified tolerance.</li>
                                        <li>The returned time should not be less than the actual current time by more than the specified tolerance.</li>
                                        <li>If the system is running slowly, the `utc_now()` function may return an outdated time.</li>
                                        <li>If the system is experiencing a timing-related issue (e.g., network lag), the `utc_now()` function may return an incorrect or outdated time.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestUtcNow::test_returns_datetime</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The function `utc_now()` returns a `datetime` object.</p>
                                <p><strong>Why Needed:</strong> This test prevents the regression of returning an incorrect or invalid datetime when calling `utc_now()`.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>result is an instance of `datetime`</li>
                                        <li>result has a valid timezone.</li>
                                        <li>result does not raise any exceptions</li>
                                        <li>result is not `None`</li>
                                        <li>result is not a string</li>
                                        <li>result is not a timedelta</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
        </div>

        <section class="source-coverage">
            <h2>Source Coverage</h2>
            <div class="source-coverage-table">
                <div class="source-coverage-header">
                    <span>File</span>
                    <span>Stmts</span>
                    <span>Miss</span>
                    <span>Cover</span>
                    <span>%</span>
                    <span>Covered Lines</span>
                    <span>Missed Lines</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/_git_info.py</span>
                    <span>2</span>
                    <span>0</span>
                    <span>2</span>
                    <span>100.0%</span>
                    <span class="source-lines">2-3</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/aggregation.py</span>
                    <span>116</span>
                    <span>5</span>
                    <span>111</span>
                    <span>95.69%</span>
                    <span class="source-lines">13, 15-19, 21, 35, 38, 44, 46, 52-53, 55-57, 59, 61-64, 69, 73-74, 77-80, 84, 87-89, 93, 103, 109-111, 113-117, 119-120, 125, 127-128, 130-131, 134-135, 141-144, 146, 148, 162, 164, 168, 170, 172, 182, 184-188, 190-191, 194, 196, 205, 217, 219-233, 235, 237, 245-246, 248-249, 251, 253-255, 259, 262-263, 265-266, 269-271, 273, 275-276, 280</span>
                    <span class="source-lines">66, 90-91, 192, 203</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/cache.py</span>
                    <span>47</span>
                    <span>3</span>
                    <span>44</span>
                    <span>93.62%</span>
                    <span class="source-lines">13, 15-19, 21, 27, 33, 39-41, 43, 53, 55-56, 58, 60-62, 68-69, 78, 86, 88, 90, 92, 94, 97, 103, 107, 118-119, 121, 123, 129, 132-136, 141, 144, 153</span>
                    <span class="source-lines">64-65, 130</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/collector.py</span>
                    <span>111</span>
                    <span>2</span>
                    <span>109</span>
                    <span>98.2%</span>
                    <span class="source-lines">19, 21-22, 24, 26-27, 33-34, 45-50, 52, 58, 60-62, 69, 78-79, 81, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127-128, 130, 132-133, 135-137, 140, 143, 155, 163-164, 167-169, 171, 173, 181-182, 185-189, 191, 198-200, 202, 209-210, 212-214, 216, 218, 227-228, 230-236, 238, 241, 250-252, 254, 261, 264-265, 268-269, 271, 277, 279, 285</span>
                    <span class="source-lines">141, 239</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/coverage_map.py</span>
                    <span>135</span>
                    <span>10</span>
                    <span>125</span>
                    <span>92.59%</span>
                    <span class="source-lines">13, 15-17, 19-22, 30, 38, 44-45, 47, 58-60, 64, 72-73, 83, 86, 88-90, 92, 94-96, 98, 101-104, 106-108, 114, 116, 118, 121-122, 127, 131-135, 137-140, 144-146, 148, 150, 152-153, 156, 160-162, 165, 167-168, 173, 176, 178-184, 187-189, 191, 196, 199-200, 202, 204, 216-217, 220, 224-225, 228-234, 236, 239, 241, 243-244, 246-248, 250, 252-254, 259-260, 263-264, 271, 273, 276-279, 281-283, 285, 299-300, 302, 308</span>
                    <span class="source-lines">62, 123, 125, 128, 157, 221, 249, 251, 257, 274</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/errors.py</span>
                    <span>35</span>
                    <span>0</span>
                    <span>35</span>
                    <span>100.0%</span>
                    <span class="source-lines">8-9, 12, 25-28, 31-36, 39-42, 45-46, 49-51, 54-55, 64-66, 68, 70, 74-76, 80, 129, 139</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/__init__.py</span>
                    <span>3</span>
                    <span>0</span>
                    <span>3</span>
                    <span>100.0%</span>
                    <span class="source-lines">4-5, 7</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/annotator.py</span>
                    <span>110</span>
                    <span>0</span>
                    <span>110</span>
                    <span>100.0%</span>
                    <span class="source-lines">4, 6-10, 12-15, 21-22, 25-28, 31, 45-46, 48-50, 54, 56-57, 59, 61-62, 64, 66-68, 71-72, 74-82, 87, 97-98, 100, 102, 104-105, 115, 127, 129-132, 137-139, 142, 165-168, 170-171, 176, 178, 180-183, 185-190, 192-193, 198-201, 203, 206, 229-232, 234, 236-237, 239-240, 245-246, 248-253, 255-256, 261-264, 266</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/base.py</span>
                    <span>78</span>
                    <span>0</span>
                    <span>78</span>
                    <span>100.0%</span>
                    <span class="source-lines">13, 15-18, 26, 40, 46, 52-53, 55, 72, 75-76, 78, 80, 101, 107-108, 110-111, 122, 128, 130, 136, 138, 147, 149, 165, 167-173, 175, 177, 186-187, 190-192, 194-195, 198-200, 203-208, 212, 214, 220-221, 224-225, 228-230, 233, 245, 247, 249-250, 252-253, 255, 257-258, 260, 262-263, 265, 267</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/gemini.py</span>
                    <span>275</span>
                    <span>18</span>
                    <span>257</span>
                    <span>93.45%</span>
                    <span class="source-lines">7, 9-13, 15-16, 23-27, 30-34, 37-42, 44-46, 48-50, 52, 57-63, 65-70, 72-73, 75-78, 80-85, 87-88, 91-97, 99-103, 105, 107-114, 121-122, 125, 128, 134, 136-139, 141-142, 144, 160-161, 167-169, 171-172, 174, 176-184, 186-188, 190-191, 193, 196, 200-208, 210-211, 213-215, 217-223, 225-227, 233-234, 238-239, 242-243, 245-248, 252-253, 260, 266-267, 269, 273-277, 279-283, 286-287, 292-293, 300-301, 303, 315, 317-318, 322, 327, 330-332, 335-343, 345-346, 348, 352-355, 357, 360-366, 368-374, 380-382, 384-387, 389, 391-392, 396-402, 405, 408-410, 412-414, 416-421, 427-428, 430-434, 437-440, 442-443, 445-447</span>
                    <span class="source-lines">89, 104, 106, 115-117, 199, 230-231, 235-237, 244, 250, 256, 367, 441, 444</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/litellm_provider.py</span>
                    <span>32</span>
                    <span>1</span>
                    <span>31</span>
                    <span>96.88%</span>
                    <span class="source-lines">7, 9, 11-12, 18, 21, 37-38, 44, 46, 49, 51-52, 54-56, 66-67, 69-70, 73, 76, 78-79, 81-82, 84, 88, 94-95, 97</span>
                    <span class="source-lines">74</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/noop.py</span>
                    <span>13</span>
                    <span>0</span>
                    <span>13</span>
                    <span>100.0%</span>
                    <span class="source-lines">8, 10, 12-13, 20, 26, 32, 34, 50, 52, 58, 60, 66</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/ollama.py</span>
                    <span>43</span>
                    <span>1</span>
                    <span>42</span>
                    <span>97.67%</span>
                    <span class="source-lines">7, 9, 11-12, 18, 24, 40-41, 47, 50, 52, 54-55, 57-60, 62-63, 66-67, 71-72, 74-75, 77, 81, 87-88, 90-92, 96, 102, 104, 114, 116-117, 127, 132, 134-135</span>
                    <span class="source-lines">69</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/schemas.py</span>
                    <span>36</span>
                    <span>1</span>
                    <span>35</span>
                    <span>97.22%</span>
                    <span class="source-lines">8, 10-12, 16, 22, 38, 42-44, 46-47, 50-53, 55, 58-59, 62-65, 67-68, 77, 84, 90, 94-98, 102, 130</span>
                    <span class="source-lines">39</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/models.py</span>
                    <span>240</span>
                    <span>10</span>
                    <span>230</span>
                    <span>95.83%</span>
                    <span class="source-lines">17-18, 21, 24-25, 34-36, 38, 40, 47-48, 61-67, 69, 71, 82-83, 95-100, 102, 104, 109-115, 118-119, 141-157, 159, 161, 167-171, 173-182, 184, 186, 188-190, 193-194, 202-203, 205, 207, 213-214, 223-225, 227, 229, 233-235, 238-239, 248-250, 252, 254, 261-262, 271-273, 275, 277, 281-283, 286-287, 324-353, 355-360, 362, 364, 382-405, 407-419, 422-423, 437-445, 447, 449, 459, 461, 464-465, 482-492, 494, 500, 502, 508-512, 514, 516, 518, 520, 522</span>
                    <span class="source-lines">172, 183, 185, 187, 460, 513, 515, 517, 519, 521</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/options.py</span>
                    <span>117</span>
                    <span>45</span>
                    <span>72</span>
                    <span>61.54%</span>
                    <span class="source-lines">106, 146, 175, 178-180, 185-187, 193-195, 201-203, 209-218, 220, 224, 233, 248, 251-267, 270-283, 286-295, 298, 300</span>
                    <span class="source-lines">13-15, 21-22, 90-94, 97-99, 102-105, 122-123, 126-132, 135-137, 140-142, 145, 156-160, 163-164, 167, 169, 222, 227, 236</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/plugin.py</span>
                    <span>156</span>
                    <span>25</span>
                    <span>131</span>
                    <span>83.97%</span>
                    <span class="source-lines">40, 43, 49, 55, 61, 67, 73, 80, 89, 95, 101, 107, 113, 121, 126, 131, 136, 142, 147, 153, 169, 173, 177, 183-184, 187-188, 190, 192, 195-197, 203-204, 212-213, 238-239, 242-243, 246, 249-250, 252-253, 256-257, 259, 261-265, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 307, 309-314, 317-318, 322-323, 331-332, 337-340, 343, 345, 348-353, 355, 357, 365-366, 387-388, 391-392, 395-397, 408-409, 412, 415-416, 419-421, 431-432, 435-437, 448-449, 452, 455, 457-458</span>
                    <span class="source-lines">13, 15-17, 19-20, 22, 28-31, 34, 160, 216, 319, 327-328, 333-334, 379-380, 400, 424, 440-441</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/prompts.py</span>
                    <span>75</span>
                    <span>5</span>
                    <span>70</span>
                    <span>93.33%</span>
                    <span class="source-lines">13, 15-17, 24, 27, 33, 35, 49, 52, 55, 58-61, 63, 65, 67, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116, 118, 132-133, 135-138, 140-141, 144-145, 148, 151-152, 154-156, 158-159, 163, 165, 180, 182, 191-194</span>
                    <span class="source-lines">80, 114, 142, 146, 149</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/render.py</span>
                    <span>50</span>
                    <span>0</span>
                    <span>50</span>
                    <span>100.0%</span>
                    <span class="source-lines">13, 15-16, 18, 24, 30-31, 34, 40, 42, 50-51, 53, 56, 65-67, 70, 79, 87, 90, 99, 101-102, 107, 110, 121-124, 126-129, 131-134, 141-143, 145, 158-163, 177, 196</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/report_writer.py</span>
                    <span>167</span>
                    <span>10</span>
                    <span>157</span>
                    <span>94.01%</span>
                    <span class="source-lines">13, 15-25, 27-29, 46, 55, 58, 67-68, 76, 83-84, 89, 98-100, 102, 105-108, 110, 116, 127-128, 130, 142, 150, 156-158, 160, 186-189, 192, 197-199, 202-203, 211, 222-223, 226-227, 230-231, 233, 235, 254, 256-259, 262-264, 266, 268, 303, 312, 314-315, 317-328, 330, 332, 340, 343-345, 348-349, 352-354, 357, 360, 368, 376, 378-379, 382, 385, 388, 391, 399, 401-402, 408, 410, 412, 414-423, 434-435, 437-439, 447-448, 453, 455, 458, 461-462, 464, 470-474, 480-481, 488, 495, 497, 499-501, 503, 506-507, 509, 515-516</span>
                    <span class="source-lines">113, 135-137, 424-425, 432, 449-451</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/fs.py</span>
                    <span>34</span>
                    <span>3</span>
                    <span>31</span>
                    <span>91.18%</span>
                    <span class="source-lines">11, 13-14, 17, 30, 33, 36, 39, 42, 45, 55-56, 58-60, 63-64, 70, 79, 82, 100, 103, 111-113, 116-117, 119-121, 123</span>
                    <span class="source-lines">40, 65, 67</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/hashing.py</span>
                    <span>36</span>
                    <span>0</span>
                    <span>36</span>
                    <span>100.0%</span>
                    <span class="source-lines">12, 14-17, 23, 32, 35, 44-48, 51, 61, 64, 73-74, 76-78, 80-81, 86, 96, 103-104, 107, 113-114, 116-121</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/ranges.py</span>
                    <span>33</span>
                    <span>0</span>
                    <span>33</span>
                    <span>100.0%</span>
                    <span class="source-lines">12, 15, 29-30, 33, 35-37, 39-40, 42, 45-47, 50, 52, 55, 65-67, 70, 81-82, 84-91, 93, 95</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/time.py</span>
                    <span>16</span>
                    <span>0</span>
                    <span>16</span>
                    <span>100.0%</span>
                    <span class="source-lines">4, 6, 9, 15, 18, 27, 30, 39-44, 46-48</span>
                    <span class="source-lines">-</span>
                </div>
            </div>
        </section>
    </div>
</body>
</html>