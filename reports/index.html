<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Test Report &bull; 387 tests</title>
    <!-- Optional: Inter font from rsms.me CDN. Falls back to system fonts if unavailable. -->
    <link rel="stylesheet" href="https://rsms.me/inter/inter.css">
    <style>
/* Modern Color Palette */
:root {
    --bg-color: #f8fafc;
    --text-primary: #1e293b;
    --text-secondary: #64748b;
    --border-color: #e2e8f0;
    --card-bg: #ffffff;
    --surface-muted: #f1f5f9;
    --primary-color: #3b82f6;
    color-scheme: light dark;

    /* Status Colors */
    --passed-bg: #dcfce7;
    --passed-text: #166534;
    --failed-bg: #fee2e2;
    --failed-text: #991b1b;
    --skipped-bg: #fef9c3;
    --skipped-text: #854d0e;
    --xfailed-bg: #ffedd5;
    --xfailed-text: #9a3412;
    --xpassed-bg: #f3e8ff;
    --xpassed-text: #6b21a8;
    --error-bg: #fee2e2;
    --error-text: #991b1b;
}

body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    background-color: var(--bg-color);
    color: var(--text-primary);
    line-height: 1.5;
    margin: 0;
    padding: 0;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 2rem;
}

/* Header */
header {
    margin-bottom: 2rem;
    border-bottom: 1px solid var(--border-color);
    padding-bottom: 1rem;
    display: flex;
    justify-content: space-between;
    align-items: center;
}

h1 {
    font-size: 1.875rem;
    font-weight: 700;
    color: var(--text-primary);
    margin: 0;
}

.meta {
    font-size: 0.875rem;
    color: var(--text-secondary);
}

/* Summary Grid */
.summary {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
    gap: 1rem;
    margin-bottom: 2rem;
}

.summary-card {
    background: var(--card-bg);
    border-radius: 0.5rem;
    padding: 1.5rem;
    box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.1), 0 1px 2px -1px rgb(0 0 0 / 0.1);
    text-align: center;
    border: 1px solid var(--border-color);
    transition: transform 0.2s;
}

.summary-card:hover {
    transform: translateY(-2px);
}

.summary-card .count {
    font-size: 2.25rem;
    font-weight: 700;
    line-height: 1;
    margin-bottom: 0.5rem;
}

.summary-card .label {
    text-transform: uppercase;
    font-size: 0.75rem;
    font-weight: 600;
    letter-spacing: 0.05em;
    color: var(--text-secondary);
}

/* Status Colors for Summary */
.summary-card.passed .count {
    color: var(--passed-text);
}

.summary-card.failed .count {
    color: var(--failed-text);
}

.summary-card.skipped .count {
    color: var(--skipped-text);
}

.summary-card.xfailed .count {
    color: var(--xfailed-text);
}

.summary-card.xpassed .count {
    color: var(--xpassed-text);
}

.summary-card.coverage .count {
    color: var(--primary-color);
}

/* Filters */
.filters {
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.5rem;
    border: 1px solid var(--border-color);
    margin-bottom: 1.5rem;
    display: flex;
    flex-direction: column;
    gap: 0.75rem;
}

.filter-input {
    flex: 1;
    padding: 0.5rem 1rem;
    border: 1px solid var(--border-color);
    border-radius: 0.375rem;
    font-size: 0.875rem;
    background: var(--card-bg);
    color: var(--text-primary);
}

.filter-input::placeholder {
    color: var(--text-secondary);
}

.filter-statuses {
    display: flex;
    flex-wrap: wrap;
    gap: 0.5rem;
}

.filter-chip {
    display: inline-flex;
    align-items: center;
    gap: 0.35rem;
    padding: 0.25rem 0.75rem;
    border-radius: 9999px;
    border: 1px solid var(--border-color);
    background: var(--surface-muted);
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.04em;
}

.filter-chip input {
    margin: 0;
}

.filter-chip.passed {
    background: var(--passed-bg);
    color: var(--passed-text);
}

.filter-chip.failed {
    background: var(--failed-bg);
    color: var(--failed-text);
}

.filter-chip.skipped {
    background: var(--skipped-bg);
    color: var(--skipped-text);
}

.filter-chip.xfailed {
    background: var(--xfailed-bg);
    color: var(--xfailed-text);
}

.filter-chip.xpassed {
    background: var(--xpassed-bg);
    color: var(--xpassed-text);
}

.filter-chip.error {
    background: var(--error-bg);
    color: var(--error-text);
}

/* Test List */
.test-list {
    display: flex;
    flex-direction: column;
    gap: 0.75rem;
}

.test-row {
    background: var(--card-bg);
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
    overflow: hidden;
}

.test-header {
    padding: 1rem;
    display: flex;
    align-items: center;
    gap: 1rem;
    cursor: pointer;
    background: var(--card-bg);
}

.test-header:hover {
    background: var(--surface-muted);
}

.status-badge {
    padding: 0.25rem 0.75rem;
    border-radius: 9999px;
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
}

.status-passed {
    background: var(--passed-bg);
    color: var(--passed-text);
}

.status-failed {
    background: var(--failed-bg);
    color: var(--failed-text);
}

.status-skipped {
    background: var(--skipped-bg);
    color: var(--skipped-text);
}

.status-xfailed {
    background: var(--xfailed-bg);
    color: var(--xfailed-text);
}

.status-xpassed {
    background: var(--xpassed-bg);
    color: var(--xpassed-text);
}

.status-error {
    background: var(--error-bg);
    color: var(--error-text);
}

.test-name {
    flex: 1;
    font-family: monospace;
    font-size: 0.9rem;
    color: var(--text-primary);
    word-break: break-all;
}

.test-meta {
    display: flex;
    gap: 1rem;
    align-items: center;
    color: var(--text-secondary);
    font-size: 0.875rem;
}

/* Details Section */
.test-details {
    padding: 0 1rem 1rem 1rem;
    border-top: 1px solid var(--border-color);
    background: var(--surface-muted);
}

.detail-section {
    margin-top: 1rem;
}

.detail-title {
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
    color: var(--text-secondary);
    margin-bottom: 0.5rem;
}

.coverage-item {
    font-family: monospace;
    font-size: 0.85rem;
    padding: 0.25rem 0;
    border-bottom: 1px solid var(--border-color);
    display: grid;
    grid-template-columns: minmax(200px, 2fr) minmax(120px, 1fr);
    gap: 1rem;
}

.coverage-list {
    background: var(--card-bg);
    border-radius: 0.375rem;
    border: 1px solid var(--border-color);
    overflow: hidden;
}

.source-coverage {
    margin-top: 2rem;
}

.source-coverage h2 {
    margin: 0 0 1rem;
    font-size: 1.5rem;
}

.source-coverage-table {
    display: grid;
    gap: 0.35rem;
}

.source-coverage-header,
.source-coverage-row {
    display: grid;
    grid-template-columns: minmax(200px, 2fr) repeat(4, minmax(60px, 0.5fr)) minmax(
            140px,
            1fr
        ) minmax(140px, 1fr);
    align-items: center;
    gap: 0.75rem;
    padding: 0.75rem 1rem;
    border-radius: 0.5rem;
}

.source-coverage-header {
    background: var(--surface-muted);
    font-size: 0.75rem;
    font-weight: 700;
    text-transform: uppercase;
    letter-spacing: 0.04em;
    color: var(--text-secondary);
}

.source-coverage-row {
    background: var(--card-bg);
    border: 1px solid var(--border-color);
    font-size: 0.85rem;
}

.source-path {
    font-family: monospace;
    word-break: break-word;
}

.source-lines {
    font-family: monospace;
    color: var(--text-secondary);
    word-break: break-word;
}

.llm-annotation {
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.375rem;
    border: 1px solid var(--border-color);
}

.llm-annotation p {
    margin: 0 0 0.5rem 0;
}

.llm-annotation p:last-child {
    margin-bottom: 0;
}

.llm-annotation ul {
    margin: 0.5rem 0 0;
    padding-left: 1.25rem;
}

.llm-annotation li {
    margin-bottom: 0.25rem;
}

.error-message {
    font-family: monospace;
    color: var(--failed-text);
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.375rem;
    border: 1px solid var(--failed-bg);
    white-space: pre-wrap;
    overflow-x: auto;
}

/* HTML5 Progress Bar for Coverage */
progress {
    width: 60px;
}

/* Utility: Hidden state for filtering */
.hidden {
    display: none !important;
}

/* Dark Mode Support */
@media (prefers-color-scheme: dark) {
    :root {
        --bg-color: #0f172a;
        --text-primary: #f1f5f9;
        --text-secondary: #94a3b8;
        --border-color: #334155;
        --card-bg: #1e293b;
        --surface-muted: #0b1220;
        --primary-color: #60a5fa;

        /* Status Colors - Adjusted for dark mode */
        --passed-bg: #14532d;
        --passed-text: #86efac;
        --failed-bg: #7f1d1d;
        --failed-text: #fca5a5;
        --skipped-bg: #713f12;
        --skipped-text: #fde047;
        --xfailed-bg: #7c2d12;
        --xfailed-text: #fdba74;
        --error-bg: #7f1d1d;
        --error-text: #fca5a5;
    }

    /* Adjust box shadows for dark mode */
    .summary-card {
        box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.3), 0 1px 2px -1px rgb(0 0 0 / 0.3);
    }
}

@media print {
    body {
        background: #ffffff;
        color: #0f172a;
    }

    .container {
        max-width: none;
        padding: 1rem 1.5rem;
    }

    header {
        border-bottom: 2px solid var(--border-color);
    }

    .filters {
        display: none;
    }

    .summary-card,
    .test-row {
        box-shadow: none;
    }

    .test-header {
        background: #ffffff;
    }

    .test-row {
        page-break-inside: avoid;
        break-inside: avoid;
    }

    .test-details {
        background: #ffffff;
    }

    .llm-annotation {
        background: var(--surface-muted);
    }

    progress {
        width: 80px;
    }
}

body.pdf-mode .filters {
    display: none;
}

body.pdf-mode .test-row {
    page-break-inside: avoid;
    break-inside: avoid;
}    </style>
    <script>
// pytest-llm-report interactive features

// Global state for filters
const activeStatuses = new Set(['passed', 'failed', 'skipped', 'xfailed', 'xpassed', 'error']);

// Filter tests based on search input and outcome filters
function filterTests() {
    const query = document.getElementById('searchInput').value.toLowerCase();
    document.querySelectorAll('.test-row').forEach(row => {
        const nodeid = row.querySelector('.test-name').textContent.toLowerCase();
        const statusMatch = row.dataset.status ? activeStatuses.has(row.dataset.status) : false;
        const matchesSearch = nodeid.includes(query);
        row.classList.toggle('hidden', !matchesSearch || !statusMatch);
    });
}

// Toggle visibility of status filters
function toggleStatus(checkbox) {
    const status = checkbox.dataset.status;
    if (checkbox.checked) {
        activeStatuses.add(status);
    } else {
        activeStatuses.delete(status);
    }
    filterTests();
}

// Initialize interactive features after DOM is ready
document.addEventListener('DOMContentLoaded', function () {
    'use strict';

    // Toggle dark mode on preference
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.documentElement.dataset.theme = 'dark';
    }

    // Default: expand all details
    document.querySelectorAll('details').forEach(details => {
        details.setAttribute('open', '');
    });

    const params = new URLSearchParams(window.location.search);
    if (params.get('pdf') === '1') {
        document.body.classList.add('pdf-mode');
    }
});    </script>
</head>
<body>
    <div class="container">
        <header>
            <div>
                <h1>Test Report</h1>
                <div class="meta">
                    Run ID: 20979585481-py3.12 &bull;
                    Generated: 2026-01-14 02:02:28 &bull;
                    Duration: 34.63s<br>
                    <strong>Plugin:</strong> v0.1.0
                        (2f498263985a34902252c53c11fb820445bd8f21)
[dirty]<br>
                    <strong>Repo:</strong> v0.1.1
                        (f1a4e8ffe140b3685a0975f68db012dfba4d6df8)
[dirty]<br>
                    <strong>LLM:</strong> ollama / llama3.2:1b
                        (minimal context,
                         382 annotated, 4 errors)
                </div>
            </div>
            <div style="text-align: right">
                <div style="font-size: 2rem; font-weight: 700; color: var(--primary-color)">
                    92.91%
                </div>
                <div class="meta">Total Coverage</div>
            </div>
        </header>

        <!-- Summary Cards -->
        <div class="summary">
            <div class="summary-card">
                <div class="count">387</div>
                <div class="label">Total Tests</div>
            </div>
            <div class="summary-card passed">
                <div class="count">387</div>
                <div class="label">Passed</div>
            </div>
            <div class="summary-card failed">
                <div class="count">0</div>
                <div class="label">Failed</div>
            </div>
            <div class="summary-card skipped">
                <div class="count">0</div>
                <div class="label">Skipped</div>
            </div>
            <div class="summary-card xfailed">
                <div class="count">0</div>
                <div class="label">XFailed</div>
            </div>
            <div class="summary-card xpassed">
                <div class="count">0</div>
                <div class="label">XPassed</div>
            </div>
            <div class="summary-card failed">
                <div class="count">0</div>
                <div class="label">Errors</div>
            </div>
        </div>

        <!-- Filters -->
        <div class="filters">
            <input type="text" id="searchInput" class="filter-input" placeholder="Search tests..." onkeyup="filterTests()">
            <div class="filter-statuses" aria-label="Filter by status">
                <label class="filter-chip passed">
                    <input type="checkbox" data-status="passed" checked onchange="toggleStatus(this)">
                    Passed
                </label>
                <label class="filter-chip failed">
                    <input type="checkbox" data-status="failed" checked onchange="toggleStatus(this)">
                    Failed
                </label>
                <label class="filter-chip skipped">
                    <input type="checkbox" data-status="skipped" checked onchange="toggleStatus(this)">
                    Skipped
                </label>
                <label class="filter-chip xfailed">
                    <input type="checkbox" data-status="xfailed" checked onchange="toggleStatus(this)">
                    XFailed
                </label>
                <label class="filter-chip xpassed">
                    <input type="checkbox" data-status="xpassed" checked onchange="toggleStatus(this)">
                    XPassed
                </label>
                <label class="filter-chip error">
                    <input type="checkbox" data-status="error" checked onchange="toggleStatus(this)">
                    Error
                </label>
            </div>
        </div>

        <!-- Test List -->
        <div class="test-list">
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_all_policy</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the aggregate function correctly aggregates all policy when there are multiple test cases.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the aggregate function may incorrectly retain only one of the test cases when there are multiple.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The number of tests in the result should be equal to the number of retained test cases.</li>
                                        <li>Each retained test case should have an outcome of 'passed'.</li>
                                        <li>All retained test cases should have a duration of 0.1 seconds.</li>
                                        <li>All retained test cases should belong to the same phase as the first test case.</li>
                                        <li>The aggregate function should correctly handle multiple test cases without retaining only one.</li>
                                        <li>The aggregate function should not retain any test cases when there are no matching tests in the input reports.</li>
                                        <li>The aggregate function should preserve the original order of the test cases within each report.</li>
                                        <li>Each retained test case should have a unique node ID that matches its parent test case.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 52, 55-56, 59, 61-63, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134, 141, 146, 148-153, 155, 157-159, 170, 217, 219-223, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_dir_not_exists</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The aggregate function should not be called when the directory does not exist.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the aggregate function fails to work correctly when the input directory does not exist.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>aggregator.aggregate() is None</li>
                                        <li>pathlib.Path.exists(aggregator.aggregate()) is False</li>
                                        <li>len(aggregator.aggregate()) == 0</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 52, 55-57, 109-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_latest_policy</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the test_aggregate_latest_policy function correctly picks the latest policy for a given test case.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in cases where two reports with different outcomes are run on the same test at different times.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The outcome of the aggregated report should be 'passed' when the last report has an 'outcome' of 'failed'.</li>
                                        <li>The number of tests in the aggregated report should be 1.</li>
                                        <li>The outcome of each individual test in the aggregated report should match the outcome of the latest report.</li>
                                        <li>The run meta should indicate that this is an aggregated run.</li>
                                        <li>The run meta should contain a count of 2 runs.</li>
                                        <li>The summary should have passed and failed counts equal to the number of tests in the aggregated report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">77 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134, 141, 146, 148-153, 155, 157-159, 170, 182, 184-188, 190-191, 194, 217, 219-223, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_no_dir_configured</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the aggregate function returns None when no directory configuration is provided.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the aggregate function throws an error or raises an exception without providing a valid directory path.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The aggregate function should return None when `aggregate_dir` is set to `None`.</li>
                                        <li>The aggregate function should not throw any exceptions or errors when `aggregate_dir` is set to `None`.</li>
                                        <li>The aggregate function should raise an error with a meaningful message when `aggregate_dir` is set to `None` and the directory does not exist.</li>
                                        <li>The aggregate function should raise an error with a meaningful message when `aggregate_dir` is set to `None` and the directory path is invalid.</li>
                                        <li>The aggregate function should not throw any exceptions or errors when `aggregate_dir` is set to `None` and the directory exists.</li>
                                        <li>The aggregate function should return None when `aggregate_dir` is set to `None` and the directory path is empty.</li>
                                        <li>The aggregate function should raise an error with a meaningful message when `aggregate_dir` is set to `None` and the directory path is not a valid file system path.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 44, 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_no_reports</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the aggregate function when no reports are provided</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where an empty report is returned without any errors or warnings</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregate` method should return `None` instead of an empty list when no reports are available.</li>
                                        <li>An error message should not be printed to the console.</li>
                                        <li>A warning message should not be printed to the console.</li>
                                        <li>The function should handle the case where the input is invalid or empty correctly.</li>
                                        <li>The function should not throw any exceptions when called with a non-empty report.</li>
                                        <li>The function should return an empty list by default when no reports are provided.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52, 55-57, 109-110, 113-114, 170)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_with_coverage_and_llm_annotations</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that coverage and LLM annotations are properly deserialized and can be re-serialized.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in core functionality</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>coverage: Assert A, Assert B, Assert C were correctly deserialized from JSON.</li>
                                        <li>llm_annotation: The scenario, why_needed, key_assertions, and confidence of the LLM annotation were correctly set.</li>
                                        <li>Serialization: The test can be re-serialized with the same coverage and LLM annotations.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">81 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134-137, 141-144, 146, 148-153, 155, 157-159, 170, 182, 184-188, 194, 217, 219-223, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">32 lines (ranges: 40-43, 104-107, 109-111, 113, 115, 161-165, 167, 169, 171, 173, 176-180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_with_source_coverage</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `aggregate` method returns a single `SourceCoverageEntry` for each source file.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where multiple source files are aggregated into one entry, leading to incorrect coverage summaries.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `source_coverage` attribute of the aggregated report should contain exactly one `SourceCoverageEntry`.</li>
                                        <li>Each `SourceCoverageEntry` in the `source_coverage` list should have a `file_path` attribute matching the source file path.</li>
                                        <li>All statements, missed, covered, and coverage percentages in each `SourceCoverageEntry` should be accurate and consistent with the original report.</li>
                                        <li>The `covered_ranges` and `missed_ranges` attributes of each `SourceCoverageEntry` should match the corresponding ranges from the original report.</li>
                                        <li>The `coverage_percent` attribute of each `SourceCoverageEntry` should be a valid percentage between 0 and 100.</li>
                                        <li>All statements, missed, covered, and coverage percentages in all `SourceCoverageEntries` should add up to exactly 100% (or match the original report's total coverage).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">66 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 148-155, 157-159, 170, 182, 184-186, 194, 217, 219-220, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_load_coverage_from_source</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loading coverage from configured source file when option is not set.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the test fails due to an unexpected behavior of the _load_coverage_from_source() method without a configured source file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function does not raise any exceptions when the aggregator.config.llm_coverage_source is None.</li>
                                        <li>The function does not raise any UserWarning when the aggregator.config.llm_coverage_source is '/nonexistent/coverage'.</li>
                                        <li>The function correctly loads coverage data from a mock .coverage file and returns it as a list of SourceCoverageEntry objects.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 245-246, 248-249, 251, 253-257, 259, 262-263, 265-266, 269-271, 273)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_recalculate_summary</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test `test_recalculate_summary` verifies that the aggregator recalculates the latest summary correctly when new test results are added.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the aggregator's ability to handle a large number of tests and recalculate the summary accurately.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total count of tests is updated correctly after adding new test results.</li>
                                        <li>The passed count remains unchanged despite new failed or skipped tests.</li>
                                        <li>The failed count increases by one, as expected.</li>
                                        <li>The skipped count remains unchanged.</li>
                                        <li>The xfailed count also increases by one, as expected.</li>
                                        <li>The xpassed count decreases by one, as expected.</li>
                                        <li>The error count is updated correctly.</li>
                                        <li>The coverage percentage is preserved and remains at 85.5% after recalculating the summary.</li>
                                        <li>The total duration of all tests is updated accurately.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 217, 219-233, 235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_skips_invalid_json</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case verifies that skipping an invalid JSON file prevents a regression.</p>
                                <p><strong>Why Needed:</strong> This test prevents the regression where the aggregator incorrectly counts all reports, including those with missing fields or invalid JSON.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregate` method should not count any report if it contains missing fields or is marked as invalid.</li>
                                        <li>The `run_meta.run_count` attribute of the aggregated result should be set to 1 instead of the total number of reports.</li>
                                        <li>The aggregator should raise a warning when encountering an invalid JSON file, indicating that it's skipping this file.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">71 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119-120, 125, 127-128, 148-153, 155, 157-159, 162, 164-166, 168, 170, 182, 184-186, 194, 217, 219-220, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation_maximal.py::TestAggregationMaximal::test_recalculate_summary_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the aggregator recalculates the summary correctly when given a set of tests with varying durations and coverage totals.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the aggregator fails to calculate the summary for tests with different durations or coverage totals.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total duration of all tests should be equal to the latest summary's total duration.</li>
                                        <li>At least one test should have a passed status and at least one test should have a failed status.</li>
                                        <li>The coverage total percent should match the latest summary's coverage total percent.</li>
                                        <li>The total number of tests should be greater than or equal to 2 (the number of tests provided in the test case).</li>
                                        <li>All test durations should be non-negative.</li>
                                        <li>All test coverage totals should be non-negative and add up to at least 100% (the maximum possible coverage total).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 44, 217, 219-225, 235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_cached_tests_are_skipped</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the caching of tests to prevent skipping them</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where cached tests are skipped unexpectedly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `mock_provider` is not called with any arguments when it's used in the test.</li>
                                        <li>The `mock_cache` is not set to None after being used in the test.</li>
                                        <li>The `mock_assembler` is not called with any arguments when it's used in the test.</li>
                                        <li>The cache is cleared before the next test call, preventing cached tests from being skipped.</li>
                                        <li>The mock provider returns a valid result for the test.</li>
                                        <li>The mock cache is set to None after being used in the test.</li>
                                        <li>The mock assembler does not throw an exception when called with arguments.</li>
                                        <li>The test logs a message indicating that no cached tests were skipped.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-81, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing concurrent annotation with multiple providers and caches</p>
                                <p><strong>Why Needed:</strong> Prevents potential performance issues due to concurrent access to shared resources.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocking the provider and cache instances ensures they are not accessed simultaneously, preventing race conditions.</li>
                                        <li>Verifying that the assembler instance is not accessed concurrently allows for efficient annotation processing.</li>
                                        <li>Ensuring that all mock objects are properly cleaned up after use prevents memory leaks.</li>
                                        <li>Testing that the annotator function can handle concurrent calls without significant performance degradation.</li>
                                        <li>Validating that the cache is properly initialized and updated before annotation processing begins.</li>
                                        <li>Confirming that the assembler instance is properly set up for concurrent annotation execution.</li>
                                        <li>Checking that the annotator function can handle multiple provider and cache instances concurrently without issues.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">64 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104-112, 129-135, 137, 139, 229-232, 234, 236-237, 239, 245-246, 248-253, 255, 261, 266)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation_handles_failures</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test concurrent annotation handles failures to verify the correctness of annotated data.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where concurrent annotations fail to handle failures in the annotator, leading to incorrect results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotator correctly handles failures by re-annotating the data with a new annotation.</li>
                                        <li>The annotator does not annotate the same data multiple times if it fails.</li>
                                        <li>The annotator correctly handles failures when the annotator is not available or has an error.</li>
                                        <li>The annotator logs the failure and continues processing other tasks without interruption.</li>
                                        <li>The annotator re-annotates the failed data with a new annotation after some time.</li>
                                        <li>The annotator does not annotate the same data multiple times if it fails within a certain time limit.</li>
                                        <li>The annotator correctly handles failures when the annotator is running in parallel with other tasks.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104-112, 129-135, 137-139, 229-232, 234, 236-237, 239, 245-246, 248-253, 255, 261-264, 266)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_progress_reporting</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the progress reporting function is called with correct arguments.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the progress reporting function is not called correctly or at all.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_provider is called with the correct mock object</li>
                                        <li>mock_cache is called with the correct mock object</li>
                                        <li>mock_assembler is called with the correct mock object</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation</span>
                        <div class="test-meta">
                            <span>12.00s</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing sequential annotation with multiple providers and cache.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case of concurrent requests or caching issues.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocking the provider and cache instance should not affect the annotator's behavior.</li>
                                        <li>The annotator should be able to handle sequential requests correctly without any issues.</li>
                                        <li>The cache should not interfere with the annotation process for sequential requests.</li>
                                        <li>The annotator should return the expected result even when multiple providers are used concurrently.</li>
                                        <li>The annotator should ignore the cache for sequential requests and focus on the provider's response.</li>
                                        <li>The annotator should be able to handle different types of providers (e.g., mock, real) without any issues.</li>
                                        <li>The annotator should not throw any exceptions when dealing with concurrent requests or caching issues.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_skips_if_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `test_skips_if_disabled` test verifies that the annotator does not perform any actions when the LLM (Language Model) is disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the annotator would incorrectly skip tests or annotations due to the LLM being disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config` object is created with the 'none' provider.</li>
                                        <li>An empty list `annotate_tests` is passed to the `annotate` function.</li>
                                        <li>No annotation is performed on any test.</li>
                                        <li>The annotator does not skip any tests or annotations.</li>
                                        <li>The LLM is properly disabled without affecting the annotator's behavior.</li>
                                        <li>The annotator performs its normal workflow when the LLM is enabled.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 45-46)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_skips_if_provider_unavailable</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the annotator skips annotation if the provider is unavailable.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the annotator fails to skip annotation when the provider is not available.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mock Provider Mock</li>
                                        <li>Skip Annotation</li>
                                        <li>No Output (no annotation)</li>
                                        <li>Provider Not Available</li>
                                        <li>Annotation Skipping</li>
                                        <li>No Error Message</li>
                                        <li>No Exception Raised</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_concurrent_with_progress_and_errors</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the annotator reports progress and the first error in concurrent mode.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when annotating multiple tests concurrently with LLM Max concurrency.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that the `annotate_concurrent` function reports both a successful annotation (2 tasks) and an error (1 task),</li>
                                        <li>Check if the first error is reported correctly (contains 'first error')</li>
                                        <li>Ensure that at least one progress message is generated ('Processing 2 test(s)' or 'LLM annotation')</li>
                                        <li>Verify that the `annotate_concurrent` function does not return all annotations immediately, but instead waits for completion and reports errors first</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 229-232, 234, 236-237, 239-242, 245-246, 248-253, 255-258, 261-264, 266)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_sequential_rate_limit_wait</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Should wait if rate limit interval has not elapsed.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in sequential annotation tasks where the rate limit interval does not have elapsed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocked time.sleep was called with a delay of 0.1s but the expected rate limit interval (1.0s) was not met.</li>
                                        <li>The mock_time.side_effect was set to [100.0, 100.1, 100.2, 100.3, 100.4] which is slower than the expected rate limit interval of 1.0s.</li>
                                        <li>The provider.get_rate_limits.return_value was called with None but it should have returned a valid rate limits object.</li>
                                        <li>The provider.annotate.return_value was called with LlmAnnotation but it should not have been called yet because the rate limit interval has not elapsed.</li>
                                        <li>The mock_time.side_effect was set to [100.0, 100.1, 100.2, 100.3, 100.4] which is slower than the expected rate limit interval of 1.0s.</li>
                                        <li>The provider.is_local.return_value was called with False but it should have been True because the task is running on a remote server.</li>
                                        <li>The provider.get_rate_limits.return_value was called with None but it should have returned a valid rate limits object.</li>
                                        <li>The provider.annotate.return_value was called with LlmAnnotation but it should not have been called yet because the rate limit interval has not elapsed.</li>
                                        <li>The mock_time.side_effect was set to [100.0, 100.1, 100.2, 100.3, 100.4] which is slower than the expected rate limit interval of 1.0s.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">23 lines (ranges: 165-168, 170-171, 173-174, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_tests_cached_progress</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the annotator reports progress when caching tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where cached tests are not reported with their progress.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that any progress messages contain '(cache): test_cached'.</li>
                                        <li>Check if the 'test_cached' scenario is included in any progress message.</li>
                                        <li>Ensure that the progress message does not contain '(no cache)'.</li>
                                        <li>Verify that the progress message contains a string starting with '(cache)'.</li>
                                        <li>Confirm that the progress message includes the correct context ('src').</li>
                                        <li>Check if the progress message includes the expected outcome ('passed') for the test_cached scenario.</li>
                                        <li>Verify that any progress messages are appended to the list 'progress_msgs'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">37 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-84, 97-98, 100, 127, 129-135, 137, 139)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_tests_provider_unavailable</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the annotator throws an error when the test provider is not available.</p>
                                <p><strong>Why Needed:</strong> To prevent a potential error when the test provider is unavailable, this test verifies that the annotator correctly handles the situation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mocks.get_provider().is_available() must be called with False as its argument</li>
                                        <li>mocks.get_provider().is_available() should return False</li>
                                        <li>mocks.get_provider().get_provider() should throw an exception</li>
                                        <li>mocks.get_provider().get_provider().should raise a CalledError</li>
                                        <li>mocks.get_provider().is_available().should not be called again after the first call</li>
                                        <li>mocks.get_provider().is_available().should return False immediately</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_coverage_v2.py::test_base_parse_response_malformed_json_after_extract</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `test_base_parse_response_malformed_json_after_extract` function checks if the extracted JSON from a malformed JSON string is invalid.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function returns incorrect results when encountering an invalid JSON string after extracting it from a response.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `annotation.error` attribute should be set to 'Failed to parse LLM response as JSON'.</li>
                                        <li>The `annotation.json` attribute should be None or empty.</li>
                                        <li>The `annotation.content` attribute should contain the invalid JSON string.</li>
                                        <li>The `annotation.type` attribute should be 'Error'.</li>
                                        <li>The `annotation.message` attribute should contain a clear error message indicating that the response cannot be parsed as JSON.</li>
                                        <li>The `annotation.errors` attribute should not be empty, if any errors are present.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 186-187, 190-191, 194-195, 220-221)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_coverage_v2.py::test_base_parse_response_non_string_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests for non-string fields in response data.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the base parsing function does not handle non-string fields correctly, potentially leading to incorrect results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The correct scenario value is '123'.</li>
                                        <li>The correct why needed list item is ['list'].</li>
                                        <li>The correct key assertion is 'a'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203-207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_gemini_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `get_gemini_provider` function returns a `GeminiProvider` instance.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of changes to the `gemini` provider configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider(config)` returns an instance of `GeminiProvider`.</li>
                                        <li>The returned value is not None.</li>
                                        <li>The returned value is a valid instance of `GeminiProvider`.</li>
                                        <li>The `provider` attribute of the returned value is set to 'gemini'.</li>
                                        <li>The `config` parameter passed to `get_provider(config)` has a non-empty string value for the `provider` field.</li>
                                        <li>The `config` parameter passed to `get_provider(config)` does not contain any invalid values.</li>
                                        <li>A valid provider configuration can be created using the `Config` class with a valid `provider` argument.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 245, 247, 249, 252, 257, 262-263, 265)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_invalid_provider</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that a ValueError is raised when an unknown LLM provider is specified in the configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where an invalid provider is used without raising an error.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider` raises a `ValueError` with the message 'Unknown LLM provider: invalid'.</li>
                                        <li>The `pytest.raises()` matcher ensures that the exception type is `ValueError`.</li>
                                        <li>The `match` parameter specifies the exact error message to match.</li>
                                        <li>The `invalid` value passed as the `provider` argument should be an unknown LLM provider.</li>
                                        <li>The `get_provider(config)` function call should raise a `ValueError` with the specified error message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 245, 247, 249, 252, 257, 262, 267)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_litellm_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `get_litellm_provider` function returns a LitELLMProvider instance.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an incorrect provider type.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_litellm_provider` is called with a Config object specifying 'litellm' as the provider.</li>
                                        <li>The returned value of `get_litellm_provider` is checked to be an instance of LitELLMProvider.</li>
                                        <li>An assertion error is raised if `get_litellm_provider` returns something other than a LitELLMProvider instance.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52-53, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_noop_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that a NoopProvider is returned when the 'provider' parameter is set to 'none'</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where a non-existent provider is used.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_provider` function returns an instance of `NoopProvider` when the `config.provider` attribute is set to `'none'`.</li>
                                        <li>The `provider` variable holds an instance of `NoopProvider` after calling `get_provider(config)`.</li>
                                        <li>The `isinstance(provider, NoopProvider)` assertion checks if the returned provider is indeed an instance of `NoopProvider`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_ollama_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `get_ollama_provider` function returns an instance of OllamaProvider.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `OllamaProvider` class is not properly instantiated due to missing dependencies.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` parameter passed to `get_provider(config)` should be an instance of `OllamaProvider`.</li>
                                        <li>The `get_provider(config)` function should return an instance of `OllamaProvider`.</li>
                                        <li>The `isinstance(provider, OllamaProvider)` assertion should pass if the returned provider is indeed an instance of `OllamaProvider`.</li>
                                        <li>The test should fail with a meaningful error message when the `provider` parameter is not an instance of `OllamaProvider`.</li>
                                        <li>The `get_provider(config)` function should raise a `TypeError` when passed an invalid `provider` type.</li>
                                        <li>The `config` object passed to `get_provider(config)` should have a valid `provider` attribute.</li>
                                        <li>The test should only fail if the `provider` parameter is not provided at all.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 245, 247, 249, 252-253, 255)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_available_caches_result</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the test provides a valid LLM provider with available caches.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case of an unavailable cache.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `is_available()` method returns True for both providers.</li>
                                        <li>The `checks` attribute is incremented by one each time `_check_availability()` is called.</li>
                                        <li>There are no assertions made on the provider outside of these methods.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 107-108, 110-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_model_name_defaults_to_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_model_name()` method of a `ConcreteProvider` instance returns the default model name specified in the configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the model name defaults to 'test-model' when no configuration is provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert provider.get_model_name() == 'test-model'</li>
                                        <li>provider.config.model should be set to 'test-model'</li>
                                        <li>provider.config.model is not empty</li>
                                        <li>provider.config.model does not contain any other model names</li>
                                        <li>provider.config.model is a string</li>
                                        <li>provider.config.model is a valid Python identifier</li>
                                        <li>provider.get_model_name() is called with the correct configuration</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 136)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_rate_limits_defaults_to_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_rate_limits` method of a `ConcreteProvider` instance returns `None` when no rate limits are specified.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where defaulting to `None` without explicit rate limits in the configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_rate_limits()` method should return `None` for an empty or unspecified configuration.</li>
                                        <li>The `get_rate_limits()` method should not raise an exception when no rate limits are specified.</li>
                                        <li>The `get_rate_limits()` method should correctly handle cases where the provider is created with a valid configuration but no rate limits are specified.</li>
                                        <li>The `get_rate_limits()` method should return `None` for a valid configuration that includes explicit rate limits.</li>
                                        <li>The `get_rate_limits()` method should not throw an exception when the provider has default rate limits.</li>
                                        <li>The `get_rate_limits()` method should correctly handle cases where multiple providers are created with different configurations and no rate limits are specified.</li>
                                        <li>The `get_rate_limits()` method should return `None` for a configuration that includes both explicit and default rate limits.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 128)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_is_local_defaults_to_false</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that `is_local()` returns False when the default is set to false.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where `is_local()` returns True when the default is set to true.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider.is_local()` method should return `False` for the default value.</li>
                                        <li>The `provider.is_local()` method should not be affected by the `local_defaults` configuration option.</li>
                                        <li>The `provider.is_local()` method should not return True when the `local_defaults` configuration option is set to false.</li>
                                        <li>The `provider.is_local()` method should raise an error when the default value is set to true and local defaults are disabled.</li>
                                        <li>The `is_local()` method should be able to distinguish between different configurations (e.g., local vs. non-local)</li>
                                        <li>The `is_local()` method should not return a cached result for the same configuration</li>
                                        <li>The `provider.is_local()` method should be able to handle cases where `local_defaults` is set to false but `use_default` is still enabled</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestHashSource::test_consistent_hash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'hash_source' function is called with the same source code, and it returns the same hash value.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where different source codes produce different hashes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>source = "def test_foo(): pass"</li>
                                        <li>assert hash_source(source) == hash_source(source)</li>
                                        <li>expected_hash_value = hash_source(source)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestHashSource::test_different_source_different_hash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the cache with different source functions.</p>
                                <p><strong>Why Needed:</strong> Prevents a bug where two different source functions could potentially produce the same hash value, leading to unexpected behavior or incorrect results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `hash_source` should return a different hash value for two different source functions.</li>
                                        <li>Different source functions should not have the same hash value.</li>
                                        <li>The cache should store and retrieve data correctly even when using different source functions.</li>
                                        <li>The test should fail if the function `hash_source` is modified to always produce the same hash value.</li>
                                        <li>Using different source functions in the test case should result in a different output.</li>
                                        <li>The cache should be able to handle multiple source functions without any issues.</li>
                                        <li>Using the same source function for both tests should not affect the outcome of the test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestHashSource::test_hash_length</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the length of the hash generated by HashSource.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the hash is too short, potentially leading to incorrect caching decisions.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The hash should be at least 16 characters long.</li>
                                        <li>The hash should not be shorter than 8 characters (assuming a minimum length of 4 for simplicity).</li>
                                        <li>The hash should not have any leading zeros.</li>
                                        <li>The hash should not contain any non-alphanumeric characters.</li>
                                        <li>The hash should not be a palindrome (e.g., 'aabbcc').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_clear</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `clear` method of LlmCache to ensure it correctly removes all cache entries.</p>
                                <p><strong>Why Needed:</strong> The test prevents a potential bug where some cache entries are not properly cleared when the cache is reset.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Ensure that the clear method returns the correct number of cache entries (2 in this case).</li>
                                        <li>Verify that the `get` method returns None for both 'test::a' and 'test::b'.</li>
                                        <li>Check if the cache is properly cleared by verifying its size before and after clearing.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 129, 132-136, 141)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_does_not_cache_errors</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that annotations with errors are not cached.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where an error-cased annotation would be cached and later retrieved, potentially causing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'test::foo' cache key should remain empty after setting the annotation.</li>
                                        <li>The 'test::foo' cache value should not match the original annotation.</li>
                                        <li>The 'test::foo' cache result should be None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 39-41, 53, 55-56, 86, 88, 118-119, 121)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_get_missing</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case 'test_get_missing' verifies that the function returns None for missing entries in the cache.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not return an error or raise an exception when trying to access a non-existent key in the cache.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return `None` when trying to access a non-existent key in the cache.</li>
                                        <li>The function should throw an exception with a meaningful message when trying to access a non-existent key in the cache.</li>
                                        <li>The function should raise a KeyError with a meaningful message when trying to access a non-existent key in the cache.</li>
                                        <li>The function should not return any value (i.e., `None`) when trying to access a non-existent key in the cache.</li>
                                        <li>The function should throw an exception with a specific error message when trying to access a non-existent key in the cache.</li>
                                        <li>The function should raise a KeyError with a specific error message when trying to access a non-existent key in the cache.</li>
                                        <li>The function should not raise any exceptions when trying to access a non-existent key in the cache.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 39-41, 53, 55-56, 118-119, 121)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_set_and_get</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that annotations are stored and retrieved from the cache correctly.</p>
                                <p><strong>Why Needed:</strong> Prevents bypass by ensuring annotations are cached before they are used in subsequent requests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>- Check if annotation is set successfully -</li>
                                        <li>- Check if annotation has the correct confidence level -</li>
                                        <li>- Verify that the same annotation can be retrieved multiple times with the same key -</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 39-41, 53, 55, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorCollectionErrors::test_collection_error_structure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that a collection error has the correct node ID and message.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where a collection error is incorrectly identified or reported.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert error.nodeid == 'test_bad.py'</li>
                                        <li>assert error.message == 'SyntaxError'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorCollectionErrors::test_get_collection_errors_initially_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that an empty collection is returned when the `get_collection_errors` method is called on a newly created `TestCollector` instance.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where an empty list is returned when there are no errors in the collection, potentially causing unexpected behavior or errors downstream.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_collection_errors()` method should return an empty list.</li>
                                        <li>No errors should be present in the collection initially.</li>
                                        <li>An error message or other indication of collection content should not be present.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_context_override_default_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the default llm_context_override is set to None for a test case.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when using LLM context override with default value of None.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The llm_context_override attribute of TestCaseResult nodeid='test.py::test_foo' should be None.</li>
                                        <li>The llm_context_override attribute of TestCaseResult nodeid='test.py::test_foo' is not set to None.</li>
                                        <li>The llm_context_override attribute of TestCaseResult nodeid='test.py::test_foo' is set to a different value than None.</li>
                                        <li>The llm_context_override attribute of TestCaseResult nodeid='test.py::test_foo' should be an instance of TestCaseResult.</li>
                                        <li>The llm_context_override attribute of TestCaseResult nodeid='test.py::test_foo' should not be None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_opt_out_default_false</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the default value of llm_opt_out is set to False for a test case with passed outcome.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the default value of llm_opt_out might be incorrectly set to True due to a change in the test environment or configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The llm_opt_out attribute of TestCaseResult is checked to be False.</li>
                                        <li>The llm_opt_out property of TestCaseResult is asserted to be False.</li>
                                        <li>The value of llm_opt_out is compared with False using the assert method.</li>
                                        <li>The TestCaseResult object passed into the test_result function has an llm_opt_out attribute that is set to False.</li>
                                        <li>A TestCaseResult object created with a passed outcome and default llm_opt_out value has its llm_opt_out attribute checked to be False.</li>
                                        <li>The llm_opt_out property of a TestCaseResult object created with a passed outcome and default llm_opt_out value is asserted to be False.</li>
                                        <li>The llm_opt_out attribute of a TestCaseResult object created with a passed outcome and default llm_opt_out value is compared with False using the assert method.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorOutputCapture::test_capture_disabled_by_default</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `capture` feature is not enabled by default.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the output capture feature is unintentionally enabled by default, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert config.capture_failed_output is False</li>
                                        <li>assert isinstance(config, Config)</li>
                                        <li>assert 'capture' in config.__dict__</li>
                                        <li>assert not hasattr(config, '_capture_enabled')</li>
                                        <li>assert config._capture_enabled is False</li>
                                        <li>assert config.capture_failed_output == False</li>
                                        <li>assert config.output_capture_type == 'disabled'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorOutputCapture::test_capture_max_chars_default</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies the default value of `capture_output_max_chars` in the `Config` class.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the default max chars is not set correctly, leading to unexpected output or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert config.capture_output_max_chars == 4000</li>
                                        <li>assert isinstance(config.capture_output_max_chars, int)</li>
                                        <li>config.capture_output_max_chars >= 1</li>
                                        <li>config.capture_output_max_chars <= 10000</li>
                                        <li>config.capture_output_max_chars != 4000</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorXfailHandling::test_xfail_failed_is_xfailed</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'xfail failures should be recorded as xfailed' verifies that failed test cases are correctly marked as such.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where a failed test case is incorrectly marked as passed instead of being marked as xfailed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `results` dictionary contains the correct key-value pairs for the failed test case.</li>
                                        <li>The `outcome` attribute of the `result` object matches 'xfailed' according to the expected behavior.</li>
                                        <li>The `wasxfail` attribute is set to 'expected failure' as expected.</li>
                                        <li>The `duration` and `longrepr` attributes are correctly initialized with default values.</li>
                                        <li>The `passed`, `failed`, `skipped` attributes match their expected values for a failed test case.</li>
                                        <li>The `nodeid` key matches the expected location of the failed test case in the `results` dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212, 216, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorXfailHandling::test_xfail_passed_is_xpassed</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'xfail passes should be recorded as xpassed' verifies that when an xfail is passed, it is correctly marked as xpassed in the test results.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where an expected failure is not properly recorded as a failed test.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'when' field of the report should be set to 'call'.</li>
                                        <li>The 'passed' field of the report should be set to True.</li>
                                        <li>The 'failed' and 'skipped' fields of the report should both be False.</li>
                                        <li>The duration of the test run should be 0.01 seconds or less.</li>
                                        <li>The longrepr field should be an empty string.</li>
                                        <li>The wasxfail field should be set to 'expected failure'.</li>
                                        <li>The outcome of the test result should be 'xpassed'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212-214)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestTestCollector::test_create_collector</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `create_collector` method of `TestCollector` class.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `TestCollector` instance is created with an empty configuration, leading to incorrect results or behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `results` attribute of the `collector` object should be an empty dictionary.</li>
                                        <li>The `collection_errors` list should be an empty list.</li>
                                        <li>The `collected_count` attribute of the `collector` object should be set to 0.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestTestCollector::test_get_results_sorted</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'get_results_sorted' verifies that the collected test results are sorted by node ID.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the order of the test results is not maintained after adding or removing tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The list of node IDs in the collected results should be in ascending order.</li>
                                        <li>The list of node IDs in the collected results should contain only 'a_test.py::test_a' and 'z_test.py::test_z'.</li>
                                        <li>The node ID 'a_test.py::test_b' is not present in the sorted list.</li>
                                        <li>The node ID 'b_test.py::test_c' is not present in the sorted list.</li>
                                        <li>The node IDs are not in ascending order, but rather in descending order.</li>
                                        <li>There are duplicate node IDs in the collected results.</li>
                                        <li>The node ID 'z_test.py::test_d' is not present in the sorted list.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestTestCollector::test_handle_collection_finish</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the test collects and deselects items correctly after collection finish.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where collected items are not deselected after collection finish.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The collector's `collected_count` should be updated to reflect the number of collected items.</li>
                                        <li>The collector's `deselected_count` should be updated to reflect the number of deselected items.</li>
                                        <li>The `collected_count` and `deselected_count` attributes should be updated correctly after calling `handle_collection_finish`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 78-79, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_disabled_via_handle_report</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test should not capture output if config is disabled and handle_report integration.</p>
                                <p><strong>Why Needed:</strong> To prevent capturing of output when the configuration is set to disable capture of failed outputs via handle_report integration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>collector.handle_runtest_logreport(report) should not be called with a report that has captured stdout.</li>
                                        <li>result.captured_stdout should be None after collector.handle_runtest_logreport(report).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stderr</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the collector captures stderr and reports it correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the collector does not capture stderr or reports incorrect stderr messages.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `captured_stderr` attribute of the `TestCaseResult` object is set to 'Some error'.</li>
                                        <li>The `report.capstderr` method sets the captured stderr message to 'Some error'.</li>
                                        <li>The `collector._capture_output(result, report)` function calls `report.capstderr` with the correct value.</li>
                                        <li>The `result.captured_stderr` attribute is not set to an empty string or None before asserting it.</li>
                                        <li>The `report.capstdout` method does not have any effect on the captured stderr message.</li>
                                        <li>The `collector._capture_output(result, report)` function calls `report.capstderr` with a value that matches the expected output.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264, 268-269)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stdout</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `test_capture_output_stdout` function captures stdout correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the captured stdout is not properly recorded.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `captured_stdout` attribute of the `TestCaseResult` object should be set to 'Some output'.</li>
                                        <li>The `report.capstdout` method should have been called with the correct value ('Some output').</li>
                                        <li>The `collector._capture_output` function should have recorded the captured stdout correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_truncated</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the collector truncates output exceeding max chars.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the collector does not truncate output exceeding the specified max_chars.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The captured stdout should be truncated to 10 characters.</li>
                                        <li>The captured stderr is empty.</li>
                                        <li>The captured stdout contains only 9 characters (1234567890).</li>
                                        <li>The captured stderr remains unchanged (empty string).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_create_result_with_item_markers</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the collector extracts item markers correctly when using item markers in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the collector might not extract item markers from the report, leading to incorrect results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `param_id` attribute of the result is set to 'param1'.</li>
                                        <li>The `llm_opt_out` attribute of the result is set to True.</li>
                                        <li>The `llm_context_override` attribute of the result is set to 'complete'.</li>
                                        <li>All required requirements in the report are extracted and included in the result.</li>
                                        <li>The item marker with name 'requirement' is correctly retrieved from the collector.</li>
                                        <li>The item marker with name 'llm_opt_out' is correctly retrieved from the collector.</li>
                                        <li>The item marker with name 'llm_context_override' is correctly retrieved from the collector.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">35 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163-164, 167-169, 171, 181-182, 185-189, 198-200, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_repr_crash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `collectors.TestCollector._extract_error` correctly handles ReprFileLocation when it causes a crash report.</p>
                                <p><strong>Why Needed:</strong> This test prevents the potential crash of the collector due to an incorrect handling of ReprFileLocation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_error` method should return 'Crash report' when `report.longrepr.__str__.return_value` equals 'Crash report'.</li>
                                        <li>The `report.longrepr` object's `__str__` method should be called with the argument `'Crash report'`.</li>
                                        <li>The `report.longrepr` object's `longrepr` attribute should have been set to a string value equal to `'Crash report'`.</li>
                                        <li>The `_extract_error` method should not raise an exception when `report.longrepr.__str__.return_value` equals 'Crash report'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_string</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `_extract_error` method of `TestCollector` returns 'Some error occurred' when a `longrepr` is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the `longrepr` is not returned correctly if an error occurs during report generation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_error` method of `TestCollector` should return 'Some error occurred' when a `longrepr` is provided.</li>
                                        <li>The `report.longrepr` attribute should be set to 'Some error occurred'.</li>
                                        <li>If an error occurs during report generation, the `_extract_error` method should still return 'Some error occurred'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_fallback</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `extract_skip_reason` method returns None when there are no longreprs.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails if there are no longreprs in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_skip_reason` method of the `Collector` class should return `None` when `report.longrepr` is `None`.</li>
                                        <li>The `_extract_skip_reason` method of the `Collector` class should not raise an exception when `report.longrepr` is `None`.</li>
                                        <li>The `report.longrepr` attribute should be set to `None` before calling `_extract_skip_reason`.</li>
                                        <li>The `report` object passed to `_extract_skip_reason` should have a `longrepr` attribute.</li>
                                        <li>The `Collector` class should not throw an exception when called with a `report` object that has no `longrepr` attribute.</li>
                                        <li>The `_extract_skip_reason` method should be able to extract skip reasons from the report even if there are no longreprs.</li>
                                        <li>The `Collector` class should correctly handle cases where `report.longrepr` is `None` without raising an exception.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250, 252)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_string</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `_extract_skip_reason` method of `TestCollector` with a mock report.</p>
                                <p><strong>Why Needed:</strong> The test prevents regression when the `longrepr` attribute is not set in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report.longrepr` attribute should be an empty string if it's not provided.</li>
                                        <li>The `report.longrepr` attribute should contain the expected value 'Just skipped' if it's provided.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_tuple</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `extract_skip_reason_tuple` function correctly extracts a skip message from a tuple containing file, line and message information.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where an incorrect or missing reason is extracted for skipped tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report.longrepr` tuple should contain all three required elements: (file, line, message).</li>
                                        <li>If the tuple does not contain a `message`, it should still be able to extract the `longrepr` string.</li>
                                        <li>If the tuple contains an invalid or missing value for any of these elements, the function should raise an error with a descriptive message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_collection_report_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> When the `handle_collection_report` method is called with a collection report that fails, it should correctly record the error and update the list of collection errors.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the collector does not handle collection reports that fail properly, potentially leading to incorrect or missing error messages.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `collection_errors` list should contain exactly one element with the specified nodeid and message.</li>
                                        <li>The first element of `collection_errors` should have the correct nodeid.</li>
                                        <li>The first element of `collection_errors` should have the correct message.</li>
                                        <li>The second assertion in `key_assertions` is redundant, as it does not add any new information to the test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 58, 60-65, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_rerun</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `handle_runtest_rerun` method correctly handles reruns by setting the `rerun` attribute of the report to 1 and asserting its count is 1, while also ensuring the final outcome is 'failed'.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the `rerun` attribute is not being set correctly after a rerun.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>res.rerun_count == 1</li>
                                        <li>assert res.final_outcome == "failed"</li>
                                        <li>res.rerun == 1</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_setup_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'handle_runtest_setup_failure' verifies that the TestCollector reports a setup error when runtest logreport fails.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the TestCollector correctly handles setup errors and logs them properly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>res.outcome == 'error'</li>
                                        <li>res.phase == 'setup'</li>
                                        <li>res.error_message == 'Setup failed'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99-103, 109-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_teardown_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> TestCollectorReportHandling::test_handle_runtest_teardown_failure verifies that the test collects an error when teardown fails after a pass.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the test might not collect errors in cases of teardown failures, potentially masking issues with cleanup.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert res.outcome == 'error'</li>
                                        <li>assert res.phase == 'teardown'</li>
                                        <li>assert res.error_message == 'Cleanup failed'</li>
                                        <li>assert call_report.wasxfail is True</li>
                                        <li>assert teardown_report.wasxfail is False</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">38 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-115, 124, 127-128, 130, 132-133, 135-137, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_model_parsing_edge_cases</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the GeminiProvider's parsing of preferred models for edge cases, including an empty list and a specific model.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case the 'Gemini' booster is not correctly handling edge cases where no preferred models are specified or when all models are specified.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function provider._parse_preferred_models() returns a list of strings containing 'm1', 'm2'.</li>
                                        <li>The function provider._parse_preferred_models() returns an empty list when the model is None.</li>
                                        <li>The function provider._parse_preferred_models() returns an empty list when the model is set to 'All'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 134, 136-139, 141-142, 385, 387, 417-424)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_rate_limiter_edge_math</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter correctly handles edge cases where there are more tokens available than requests.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the rate limiter allows for excessive token usage without triggering an error.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert limiter.next_available_in(60) > 0</li>
                                        <li>assert limiter.next_available_in(10) == 0</li>
                                        <li>assert limiter.tokens() < 150</li>
                                        <li>assert limiter.request_count() < 50</li>
                                        <li>assert len(limiter.history()) == 2</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">35 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_models_to_dict_variants</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `to_dict()` method of SourceCoverageEntry and LlmAnnotation classes to ensure they return expected values for coverage percent, error messages, and run metadata.</p>
                                <p><strong>Why Needed:</strong> This test prevents regressions where coverage percent or error message is not correctly returned when using these classes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `coverage_percent` attribute of SourceCoverageEntry should be equal to the provided value (50.0).</li>
                                        <li>The `error` attribute of LlmAnnotation should be equal to the provided value ('timeout').</li>
                                        <li>The `duration` attribute of RunMeta should be equal to the provided value (1.0).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 71-78, 104-107, 109, 111-113, 115, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_create_mapper</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `CoverageMapper` class initializes correctly with a provided configuration.</p>
                                <p><strong>Why Needed:</strong> To prevent a potential bug where the mapper's warnings are not properly initialized or populated.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert mapper.config is config</li>
                                        <li>assert mapper.warnings == []</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 44-45)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_get_warnings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `get_warnings` method of the `CoverageMapper` class should be able to retrieve a list of warnings from the coverage report.</p>
                                <p><strong>Why Needed:</strong> This test prevents bugs or regressions where the `get_warnings` method returns an incorrect type (e.g., a non-list), potentially causing issues downstream in the application.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `warnings` variable is expected to be of type `list`.</li>
                                        <li>The `warnings` list contains at least one warning.</li>
                                        <li>Each warning in the `warnings` list has the correct structure (i.e., it's a dictionary with keys 'message' and 'code').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 44-45, 308)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_map_coverage_no_coverage_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `map_coverage` returns an empty dictionary when no coverage file exists.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where the test fails due to missing coverage data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return an empty dictionary.</li>
                                        <li>The `Path.exists` mock should return False.</li>
                                        <li>The `glob.glob` mock should return an empty list.</li>
                                        <li>The `map_coverage` function should not raise any exceptions.</li>
                                        <li>The test should have at least one warning.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_all_phases</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Should include all phases when `include_phase=all` configuration is used.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the coverage map does not include all phases when `include_phase=all` is specified.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `_extract_nodeid` should return the correct node ID for each phase.</li>
                                        <li>The function `_extract_nodeid` should handle cases where the phase name contains a dot (.) correctly.</li>
                                        <li>The function `_extract_nodeid` should include all phases in the coverage map even when `include_phase=all` is specified.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_empty_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 44-45, 216-217)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_filters_setup</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `test_extract_nodeid_filters_setup` test case extracts node IDs for a specific module and phase.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `CoverageMapper` does not extract node IDs from setup phases, potentially leading to incorrect coverage reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `_extract_nodeid` is called with the correct arguments (`'test.py::test_foo|setup'`) and returns `None` as expected.</li>
                                        <li>The test asserts that the extracted node ID is `None` for the specified module and phase.</li>
                                        <li>The `CoverageMapper` instance has a valid configuration object with the `include_phase` set to `'run'`.</li>
                                        <li>The `nodeid` variable is assigned the correct value after calling `_extract_nodeid` on the `mapper` instance.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 44-45, 216, 220, 224-225, 228-230)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_with_run_phase</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `test_extract_nodeid_with_run_phase` function correctly extracts a node ID from the run phase context.</p>
                                <p><strong>Why Needed:</strong> This test prevents bugs or regressions where the coverage map might not accurately reflect the code's execution phases.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The extracted node ID should match the expected value `test.py::test_foo`.</li>
                                        <li>The function `_extract_nodeid` is correctly called with the correct arguments `('test.py::test_foo|run')`.</li>
                                        <li>The coverage map is updated to reflect the accurate execution phase of the code being tested.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_full_logic</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `extract_contexts` method of `CoverageMapper` correctly extracts all contexts in a given file.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the coverage map does not include all paths in the `_extract_contexts` method, potentially leading to incorrect analysis or missed context areas.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return a list of covered lines for each context.</li>
                                        <li>The function should return an empty list for files that do not have any contexts.</li>
                                        <li>Each line within a covered context should be counted correctly and in the correct order.</li>
                                        <li>The function should handle cases where multiple files are covered by the same context.</li>
                                        <li>The function should ignore non-python files when extracting contexts.</li>
                                        <li>The function should return an empty list for files that do not have any contexts, even if they contain code.</li>
                                        <li>The function should correctly handle cases where a file has multiple lines with the same line number.</li>
                                        <li>The function should preserve the original order of covered lines within each context.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">57 lines (ranges: 44-45, 118, 121-122, 127, 131-135, 137-140, 144, 148, 150, 152-153, 156, 160-163, 165, 167-168, 173, 176, 178-184, 187-189, 191-194, 196, 199-200, 202, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">13 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65-67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_no_contexts</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `ExtractContexts` method returns an empty dictionary when there are no test contexts.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the method incorrectly assumes all files have test contexts and returns an incorrect result.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_data.measured_files.return_value == ['app.py']</li>
                                        <li>mock_data.contexts_by_lineno.return_value == {}</li>
                                        <li>result == {}</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 44-45, 118, 121-122, 127, 131-135, 144-146)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_nodeid_variants</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `CoverageMapper` with different node IDs and phases.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in coverage analysis when missing lines or without specific phases.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_nodeid()` method should return the expected node ID for each phase.</li>
                                        <li>The `_extract_nodeid()` method should return `None` for nodes with no specified phase.</li>
                                        <li>The context without a pipe character (`|`) should be correctly identified as having no phase.</li>
                                        <li>The test should pass even when the input string contains only one node ID (e.g., `test.py::test`).</li>
                                        <li>The test should fail when the input string contains multiple nodes with different phases (e.g., `test.py::test|setup` and `test.py::test|run`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231-234, 236, 239)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_no_files</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_load_coverage_data_no_files' verifies that the test fails when no coverage files exist.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the test would silently fail or produce incorrect results when there are no coverage files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `_load_coverage_data()` returns `None` when no .coverage files exist.</li>
                                        <li>The warning message is 'W001' as expected.</li>
                                        <li>There is only one warning raised in this test case.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 44-45, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_read_error</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `CoverageMapper` raises an error when trying to load corrupt coverage files.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the `CoverageMapper` fails to handle corrupted coverage data, potentially leading to unexpected behavior or errors in downstream applications.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `_load_coverage_data()` should raise an exception with the message 'Corrupt coverage file' when trying to read from a mock CoverageData instance that raises an error on read.</li>
                                        <li>Any warnings generated by the `CoverageMapper` should include the string 'Failed to read coverage data'</li>
                                        <li>The function `mapper.warnings` should contain at least one warning object with the message 'Corrupt coverage file'</li>
                                        <li>The function `mapper.warnings` should not contain any other error messages</li>
                                        <li>The function `_load_coverage_data()` should return None</li>
                                        <li>Any exceptions raised by the mock CoverageData instance should be caught and ignored</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 44-45, 72-73, 83, 86, 88, 92, 94-96, 107-111, 114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_with_parallel_files</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test should handle parallel coverage files from xdist and verify the correct number of updates.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in handling parallel coverage files, which is crucial for ensuring accurate coverage data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `update` method of the `CoverageData` class was called twice with different mock instances.</li>
                                        <li>The number of calls to `update` should be greater than or equal to 2.</li>
                                        <li>The mock instances returned by `mock_data_cls.side_effect` should not have been garbage collected before being used in `_ = mapper._load_coverage_data()`</li>
                                        <li>The coverage data loaded from the parallel files should contain both mock instances.</li>
                                        <li>The number of unique coverage data objects (i.e., different instances) should be greater than or equal to 2.</li>
                                        <li>The `update` method of the `CoverageData` class was called with at least one mock instance.</li>
                                        <li>The `update` method of the `CoverageData` class was called without any mock instances being garbage collected</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 44-45, 72-73, 83, 86, 88, 92, 94, 98, 101-104, 106)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_coverage_no_data</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `map_coverage` method when it returns an empty dictionary.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails with an incorrect assertion when no coverage data is available.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_load_coverage_data` method should return None or an empty dictionary when called without any arguments.</li>
                                        <li>The `map_coverage` method should correctly handle this case and return an empty dictionary.</li>
                                        <li>The test should pass even if the `_load_coverage_data` method returns a non-empty dictionary, as it is not relevant to the expected behavior of the `map_coverage` method.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 44-45, 58-60)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_analysis_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the CoverageMapper handles analysis2 errors during source coverage analysis.</p>
                                <p><strong>Why Needed:</strong> To prevent unexpected behavior when analyzing code with errors, such as skipping files with errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `map_source_coverage` method should return an empty list of entries for mock_cov.analysis2.side_effect == Exception('Analysis failed')</li>
                                        <li>The `map_source_coverage` method should not skip any files in the case of analysis2 errors</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 44-45, 243-244, 246-248, 250, 252-254, 259, 261, 263-268, 271, 299-300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_comprehensive</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the test maps all source files in `map_source_coverage` to their corresponding coverage statistics.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where some source files are not covered by the analysis, leading to incomplete coverage reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `map_source_coverage` returns exactly one entry with file path `app.py` and its corresponding coverage statistics.</li>
                                        <li>The first assertion checks that the number of entries returned is equal to 1.</li>
                                        <li>The second assertion verifies that the file path matches `app.py`.</li>
                                        <li>The third assertion ensures that the statements in the entry are equal to 3.</li>
                                        <li>The fourth assertion confirms that the covered percentage is equal to 66.67.</li>
                                        <li>The fifth assertion checks that there are no missing entries with a coverage percentage greater than 0.</li>
                                        <li>The sixth assertion verifies that the coverage percentage of the first entry matches 66.67.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">32 lines (ranges: 44-45, 243-244, 246-248, 250, 252, 259-261, 273, 276-279, 281-283, 285-293, 295, 299-300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors.py::test_make_warning</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the make_warning factory function to ensure it returns a WarningCode.W001_NO_COVERAGE instance with correct message and detail.</p>
                                <p><strong>Why Needed:</strong> To prevent a potential bug where an unknown warning is returned instead of a specific one, such as W001_NO_COVERAGE.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `make_warning` should return an instance of class WarningCode.W001_NO_COVERAGE with the correct code.</li>
                                        <li>The message of the returned warning should contain 'No .coverage file found'.</li>
                                        <li>The detail of the returned warning should be 'test-detail'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors.py::test_warning_code_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that warning codes have correct values.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the wrong warning code is assigned to an error condition.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>{'message': 'Assertion failed: WarningCode.W001_NO_COVERAGE.value == "W001"'}</li>
                                        <li>{'message': 'Assertion failed: WarningCode.W101_LLM_ENABLED.value == "W101"'}</li>
                                        <li>{'message': 'Assertion failed: WarningCode.W201_OUTPUT_PATH_INVALID.value == "W201"'}</li>
                                        <li>{'message': 'Assertion failed: WarningCode.W301_INVALID_CONFIG.value == "W301"'}</li>
                                        <li>{'message': 'Assertion passed: WarningCode.W401_AGGREGATE_DIR_MISSING.value == "W401"'}</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors.py::test_warning_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `to_dict()` method of `Warning` class.</p>
                                <p><strong>Why Needed:</strong> Prevent a warning that occurs when trying to convert a `Warning` object to a dictionary without setting 'detail' attribute.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `code` attribute is set correctly and matches the expected value.</li>
                                        <li>The `message` attribute is set correctly and matches the expected value.</li>
                                        <li>The `detail` attribute is not set, which should prevent this warning from occurring.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 70-72, 74-76)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_known_code</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the `make_warning` function creates a warning with the correct code and message.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the warning is not created correctly when the code is known to be valid.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The warning has the expected code `WarningCode.W101_LLM_ENABLED`.</li>
                                        <li>The warning has the expected message from `WARNING_MESSAGES` dictionary.</li>
                                        <li>The warning detail is None, indicating that no additional information is provided.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_unknown_code</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Make Warning: Unknown Code</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the code is not covered by warnings, leading to unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `make_warning` does not throw an exception when given an unknown warning code.</li>
                                        <li>The function `make_warning` uses the fallback message 'Unknown warning.' for unknown warning codes.</li>
                                        <li>The function `make_warning` restores the original message after using it to make a warning.</li>
                                        <li>The function `make_warning` correctly handles missing warning messages in the dictionary.</li>
                                        <li>The function `make_warning` preserves the original message when restoring it.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_with_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_make_warning_with_detail' verifies creating a warning with detailed message.</p>
                                <p><strong>Why Needed:</strong> This test prevents the creation of warnings without any detail, which can lead to unexpected behavior or errors in the application.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>w.code == WarningCode.W301_INVALID_CONFIG</li>
                                        <li>w.detail == 'Bad value'</li>
                                        <li>assert w.detail is not None</li>
                                        <li>assert isinstance(w.detail, str)</li>
                                        <li>assert len(w.detail) > 0</li>
                                        <li>assert w.detail != '',</li>
                                        <li>assert w.detail != 'Bad value'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestWarningCodes::test_codes_are_strings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Ensures that all enum values are strings and start with 'W', preventing potential warnings.</p>
                                <p><strong>Why Needed:</strong> This test prevents WarningsCodes from being converted to integers without proper string representation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>code.value should be a string.</li>
                                        <li>code.value should start with 'W'.</li>
                                        <li>All enum values must have a string representation.</li>
                                        <li>WarningCode enum values cannot be converted to integers without explicit conversion.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_no_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the warning to dictionary serialization without detail.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where warnings are not properly serialized to dictionaries, potentially leading to incorrect data being stored in test results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'code' key should contain the warning code.</li>
                                        <li>The 'message' key should contain the warning message.</li>
                                        <li>Both keys should match the expected values ('W001' and 'No coverage', respectively).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 70-72, 74, 76)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_with_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that WarningDataClass can be serialized correctly with detail.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the warning data is not properly serialized to JSON.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'code' key in the dictionary should match the actual code value.</li>
                                        <li>The 'message' key in the dictionary should match the actual message value.</li>
                                        <li>The 'detail' key in the dictionary should match the actual detail value.</li>
                                        <li>The 'WarningCode.W001_NO_COVERAGE' key should be present in the dictionary and have the correct value.</li>
                                        <li>The 'Check setup' key should also be present in the dictionary with the correct value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 70-72, 74-76)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestIsPythonFile::test_non_python_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies whether a given file extension is 'python' or not.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues where the `is_python_file` function incorrectly identifies non-.py files as Python files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return False for files with extensions other than '.py'.</li>
                                        <li>The function should return False for files with extensions like 'foo/bar.txt' or 'foo/bar.pyc'.</li>
                                        <li>The function should not incorrectly identify 'foo/bar.py' as a non-.py file.</li>
                                        <li>The function should handle case-insensitive matching of file extensions.</li>
                                        <li>The function should correctly handle files with multiple '.' characters in their extension.</li>
                                        <li>The function should return False for files without any extension (e.g., just 'foo.txt').</li>
                                        <li>The function should raise an error when given a non-existent Python file (e.g., 'non_existent.py').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 79)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestIsPythonFile::test_python_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `is_python_file` function returns True for a `.py` file.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function incorrectly identifies non-`.py` files as Python files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return `True` when given a string representing a `.py` file.</li>
                                        <li>The function should raise an error or return a specific value indicating that it cannot identify the file type.</li>
                                        <li>The function should handle cases where the input is not a valid Python file (e.g., a non-`.py` file with Python code).</li>
                                        <li>The function should correctly handle nested directories and relative paths in the file path.</li>
                                        <li>The function should ignore case when comparing file extensions (e.g., `.Py` vs. `py`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 79)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestMakeRelative::test_makes_path_relative</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test makes absolute path relative by creating a subdirectory and verifying the resulting file path is correct.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the `make_relative` function does not correctly handle cases where the input file path has an absolute component.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `make_relative` function should create a new file path that includes the subdirectory specified in the first argument and the original file name.</li>
                                        <li>The resulting file path should be relative to the base directory of the test.</li>
                                        <li>The parent directory of the input file path should not be included in the output file path.</li>
                                        <li>The `touch` method should create a new file with the correct permissions (in this case, no modifications).</li>
                                        <li>The `mkdir` method should create the subdirectory if it does not already exist.</li>
                                        <li>The `exist_ok` parameter should prevent an error from being raised if the parent directory already exists.</li>
                                        <li>The resulting file path should have a relative extension ('.py').</li>
                                        <li>The original file name should be included in the output file path (e.g., 'file.py').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function returns an incorrect normalized path when no base is provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of `make_relative('foo/bar')` should be 'foo/bar'.</li>
                                        <li>The current implementation does not handle cases where the input directory contains only subdirectories.</li>
                                        <li>A normalization step would be required to ensure consistency across different operating systems and file systems.</li>
                                        <li>Without this check, the function could return an incorrect normalized path in certain edge cases.</li>
                                        <li>This test verifies that the `make_relative` function behaves correctly when no base is provided.</li>
                                        <li>The test ensures that the function returns a normalized path as expected.</li>
                                        <li>It also checks if the current implementation handles subdirectories correctly.</li>
                                        <li>In case of subdirectories, the function should still return a normalized path.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 30, 33, 36, 39, 42, 55-56)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestNormalizePath::test_already_normalized</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that a normalized path is returned for an already-normalized input.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues where the `normalize_path` function may incorrectly handle already-normalized paths, potentially leading to unexpected behavior or errors in downstream applications.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Input: 'foo/bar'</li>
                                        <li>Output: 'foo/bar'</li>
                                        <li>Path is already normalized</li>
                                        <li>Normalization does not affect the path's readability or usability</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestNormalizePath::test_forward_slashes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests that the `normalize_path` function correctly converts forward slashes in file paths.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly handles forward slashes in file paths.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input string should be converted to 'foo/bar' without any changes.</li>
                                        <li>The resulting path should have the same structure as the original input.</li>
                                        <li>Any backslashes in the input string should be replaced with forward slashes.</li>
                                        <li>The function should not add any additional slashes to the end of the path.</li>
                                        <li>The function should handle paths with multiple consecutive forward slashes correctly.</li>
                                        <li>The function should ignore leading or trailing whitespace characters in the input string.</li>
                                        <li>The function should preserve the original directory structure when converting backslashes to forward slashes.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `normalize_path` function to remove trailing slashes from paths.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where a path with a trailing slash is returned unmodified.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input path should be stripped of any trailing slashes.</li>
                                        <li>The resulting normalized path should have no leading or trailing slashes.</li>
                                        <li>Any empty string should not be returned as the normalized path.</li>
                                        <li>A path with only one slash should remain unchanged.</li>
                                        <li>A path with multiple consecutive slashes should also remain unchanged.</li>
                                        <li>A path with a single slash followed by another slash should be considered as having only one slash.</li>
                                        <li>The function should handle paths with leading or trailing whitespace correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_custom_exclude_patterns</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies whether a path matches custom exclusion patterns.</p>
                                <p><strong>Why Needed:</strong> Prevents potential issues where paths are incorrectly excluded due to custom patterns.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'should_skip_path' function should return True when the path matches any of the custom exclusion patterns.</li>
                                        <li>The 'should_skip_path' function should return False when the path does not match any of the custom exclusion patterns.</li>
                                        <li>The 'exclude_patterns' parameter should be able to exclude specific file extensions or patterns from being skipped.</li>
                                        <li>Custom exclusion patterns should be able to override the default behavior of the 'should_skip_path' function.</li>
                                        <li>The test should cover both cases where a path matches and does not match any custom exclusion patterns.</li>
                                        <li>The test should use the correct syntax for passing custom exclusion patterns as arguments to the 'exclude_patterns' parameter.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116-117, 119-121, 123)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_normal_path</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_fs.py::TestShouldSkipPath::test_normal_path</p>
                                <p><strong>Why Needed:</strong> To prevent skipping of normal paths due to incorrect implementation of 'should_skip_path' function.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'should_skip_path' function should return False for the path 'src/module.py'.</li>
                                        <li>The 'should_skip_path' function should not throw an exception or raise an error when given a valid path.</li>
                                        <li>The 'should_skip_path' function should maintain its original behavior for other paths (e.g., directories, files).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_git</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `should_skip_path` function correctly identifies `.git` directories.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the function incorrectly considers non-`.git` directories as paths to be skipped.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert should_skip_path('.git/objects/foo') is True</li>
                                        <li>assert not should_skip_path('non_git_directory.txt')</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_pycache</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `should_skip_path` function correctly identifies and skips `__pycache__` directories.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the `should_skip_path` function incorrectly includes `__pycache__` directories in its skip list, leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file path should be relative and start with `__pycache__`.</li>
                                        <li>The file path should not contain a `.pyc` extension.</li>
                                        <li>The file path should be located within the `foo/__pycache__` directory.</li>
                                        <li>The file path should not be located at the root of the test directory.</li>
                                        <li>The file path should be skipped by the `should_skip_path` function.</li>
                                        <li>The file path should not cause any errors or unexpected behavior when passed to the `should_skip_path` function.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_venv</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_fs.py::TestShouldSkipPath::test_skips_venv</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the `should_skip_path` function incorrectly identifies venv directories as being to be skipped.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return True for venv directories, but it currently returns False.</li>
                                        <li>The function should return True for .venv directories, but it currently returns False.</li>
                                        <li>The function should handle the case where a directory contains multiple venv subdirectories correctly.</li>
                                        <li>The function should not incorrectly identify a non-existent venv directory as being to be skipped.</li>
                                        <li>The function should raise an exception when given a non-venv directory path instead of returning False.</li>
                                        <li>The function should ignore the presence of a parent directory containing a venv subdirectory in its own case.</li>
                                        <li>The function should handle the case where a Python package is installed in a virtual environment but not in the current working directory correctly.</li>
                                        <li>The function should raise an exception when given a non-venv directory path with a non-existent parent directory.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_pruning</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that pruning clears request and token usage times when a request is added in the past</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where adding a request in the past would cause pruning to clear both request and token usage times.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>_request_times should be empty after pruning</li>
                                        <li>_token_usage should be empty after pruning</li>
                                        <li>limiter._prune(time.monotonic()) should not modify _request_times or _token_usage</li>
                                        <li>assert len(limiter._request_times) == 0 and assert len(limiter._token_usage) == 0 after pruning</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 39-42, 81-85, 87-88)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_rpm_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter prevents requests from exceeding the specified limit when the limit is set to 1 request per minute.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where a client can send multiple requests in quick succession, potentially overwhelming the server and causing it to become unavailable.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `next_available_in` method returns a value greater than 0 when the limit is set to 1 request per minute.</li>
                                        <li>The `next_available_in` method returns a value less than or equal to 60.0 seconds when the limit is set to 1 request per minute.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95, 97, 100-102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_tpm_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter prevents a regression when tokens are not immediately available.</p>
                                <p><strong>Why Needed:</strong> This test verifies that the rate limiter does not allow tokens to be used before they become available, preventing potential abuse and ensuring fair usage of the API.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The next_available_in method returns a non-zero value (greater than 0) when there are tokens left.</li>
                                        <li>The _token_usage list is updated correctly after recording tokens.</li>
                                        <li>The number of tokens used in the last 20 seconds is less than or equal to 2.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">33 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-94, 100-101, 103, 105, 107-108, 110-114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_wait_for_slot</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the `wait_for_slot` method sleeps when a request is recorded.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the rate limiter does not sleep after a request is recorded, potentially leading to performance issues or unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>limiter.wait_for_slot() was called with mock_sleep</li>
                                        <li>the `time.sleep` function was called exactly once</li>
                                        <li>no other requests were made during the test duration</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">31 lines (ranges: 39-42, 45-46, 48, 52-54, 58-59, 61-63, 73, 76-78, 81-82, 84, 87-88, 92-93, 95, 97, 100-102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_record_zero_tokens</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the limiter records zero tokens when no tokens are available.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the limiter does not record tokens for the first few minutes of usage.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of `_token_usage` is equal to 0 after calling `record_tokens(0)`.</li>
                                        <li>The number of records in `_token_usage` is exactly 1 (i.e., zero tokens were recorded).</li>
                                        <li>The limiter's token usage does not exceed the specified limit (100 tokens per minute) for at least one minute.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39-42, 66-67)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_requests_per_day_exhaustion</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the test Gemini Limiter requests per day exhaustion.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential rate limit exceeded error when exceeding daily request limits.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `GeminiRateLimitExceeded` exception is raised with the correct message 'requests_per_day'.</li>
                                        <li>The `wait_for_slot` method returns an instance of `_GeminiRateLimitExceeded` exception.</li>
                                        <li>The `record_request` method does not raise a `GeminiRateLimitExceeded` exception when exceeding daily limits.</li>
                                        <li>The `wait_for_slot` method waits for the requested slot to become available, allowing for requests beyond the limit.</li>
                                        <li>The `requests_per_day` attribute is set to 1, indicating a single request per day.</li>
                                        <li>The test does not verify that the `GeminiRateLimitExceeded` exception is raised with an error message related to 'requests_per_day'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">24 lines (ranges: 32-34, 39-42, 45-46, 48-50, 58-60, 73, 76-78, 81-82, 84, 87-88)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_tpm_fallback_wait</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the tpm wait time fallback feature of GeminiRateLimiter.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the rate limiter fails to detect and prevent excessive TPM usage without sufficient tokens.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The current rate limit is not being exceeded by the request tokens.</li>
                                        <li>The token usage is not empty after filling up the TPM.</li>
                                        <li>The time until TPM becomes available is greater than 0 seconds.</li>
                                        <li>_seconds_until_tpm_available returns a positive value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">24 lines (ranges: 39-42, 66, 68-70, 81-82, 84, 87-88, 100-101, 103, 105, 107-108, 110-114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_provider_rpm_cooldown</span>
                        <div class="test-meta">
                            <span>556ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that RPM rate limit cooldown handling is correctly implemented.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the RPM rate limit cooldown might not be properly handled, leading to unexpected behavior or errors in subsequent tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'models/gemini-pro' model should be present in the provider's cooldowns.</li>
                                        <li>The cooldown time for the 'models/gemini-pro' model should be greater than 1000.0 seconds (1 minute).</li>
                                        <li>The provider's cooldowns dictionary should contain the 'models/gemini-pro' model.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">23 lines (ranges: 52-53, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">117 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-215, 217-223, 225-227, 233-234, 238-240, 242-243, 274-277, 280, 282-290, 292-295, 297-298, 300-301, 346, 348-350, 352-353, 381-382, 385-386)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_rate_limit_retry</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the GeminiProvider annotates a rate limit retry scenario correctly</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the GeminiProvider's ability to handle rate limit retries.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation should contain the correct scenario 'Recovered Scenario'</li>
                                        <li>The mock_post call count should be equal to 2</li>
                                        <li>The annotation should not have an error message</li>
                                        <li>The annotation should contain a valid model name</li>
                                        <li>The annotation should contain supported generation methods</li>
                                        <li>The annotation should contain a valid content snippet</li>
                                        <li>The annotation should contain the correct part of the text</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">181 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-215, 217-222, 225-227, 233-234, 238-240, 242-243, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330-333, 335-341, 343, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_success</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that _annotate_internal returns the correct annotation when a successful response is received from _call_gemini.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case of an unexpected error or incorrect response from _parse_response.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation returned by _annotate_internal has the expected scenario and no error.</li>
                                        <li>The annotation does not contain any error information.</li>
                                        <li>The annotation's `scenario` field matches the expected value.</li>
                                        <li>_parse_response returns a Mock object with the correct scenario and no error.</li>
                                        <li>The annotation's `error` field is None.</li>
                                        <li>The annotation's `text` field contains the expected text.</li>
                                        <li>The annotation's `tokens` field does not contain any tokens.</li>
                                        <li>The annotation's `_call_gemini` method returns the correct JSON structure.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">173 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-223, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-343, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_availability</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the GeminiProvider class checks availability correctly when no environment variables are set.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider's availability check fails due to missing environment variables, potentially leading to incorrect API token usage or other issues.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider._check_availability() method returns False if no environment variables are set.</li>
                                        <li>The provider._check_availability() method should return True when environment variables are set with a valid API token.</li>
                                        <li>The provider._check_availability() method should raise an AssertionError or other exception when environment variables are not set and the API token is invalid or missing.</li>
                                        <li>The provider._check_availability() method should not throw an AssertionError if environment variables are set but the API token is invalid or missing.</li>
                                        <li>The provider._check_availability() method should not return True if environment variables are set with a valid API token, but the API token is invalid or missing.</li>
                                        <li>The provider._check_availability() method should raise an AssertionError when environment variables are set with a valid API token, but the API token is invalid or missing.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 134, 136-139, 141-142, 266-267, 269)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpd_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter prevents excessive requests within a certain time frame.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential infinite loop of requests due to the rate limiter not being able to handle high request rates.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The next_available_in() method should return None after 100 requests.</li>
                                        <li>The limiter's record_request() method should be called before calling next_available_in().</li>
                                        <li>The limiter's next_available_in() method should not be called within the same request.</li>
                                        <li>The rate limit should not be exceeded for any given time period (in this case, 100 requests).</li>
                                        <li>The limiter's record_request() method should increment the request count and then call next_available_in().</li>
                                        <li>The limiter's next_available_in() method should return None after calling record_request().</li>
                                        <li>The rate limit should not be exceeded within a certain time frame (in this case, 100 requests).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 39-42, 45-46, 48-50, 73, 76-78, 81-82, 84, 87-88)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpm_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the rate limiter does not block subsequent requests when there are no available slots in the current window.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where subsequent requests to the same endpoint would block due to insufficient available slots in the rate limit window.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The next_available_in function should return 0.0 after three requests without any available slots in the rate limit window.</li>
                                        <li>The limiter.record_request() function should not be called before the first request has a valid available slot in the rate limit window.</li>
                                        <li>The wait value returned by next_available_in should be greater than 0 and less than or equal to 60.0 after three requests without any available slots in the rate limit window.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">27 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-97, 100-102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeConfigHash::test_different_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that different configuration providers yield different hashes.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of provider switching, where the hash may change.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `compute_config_hash` should return a different hash for two instances with different config providers.</li>
                                        <li>The function `compute_config_hash` should not return the same hash when both providers are 'none'.</li>
                                        <li>The function `compute_config_hash` should not return the same hash when one provider is 'ollama' and the other is 'none'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 96-101, 103-104)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeConfigHash::test_returns_short_hash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the computed hash is short and returns a 16-character string.</p>
                                <p><strong>Why Needed:</strong> Prevents potential security vulnerabilities by ensuring the hash length is sufficient to prevent collisions.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the computed hash should be exactly 16 characters.</li>
                                        <li>The hash should not contain any non-ASCII characters.</li>
                                        <li>The first character of the hash should be a lowercase letter.</li>
                                        <li>The second character of the hash should be a lowercase letter.</li>
                                        <li>The third character of the hash should be a digit.</li>
                                        <li>The fourth character of the hash should be an uppercase letter.</li>
                                        <li>The fifth character of the hash should be a digit.</li>
                                        <li>All characters in the hash should be either lowercase letters or digits.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 96-101, 103-104)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeFileSha256::test_consistent_with_bytes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the computed SHA-256 hash of a file matches its content hash when the same file is hashed multiple times with different inputs.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the computed SHA-256 hash does not match the content hash even if the input file is the same.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The computed SHA-256 hash of the file should be equal to its content hash.</li>
                                        <li>The content hash of the file should be equal to the computed SHA-256 hash.</li>
                                        <li>The computed SHA-256 hash of a different file with the same input should also match the content hash.</li>
                                        <li>If the input file is modified, the computed SHA-256 hash should still match the content hash.</li>
                                        <li>If the input file is deleted or renamed, the computed SHA-256 hash should still match the content hash.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 32, 44-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeFileSha256::test_hashes_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies the correctness of computing a SHA-256 hash for a file.</p>
                                <p><strong>Why Needed:</strong> Prevents potential issues where incorrect or incomplete file contents are passed to `compute_file_sha256` function, potentially leading to incorrect hashes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the computed hash should be exactly 64 bytes.</li>
                                        <li>The computed hash should not contain any zeros (indicating an empty string).</li>
                                        <li>Any non-zero values in the file contents should be present in the hash output.</li>
                                        <li>No leading zeros in the hash output are expected, as they indicate a truncated hash.</li>
                                        <li>The hash is not empty and does not start with '0x' prefix.</li>
                                        <li>The hash contains only hexadecimal digits (a-z, A-Z, 0-9).</li>
                                        <li>Any non-hexadecimal characters in the file contents should be ignored during hashing.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 44-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeHmac::test_different_key</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that different keys produce different signatures.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where two different keys may generate the same signature, which could be exploited by an attacker to bypass security measures.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The computed signature for key1 is different from the computed signature for key2.</li>
                                        <li>The computed signature for key1 is not equal to the expected value (which should be different due to the different keys).</li>
                                        <li>The computed signature for key2 is different from the computed signature for key1.</li>
                                        <li>The computed signature for key2 is not equal to the expected value (which should be different due to the different keys).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 61)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeHmac::test_with_key</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies the computation of HMAC using a secret key.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where an attacker could exploit the lack of padding or authentication in the HMAC calculation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the generated signature is 64 bytes.</li>
                                        <li>The signature is not empty.</li>
                                        <li>The signature contains only hexadecimal digits.</li>
                                        <li>No leading zeros are present in the signature.</li>
                                        <li>No trailing zeros are present in the signature.</li>
                                        <li>The signature does not contain any whitespace characters.</li>
                                        <li>The signature does not contain any newline characters.</li>
                                        <li>The signature does not contain any special characters other than those allowed by the HMAC algorithm.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 61)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeSha256::test_consistent</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'compute_sha256' function is called with the same input and produces the same output.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where different inputs to the `compute_sha256` function produce different hashes, potentially leading to incorrect results or security vulnerabilities.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The two hash values should be equal.</li>
                                        <li>The contents of the input bytes should not change the output hash.</li>
                                        <li>The hash value should remain constant for a given set of input data.</li>
                                        <li>Different inputs to `compute_sha256` function should produce different hashes.</li>
                                        <li>The hash value should not be affected by changes in the input data, such as padding or encoding.</li>
                                        <li>The hash value should not be changed by appending or modifying the input bytes.</li>
                                        <li>The hash value should remain unchanged if the input is a known constant (e.g., an empty string).</li>
                                        <li>The hash value should not change when the input is modified in-place (e.g., using `b'...' = b'...'`) but the function is called again.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeSha256::test_length</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The hash of the input string 'test' is expected to have a length of 64 characters.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential SHA-256 hashing issue where the output may not be exactly 64 bytes due to various factors such as encoding or padding errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The hash should be a string of 64 hexadecimal characters.</li>
                                        <li>The length of the hash should be exactly 64 characters.</li>
                                        <li>The hash should have no leading zeros.</li>
                                        <li>The hash should not contain any null bytes.</li>
                                        <li>The hash should only contain hexadecimal digits (0-9, A-F, a-f).</li>
                                        <li>The hash should not contain whitespace or special characters.</li>
                                        <li>The hash should be a valid SHA-256 hash.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestGetDependencySnapshot::test_includes_pytest</span>
                        <div class="test-meta">
                            <span>81ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `get_dependency_snapshot` function includes the 'pytest' package.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the 'pytest' package is not included in the dependency snapshot.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'pytest' package should be present in the dependency snapshot.</li>
                                        <li>The 'pytest' package should be listed as an item in the snapshot.</li>
                                        <li>The snapshot should include all required dependencies, including 'pytest'.</li>
                                        <li>The test should fail if the 'pytest' package is not included in the dependency snapshot.</li>
                                        <li>The test should fail if the 'pytest' package is missing from the snapshot.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 113-114, 116-121)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestGetDependencySnapshot::test_returns_dict</span>
                        <div class="test-meta">
                            <span>82ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_dependency_snapshot` function returns a dictionary.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function may not return a dictionary if it encounters an error or edge case.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>snapshot is an instance of dict</li>
                                        <li>snapshot has no attributes other than __dict__</li>
                                        <li>the `isinstance` check passes with `snapshot` as dict</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 113-114, 116-121)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_loads_key</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `load_hmac_key` function correctly loads a key from a file.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the HMAC key is not loaded correctly from the file, potentially leading to incorrect authentication.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>A bytes object containing the expected HMAC key should be returned by the `load_hmac_key` function.</li>
                                        <li>The HMAC key in the file should match the expected value.</li>
                                        <li>The `load_hmac_key` function should raise an error if the file is not found or cannot be read.</li>
                                        <li>The `load_hmac_key` function should correctly decode the HMAC key from the file bytes.</li>
                                        <li>The `load_hmac_key` function should handle cases where the file is empty or contains only whitespace.</li>
                                        <li>The `load_hmac_key` function should raise an error if the file does not contain a valid HMAC key.</li>
                                        <li>The `load_hmac_key` function should correctly load the HMAC key from the first line of the file (assuming it's the only one).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 73, 76-77, 80-81)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_missing_key_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the function returns None when a missing key file is provided.</p>
                                <p><strong>Why Needed:</strong> To prevent a potential bug where the test fails due to an unexpected exception being raised when trying to load a non-existent key file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `load_hmac_key` function should be able to successfully load the HMAC key from the provided configuration.</li>
                                        <li>The `load_hmac_key` function should not raise any exceptions if the key file does not exist.</li>
                                        <li>The test should fail with a clear error message indicating that the key file is missing.</li>
                                        <li>The test should verify that the expected exception (KeyFileNotFoundError) is raised when trying to load a non-existent key file.</li>
                                        <li>The `Config` class should be able to handle the case where the key file does not exist without raising an exception.</li>
                                        <li>The test should only fail if the key file exists, and not if it's just missing for some reason (e.g. due to permissions issues).</li>
                                        <li>The test should provide a clear indication that the key file is missing by verifying its absence in the configuration object.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 73, 76-78)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_no_key_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `load_hmac_key` function returns None when no key file is specified.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function does not handle cases without a key file configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `load_hmac_key` function should return `None` if no key file is provided.</li>
                                        <li>No exception should be raised when no key file is specified.</li>
                                        <li>The function should correctly identify and return `None` for invalid input.</li>
                                        <li>The function should not throw an error or raise an exception when given a valid configuration but no key file.</li>
                                        <li>The function's behavior should remain consistent across different test environments.</li>
                                        <li>No unexpected side effects should occur when loading the HMAC key without a file.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 73-74)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_aggregation_defaults</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the aggregation configuration defaults.</p>
                                <p><strong>Why Needed:</strong> This test ensures that aggregation has sensible defaults, preventing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.aggregate_dir is None</li>
                                        <li>config.aggregate_policy == 'latest'</li>
                                        <li>config.aggregate_include_history is False</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_capture_failed_output_default_false</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `capture_failed_output` default value for the integration gate is set to `False`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the default capture output is enabled by mistake.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.capture_failed_output should be `None` or `False`.</li>
                                        <li>The `capture_failed_output` configuration option does not have any effect on the integration gate's behavior.</li>
                                        <li>The integration gate will behave correctly when `capture_failed_output` is set to `True`.</li>
                                        <li>Setting `capture_failed_output` to `False` ensures that captured output is opt-in.</li>
                                        <li>If `capture_failed_output` is set to `True`, no test assertions are performed.</li>
                                        <li>The default value for `capture_failed_output` should be consistent across all integration gate configurations.</li>
                                        <li>Other configuration options, such as `capture_failed_output` itself, do not affect the integration gate's behavior.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_context_mode_default_minimal</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the context mode is set to 'minimal' by default.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the context mode defaults to 'none', which may cause issues with certain integration gate configurations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function get_default_config() returns an instance of Config with llm_context_mode set to 'minimal'.</li>
                                        <li>The value of config.llm_context_mode is equal to 'minimal'.</li>
                                        <li>If the context mode defaults to 'none', then config.llm_context_mode should be set to 'minimal'.</li>
                                        <li>If the context mode is not 'minimal' when it's set, then config.llm_context_mode should not be 'minimal'.</li>
                                        <li>The function get_default_config() returns an instance of Config with llm_context_mode set to a value other than 'minimal'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_llm_not_enabled_by_default</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the LLM is not enabled by default in the configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where LLM is enabled by default due to a misconfiguration or bug.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.is_llm_enabled() should return False</li>
                                        <li>config.get_llm_enabled_value() should be False</li>
                                        <li>get_default_config().llm_enabled() should be False</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 107, 147, 224, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_omit_tests_default_true</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `TestConfigDefaults` class's `omit_tests_from_coverage` attribute is set to `True` by default.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the coverage threshold is not correctly applied when omitting tests from coverage.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.omit_tests_from_coverage is True</li>
                                        <li>assert config.omit_tests_from_coverage == True</li>
                                        <li>config.omit_tests_from_coverage should be set to `True` by default</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_provider_default_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the provider defaults to 'none' when no value is provided.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the provider is set to an invalid or unexpected value.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config.provider` attribute should be equal to 'none'.</li>
                                        <li>The `provider` attribute of the configuration object should have a value of 'none'.</li>
                                        <li>The `get_default_config()` function returns a configuration object with a `provider` attribute set to 'none'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_secret_exclude_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that secret files are excluded by default from the LLM context.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where sensitive information like secret files might be inadvertently included in the LLM context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_default_config()` returns an object with an attribute `llm_context_exclude_globs` that contains a list of globs to exclude.</li>
                                        <li>Any string in the list of globs is either 'secret' or ends with '.env'.</li>
                                        <li>The function `any()` checks if any element in the list matches the specified condition (either 'secret' or ending with '.env').</li>
                                        <li>The function `any()` returns True as soon as it finds a match, rather than checking all elements in the list.</li>
                                        <li>Without this test, there's a possibility that sensitive information could be accidentally included in the LLM context due to a programming error.</li>
                                        <li>This test ensures that the default configuration is correct and does not include any secret files by default.</li>
                                        <li>The function `get_default_config()` is called with no arguments, which returns an object with the specified attributes.</li>
                                        <li>The attribute `llm_context_exclude_globs` is checked for being a list of strings.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_deterministic_output</span>
                        <div class="test-meta">
                            <span>10ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the test_deterministic_output function reports deterministic output.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when the test_deterministic_output function is updated to use a different sorting algorithm.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The nodeids in the report.json file are sorted by nodeid.</li>
                                        <li>The nodeids in the report.json file are not duplicated.</li>
                                        <li>The nodeids in the report.json file contain only unique values.</li>
                                        <li>The nodeids in the report.json file are sorted correctly even if there are duplicate nodeids.</li>
                                        <li>The nodeids in the report.json file are sorted alphabetically by nodeid.</li>
                                        <li>The nodeids in the report.json file do not contain any duplicates or duplicates with different sorting order.</li>
                                        <li>The nodeids in the report.json file contain only unique values and are sorted correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">78 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">117 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_empty_test_suite</span>
                        <div class="test-meta">
                            <span>10ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing an empty test suite produces a valid report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that an empty test suite generates a correct report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total count of tests in the report is zero.</li>
                                        <li>There are no failed tests in the report.</li>
                                        <li>All test suites have been successfully written to the report file.</li>
                                        <li>The report contains only one section with 'total' key set to zero.</li>
                                        <li>No summary or other sections are present in the report.</li>
                                        <li>The report does not contain any duplicate keys.</li>
                                        <li>Each test is properly formatted and does not exceed 100 characters.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_html_report_generation</span>
                        <div class="test-meta">
                            <span>34ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The full pipeline generates an HTML report for all test cases.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression if the full pipeline is not generating a report for some tests, potentially due to a bug or configuration issue.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file "report.html" exists at the specified path.</li>
                                        <li>The content of "report.html" contains the string '<html', indicating it's an HTML document.</li>
                                        <li>The content of "report.html" contains the string 'test_pass', which is expected for passed test cases.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">113 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_json_report_generation</span>
                        <div class="test-meta">
                            <span>54ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the full pipeline's ability to generate a valid JSON report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression or bugs that may cause the generation of an invalid JSON report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The report should contain a 'schema_version' key with the correct value.</li>
                                        <li>The report should have a 'summary' section with the expected number of tests, passed, failed, and skipped.</li>
                                        <li>All test results should be included in the report.</li>
                                        <li>The report path should exist at the specified location.</li>
                                        <li>The JSON data should contain the required keys ('schema_version', 'summary', 'total', 'passed', 'failed', 'skipped')</li>
                                        <li>Test results should have the expected outcome (1 passed, 2 failed, and 0 skipped).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/_git_info.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 2-3)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">78 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">133 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-322, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_report_root_has_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the ReportRoot object has all required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the report root is missing required fields, potentially leading to incorrect or incomplete reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>schema_version</li>
                                        <li>run_meta</li>
                                        <li>summary</li>
                                        <li>tests</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_aggregation_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'RunMeta has aggregation fields' verifies that the RunMeta object contains 'is_aggregated', 'run_count' and possibly other aggregation policy information.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the 'is_aggregated' or 'run_count' fields are missing from the RunMeta object, potentially leading to incorrect analysis results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'is_aggregated' field is present in the data.</li>
                                        <li>The 'run_count' field is present in the data.</li>
                                        <li>The aggregation policy information ('run_count') is included when 'is_aggregated' is True.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_status_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'RunMeta has run status fields' verifies that the test run meta object contains all required status fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the `RunMeta` object is not properly initialized with status fields, potentially causing issues downstream in the integration process.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'exit_code' field should be present in the data.</li>
                                        <li>The 'interrupted' field should be present in the data.</li>
                                        <li>The 'collect_only' field should be present in the data.</li>
                                        <li>The 'collected_count' field should be present in the data.</li>
                                        <li>The 'selected_count' field should be present in the data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_schema_version_defined</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the schema version is defined and matches a semver-like format.</p>
                                <p><strong>Why Needed:</strong> This test prevents regressions where the schema version is not defined or does not match a semver-like format.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `SCHEMA_VERSION` attribute of the TestSchema object should be set to a valid version string.</li>
                                        <li>The `SCHEMA_VERSION` attribute should contain at least one dot (`.`) character, indicating that it is semver-like.</li>
                                        <li>The `SCHEMA_VERSION` attribute should match a valid semver-like format according to the SemVer specification.</li>
                                        <li>If the schema version does not contain a dot, it should be replaced with an empty string or a default value (e.g., `'0.1.0'`).</li>
                                        <li>If the schema version is less than 1.0.0, it should be updated to the next available semver release.</li>
                                        <li>If the schema version is greater than 2.0.0, it should be updated to the latest available semver release.</li>
                                        <li>The `SCHEMA_VERSION` attribute should not change between test runs or when the TestSchema object is cloned.</li>
                                        <li>If the `SCHEMA_VERSION` attribute is set to a value that is not compatible with the current version of the application (e.g., 1.0.2), it should be updated to the latest available semver release.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_test_case_has_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `TestSchemaCompatibility` class is tested to ensure that it correctly validates the presence of required fields in a test case.</p>
                                <p><strong>Why Needed:</strong> This test prevents regressions where a test case might not have all necessary fields for schema compatibility checks.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'nodeid' field should be present in the `data` dictionary.</li>
                                        <li>The 'outcome' field should also be present in the `data` dictionary.</li>
                                        <li>The 'duration' field should be present in the `data` dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_gemini_returns_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_provider` function returns an instance of `GeminiProvider` when the `provider` parameter is set to `'gemini'`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the LLM model does not return a provider instance for the 'gemini' configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider(config)` should return an instance of `GeminiProvider`.</li>
                                        <li>The class name of the returned provider instance should be `'GeminiProvider'`.</li>
                                        <li>The method `__class__.__name__` of the returned provider instance should match `'GeminiProvider'`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 245, 247, 249, 252, 257, 262-263, 265)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_litellm_returns_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_provider` function returns an instance of LiteLLMProvider when the 'provider' parameter is set to 'litellm'.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider is not correctly identified as 'LiteLLMProvider', potentially leading to incorrect model selection or usage.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_provider` function should return an instance of `LiteLLMProvider` when the 'provider' parameter is set to 'litellm'.</li>
                                        <li>The provider name should match 'LiteLLMProvider'.</li>
                                        <li>The class name of the returned object should be 'LiteLLMProvider'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52-53, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_none_returns_noop</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> test_get_provider_with_none_config returns NoopProvider.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the LLM is not properly initialized with a None provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_provider` function should return an instance of `NoopProvider` when the `provider` parameter is set to 'none'.</li>
                                        <li>The `provider` attribute of the returned `NoopProvider` instance should be `None`.</li>
                                        <li>The `Config` class's `provider` parameter should not be used in this test.</li>
                                        <li>The `get_provider` function should raise an error when given a non-string provider value.</li>
                                        <li>The `get_provider` function should return `None` when the provided configuration is invalid.</li>
                                        <li>The `NoopProvider` instance should have no attributes or methods.</li>
                                        <li>The `provider` attribute of the `NoopProvider` instance should be `None`.</li>
                                        <li>The `Config` class's `provider` parameter should not be used in this test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_ollama_returns_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that when using Ollama as a provider, the returned class is indeed OllamaProvider.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the correct provider class is not imported due to an import error.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider.__class__.__name__ == 'OllamaProvider'</li>
                                        <li>provider is of type OllamaProvider</li>
                                        <li>The provider instance has a valid __class__.__name__ attribute</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 245, 247, 249, 252-253, 255)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_unknown_raises</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `get_provider` function raises a `ValueError` when an unknown provider is provided.</p>
                                <p><strong>Why Needed:</strong> To prevent unexpected behavior where an unknown provider might be used without proper validation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_provider` function should raise a `ValueError` with the message 'unknown' when called with an unknown provider.</li>
                                        <li>The error message should include the string 'unknown'.</li>
                                        <li>The error message should not contain any other strings besides 'unknown'.</li>
                                        <li>The error message should be case-insensitive (e.g., 'Unknown', 'unknown').</li>
                                        <li>The `Config` object passed to `get_provider` should have a valid provider.</li>
                                        <li>The `Config` object passed to `get_provider` should have a provider that is not 'unknown'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 245, 247, 249, 252, 257, 262, 267)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestLlmProviderContract::test_noop_implements_interface</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that NoopProvider implements LlmProvider interface.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of NoopProvider not implementing required methods.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider should have the required methods (annotate, is_available, get_model_name, config).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestNoopProvider::test_annotate_returns_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the NoopProvider returns an empty annotation when no annotation is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the provider does not return any annotation even if it's supposed to be empty.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>annotation is of type LlmAnnotation</li>
                                        <li>scenario is an empty string</li>
                                        <li>why_needed is an empty string</li>
                                        <li>key_assertions are an empty list</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestNoopProvider::test_get_model_name_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the NoopProvider returns an empty string when given an empty configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the model name is not returned correctly when the config is empty.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function provider.get_model_name() should return an empty string.</li>
                                        <li>The function provider.get_model_name() should be called with an argument that is an empty Config object.</li>
                                        <li>The function provider.get_model_name() should raise a ValueError with an appropriate message when given an invalid config.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 66)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestNoopProvider::test_is_available</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the NoopProvider instance is always available.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the provider might not be available due to configuration issues.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `is_available()` method should return True for all instances of the NoopProvider.</li>
                                        <li>The `is_available()` method should raise an AssertionError if it returns False.</li>
                                        <li>The `is_available()` method should not throw any exceptions when called on a valid instance of the NoopProvider.</li>
                                        <li>The `is_available()` method should behave consistently across different configurations.</li>
                                        <li>The `is_available()` method should return True for all instances with default configuration.</li>
                                        <li>The `is_available()` method should return False for instances with custom configuration that disables availability.</li>
                                        <li>The `is_available()` method should not be affected by the presence of any exceptions in its implementation.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 107, 110-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 58)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_emits_summary</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the annotation summary is printed when annotations run.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the annotation summary is not printed, potentially causing confusion or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider` from `pytest_llm_report.llm.annotator` returns a `FakeProvider` instance.</li>
                                        <li>The `annotate_tests` function calls `get_provider` with the correct configuration.</li>
                                        <li>The captured output contains the expected string 'Annotated 1 test(s) via litellm'.</li>
                                        <li>The `test_annotate_tests_emits_summary` function is called with the correct arguments and returns a `TestCaseResult` instance.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_reports_progress</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the LLM annotator reports progress via a callback.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the LLM annotator does not report progress, potentially causing issues with test reporting.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test verifies that the LLM annotation starts and ends with the correct messages.</li>
                                        <li>The test checks if the correct message is appended to the list of messages.</li>
                                        <li>The test verifies that the correct scenario is included in the message.</li>
                                        <li>The test checks if the progress callback is set correctly.</li>
                                        <li>The test verifies that the progress callback appends the correct message to the list.</li>
                                        <li>The test checks if the correct node ID is included in the message.</li>
                                        <li>The test verifies that the LLM annotation progresses as expected.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_respects_opt_out_and_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests that LLM annotations respect opt-out and limit settings.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that LLM annotations do not include opt-out tests or exceed the maximum number of tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'tests/test_a.py::test_a' node should be included in the LLM annotation.</li>
                                        <li>The 'tests/test_b.py::test_b' node should be excluded from the LLM annotation due to the 'llm_opt_out=True' parameter.</li>
                                        <li>The 'tests/test_c.py::test_c' node should not be included in the LLM annotation.</li>
                                        <li>The number of nodes in the LLM annotation should not exceed 1 (the maximum limit set by llm_max_tests=1).</li>
                                        <li>The 'tests/test_a.py::test_a' node should have an LLM annotation, while the other two nodes should not.</li>
                                        <li>The 'tests/test_b.py::test_b' and 'tests/test_c.py::test_c' nodes should not have any LLM annotations.</li>
                                        <li>The number of LLM annotations should be 1 (the maximum allowed).</li>
                                        <li>The 'tests/test_a.py::test_a' node should be the only one included in the LLM annotation.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 45, 48-49, 56-57, 59, 61-62, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_respects_rate_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the LLM annotator respects the requests-per-minute rate limit.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the LLM annotator exceeds the allowed requests per minute.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider's `calls` attribute should contain the expected list of node IDs.</li>
                                        <li>The `sleep_calls` attribute should contain the expected sleep duration in seconds.</li>
                                        <li>The `provider.calls` attribute should match the expected list of node IDs.</li>
                                        <li>The `sleep_calls` attribute should not exceed 2.0 seconds.</li>
                                        <li>The `provider.calls` attribute should be an array containing only the expected node ID.</li>
                                        <li>The `time.sleep(2.0)` call should have been made at least once during the test execution.</li>
                                        <li>The `time.sleep(0.0)` call should have been made before the first `time.sleep(2.0)` call.</li>
                                        <li>The `time.sleep` function should not be called more than 30 times in a minute.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-173, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_skips_unavailable_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that annotation fails when unavailable provider is specified.</p>
                                <p><strong>Why Needed:</strong> To prevent skipping of annotation tests due to unavailability of providers.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `llm.annotator.get_provider` should be called with a valid configuration.</li>
                                        <li>The `is_available` method of the `UnavailableProvider` class should return False.</li>
                                        <li>The message 'is not available' should be captured in the output of `capsys.readouterr()`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_uses_cache</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that annotations are cached between runs and that the annotation is correctly retrieved.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the annotation is lost after a cache flush or restart.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` attribute of the `TestCaseResult` instance should be set to `tests/test_sample.py::test_case` before calling `get_provider`.</li>
                                        <li>The `llm_annotation` attribute of the `TestCaseResult` instance should not be `None` after calling `annotate_tests` with a cached provider.</li>
                                        <li>The scenario of the annotation in the `TestCaseResult` instance should match the scenario specified by the cache directory.</li>
                                        <li>The `provider` attribute of the `TestCaseResult` instance should be set to `tests/test_sample.py::test_case` before calling `get_provider` again after setting it to a different provider.</li>
                                        <li>The `llm_annotation` attribute of the `TestCaseResult` instance should not be `None` after calling `annotate_tests` with a non-cached provider.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">30 lines (ranges: 39-41, 53, 55-56, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-81, 87-92, 97-98, 100, 102, 104, 115-122, 127, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the schema requires both "scenario" and "why_needed" fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the schema is not enforced correctly, potentially leading to incorrect validation of contracts.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>required</li>
                                        <li>scenario</li>
                                        <li>why_needed</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_from_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `AnnotationSchema` can correctly parse a dictionary containing required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential bugs in the `AnnotationSchema` where it may not be able to accurately determine the schema from a dictionary, potentially leading to incorrect or missing key assertions.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>checks password</li>
                                        <li>checks username</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> An annotation schema is created with an empty dictionary.</p>
                                <p><strong>Why Needed:</strong> The test prevents a potential bug where the schema creation process fails when given an empty input.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>schema.scenario = "" (empty string)</li>
                                        <li>schema.why_needed = "" (empty string) (to prevent schema creation failure)</li>
                                        <li>schema.scenario == "" (checks that the scenario is set to an empty string)</li>
                                        <li>schema.why_needed == "" (checks that the why needed message is set to an empty string)</li>
                                        <li>schema.scenario != "" (checks that the scenario is not set to a non-empty string)</li>
                                        <li>schema.why_needed != "" (checks that the why needed message is not set to a non-empty string)</li>
                                        <li>schema.scenario == "" (checks that the schema has an empty string as its value)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_partial</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the AnnotationSchema class correctly handles partial inputs by verifying if it matches the expected scenario.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the AnnotationSchema class does not handle partial inputs correctly, potentially leading to incorrect results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>schema.scenario == 'Partial only'</li>
                                        <li>assert schema.why_needed == ''</li>
                                        <li>schema.scenario should be equal to 'Partial only' according to the expected annotation</li>
                                        <li>schema.why_needed should be an empty string according to the expected annotation</li>
                                        <li>The AnnotationSchema class correctly handles partial inputs by verifying if it matches the expected scenario</li>
                                        <li>The AssertionSchema class correctly handles partial inputs by verifying if it matches the expected scenario</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_has_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the annotation schema has required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the annotation schema is missing required fields, potentially causing errors or inconsistencies during validation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert 'scenario' in ANNOTATION_JSON_SCHEMA['properties']</li>
                                        <li>assert 'why_needed' in ANNOTATION_JSON_SCHEMA['properties']</li>
                                        <li>assert 'key_assertions' in ANNOTATION_JSON_SCHEMA['properties']</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> TestAnnotationSchema::test_schema_to_dict verifies that the AnnotationSchema instance correctly serializes to a dictionary.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the AnnotationSchema instance can be serialized and deserialized correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assertion 1: The 'scenario' key in the dictionary matches the expected value.</li>
                                        <li>assertion 2: The 'why_needed' key in the dictionary matches the expected value.</li>
                                        <li>assertion 3: The 'key_assertions' list in the dictionary contains all the expected assertions.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 90-92, 94-96, 98)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_from_factory</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the Factory returns a NoopProvider when the provider is set to 'none'.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the Factory returns an incorrect provider for the 'none' provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider(config)` should return a NoopProvider instance.</li>
                                        <li>The `provider` attribute of the returned `NoopProvider` instance should be set to 'none'.</li>
                                        <li>The `provider` attribute of the returned `NoopProvider` instance should not be set to any other provider.</li>
                                        <li>The `provider` attribute of the returned `NoopProvider` instance should have a value of 'none'.</li>
                                        <li>The `provider` attribute of the returned `NoopProvider` instance should not have a value of an empty string or None.</li>
                                        <li>The `provider` attribute of the returned `NoopProvider` instance should not be set to any other provider.</li>
                                        <li>The `provider` attribute of the returned `NoopProvider` instance should have a value that is different from 'none'.</li>
                                        <li>If the Factory returns an incorrect provider, it should raise an assertion error.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_is_llm_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `NoopProvider` class correctly inherits from `LlmProvider`.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the `NoopProvider` class is incorrectly implemented as an instance of `LlmProvider` instead of its intended interface.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` variable should be an instance of `LlmProvider`.</li>
                                        <li>The `provider` variable should not have any additional attributes or methods beyond what is defined in the `LlmProvider` interface.</li>
                                        <li>The `provider` variable's type should match the expected type (`LlmProvider`) without any conversions or casts.</li>
                                        <li>No other attributes or methods of `provider` should be present, as they are not part of the `LlmProvider` interface.</li>
                                        <li>Any additional imports or dependencies required by the `NoopProvider` class should not affect its functionality in this test.</li>
                                        <li>The `config` object passed to `NoopProvider` should not have any unexpected side effects on the LLM provider implementation.</li>
                                        <li>The `noop` function provided with `NoopProvider` should be called correctly and without any errors.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_returns_empty_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The NoopProvider returns an empty annotation when the test function does not contain any annotations.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the NoopProvider would incorrectly return an annotation for a test function with no annotations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert result.scenario == "" (empty string)</li>
                                        <li>assert result.why_needed == "" (empty string)</li>
                                        <li>assert result.key_assertions == [] (no key assertions performed)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_annotate_returns_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Annotating a TestCaseResult object with the correct configuration and provider.</p>
                                <p><strong>Why Needed:</strong> The test prevents regression in case of incorrect annotation configuration or provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `result` object has attributes `scenario`, `why_needed`, and `key_assertions`.</li>
                                        <li>The `result` object has the expected attributes for a TestCaseResult.</li>
                                        <li>The `result` object's `why_needed` attribute is set correctly based on the annotation configuration.</li>
                                        <li>The `result` object's `key_assertions` list contains the expected checks.</li>
                                        <li>The `result` object's `scenario` attribute is not empty.</li>
                                        <li>The `result` object's `why_needed` attribute does not contain any invalid values.</li>
                                        <li>The `result` object's `key_assertions` list does not contain any missing or invalid assertions.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_handles_empty_code</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the ProviderContract handles an empty code by returning a successful annotation.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the contract does not handle empty code correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider should return a valid result for the given input.</li>
                                        <li>The result should be non-null.</li>
                                        <li>The annotation outcome should be 'passed'.</li>
                                        <li>No exception should be raised when providing an empty code.</li>
                                        <li>The contract should not throw any errors or exceptions when handling empty code.</li>
                                        <li>The test should pass without any failures or errors.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_handles_none_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `provider.annotate` method should return a non-None value when given an annotation with `None` context.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `provider.annotate` method returns `None` due to the presence of `None` in the annotation context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `result` variable should not be `None` after calling `provider.annotate(test, 'code', None)`.</li>
                                        <li>The `result` variable is expected to have a non-`None` value.</li>
                                        <li>The `provider.annotate` method should return a valid object (e.g., `TestCaseResult`) instead of `None`.</li>
                                        <li>The annotation context (`'code'`) should be properly propagated to the `TestCaseResult` instance.</li>
                                        <li>The test result should not be affected by the presence of `None` in the annotation context.</li>
                                        <li>The `test_result` object returned by `provider.annotate` should have a valid state (e.g., `passed`, `failed`, etc.)</li>
                                        <li>The test case should pass without any errors or exceptions when given an annotation with `None` context.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_has_annotate_method</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that all providers have an 'annotate' method.</p>
                                <p><strong>Why Needed:</strong> Prevent regression in the contract where providers are not annotated with a specific method.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider has an attribute named 'annotate'.</li>
                                        <li>The provider is callable.</li>
                                        <li>The provider has an 'annotate' method.</li>
                                        <li>The provider's 'annotate' method is callable.</li>
                                        <li>All providers have the same 'annotate' method.</li>
                                        <li>The 'annotate' method is present in all tested providers.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 52-53, 245, 247, 249-250, 252-253, 255, 257-258, 260, 262-263, 265)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_handles_context_too_large</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `annotate` method handles large contexts correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the `annotate` method throws an exception when dealing with very large contexts.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `annotate` method should not throw an exception when given a context of size up to 1000.</li>
                                        <li>The `annotate` method should return an empty list for a context larger than 1000.</li>
                                        <li>The `annotate` method should raise a `ValueError` with a meaningful error message for a context larger than 1000.</li>
                                        <li>The `annotate` method should not throw an exception when given a context of size 0.</li>
                                        <li>The `annotate` method should return the expected result (an empty list) for a context of size 1.</li>
                                        <li>The `annotate` method should raise a `ValueError` with a meaningful error message for a context larger than 1000.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52-53, 72, 75-76, 80, 165, 167, 175)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">155 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-221, 233, 245-248, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417-418, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_dependency</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LiteLLMProvider annotates missing dependencies correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider does not report missing dependencies and instead installs them silently.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation message is correct and informative.</li>
                                        <li>The error message includes the name of the missing dependency (litellm).</li>
                                        <li>The annotation message provides instructions on how to install the missing dependency (pip install litellm).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 134, 136-139, 141-142, 160-164)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_token</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `annotate` method fails when an API token is missing.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `annotate` method throws an error due to an unprovided API token.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `annotation.error` attribute should be set to 'GEMINI_API_TOKEN is not set'.</li>
                                        <li>The `provider.annotate(test, ...)` call should raise an exception.</li>
                                        <li>The `test_case()` function should throw a `AssertionError` with the message 'GEMINI_API_TOKEN is not set'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 134, 136-139, 141-142, 160-161, 167-169)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_records_tokens</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that tokens are recorded on the limiter correctly.</p>
                                <p><strong>Why Needed:</strong> Prevents regressions where token usage is not recorded.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'json' key in the captured dictionary contains the expected response data.</li>
                                        <li>The 'totalTokenCount' key in the captured dictionary matches the expected value.</li>
                                        <li>The 'candidates' list in the captured dictionary includes at least one record with a valid token count.</li>
                                        <li>The 'usageMetadata' dictionary contains the expected rate limits.</li>
                                        <li>The 'models?' query parameter is present in the URL and has the correct model name.</li>
                                        <li>The 'rateLimits?' query parameter is present in the URL and has the correct rate limit value.</li>
                                        <li>The 'tokensPerMinute?' query parameter is present in the URL and matches the expected value.</li>
                                        <li>The 'tokenUsage' list in the captured dictionary contains at least one record with a valid token count.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">183 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-223, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-343, 346, 348-350, 352-355, 360-366, 368, 370-371, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_retries_on_rate_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `annotate` method of the `GeminiProvider` class should retry when rate limiting occurs.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `annotate` method fails to retry after rate limiting, causing the model to be stuck in an infinite loop.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `annotate` method should call `self._retry_on_rate_limit` before making the annotation request.</li>
                                        <li>The `annotate` method should call `self._retry_on_rate_limit` again after a certain amount of time (e.g. 1 second) if the initial retry fails.</li>
                                        <li>The `annotate` method should raise an exception when rate limiting occurs and retries are not allowed.</li>
                                        <li>The `annotate` method should log a warning message indicating that rate limiting occurred, but no retry was made.</li>
                                        <li>The `_retry_on_rate_limit` method should be called with the correct arguments (e.g. `self._rate_limit`, `self._max_retries`) when rate limiting occurs.</li>
                                        <li>The `_retry_on_rate_limit` method should call `self._logger.warning` to log a warning message indicating that rate limiting occurred, but no retry was made.</li>
                                        <li>The `annotate` method should not make any requests during the retry period (e.g. 1 second).</li>
                                        <li>The `annotate` method should raise an exception when it is called multiple times in quick succession while rate limiting occurs.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">181 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 233-234, 238-240, 242-243, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330-333, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-369, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_rotates_models_on_daily_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `annotate` method of the `GeminiProvider` class rotates models on the daily limit when used with a large number of annotations.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the model rotation is not applied correctly to large datasets, leading to performance issues or incorrect results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `annotate` method should rotate models on the daily limit for all annotations in the dataset.</li>
                                        <li>The number of rotated models should be equal to the total number of annotations minus one (for the last annotation),</li>
                                        <li>The model rotation should occur immediately after the first annotation, not after a certain delay or threshold.</li>
                                        <li>The `annotate` method should handle cases where there are multiple annotations with the same label correctly.</li>
                                        <li>The model rotation should be applied to all models in the dataset, regardless of their labels or types.</li>
                                        <li>The test should pass for both positive and negative annotations (e.g., 1000 positive annotations and -500 negative annotations).</li>
                                        <li>The `annotate` method should not rotate models on the daily limit if there are no annotations in the dataset.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">177 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419-420, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_skips_on_daily_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the annotate function skips daily limits when provided with a valid provider.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the annotate function fails to skip daily limits due to incorrect handling of the provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotate function should not attempt to annotate data for providers with daily limits.</li>
                                        <li>The annotate function should return an error message indicating that daily limits are exceeded.</li>
                                        <li>The annotate function should update the provider's annotation status to 'skipped' when a valid provider is provided.</li>
                                        <li>The annotate function should not attempt to annotate data for providers without daily limits.</li>
                                        <li>The annotate function should return an error message indicating that no valid provider was found.</li>
                                        <li>The annotate function should update the provider's annotation status to 'skipped' when a valid provider with a daily limit is provided.</li>
                                        <li>The annotate function should only skip annotations for providers with daily limits, not for those without.</li>
                                        <li>The annotate function should handle cases where the provider has no daily limit or an invalid daily limit value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">184 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-208, 210-211, 213-214, 217-222, 225-227, 252-254, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_success_with_mock_response</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LiteLLM provider annotates a successful response with the correct key assertions and confidence level.</p>
                                <p><strong>Why Needed:</strong> Prevents regression by ensuring that the annotation is correctly configured for success scenarios.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>status ok</li>
                                        <li>redirect</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">177 lines (ranges: 39-42, 45-46, 48-49, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_exhausted_model_recovers_after_24h</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the exhausted model recovers after 24 hours.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the model does not recover from exhaustion within 24 hours.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The model's performance metrics (e.g. accuracy, F1 score) should return to normal within 24 hours of being exhausted.</li>
                                        <li>The model's training time should decrease by at least 50% after 24 hours of exhaustion.</li>
                                        <li>The model's inference time should increase by less than 20% after 24 hours of exhaustion.</li>
                                        <li>The model's memory usage should decrease by at least 30% after 24 hours of exhaustion.</li>
                                        <li>The model's GPU utilization should decrease by at least 40% after 24 hours of exhaustion.</li>
                                        <li>The model's disk I/O time should increase by less than 15% after 24 hours of exhaustion.</li>
                                        <li>The model's memory allocation should not exceed the available memory for more than 24 hours.</li>
                                        <li>The model's CPU utilization should not exceed 80% for more than 24 hours.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">190 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-188, 190-191, 193-194, 196, 200-208, 210-211, 213-214, 217-222, 225-227, 252-254, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_fetch_available_models_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `fetch_available_models` method throws an error when no models are available.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the `fetch_available_models` method might return incorrect results or raise an exception due to a missing model.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `fetch_available_models` method raises a `ValueError` with a meaningful error message when no models are available.</li>
                                        <li>The method does not throw an exception when all models are available.</li>
                                        <li>The method returns the correct list of available models when all models are available.</li>
                                        <li>The method throws an exception with a specific error message when all models are available.</li>
                                        <li>The `fetch_available_models` method raises an exception with a more informative error message than `ValueError`.</li>
                                        <li>The `fetch_available_models` method does not raise an exception when the input is invalid or empty.</li>
                                        <li>The `fetch_available_models` method returns an empty list when all models are available.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 134, 136-139, 141-142, 280, 282-283, 286-290, 292-295, 297-298, 300-301, 346, 348-350, 352-355, 360-363, 374-377, 385, 387, 391-392, 396-402, 405, 408-410, 412-414, 417-418, 428, 430-432, 435-436)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_model_list_refreshes_after_interval</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The model list should refresh after an interval of 10 seconds.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the model list does not update after a certain time period, potentially causing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>model_list is updated with new models within the specified interval</li>
                                        <li>no stale models are present in the model list</li>
                                        <li>the model list size remains constant despite changes to the underlying data source</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">169 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-369, 374-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_handles_completion_error</span>
                        <div class="test-meta">
                            <span>6.00s</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the LiteLLMProvider annotates completion errors correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the annotation does not surface completion errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation should contain an error message indicating a completion error.</li>
                                        <li>The annotation should raise a RuntimeError when encountering a completion error.</li>
                                        <li>The annotation should include the string 'boom' in its error message.</li>
                                        <li>The annotation should not ignore completion errors and instead surface them correctly.</li>
                                        <li>The annotation should not silently fail when encountering a completion error.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 78-79, 81-82, 84-85)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_invalid_key_assertions</span>
                        <div class="test-meta">
                            <span>6.00s</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the LiteLLMProvider annotates invalid key_assertions payloads correctly.</p>
                                <p><strong>Why Needed:</strong> To prevent unexpected behavior when invalid key_assertions are provided to the annotate method.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'key_assertions' parameter must be a list of strings.</li>
                                        <li>Invalid response: key_assertions must be a list</li>
                                        <li>Key assertions should not contain any string values (e.g., None, empty strings).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207-209)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69, 73, 76, 81-82, 84-85)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_missing_dependency</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The LiteLLMProvider should report a missing dependency when trying to annotate a case where the provider is not installed.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the LiteLLMProvider does not correctly handle cases where it is not installed, leading to incorrect error messages.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation will contain an error message indicating that the `litellm` dependency is missing and how to install it.</li>
                                        <li>The annotation will include a specific URL pointing to the installation instructions for `litellm`.</li>
                                        <li>The annotation will correctly identify the provider as 'litellm' even if it has not been installed.</li>
                                        <li>The annotation will not report any other error messages or warnings when trying to annotate a case where the provider is missing.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 37-41)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_success_with_mock_response</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LiteLLMProvider annotates a successful response with the correct key assertions and confidence level.</p>
                                <p><strong>Why Needed:</strong> Prevents regressions by ensuring that the annotation is correctly configured for a successful response.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>status ok</li>
                                        <li>redirect</li>
                                        <li>confidence >= 0.8</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69-70)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_is_available_with_module</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the LiteLLM provider detects installed modules correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the provider does not detect installed modules, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `is_available()` method of the `LiteLLMProvider` instance should return `True` when the `litellm` module is available in the system's module list.</li>
                                        <li>The `sys.modules` dictionary should contain a reference to the `fake_litellm` SimpleNamespace created during setup.</li>
                                        <li>The `is_available()` method should not raise an exception or throw any other error when the `litellm` module is installed.</li>
                                        <li>The provider's configuration should be able to detect the installed `litellm` module without requiring manual intervention.</li>
                                        <li>The test should pass even if the `litellm` module is installed but not imported directly (e.g., as a package).</li>
                                        <li>The provider's behavior should remain consistent across different Python versions and environments.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 107, 110-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 94-95, 97)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_fallbacks_on_context_length_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the annotate fallbacks on context length error are correctly handled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the LLM provider may not be able to handle context length errors properly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>When an invalid context is provided, it should throw a ContextLengthError with a meaningful message.</li>
                                        <li>The annotate method should raise a ValueError when the context length exceeds the maximum allowed value.</li>
                                        <li>The error message should include information about the original input that caused the error.</li>
                                        <li>The test should be able to reproduce the error using different inputs (e.g., different contexts, different LLM providers).</li>
                                        <li>The error should not be silently ignored or masked by other parts of the code.</li>
                                        <li>The annotate method should handle context length errors in a way that is consistent with the expected behavior for valid inputs.</li>
                                        <li>The test should pass even when the LLM provider returns an error response (e.g., 500 Internal Server Error).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">33 lines (ranges: 52-53, 72, 75-76, 78, 165, 167-173, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-60, 62-63, 66-67)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_handles_call_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the annotate method returns an error message when a call to Ollama raises a RuntimeError.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in handling call errors by ensuring that the annotation function correctly identifies and reports any errors that occur during the annotation process.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation function should return the string 'Failed after 3 retries. Last error: boom' as its error message when a call to Ollama raises a RuntimeError.</li>
                                        <li>The annotation function should not report an error if the call to Ollama is successful.</li>
                                        <li>The annotation function should correctly handle multiple retries and update the error message accordingly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-59, 71-72, 74-75, 77-78)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_missing_httpx</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that OllamaProvider annotates missing httpx dependency correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the provider incorrectly reports an error message for missing httpx, leading to incorrect user experience.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation error is set to 'httpx not installed. Install with: pip install httpx'.</li>
                                        <li>The provider correctly reports that the httpx dependency is missing.</li>
                                        <li>The test passes if the annotation error matches this expected message.</li>
                                        <li>The test fails if the annotation error does not match this expected message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 40-44)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_success_full_flow</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Ollama provider full annotation flow with mocked HTTP to test annotate success for full flow</p>
                                <p><strong>Why Needed:</strong> Prevents regression in annotating full LLM flow due to potential issues with status checks and token validation</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>check status of response after successful login</li>
                                        <li>validate token sent by Ollama provider during annotation process</li>
                                        <li>assert that the annotated function returns True without any errors</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-60, 62-63, 114, 116-123, 127-130, 132, 134-135)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_success</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Ollama provider makes correct API call when calling OLLAMA successfully.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the OLLAMA provider fails to make a successful API call.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'url' captured in the captured dictionary is set to 'http://localhost:11434/api/generate'.</li>
                                        <li>The 'json' captured in the captured dictionary contains the correct model and prompt data.</li>
                                        <li>The 'system' captured in the captured dictionary contains the correct system prompt data.</li>
                                        <li>The 'stream' captured in the captured dictionary is False.</li>
                                        <li>The API call timeout is set to 60 seconds as configured in the provider settings.</li>
                                        <li>The response from OLLAMA matches the expected response.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 114, 116-123, 127-130, 132, 134-135)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_uses_default_model</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the default model is used when not specified for the Ollama provider.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the default model of Ollama is not used.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The captured response from the API should contain an empty string as the 'model' key.</li>
                                        <li>The captured response from the API should contain 'llama3.2' as the 'model' value.</li>
                                        <li>The captured response from the API should not contain any other values for the 'model' key.</li>
                                        <li>The captured response from the API should be an object with a single property 'model' of type str.</li>
                                        <li>The captured response from the API should have no other properties or values in the 'model' field.</li>
                                        <li>The captured response from the API should not contain any additional keys or values for the 'model' key.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 114, 116-123, 127-130, 132, 134-135)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the Ollama provider returns False when the server is unavailable.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the provider incorrectly assumes the server is running and always returns True for availability checks.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_check_availability()` method of the `OllamaProvider` instance should return `False` when the server is not available.</li>
                                        <li>The `Config` instance passed to the `OllamaProvider` constructor should have a `server_url` attribute that points to a non-existent URL.</li>
                                        <li>The `get()` method called on the `fake_httpx` object raised as a `ConnectionError` when it attempts to connect to the server.</li>
                                        <li>The `httpx` module imported from the `sys.modules` dictionary is not set to point to the fake HTTPX instance.</li>
                                        <li>The `_check_availability()` method of the `OllamaProvider` instance should raise an exception or return a specific value indicating failure when the server is unavailable.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 87-88, 90-91, 93-94)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_non_200</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the Ollama provider returns False for non-200 status codes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the provider incorrectly returns True for non-200 status codes, potentially leading to incorrect usage or unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The method _check_availability() of the OllamaProvider instance is called with no arguments.</li>
                                        <li>The status code returned by the FakeResponse object is not equal to 200.</li>
                                        <li>The provider's availability is set to False using the monkeypatched sys.modules['httpx'].</li>
                                        <li>The provider's availability is checked using the _check_availability() method.</li>
                                        <li>The assertion that provider._check_availability() returns a boolean value (True or False) is performed.</li>
                                        <li>The assertion that provider._check_availability() returns 200 is not performed because it does not match the expected status code.</li>
                                        <li>The assertion that provider._check_availability() returns False is performed.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 87-88, 90-92)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_success</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the Ollama provider checks availability via /api/tags endpoint successfully.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the provider does not check for availability before returning data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider should return True when checking for availability.</li>
                                        <li>The provider should return False when checking for inactivity.</li>
                                        <li>The provider's response status code should be 200 (OK) when checking for tags.</li>
                                        <li>The provider's response body should contain the expected list of available Ollama models.</li>
                                        <li>The provider's response headers should contain the expected 'Content-Type' and 'X-Content-Type-Options: nosniff' headers.</li>
                                        <li>The provider's response status code should be 200 (OK) when checking for tags with a specific tag name.</li>
                                        <li>The provider's response body should contain the expected list of available Ollama models with the specified tag name.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 87-88, 90-92)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_is_local_returns_true</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The Ollama provider should always return `is_local=True`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider incorrectly returns `False` for local configurations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider.is_local() == True</li>
                                        <li>provider.config.provider == 'ollama'</li>
                                        <li>provider.config is not None</li>
                                        <li>not provider.config.provider == 'unknown' or 'non-ollama'</li>
                                        <li>config is a valid instance of Config with provider set to 'ollama'</li>
                                        <li>is_local() should be True for local configurations</li>
                                        <li>is_local() should return False for non-local configurations</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_json</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `OllamaProvider` class throws an error when parsing a response with invalid JSON.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the Ollama provider incorrectly reports valid responses as invalid JSON.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The expected error message is 'Failed to parse LLM response as JSON'.</li>
                                        <li>The `annotation.error` attribute contains the correct error message.</li>
                                        <li>The `provider._parse_response()` method returns an instance of `OllamaProviderError` with the specified error message.</li>
                                        <li>The `OllamaProvider` class has a docstring that explains why it throws this error.</li>
                                        <li>The test verifies that the provider correctly reports invalid JSON responses.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 52-53, 186-187, 190-192)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-52, 55)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_key_assertions</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The Ollama provider rejects invalid key_assertions payloads.</p>
                                <p><strong>Why Needed:</strong> To prevent the Ollama provider from incorrectly handling or rejecting malformed key_assertions payloads.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>must be a list</li>
                                        <li>contains only valid keys</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207-209)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The provided test verifies that the Ollama provider correctly parses JSON from markdown code fences.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider incorrectly extracts JSON or fails to parse certain types of data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>response_json_is_valid</li>
                                        <li>provider_returns_code_fence</li>
                                        <li>provider_extracts_correct_data</li>
                                        <li>provider_does_not_return_empty_string</li>
                                        <li>provider_returns_non_empty_string_if_missing</li>
                                        <li>provider_returns_properly_formatted_json</li>
                                        <li>provider_extracts_correct_key_value_pairs</li>
                                        <li>provider_extracts_correct_object_types</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 38, 42-44, 46-47)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_plain_fence</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The provided test verifies that the Ollama provider correctly parses a JSON response in a plain markdown fence without any language specification.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues where the provider may incorrectly parse or ignore non-language marked-up text as JSON.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `response` variable is assigned a string containing a valid JSON representation of a plain markdown fence.</li>
                                        <li>The `provider.parse_response_json(response)` call returns True, indicating that the JSON was successfully parsed.</li>
                                        <li>The `response` variable contains only whitespace characters and Markdown syntax (e.g., ```, ``), which should be ignored by the provider.</li>
                                        <li>The `config.provider` attribute is set to 'ollama', which indicates that the provider has been configured with an Ollama instance.</li>
                                        <li>The `provider.config` attribute provides access to the configuration object, allowing for customization of the provider's behavior.</li>
                                        <li>The `provider.parse_response_json_in_plain_fence` method is called with a valid JSON response, demonstrating its effectiveness in this scenario.</li>
                                        <li>Any errors or exceptions raised during the parsing process are likely to be caught and handled by the provider's error handling mechanisms.</li>
                                        <li>The test ensures that the provider can handle various edge cases, such as malformed or incomplete JSON responses.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 38, 42-44, 46-47)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_success</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Ollama provider parses valid JSON responses with success.</p>
                                <p><strong>Why Needed:</strong> Prevents bugs that occur when parsing invalid or malformed JSON responses.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert a is not None</li>
                                        <li>assert b is not None</li>
                                        <li>assert response_data['scenario'] == 'Tests feature'</li>
                                        <li>assert response_data['why_needed'] == 'Stops bugs'</li>
                                        <li>assert annotation.confidence == 0.8</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestArtifactEntry::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `CoverageEntry.to_dict()` correctly serializes the object.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the serialized representation of `CoverageEntry` is incorrect, potentially leading to data corruption or inconsistencies in the test results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key should contain the expected value.</li>
                                        <li>The 'line_ranges' key should contain the expected string representation.</li>
                                        <li>The 'line_count' key should contain the expected integer value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 254-257)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestCollectionError::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `CoverageEntry.to_dict()` returns the expected data structure.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `CoverageEntry` object is not properly serialized to JSON.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key should contain the correct value.</li>
                                        <li>The 'line_ranges' key should contain the correct values.</li>
                                        <li>The 'line_count' key should contain the correct value.</li>
                                        <li>The 'coverage' key should be missing.</li>
                                        <li>The 'start' and 'end' keys for line ranges should be present with correct values.</li>
                                        <li>All required keys ('file_path', 'line_ranges', 'line_count') should be included in the output JSON.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 207-209)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestCoverageEntry::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `CoverageEntry.to_dict()` correctly serializes a CoverageEntry object.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when updating the `CoverageEntry` class to include additional fields in its serialization.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' field of the serialized CoverageEntry should match the original file path.</li>
                                        <li>The 'line_ranges' field of the serialized CoverageEntry should match the expected line ranges.</li>
                                        <li>The 'line_count' field of the serialized CoverageEntry should match the original value.</li>
                                        <li>Additional fields (if any) in the `CoverageEntry` class should not be included in the serialization.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 40-43)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_empty_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> An empty annotation should be created with default values.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where an empty annotation would have no confidence or error value.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `annotation` object has the expected attributes: `scenario`, `why_needed`, and `error`.</li>
                                        <li>The `confidence` attribute is set to `None` as it should be for an empty annotation.</li>
                                        <li>The `error` attribute is also set to `None` as it should be for an empty annotation.</li>
                                        <li>The `key_assertions` list is empty, indicating that the test does not verify any specific key assertions.</li>
                                        <li>The `annotation` object has no attributes other than `scenario`, `why_needed`, and `error`.</li>
                                        <li>The `confidence` attribute is missing from the `annotation` object.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_to_dict_minimal</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `to_dict` method of `LlmAnnotation` returns a dictionary with required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the minimal annotation format includes all necessary information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The dictionary must contain 'scenario' key.</li>
                                        <li>The dictionary must contain 'why_needed' key.</li>
                                        <li>The dictionary must contain 'key_assertions' key.</li>
                                        <li>The dictionary should not contain 'confidence' key when it is None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 104-107, 109, 111, 113, 115)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_to_dict_with_all_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test to dictionary with all fields</p>
                                <p><strong>Why Needed:</strong> Prevents LlmAnnotation model from being misconfigured or incorrectly annotated.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'scenario' field is correctly set to the expected value.</li>
                                        <li>The 'confidence' field matches the expected confidence level (0.95).</li>
                                        <li>The 'context_summary' dictionary contains the correct mode and bytes values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 104-107, 109-111, 113-115)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_default_report</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test default report has expected schema version and empty lists.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the report is missing the required 'schema_version' key or contains non-empty 'tests', 'warnings', or 'collection_errors' lists.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert d['schema_version'] == SCHEMA_VERSION</li>
                                        <li>assert d['tests'] == []</li>
                                        <li>assert 'warnings' not in d</li>
                                        <li>assert 'collection_errors' not in d</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_report_with_collection_errors</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Report with Collection Errors should be verified to include collection errors.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the report does not accurately reflect collection errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'collection_errors' key in the report dictionary should contain exactly one item.</li>
                                        <li>The value of the 'nodeid' key in the first item of the 'collection_errors' list should be 'test_bad.py'.</li>
                                        <li>All items in the 'collection_errors' list should have a non-empty 'nodeid' attribute.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">58 lines (ranges: 207-209, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508-510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_report_with_warnings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test reports with warnings should be handled correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the report does not include all warnings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of `d['warnings']` is equal to 1.</li>
                                        <li>The code in `d['warnings'][0]` matches 'W001'.</li>
                                        <li>All warnings are included in the report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">60 lines (ranges: 229-231, 233, 235, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_tests_sorted_by_nodeid</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests should be sorted by nodeid in output.</p>
                                <p><strong>Why Needed:</strong> This test prevents regressions where the order of tests is not guaranteed to match their original nodeids.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The list of nodeids matches the expected order.</li>
                                        <li>All nodeids are present in the list.</li>
                                        <li>Nodeids are sorted correctly (a -> m, a -> z).</li>
                                        <li>No duplicate nodeids are present in the list.</li>
                                        <li>All tests have unique nodeids.</li>
                                        <li>Test nodes are not duplicated in the list.</li>
                                        <li>The order of tests is preserved.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">71 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportWarning::test_to_dict_with_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `to_dict` method of `ReportWarning` returns a dictionary with 'detail' key.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the detailed warning message is not included in the dictionary returned by the `to_dict` method, potentially causing confusion or errors when working with warnings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of 'detail' in the dictionary returned by `warning.to_dict()` should be '/path/to/file'.</li>
                                        <li>The key 'detail' exists in the dictionary returned by `warning.to_dict()` and its value is indeed '/path/to/file'.</li>
                                        <li>The value of 'detail' in the dictionary returned by `warning.to_dict()` does not contain any additional information.</li>
                                        <li>The warning object passed to `ReportWarning` has a valid `to_dict` method that returns a dictionary with the required keys.</li>
                                        <li>The `to_dict` method of `ReportWarning` is correctly implemented and does not return an empty dictionary or raise an exception.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 229-231, 233-235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportWarning::test_to_dict_without_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_to_dict_without_detail' verifies that a ReportWarning instance can be converted to a dictionary without including its detail information.</p>
                                <p><strong>Why Needed:</strong> This test prevents a warning about missing detail from being included in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'code' key should contain the expected value 'W001'.</li>
                                        <li>The 'message' key should contain the expected value 'No coverage'.</li>
                                        <li>The 'detail' key should be absent from the dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 229-231, 233, 235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_aggregation_fields_present</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that RunMeta has aggregation fields.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where RunMeta is missing required aggregation fields.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert d['run_id'] == 'run-123'</li>
                                        <li>assert d['run_group_id'] == 'group-456'</li>
                                        <li>assert d['is_aggregated'] is True</li>
                                        <li>assert d['aggregation_policy'] == 'merge'</li>
                                        <li>assert d['run_count'] == 3</li>
                                        <li>assert len(d['source_reports']) == 2</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 277-279, 281-283, 364-380, 382, 385, 387, 390, 393, 395, 397, 399-405, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_llm_fields_excluded_when_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM fields are excluded when annotations are disabled.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where LLMs' annotation settings are not properly handled when annotations are disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm_annotations_enabled' key is present in the data.</li>
                                        <li>The 'llm_provider' key is present in the data.</li>
                                        <li>The 'llm_model' key is present in the data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_llm_traceability_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that LLM traceability fields are included when enabled for a RunMeta object.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the case where LLMs are not properly configured or annotated, potentially leading to incorrect results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `llm_annotations_enabled` is True.</li>
                                        <li>The value of `llm_provider` is set to 'ollama'.</li>
                                        <li>The value of `llm_model` is set to 'llama3.2:1b'.</li>
                                        <li>The value of `llm_context_mode` is set to 'complete'.</li>
                                        <li>The value of `llm_annotations_count` is 10.</li>
                                        <li>The value of `llm_annotations_errors` is 2.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407-419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_non_aggregated_excludes_source_reports</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that non-aggregated reports do not include source_reports.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where non-aggregated reports incorrectly include source_reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'source_reports' key should be absent from the report dictionary.</li>
                                        <li>The 'is_aggregated' value should be set to False for non-aggregated reports.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_run_meta_to_dict_full</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test RunMeta to dict with all optional fields.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in RunMeta serialization when including legacy fields.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that the 'git_sha' field is correctly set to 'abc1234'.</li>
                                        <li>Assert that 'git_dirty' is True.</li>
                                        <li>Verify that 'repo_version' and 'repo_git_sha' are correctly set to '1.0.0'.</li>
                                        <li>Assert that 'repo_git_dirty' is False.</li>
                                        <li>Verify that 'plugin_git_sha' is correctly set to 'def5678'.</li>
                                        <li>Assert that 'plugin_git_dirty' is False.</li>
                                        <li>Verify the length of 'source_reports' is 1 as expected.</li>
                                        <li>Verify all required fields are present in the output dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">49 lines (ranges: 277-279, 281-283, 364-380, 382-405, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_run_status_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test RunMeta to include run status fields.</p>
                                <p><strong>Why Needed:</strong> The test prevents a regression where the 'RunMeta' object does not contain all necessary run status fields.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'exit_code' field is set to 1.</li>
                                        <li>The 'interrupted' field is True.</li>
                                        <li>The 'collect_only' field is True.</li>
                                        <li>The 'collected_count' field equals 10.</li>
                                        <li>The 'selected_count' field equals 8.</li>
                                        <li>The 'deselected_count' field equals 2.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSchemaVersion::test_schema_version_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the schema version is correctly formatted as a semver string.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the schema version is not in a valid semver format.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The schema version should be split into three parts (e.g., '1.2.3')</li>
                                        <li>Each part of the schema version should be a digit (0-9)</li>
                                        <li>The first part of the schema version should not be zero (e.g., '1', not '0')</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSchemaVersion::test_schema_version_in_report_root</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `ReportRoot` class includes the schema version in its report root.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the schema version is not included in the report root, potentially causing issues with data integrity or reporting accuracy.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `schema_version` attribute of the `ReportRoot` instance should be equal to `SCHEMA_VERSION`.</li>
                                        <li>The value of `schema_version` should match the actual schema version stored in the report root.</li>
                                        <li>The `to_dict()` method of the `ReportRoot` instance should return a dictionary with a `schema_version` key that matches `SCHEMA_VERSION`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSourceCoverageEntry::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> CoverageEntry should serialize correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the coverage entry's line ranges are not properly serialized to JSON.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key is set to 'src/foo.py'.</li>
                                        <li>The 'line_ranges' key is set to '1-3, 5, 10-15'.</li>
                                        <li>The 'line_count' key is set to 10.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 71-78)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSourceReport::test_to_dict_minimal</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `to_dict` method of LlmAnnotation returns a dictionary with required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents bugs or regressions where the minimal annotation is missing certain required fields.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'scenario' field should be present in the dictionary.</li>
                                        <li>The 'why_needed' field should be present in the dictionary.</li>
                                        <li>The 'key_assertions' field should be present in the dictionary.</li>
                                        <li>The 'confidence' field should not be included in the dictionary when it is None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 277-279, 281, 283)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSourceReport::test_to_dict_with_run_id</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test SourceReport to_dict_with_run_id verifies that the 'run_id' key is present in the output dictionary.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the 'run_id' field is not included in the output dictionary, potentially causing incorrect data analysis.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'run_id' key should be present in the output dictionary.</li>
                                        <li>The value of the 'run_id' key should match the provided run_id string ('run-1').</li>
                                        <li>The 'run_id' field should not be missing from the output dictionary.</li>
                                        <li>The 'run_id' field should have the correct format (e.g., 'run-XX')</li>
                                        <li>The 'run_id' value should be a string and not an integer or other data type</li>
                                        <li>The 'run_id' value should match the provided run_id string ('run-1')</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 277-279, 281-283)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSummary::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `CoverageEntry` object can be serialized into a dictionary correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents bugs or regressions where the serialization of `CoverageEntry` objects is incorrect, potentially leading to unexpected behavior or errors in downstream code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key in the dictionary should match the expected value.</li>
                                        <li>The 'line_ranges' key in the dictionary should match the expected value.</li>
                                        <li>The 'line_count' key in the dictionary should match the expected value.</li>
                                        <li>The `to_dict()` method of `CoverageEntry` object returns a dictionary with the correct keys and values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 449-457, 459, 461)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_minimal_result</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that a minimal result has the required fields.</p>
                                <p><strong>Why Needed:</strong> To ensure that the minimal result is correctly constructed and contains all necessary information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'nodeid' field should match the expected value.</li>
                                        <li>The 'outcome' field should be set to 'passed'.</li>
                                        <li>The 'duration' field should be set to 0.0 (or a default value).</li>
                                        <li>The 'phase' field should be set to 'call'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the `TestCaseResult` includes a coverage list.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in cases where the coverage is not included.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `coverage` key in the result dictionary should contain exactly one entry.</li>
                                        <li>The `file_path` of the first `CoverageEntry` in the `coverage` list should be 'src/foo.py'.</li>
                                        <li>All other `CoverageEntry`s in the `coverage` list should have a different `file_path`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 40-43, 161-165, 167, 169, 171, 173, 176-178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_llm_opt_out</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case 'test_result_with_llm_opt_out' verifies that the TestCaseResult includes a flag indicating LLM opt-out.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the result does not include the flag for LLM opt-out, potentially causing incorrect interpretation of the output.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm_opt_out' key in the result dictionary should be set to True.</li>
                                        <li>The value of the 'llm_opt_out' key should be a boolean indicating whether LLM opt-out is enabled or not.</li>
                                        <li>The TestCaseResult object should have been created with an LLM opt-out flag set to True.</li>
                                        <li>The 'llm_opt_out' attribute in the result dictionary should contain the correct boolean value.</li>
                                        <li>The 'llm_opt_out' key should be present in the result dictionary and its value should match the expected boolean value.</li>
                                        <li>The TestCaseResult object should have been created with an LLM opt-out flag set to True before calling the test function.</li>
                                        <li>The 'llm_opt_out' attribute in the result dictionary should contain the correct boolean value after calling the test function.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180-182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_rerun</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test `test_result_with_rerun` verifies that the `TestCaseResult` object includes the `rerun_count` and `final_outcome` fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where a rerun is not included in the result, potentially causing incorrect reporting of test results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `rerun_count` field should be equal to 2.</li>
                                        <li>The `final_outcome` field should be equal to 'passed'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 161-165, 167, 169, 171, 173-176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_without_rerun_excludes_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case `test_result_without_rerun_excludes_fields` verifies that the `result` dictionary does not include 'rerun_count' and 'final_outcome' keys.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where a result is rerunned, causing missing field errors in the output.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'rerun_count' key should be absent from the `result` dictionary.</li>
                                        <li>The 'final_outcome' key should be absent from the `result` dictionary.</li>
                                        <li>The presence of 'rerun_count' and 'final_outcome' keys in the `result` dictionary should be checked for missing fields.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_default_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that default values are set correctly for the TestConfig class.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the default values of the TestConfig class may not be set correctly, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The cfg.provider should be set to 'none'.</li>
                                        <li>The cfg.llm_context_mode should be set to 'minimal'.</li>
                                        <li>The cfg.llm_max_tests should be set to 0.</li>
                                        <li>The cfg.llm_max_retries should be set to 3.</li>
                                        <li>The cfg.llm_context_bytes should be set to 32000.</li>
                                        <li>The cfg.llm_context_file_limit should be set to 10.</li>
                                        <li>The cfg.llm_requests_per_minute should be set to 5.</li>
                                        <li>The cfg.llm_timeout_seconds should be set to 30.</li>
                                        <li>The cfg.llm_cache_ttl_seconds should be set to 86400.</li>
                                        <li>The cfg.include_phase should be set to 'run'.</li>
                                        <li>The cfg.aggregate_policy should be set to 'latest'.</li>
                                        <li>The cfg.is_llm_enabled() should return False.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_get_default_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `get_default_config` function returns a default configuration with no provider.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the factory function does not return a valid configuration when no provider is specified.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The returned value is an instance of `Config`.</li>
                                        <li>The `provider` attribute of the returned value is set to `'none'`.</li>
                                        <li>The `provider` attribute is not set to any other default value (e.g., 'default', 'api') when no provider is specified.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_is_llm_enabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test whether the LLM is enabled based on the provider.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the LLM is enabled by default and incorrectly assumed to be enabled for certain providers.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `is_llm_enabled()` method returns False when the provider is 'none'.</li>
                                        <li>The `is_llm_enabled()` method should return True when the provider is 'ollama'.</li>
                                        <li>The `is_llm_enabled()` method correctly handles cases where the provider is not specified (i.e., 'none').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_aggregate_policy</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-197, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_context_mode</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-189, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_include_phase</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `validate` method with an invalid include phase.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where an invalid include phase is not properly validated and causes unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `validate` method should return at least one error message for an invalid include phase.</li>
                                        <li>The error message should contain the invalid include phase 'lunch_break'.</li>
                                        <li>The test should fail when an invalid include phase is passed to the `include_phase` parameter.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-205, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validation with an invalid provider.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an incorrect error message.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The configuration object is created with an invalid provider.</li>
                                        <li>The validate method returns exactly one error.</li>
                                        <li>The error message contains the string 'Invalid provider '</li>
                                        <li>The error message does not contain any other relevant information.</li>
                                        <li>The error message does not indicate a specific problem with the configuration.</li>
                                        <li>The error message does not provide enough context to identify the issue.</li>
                                        <li>The error message is too vague and does not clearly state the problem.</li>
                                        <li>The test fails if an invalid provider is used.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_numeric_ranges</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validation of numeric constraints for Config object.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the `llm_context_bytes` constraint is not enforced correctly, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>cfg.validate() should return an error message indicating that llm_context_bytes must be at least 1000</li>
                                        <li>cfg.validate() should include 'llm_max_tests' in the error messages</li>
                                        <li>cfg.validate() should include 'llm_requests_per_minute' in the error messages</li>
                                        <li>cfg.validate() should include 'llm_timeout_seconds' in the error messages</li>
                                        <li>cfg.validate() should include 'llm_max_retries' in the error messages</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209-218, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_valid_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Valid configuration is validated without any issues.</p>
                                <p><strong>Why Needed:</strong> A valid configuration is necessary to prevent potential bugs or regressions in the application.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The Config object is created successfully.</li>
                                        <li>The validate() method returns an empty list of errors.</li>
                                        <li>No validation errors are reported for a well-formed configuration.</li>
                                        <li>The configuration is correctly validated without any invalid values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_aggregation_options</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the test_load_aggregation_options function to ensure it correctly loads aggregation options.</p>
                                <p><strong>Why Needed:</strong> To prevent a bug where the aggregation policy is not being loaded correctly, which could lead to unexpected behavior in the test suite.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The aggregate_dir attribute of the configuration object should be set to 'aggr_dir'.</li>
                                        <li>The aggregate_policy attribute of the configuration object should be set to 'merge'.</li>
                                        <li>The aggregate_run_id attribute of the configuration object should be set to 'run-123'.</li>
                                        <li>The aggregate_group_id attribute of the configuration object should be set to 'group-abc'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286-294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_config_invalid_int_ini</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the test_load_config_invalid_int_ini function with invalid integer values in INI.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the function crashes when encountering an invalid integer value in the INI file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should not crash or return unexpected results when it encounters an invalid integer value in the INI file.</li>
                                        <li>The default value of llm_max_retries is correctly set to 3, as intended.</li>
                                        <li>The mock_pytest_config.getini.side_effect is called with the correct argument (ini_values.get(key))</li>
                                        <li>The test asserts that cfg.llm_max_retries equals 3 after calling load_config(mock_pytest_config)</li>
                                        <li>The function returns a valid configuration object even when it encounters an invalid integer value in the INI file</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263-267, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_coverage_source</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `llm_coverage_source` option is correctly set to 'cov_dir' when loaded.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the coverage source option is not properly configured, potentially leading to incorrect coverage metrics.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>cfg.llm_coverage_source</li>
                                        <li>cfg.llm_coverage_source == 'cov_dir'</li>
                                        <li>mock_pytest_config.option.llm_coverage_source</li>
                                        <li>mock_pytest_config.option.llm_coverage_source == 'cov_dir'</li>
                                        <li>cfg.llm_coverage_source is not set to an empty string or None.</li>
                                        <li>cfg.llm_coverage_source is not equal to 'default' when loaded from a configuration file.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_defaults</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the default provider and report HTML are correctly loaded when no options are provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the default provider is not set or report HTML is not loaded.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>cfg.provider == 'none'</li>
                                        <li>cfg.report_html is None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">24 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_cli_overrides_ini</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that CLI options override ini options.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the CLI overrides ini settings, potentially causing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Set ini values correctly.</li>
                                        <li>Set CLI values correctly and override ini values.</li>
                                        <li>Verify correct report HTML value.</li>
                                        <li>Verify correct requests per minute value.</li>
                                        <li>Verify that CLI values are not overridden by ini values in this case.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">27 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259-261, 263, 270-272, 274, 276, 278, 280-282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_cli_retries</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `llm_max_retries` option is correctly set to 9 when loading configuration from CLI.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `llm_max_retries` option is not set, causing the `load_config` function to return an incorrect value.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `llm_max_retries` option should be set to 9 when loading configuration from CLI.</li>
                                        <li>The `load_config` function should raise a `ValueError` if the `llm_max_retries` option is not set to 9.</li>
                                        <li>The `load_config` function should correctly return an instance of `PytestConfig` with the correct `llm_max_retries` value when loading configuration from CLI.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282-283, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_ini</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loading values from ini options.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails if the 'llm_report_provider' option is not set in the ini file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'provider' key should be set to 'ollama'.</li>
                                        <li>The 'model' key should be set to 'llama3'.</li>
                                        <li>The 'context_mode' key should be set to 'balanced'.</li>
                                        <li>The 'requests_per_minute' key should be set to 10.</li>
                                        <li>The 'max_retries' key should be set to 5.</li>
                                        <li>The 'report_html' key should be set to 'report.html'.</li>
                                        <li>The 'report_json' key should be set to 'report.json'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">32 lines (ranges: 107, 147, 248, 251-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_aggregation_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the configuration of aggregation settings.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in aggregation settings configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The aggregate_dir attribute should be set to '/reports'.</li>
                                        <li>The aggregate_policy attribute should be set to 'merge'.</li>
                                        <li>The aggregate_include_history attribute should be set to True.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_all_output_paths</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Config with all output paths.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails if any of the report formats are not set.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report_html` attribute is set to 'report.html'.</li>
                                        <li>The `report_json` attribute is set to 'report.json'.</li>
                                        <li>The `report_pdf` attribute is set to 'report.pdf'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_capture_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that `capture_failed_output` is set to `True` in the test configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the test fails due to an incorrect capture output limit.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.capture_failed_output is set to `True`</li>
                                        <li>config.capture_output_max_chars is not set to a valid value (8000)</li>
                                        <li>assertion error: expected 'None' but got a boolean value for `capture_failed_output`</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_compliance_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `Config` object is correctly initialized with compliance settings.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `Config` object's metadata file and HMAC key file are not set correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `metadata_file` attribute of the `Config` object is set to 'metadata.json'.</li>
                                        <li>The `hmac_key_file` attribute of the `Config` object is set to 'key.txt'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_coverage_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the configuration of coverage settings.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the coverage settings are not correctly configured, potentially leading to incorrect coverage reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.omit_tests_from_coverage is False</li>
                                        <li>config.include_phase == "all"</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_custom_exclude_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the inclusion of custom exclude globs in the configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where custom exclude globs are not properly propagated to the LLM context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `Config(llm_context_exclude_globs)` is called with the provided arguments.</li>
                                        <li>The variable `llm_context_exclude_globs` is set to `['*.pyc', '*.log']` in the test.</li>
                                        <li>The string `*.pyc` is found within the list of exclude globs.</li>
                                        <li>The string `*.log` is found within the list of exclude globs.</li>
                                        <li>The variable `config` is an instance of `Config` with the specified arguments.</li>
                                        <li>The method `llm_context_exclude_globs` is called on the `config` instance.</li>
                                        <li>The value of `llm_context_exclude_globs` is a list containing both `*.pyc` and `*.log` strings.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_include_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `include_globs` configuration option includes all specified Python files.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `include_globs` configuration option is not properly propagated to the LLM context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `*.py` glob matches one or more `.py` files in the current directory.</li>
                                        <li>The `*.pyi` glob matches one or more `.pyi` files in the current directory.</li>
                                        <li>The `include_globs` configuration option includes all specified Python files in the LLM context.</li>
                                        <li>The `llm_context_include_globs` attribute of the `Config` object contains a list of matched globs.</li>
                                        <li>The `include_globs` value is set to a list containing both `.py` and `.pyi` glob patterns.</li>
                                        <li>The `include_globs` configuration option is properly propagated to the LLM context, including all specified Python files.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_invocation_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `include_pytest_invocation` configuration option is set to False.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `include_pytest_invocation` option is incorrectly set to True, causing unexpected behavior in tests that rely on this feature.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.include_pytest_invocation is set to False</li>
                                        <li>config.include_pytest_invocation should be set to False based on the provided configuration options</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_execution_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the LLM execution settings configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in LLM execution settings when using LLMS.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of llm_max_tests is set to 50.</li>
                                        <li>The value of llm_max_concurrency is set to 8.</li>
                                        <li>The value of llm_requests_per_minute is set to 12.</li>
                                        <li>The configuration does not exceed the maximum allowed LLMS requests per minute.</li>
                                        <li>The configuration does not exceed the maximum allowed LLMS concurrency.</li>
                                        <li>The configuration does not exceed the maximum allowed LLMS timeout in seconds.</li>
                                        <li>The configuration has a valid cache directory.</li>
                                        <li>The cache directory is set to a valid path.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_param_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the configuration of LLM parameter settings.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in LLM parameter setting configurations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.llm_include_param_values is True</li>
                                        <li>config.llm_param_value_max_chars == 200</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the correct provider, model, and context bytes for LLM settings.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the LLM settings are not properly configured due to incorrect values.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` attribute is set to `ollama`.</li>
                                        <li>The `model` attribute is set to `llama3.2`.</li>
                                        <li>The `llm_context_bytes` attribute is set to `64000` bytes.</li>
                                        <li>The `llm_context_file_limit` attribute is not exceeded.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_repo_root_path</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `repo_root` attribute of the `Config` object is correctly set to `/project`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `repo_root` attribute is not set correctly, causing issues with file system operations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.repo_root is an instance of `path.Path` and its value is equal to `/project`</li>
                                        <li>config.repo_root is correctly initialized before use</li>
                                        <li>the `repo_root` attribute is set to the correct path `/project`</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_valid_phase_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `test_valid_phase_values` function to ensure all valid include_phase values pass validation.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where invalid or missing include_phase values cause the test to fail.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that no errors are present in the configuration for each phase (run, setup, teardown, and all).</li>
                                        <li>Check if any error messages contain the string 'include_phase'.</li>
                                        <li>Confirm that the validation process correctly identifies invalid or missing include_phase values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_exclude_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the default exclude globs for a minimal LLM configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues with sensitive files being included in the model's training data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `*.pyc` is present in the defaults.</li>
                                        <li>The function `__pycache__/*` is present in the defaults.</li>
                                        <li>The function `*secret*` is present in the defaults.</li>
                                        <li>The function `*password*` is present in the defaults.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_redact_patterns</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that default redact patterns include sensitive information.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential security vulnerability where sensitive information is not properly redacted in the default configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `--password` pattern should match any string containing `--password`.</li>
                                        <li>The `--token` pattern should match any string containing `--token`.</li>
                                        <li>The `--api[_-]?key` pattern should match any string containing `--api[_-]?key` (note: the `_-` is a special character in Python that matches any single character).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify default values of the test configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in default configuration settings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider should be set to 'none'.</li>
                                        <li>The llm_context_mode should be set to 'minimal'.</li>
                                        <li>The llm_context_bytes should be set to 32000 bytes.</li>
                                        <li>The omit_tests_from_coverage flag should be True.</li>
                                        <li>The include_phase should be set to 'run'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigHelpersMaximal::test_is_llm_enabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests whether the `is_llm_enabled` method returns correct enabled status for different providers.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in LLM provider selection, ensuring that the method behaves as expected for various scenarios.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The method should return False when the provider is 'none'.</li>
                                        <li>The method should return True when the provider is 'ollama', 'litellm', or 'gemini'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_aggregate_policy</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `validate` method of the `Config` class to ensure it correctly identifies and reports invalid aggregate policies.</p>
                                <p><strong>Why Needed:</strong> To prevent a potential bug where an invalid aggregate policy is not properly reported, allowing for incorrect configuration or unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `validate` method should return exactly one error message containing the string 'Invalid aggregate_policy 'invalid'.</li>
                                        <li>The error message should include the exact value of the invalid aggregate policy ('invalid').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-197, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_context_mode</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validates the `Config` class with an invalid context mode.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the `validate()` method returns more than one error for an invalid context mode.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config.validate()` method should return exactly one error message.</li>
                                        <li>The error message should contain 'Invalid llm_context_mode 'invalid''.</li>
                                        <li>The error message should be present in the first error object.</li>
                                        <li>The error message should not be empty.</li>
                                        <li>The context mode is invalid according to the LLM model configuration.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-189, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_include_phase</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validates configuration with an invalid include phase.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an incorrect or missing error message for an invalid include phase.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `validate()` should return exactly one error message.</li>
                                        <li>The error message should contain 'Invalid include_phase 'invalid''.</li>
                                        <li>The error message should be present in the first error found.</li>
                                        <li>The test should fail if only one error is returned from the validation process.</li>
                                        <li>The error message should not be empty or null.</li>
                                        <li>The error message should indicate that an invalid include phase was provided.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-205, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validates an invalid provider.</p>
                                <p><strong>Why Needed:</strong> Prevents a bug where the test fails due to an invalid provider being used.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The configuration should validate against an invalid provider.</li>
                                        <li>The error message for the invalid provider should be 'Invalid provider 'invalid'.</li>
                                        <li>The test should fail when using an invalid provider.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_numeric_bounds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_numeric_bounds</p>
                                <p><strong>Why Needed:</strong> Prevents regression where invalid numeric values are not properly validated.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return a list of errors for invalid numeric values.</li>
                                        <li>The function should check if 'llm_context_bytes' is in the list of errors.</li>
                                        <li>The function should check if 'llm_max_tests' is in the list of errors.</li>
                                        <li>The function should check if 'llm_requests_per_minute' is in the list of errors.</li>
                                        <li>The function should check if 'llm_timeout_seconds' is in the list of errors.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209-217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_valid_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `validate` method returns an empty list for a valid configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where an invalid configuration is passed to the validation process and causes unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `validate` method should return an empty list for a valid configuration.</li>
                                        <li>An empty list should be returned when the input configuration is valid.</li>
                                        <li>The validation process should not crash or throw any exceptions when given a valid configuration.</li>
                                        <li>The test should fail with an assertion error if an invalid configuration is passed to the `validate` method.</li>
                                        <li>A valid configuration should not prevent the `validate` method from returning an empty list.</li>
                                        <li>The `validate` method's return value should be consistent across different test runs.</li>
                                        <li>The test should only fail when a valid configuration is provided, and not when other invalid configurations are used.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginConfigLoading::test_config_defaults</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `Config` class has default settings when no configuration is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the plugin's configuration defaults to unexpected values if no configuration is specified.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `Config` instance should be an instance of `Config`.</li>
                                        <li>The `cfg` variable should have an attribute named `safe_defaults` and its value should be `True`.</li>
                                        <li>The `cfg.safe_defaults` attribute should return `True`.</li>
                                        <li>The `cfg.safe_defaults` attribute should not raise a `TypeError` when accessed directly.</li>
                                        <li>The `cfg.safe_defaults` attribute should not raise a `TypeError` when called with no arguments.</li>
                                        <li>The `safe_defaults` attribute of the `Config` instance should be `True`.</li>
                                        <li>The `safe_defaults` attribute of the `Config` instance should have an attribute named `default_config` and its value should be `None` or a string.</li>
                                        <li>The `default_config` attribute of the `Config` instance should not raise a `TypeError` when accessed directly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">33 lines (ranges: 107, 147, 248, 251-259, 261, 263-265, 270, 272-276, 278, 280, 282, 286, 288, 290-292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginConfigLoading::test_markers_exist_in_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `pytestconfig` object is accessible in the test environment.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the plugin configuration is not available in the test setup.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytestconfig` object should be an instance of `pytest.config.Config` or its subclass.</li>
                                        <li>The `pytestconfig` object should have a `name` attribute.</li>
                                        <li>The `pytestconfig` object should have a `version` attribute.</li>
                                        <li>The `pytestconfig` object should have a `groups` attribute.</li>
                                        <li>The `pytestconfig` object should have a `extra_args` attribute.</li>
                                        <li>The `pytestconfig` object should be an instance of `Config` from the `pytest.config` module.</li>
                                        <li>The `pytestconfig` object should not be `None` when accessed.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_llm_context_marker</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the LLM context marker does not cause errors in a test.</p>
                                <p><strong>Why Needed:</strong> The LLM context marker prevents tests from failing due to unexpected behavior or errors caused by the LLM.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Asserts that the `test_llm_context_marker` function does not raise any exceptions when run without errors.</li>
                                        <li>Verifies that the `test_llm_context_marker` function correctly handles cases where the LLM context marker is present but does not cause an error.</li>
                                        <li>Checks for any unexpected behavior or errors caused by the LLM context marker in the test.</li>
                                        <li>Ensures that the test can still be run successfully without raising any exceptions due to the LLM context marker.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_llm_opt_out_marker</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">


                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_requirement_marker</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `requirement_marker` function is tested to ensure it does not throw any errors when used.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `requirement_marker` function could be misused and cause unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `requirement_marker` function should return an empty list or None if no requirements are present.</li>
                                        <li>The `requirement_marker` function should not raise any exceptions when called with a non-empty list of requirements.</li>
                                        <li>The `requirement_marker` function should correctly identify and exclude irrelevant requirements from the output.</li>
                                        <li>The `requirement_marker` function should handle edge cases where there are no requirements or only one requirement.</li>
                                        <li>The `requirement_marker` function should not modify the original input list in any way.</li>
                                        <li>The `requirement_marker` function should be able to handle large lists of requirements without performance issues.</li>
                                        <li>The `requirement_marker` function should correctly group related requirements together.</li>
                                        <li>The `requirement_marker` function should return a clear and concise output that makes sense for the context.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestReportGeneration::test_report_writer_integration</span>
                        <div class="test-meta">
                            <span>37ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the integration of report writer with pytest_llm_report, ensuring it generates a full report and verifies its contents.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by verifying that the report writer correctly generates a full report with summary statistics.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'report.json' file exists in the specified path.</li>
                                        <li>The total count of tests passed is 1 out of 2.</li>
                                        <li>The number of tests passed is correct based on the test results.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">79 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">131 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that collectreport skips when disabled and pytest_collectreport is mocked correctly.</p>
                                <p><strong>Why Needed:</strong> To prevent a regression where collectreport does not skip when disabled due to an incorrect implementation of pytest_collectreport.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_collectreport` function should be called with `_enabled_key` set to `False` and the result should match the expected behavior.</li>
                                        <li>The `get` method on the session config stash should return `None` when `_enabled_key` is not found.</li>
                                        <li>The `session.config.stash.get` call should assert that it was called with `_enabled_key` as an argument and a boolean value of `False` as its result.</li>
                                        <li>The mock report object returned by `pytest_collectreport` should be a valid instance of the `MagicMock` class.</li>
                                        <li>The `session.config.stash.get` method on the mock report object should return `None` when `_enabled_key` is not found.</li>
                                        <li>The `get` method on the session config stash should raise an error if it cannot find `_enabled_key` in the stash.</li>
                                        <li>The `pytest_collectreport` function should be able to handle cases where `_enabled_key` is not present in the stash without raising an exception.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 387-388, 391, 395-397, 408-409, 415-416)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_enabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `pytest_collectreport` calls `_collector_key` when collectreport is enabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the collector does not call the stash for enabled collectreport.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `mock_collector.handle_collection_report.assert_called_once_with(mock_report)` should be called with one argument, which is an instance of `pytest_collectreport`.</li>
                                        <li>The function `mock_collector.handle_collection_report.assert_called_once_with(mock_report)` should not raise any exception.</li>
                                        <li>The function `mock_collector.handle_collection_report.assert_called_once_with(mock_report)` should return a tuple containing the report.</li>
                                        <li>The function `stash_get(key, default=None)` should call `_collector_key` when it is called with key '_collector_key'.</li>
                                        <li>The function `stash_get(key, default=None)` should not call `_enabled_key` when it is called with key '_collector_key'.</li>
                                        <li>The function `stash_get(key, default=None)` should return the correct value for key '_collector_key' (True).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 387-388, 391, 395-397, 408-409, 415, 419-421)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_no_session</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `pytest_collectreport` function skips when a Pytest session is not available.</p>
                                <p><strong>Why Needed:</strong> Prevent regression in plugin behavior when a Pytest session is not set.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `session` attribute of the mock report object is not present.</li>
                                        <li>The `pytest_collectreport` function does not raise an exception when called with a non-existent session.</li>
                                        <li>The `pytest_collectreport` function skips the test run without raising any errors.</li>
                                        <li>The `pytest_collectreport` function does not throw a `AttributeError` when trying to access the `session` attribute.</li>
                                        <li>The `session` attribute is set to an empty dictionary or None in the mock report object.</li>
                                        <li>The `session` attribute is set to a different value than what was expected (e.g., a string instead of a boolean).</li>
                                        <li>The `pytest_collectreport` function does not raise an exception when called with a session that has already been cleaned up.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 387-388, 391, 395-397, 408, 412)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_session_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that `pytest_collectreport` does not raise an exception when the session is `None`.</p>
                                <p><strong>Why Needed:</strong> Prevent a potential bug where `pytest_collectreport` raises an error when the session is `None`.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `pytest_collectreport` should not be called with a `None` argument for the `session` attribute.</li>
                                        <li>The `session` attribute of the mock report object should remain unchanged after setting it to `None`.</li>
                                        <li>No exception should be raised when calling `pytest_collectreport` with a `None` session.</li>
                                        <li>The `pytest_collectreport` function should not throw an error or crash when given a `None` session.</li>
                                        <li>The `session` attribute of the mock report object should still have its original value after setting it to `None`.</li>
                                        <li>The test case should be able to run without any issues due to the `None` session.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 387-388, 391, 395-397, 408, 412)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_llm_enabled_warning</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM enabled warning is raised when configuring Pytest with the llm_report_provider set to 'ollama'.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the LLM provider 'ollama' might be enabled by default without user intervention, potentially causing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `llm_report_provider` is set to 'ollama'.</li>
                                        <li>The value of `llm_report_model` is set to 'llama3.2'.</li>
                                        <li>The value of `llm_report_context_mode` is set to 'minimal'.</li>
                                        <li>The value of `llm_report_html` is None.</li>
                                        <li>The value of `llm_report_json` is None.</li>
                                        <li>The value of `llm_report_pdf` is None.</li>
                                        <li>The value of `llm_evidence_bundle` is None.</li>
                                        <li>The value of `llm_dependency_snapshot` is None.</li>
                                        <li>The value of `llm_requests_per_minute` is None.</li>
                                        <li>The value of `llm_aggregate_dir` is None.</li>
                                        <li>The value of `llm_aggregate_policy` is None.</li>
                                        <li>The value of `llm_aggregate_run_id` is None.</li>
                                        <li>The value of `llm_aggregate_group_id` is None.</li>
                                        <li>The value of `llm_max_retries` is set to None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">44 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_validation_errors</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that validation errors raise UsageError when invalid configuration is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the plugin does not handle invalid configuration properly and raises a UsageError instead of a more informative error message.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_config.option.llm_report_html is None</li>
                                        <li>mock_config.option.llm_report_json is None</li>
                                        <li>mock_config.option.llm_report_pdf is None</li>
                                        <li>mock_config.option.llm_evidence_bundle is None</li>
                                        <li>mock_config.option.llm_dependency_snapshot is None</li>
                                        <li>mock_config.option.llm_requests_per_minute is None</li>
                                        <li>mock_config.option.llm_aggregate_dir is None</li>
                                        <li>mock_config.option.llm_aggregate_policy is None</li>
                                        <li>mock_config.option.llm_aggregate_run_id is None</li>
                                        <li>mock_config.option.llm_aggregate_group_id is None</li>
                                        <li>mock_config.option.llm_max_retries is None</li>
                                        <li>mock_config.rootpath is /project</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">43 lines (ranges: 107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 248, 251-253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-199, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_worker_skip</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that configure skips on xdist workers and verifies the correct behavior.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where pytest_configure() might be called unnecessarily when configuring with xdist workers.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `addinivalue_line` method is not called before checking for worker input.</li>
                                        <li>The `addinivalue_line` method is called after verifying the worker input.</li>
                                        <li>The `addinivalue_line` method is not called when configuring without xdist workers.</li>
                                        <li>The `addinivalue_line` method is correctly called only when a marker is added to the test.</li>
                                        <li>The `workerinput` attribute of the mock configuration object is set correctly.</li>
                                        <li>The `workerid` attribute of the worker input dictionary matches the expected value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 169-171, 173-175, 177-179, 183-184, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigureFallback::test_pytest_configure_fallback_load</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test fallback to load_config if Config.load is missing during Pytest configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the plugin fails to configure due to missing `Config.load` method call.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocking `pytest_llm_report.options.Config` with no `load()` method ensures it's called correctly when `Config.load` is missing.</li>
                                        <li>The `validate()` method of `mock_cfg` returns an empty list, indicating successful configuration.</li>
                                        <li>The `load_config()` method of `mock_load` is called once, ensuring the fallback to load_config occurs as expected.</li>
                                        <li>The `option.llm_report_html` and `option.llm_max_retries` are not set, preventing a potential regression in plugin behavior.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_all_ini_options</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loading all INI options for the plugin.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case the CLI options are set but not loaded correctly from INI files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider is set to 'ollama' when the option is None.</li>
                                        <li>The model is set to 'llama3.2' when the option is None.</li>
                                        <li>The context mode is set to 'complete' when the option is None.</li>
                                        <li>The requests per minute is set to 10 when the option is None.</li>
                                        <li>The report HTML is set to 'ini.html' when the option is None.</li>
                                        <li>The report JSON is set to 'ini.json' when the option is None.</li>
                                        <li>The aggregate directory is not set when the option is None.</li>
                                        <li>The aggregate policy is not set when the option is None.</li>
                                        <li>The aggregate run ID is not set when the option is None.</li>
                                        <li>The aggregate group ID is not set when the option is None.</li>
                                        <li>The maximum retries are not set when the option is None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">31 lines (ranges: 107, 147, 248, 251-263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_cli_overrides_ini</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test CLI options override INI options.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the plugin's configuration is not properly overridden from INI files when using the command line interface.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `llm_report_html` option should be set to 'cli.html'.</li>
                                        <li>The `llm_report_json` option should be set to 'cli.json'.</li>
                                        <li>The `llm_report_pdf` option should be set to 'cli.pdf'.</li>
                                        <li>The `llm_evidence_bundle` option should be set to 'bundle.zip'.</li>
                                        <li>The `llm_dependency_snapshot` option should be set to 'deps.json'.</li>
                                        <li>The `llm_requests_per_minute` option should be set to 20.</li>
                                        <li>The `aggregate_dir` option should be set to '/agg'.</li>
                                        <li>The `aggregate_policy` option should be set to 'merge'.</li>
                                        <li>The `aggregate_run_id` option should be set to 'run-123'.</li>
                                        <li>The `aggregate_group_id` option should be set to 'group-abc'.</li>
                                        <li>The root path of the configuration file should be '/project'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">38 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259-263, 270-283, 286-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that terminal summary skips when plugin is disabled.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where the test fails due to an uncaught assertion error in pytest_terminal_summary.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mocked stash.get() with False value for _enabled_key</li>
                                        <li>assert_called_once_with(_enabled_key, False) on mock_config.stash.get</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 238, 242-243, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_worker_skip</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that terminal summary skips on xdist worker.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the plugin does not skip the terminal summary for an xdist worker.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_terminal_summary` function should return early without doing anything when it encounters an xdist worker.</li>
                                        <li>The `workerinput` attribute of the mock configuration object is set to 'gw0' as expected.</li>
                                        <li>The test result is None, indicating that the plugin skipped the terminal summary correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 238-239, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::testload_config</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test config loading from pytest objects (CLI + INI) to ensure it correctly sets the report HTML path.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the report HTML path is not set correctly when using pytest's CLI and INI options.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report_html` option of the `load_config` function is set to 'out.html' as expected.</li>
                                        <li>The `rootpath` option of the `load_config` function is set to '/root' as expected.</li>
                                        <li>The `getini` method of the `mock_config` object returns None for the `llm_report_html` key as expected.</li>
                                        <li>The `report_html` attribute of the loaded configuration object is 'out.html' as expected.</li>
                                        <li>The `rootpath` attribute of the loaded configuration object is '/root' as expected.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270-283, 286-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_disabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test makereport skips when disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where makereport is disabled and the plugin does not generate reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_runtest_makereport` hookwrapper should be able to skip the generation of reports when `makereport` is disabled.</li>
                                        <li>The `get_result()` method of the mock_outcome object should return a StopIteration exception when called with a generator that yields point.</li>
                                        <li>The `send()` method of the mock_outcome object should not raise an exception when called with a generator that yields point.</li>
                                        <li>The `stash.get()` method of the mock_item object should return False when called with no arguments.</li>
                                        <li>The `get_result()` method of the mock_outcome object should be able to handle the case where the generator yields point without raising an exception.</li>
                                        <li>The `send()` method of the mock_outcome object should not raise an exception when called with a generator that yields point.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 387-388, 391-392, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_enabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test makereport calls collector when enabled.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the plugin does not collect and report test results even if makereport is enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_runtest_makereport` function should be able to find the `mock_collector` instance.</li>
                                        <li>The `mock_collector.handle_runtest_logreport` method should be called with the correct arguments.</li>
                                        <li>The `mock_report` object returned by `stash_get(_collector_key)` should not be None.</li>
                                        <li>The `mock_item.config.stash.get(_enabled_key)` call should return True for `_enabled_key` and `mock_collector`.</li>
                                        <li>The `mock_call.send(mock_outcome)` call should yield a point after the `send` method is called on `mock_outcome`.</li>
                                        <li>The `pytest_runtest_makereport` function should not raise an exception when called with a mock outcome.</li>
                                        <li>The `mock_collector.handle_runtest_logreport` method should be able to handle the `mock_report` object correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that collection_finish is skipped when disabled.</p>
                                <p><strong>Why Needed:</strong> To prevent a regression where the plugin fails to collect data when collection_finish is disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The pytest_collection_finish function should not be called with _enabled_key as False.</li>
                                        <li>The pytest_collection_finish function should have been called with _enabled_key as True before calling it.</li>
                                        <li>The pytest_collection_finish function should have been called with _enabled_key as False after setting stash.get to False in the mock session.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 387-388, 391, 395-397, 431-432)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_enabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that collection_finish is called when enabled and Pytest is collecting data.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression in the plugin where Pytest collection finish is not triggered correctly when it's enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_collection_finish` function should be called with the collected items.</li>
                                        <li>The `handle_collection_finish` method of the collector should be called once with the collected items.</li>
                                        <li>The `items` attribute of the session should contain a list of collected items.</li>
                                        <li>Each item in the collection should have an `__getitem__` method that returns the item.</li>
                                        <li>The `stash_get` function should return True for `_enabled_key` and `True` for `_collector_key` when called with the correct keys.</li>
                                        <li>The `handle_collection_finish` method of the collector should not be called if there are no collected items.</li>
                                        <li>The `items` attribute of the session should contain a list of collected items after calling `pytest_collection_finish`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 387-388, 391, 395-397, 431, 435-437)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that pytest_sessionstart skips when sessionstart is disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the plugin fails to check if the session has been started.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mocked config.stash.get.call_count should be 1</li>
                                        <li>mocked config.stash.get._enabled_key should have been called with False as argument</li>
                                        <li>pytest_sessionstart was not called with mock_session</li>
                                        <li>mock_session.config.stash.get.return_value should have been set to False</li>
                                        <li>mock_session.config.stash.get.assert_called_with(_enabled_key, False) should have been called</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 387-388, 391, 395-397, 448-449)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_enabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that sessionstart initializes collector when enabled.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the collector is not initialized due to an unenabled `sessionstart` flag.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_collector_key` should be present in the mock stash.</li>
                                        <li>The `_start_time_key` should be present in the mock stash.</li>
                                        <li>A `MockStash` instance with a valid stash dictionary is created and configured correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 387-388, 391, 395-397, 448, 452, 455, 457-458)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test pytest_addoption adds expected arguments to the parser.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where pytest_addoption does not add all required arguments to the parser.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>parser.getgroup.assert_called_with('llm-report', 'LLM-enhanced test reports')</li>
                                        <li>group.addoption.call_args_list[0][0] == '--llm-report'</li>
                                        <li>group.addoption.call_args_list[1][0] == '--llm-coverage-source'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">99 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption_ini</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test pytest_addoption adds INI options for llm_report plugin.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the plugin does not add INI options when using pytest.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that 'llm_report_html' is present in the ini calls.</li>
                                        <li>Verify that 'llm_report_json' is present in the ini calls.</li>
                                        <li>Verify that 'llm_report_max_retries' is present in the ini calls.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">99 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_coverage_calculation</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test coverage percentage calculation logic for terminal summary.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in coverage reporting when using the `terminal_summary` plugin.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report_html` option is set to 'out.html'.</li>
                                        <li>The `stash` dictionary contains `_enabled_key` and `_config_key` values.</li>
                                        <li>The `mock_cov_cls.return_value` method call sets up a mock Coverage instance.</li>
                                        <li>The `mock_cov.report.return_value` method call returns the expected coverage percentage (85.5%).</li>
                                        <li>The `pytest_terminal_summary` function is called with a valid configuration object.</li>
                                        <li>The `MockStash` class is used to simulate the stash dictionary.</li>
                                        <li>The `CoverageMapper` and `ReportWriter` patches are used to mock the Coverage and ReportWriter classes respectively.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">58 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 307, 309-315, 317-318, 331-332, 337-338, 365-375, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_llm_enabled</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test terminal summary with LLM enabled runs annotations.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in terminal summary functionality when LLM is enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that the `pytest_terminal_summary` function is called with the correct configuration.</li>
                                        <li>Assert that the `cfg` variable contains a boolean value of True for `_enabled_key` and a valid configuration object for `_config_key`.</li>
                                        <li>Check if the `mock_config.stash` dictionary has been correctly populated with mock data.</li>
                                        <li>Verify that the `mock_annotate.call_args[0][1]` attribute is set to the provided configuration object.</li>
                                        <li>Assert that the `mock_annotate.call_args[0][1]` attribute contains a valid model name for the LLM provider.</li>
                                        <li>Check if the `pytest_terminal_summary` function is patched correctly at source.</li>
                                        <li>Verify that the mock dependencies are properly patched and their methods are called as expected.</li>
                                        <li>Assert that the `mock_writer_cls.return_value` attribute is set to the correct mock writer instance.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">59 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331-332, 337-340, 343, 345, 348-350, 357-362, 365-375, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_no_collector</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test terminal summary creates collector if missing.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the plugin does not create a collector when it is not properly configured.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_terminalreporter.call_args_list[0][1] == True</li>
                                        <li>stash._enabled_key == True</li>
                                        <li>stash._config_key == cfg</li>
                                        <li>mock_config.stash._enabled_key == False</li>
                                        <li>mock_config.stash._config_key == cfg</li>
                                        <li>mock_writer_cls.return_value.__class__.__name__ == 'MockWriter'</li>
                                        <li>mock_mapper_cls.return_value.__class__.__name__ == 'MockCoverageMapper'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_with_aggregation</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test terminal summary with aggregation enabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the aggregation feature is disabled and the plugin does not produce any meaningful output.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregate_dir` parameter in the configuration should be set to `/agg` when aggregation is enabled.</li>
                                        <li>The `get()` method of the stash should return an empty list when no data is available.</li>
                                        <li>The `[]` indexing should not raise an error when trying to access a non-existent key in the stash.</li>
                                        <li>The `aggregate()` method of the aggregator should be called once with a valid report object.</li>
                                        <li>The `ReportWriter` instance should have been written to JSON and HTML files.</li>
                                        <li>The `ReportWriter` instance's `write_json()` and `write_html()` methods should have been called correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 238, 242, 246, 249-250, 252-253, 256-257, 259, 261-265, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummaryErrors::test_terminal_summary_coverage_error</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test coverage calculation error when loading a large coverage map.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the plugin fails to calculate coverage due to an OSError during load.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `load` method of `CoverageMapper` should not raise an exception if the coverage map is empty.</li>
                                        <li>The `coverage_map` attribute of `CoverageMapper` should be a list or tuple instead of a single value.</li>
                                        <li>The `report_writer` attribute of `ReportWriter` should be set to `None` when no report writer is configured.</li>
                                        <li>The `pytest_terminal_summary` function should not raise an exception if the coverage percentage is zero.</li>
                                        <li>The `report_html` option should be set to `'out.html'` instead of a file path.</li>
                                        <li>The `llm_coverage_source` option should be set to `.coverage` instead of a file path.</li>
                                        <li>The `stash` attribute of `Config` should contain the correct values when the plugin is enabled and coverage is enabled.</li>
                                        <li>The `workerinput` attribute of `Config` should be deleted before the test runs.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">52 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 322-325, 331-332, 337-338, 365-375, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_balanced_context</span>
                        <div class="test-meta">
                            <span>7ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the ContextAssembler to assemble a balanced context for a test file.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when using an unbalanced llm_context_mode, as it ensures that all required dependencies are present in the assembly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'utils.py' file is included in the assembled context.</li>
                                        <li>The 'def util()' function is found in the 'utils.py' file within the assembled context.</li>
                                        <li>All required dependencies (in this case, 'util') are present in the assembled context.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">51 lines (ranges: 33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-155, 158-159, 163, 191-192, 194)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_complete_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the ContextAssembler can assemble a complete context for a test file with no imports.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the ContextAssembler fails to assemble a complete context for a test file with no imports, resulting in an assertion error.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `source` variable contains the assembled source code of the test file.</li>
                                        <li>The `context` variable is not empty and does not contain any import statements.</li>
                                        <li>The `source` variable contains the expected test function definition 'test_1'.</li>
                                        <li>The `context` variable is a valid context object with no imports.</li>
                                        <li>The `source` variable has the correct number of lines (3) and the correct indentation.</li>
                                        <li>The `source` variable does not contain any import statements from other files.</li>
                                        <li>The `context` variable contains the expected test function definition 'test_1' in the correct location.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">34 lines (ranges: 33, 49, 52, 55, 58, 60, 63, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 132-133, 180)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_minimal_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Assembling a minimal context for testing test_1 in test_a.py</p>
                                <p><strong>Why Needed:</strong> Prevents regression when minimal context is not used by the llm.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'test_1' function should be found in the source code of test_a.py.</li>
                                        <li>The assembly result should have a nodeid of 'test_a.py::test_1'.</li>
                                        <li>The context should be an empty dictionary when assembling the minimal context.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">30 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_balanced_context_limits</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the ContextAssembler does not exceed the specified context limits when assembling a test file.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the ContextAssembler exceeds the specified context limit, causing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The assembler should be able to assemble the test file without truncating any part of it.</li>
                                        <li>The assembled context should contain all original content from the test file.</li>
                                        <li>The length of the assembled context should not exceed the specified limit (40 bytes in this case).</li>
                                        <li>Any truncated content should be clearly indicated within the assembled context.</li>
                                        <li>The assembler should handle files with varying lengths without truncating them.</li>
                                        <li>The assembler should preserve the original line count and coverage information from the test file.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">34 lines (ranges: 33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-84, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-156, 158-159, 163, 191-192, 194)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_get_test_source_edge_cases</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the ContextAssembler correctly handles non-existent files and nested test names with parameters.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the ContextAssembler incorrectly returns an empty string for a file that does not exist or has a nested test name with parameters.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert 'def test_param' in source</li>
                                        <li>assert 'test_p.py::test_param[1]' in source</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 33, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_should_exclude</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the ContextAssembler excludes certain files from the LLM context.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the ContextAssembler incorrectly includes sensitive files in the LLM context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>asserts that 'test.pyc' is excluded because it's a Python bytecode file and should be excluded by default.</li>
                                        <li>asserts that 'secret/key.txt' is excluded because it contains sensitive information that should not be included in the LLM context.</li>
                                        <li>asserts that 'public/readme.md' is excluded because it does not contain any sensitive information that would require its inclusion in the LLM context.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 33, 191-194)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_consecutive_lines</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Consecutive lines in the input list should be compressed to a single range.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when consecutive lines are present in the input data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `compress_ranges([1, 2, 3])` returns the correct range for consecutive lines (e.g., '1-3'),</li>
                                        <li>the function should handle cases where there are no consecutive lines or multiple non-consecutive lines in the input list.</li>
                                        <li>the function should correctly handle edge cases such as empty lists or lists with only one element.</li>
                                        <li>the function should return an error message for invalid input data (e.g., a list containing non-integer values).</li>
                                        <li>the function should preserve the original order of elements within each range.</li>
                                        <li>the function should be able to handle ranges that span multiple lines in the input data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_duplicates</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the function correctly handles duplicate ranges.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function may incorrectly identify or handle duplicate ranges as separate.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert compress_ranges([1, 2, 2, 3, 3, 3]) == '1-3'</li>
                                        <li>assert compress_ranges([1, 1, 2, 2, 3, 3]) == '1-3'</li>
                                        <li>assert compress_ranges([1, 2, 2, 2, 3, 3]) == '1-3'</li>
                                        <li>assert compress_ranges([1, 2, 3, 3, 3, 3]) == '1-3'</li>
                                        <li>assert compress_ranges([]) == ''</li>
                                        <li>assert compress_ranges([1]) == '1-'</li>
                                        <li>assert compress_ranges([1, 1]) == '1-'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_empty_list</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `compress_ranges` function with an empty input.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where an empty list is returned, causing unexpected behavior in downstream code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `compress_ranges` function should return an empty string for an empty input.</li>
                                        <li>The `compress_ranges` function should not raise any exceptions when given an empty input.</li>
                                        <li>The `compress_ranges` function should maintain its original behavior when given a non-empty list as input.</li>
                                        <li>The `compress_ranges` function should handle the case where the input is a single-element list correctly.</li>
                                        <li>The `compress_ranges` function should be able to identify and return the correct range for an empty list.</li>
                                        <li>The `compress_ranges` function should not produce any unexpected output when given an empty list as input.</li>
                                        <li>The `compress_ranges` function should maintain its original order of operations when given a non-empty list as input.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 29-30)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_mixed_ranges</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the function `compress_ranges` with a mixed range of values.</p>
                                <p><strong>Why Needed:</strong> The test prevents regression in cases where ranges are mixed with single values.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output should be '1-3, 5, 10-12, 15' as per the expected format.</li>
                                        <li>Each range value should be correctly separated by a comma.</li>
                                        <li>Single values within ranges should not affect the overall output.</li>
                                        <li>No single values should appear in the output without being part of a range.</li>
                                        <li>The function should handle cases where there are no ranges (i.e., all values are singles).</li>
                                        <li>The function should still produce the expected output even if the input list contains duplicate values within ranges.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_non_consecutive_lines</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Non-consecutive lines in the input list are expected to be comma-separated.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when non-consecutive lines are present in the input data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return a string with comma-separated values for non-consecutive lines.</li>
                                        <li>The function should handle cases where there is only one value in the list.</li>
                                        <li>Non-consecutive lines should be separated by commas, not spaces or other characters.</li>
                                        <li>The function should ignore leading zeros and trailing spaces when compressing the input list.</li>
                                        <li>Non-consecutive lines with different lengths should also be comma-separated.</li>
                                        <li>Leading zeros should be ignored when comparing compressed lists to the expected output.</li>
                                        <li>Trailing spaces should be ignored when comparing compressed lists to the expected output.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_single_line</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_ranges.py::TestCompressRanges::test_single_line</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the single-line function would incorrectly compress ranges.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input list should contain only one element.</li>
                                        <li>The compressed range should be equal to the original range.</li>
                                        <li>No range notation should be used in the output string.</li>
                                        <li>The output string should not contain any extraneous characters or whitespace.</li>
                                        <li>The function should handle empty lists correctly.</li>
                                        <li>The function should return an error message for invalid input (e.g. non-numeric values).</li>
                                        <li>The function should preserve the original order of elements in the list.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 29, 33, 35-37, 39, 50, 52, 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_two_consecutive</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `compress_ranges` function with two consecutive line numbers.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where two consecutive line numbers are compressed to a single number.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input list contains at least two elements.</li>
                                        <li>At least one element in the input list is an integer.</li>
                                        <li>All elements in the input list are integers.</li>
                                        <li>The function correctly compresses two consecutive line numbers.</li>
                                        <li>The compressed range does not contain any non-integer values.</li>
                                        <li>The function handles edge cases where the input list contains only one or zero elements.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_unsorted_input</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `compress_ranges` function with an unsorted input.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly handles unsorted ranges.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should correctly compress the ranges and return a string in the format 'start-end',</li>
                                        <li>   e.g. '1-3' for the range [5, 1, 3].</li>
                                        <li>   or '2-4' for the range [2, 4] if the input is already sorted.</li>
                                        <li>The function should handle duplicate ranges correctly and not produce any errors.</li>
                                        <li>   e.g. ['1-3', '2-4'] for the input [5, 1, 3, 2].</li>
                                        <li>   or ['2-4', '1-3'] for the input [2, 4] if the input is already sorted.</li>
                                        <li>The function should return an empty string if the input is empty.</li>
                                        <li>   e.g. '' for the input [].</li>
                                        <li>   or 'None' for the input [None, None, None].</li>
                                        <li>The function should handle non-integer values correctly and not produce any errors.</li>
                                        <li>   e.g. ['1', '2'] for the input [5, 1, 3, 2] with non-integer values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_empty_string</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that an empty string expands to an empty list.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the function returns incorrect results for empty input strings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Input: An empty string</li>
                                        <li>Expected output: An empty list</li>
                                        <li>Check if the function correctly handles empty strings by comparing its result with an empty list</li>
                                        <li>Verify that the function does not throw any errors when given an empty string as input</li>
                                        <li>Check for any potential edge cases, such as a single-character string or a string containing only whitespace characters</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 81-82)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_mixed</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `expand_ranges` function correctly handles mixed ranges and singles.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly treats single numbers as if they were ranges.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should split the string into individual elements (1, 5, 10-12) and convert them to integers before returning a list.</li>
                                        <li>The function should correctly handle single numbers in the range (1, 3).</li>
                                        <li>The function should not treat single numbers as if they were ranges (e.g., 2 is considered a range of [1, 3]).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 81, 84-91, 93, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_range</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `expand_ranges` function is expected to expand a given range of numbers into a list.</p>
                                <p><strong>Why Needed:</strong> This test prevents the function from expanding the range incorrectly, potentially leading to incorrect results or errors in downstream code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input string should be in the format 'start-end' (e.g., '1-3')</li>
                                        <li>The start value should be less than or equal to the end value</li>
                                        <li>All values between the start and end should be included in the output list</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 81, 84-91, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_roundtrip</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that `compress_ranges` and `expand_ranges` are inverses by comparing the original list with its roundtrip expansion.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the order of elements in the compressed or expanded range is not preserved, leading to incorrect results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The original list `[1, 2, 3, 5, 10, 11, 12, 15]` should be equal to its roundtrip expansion `original`.</li>
                                        <li>The compressed range `[1, 2, 3, 5, 10]` should be equal to the expanded range `[1, 2, 3, 5, 10, 11, 12, 15]`.</li>
                                        <li>All elements in the original list should be present in the roundtrip expansion.</li>
                                        <li>No element outside the original list should be present in the roundtrip expansion.</li>
                                        <li>The order of elements within each range should be preserved.</li>
                                        <li>No duplicate ranges should be created during the roundtrip process.</li>
                                        <li>All ranges should have a unique start and end value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">27 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67, 81, 84-91, 93, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_single_number</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'expand_ranges' function is expected to handle a single input number correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents the function from producing an incorrect result for a single number, potentially leading to unexpected behavior or errors in downstream calculations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Input: '5'</li>
                                        <li>Expected output: [5]</li>
                                        <li>Expansion of ranges should not be necessary</li>
                                        <li>Single input number does not require expansion</li>
                                        <li>No need to check if the input is a valid number</li>
                                        <li>The function should return an empty list for single numbers</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 81, 84-87, 93, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestFormatDuration::test_milliseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the function formats duration in milliseconds for values less than 1 second.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function does not correctly format durations below 1 second as '500ms' instead of '1ms'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return '500ms' when given an input of 0.5 seconds.</li>
                                        <li>The function should return '1ms' when given an input of 0.001 seconds.</li>
                                        <li>The function should return '0ms' when given an input of 0.0 seconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestFormatDuration::test_seconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the duration function correctly formats seconds for values greater than or equal to 1 second.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not handle cases where the input is less than 1 second correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output of format_duration(1.23) should be '1.23s'.</li>
                                        <li>The output of format_duration(60.0) should be '60.00s'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestOutcomeToCssClass::test_all_outcomes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> All outcomes should map to CSS classes.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in CSS class mapping for different outcome types.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `outcome_to_css_class` correctly maps all outcome strings to their corresponding CSS classes.</li>
                                        <li>It handles the special case of 'xfailed' by returning 'outcome-xfailed'.</li>
                                        <li>For outcomes like 'passed', it returns 'outcome-passed'.</li>
                                        <li>The function also correctly maps 'error' to 'outcome-error'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 79-85, 87)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the 'outcome_to_css_class' function with an unknown outcome.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function returns incorrect CSS classes for unknown outcomes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return 'outcome-unknown' when given an unknown outcome.</li>
                                        <li>The function should not return any other class (e.g. 'outcome-error', 'outcome-success')</li>
                                        <li>The function should handle cases where the outcome is not recognized by returning a default CSS class ('outcome-unknown').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 79-85, 87)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_basic_report</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the rendering of a basic report with fallback HTML.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the rendered report contains incomplete or incorrect information due to missing or malformed HTML.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The '<!DOCTYPE html>' header is present in the rendered HTML.</li>
                                        <li>The 'Test Report' title is displayed in the rendered HTML.</li>
                                        <li>The 'test::passed' and 'test::failed' nodeids are found in the rendered HTML.</li>
                                        <li>The 'PASSED' and 'FAILED' labels are displayed in the rendered HTML.</li>
                                        <li>The 'Plugin:</strong> v0.1.0' and 'Repo:</strong> v1.2.3' text is present in the rendered HTML.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">52 lines (ranges: 65-67, 79-85, 87, 121-124, 126-127, 131-132, 141-143, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the test renders coverage information.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression and ensures that the test renders coverage information correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The report root contains a 'Summary' object with a total of 1 test and 1 passed test.</li>
                                        <li>The 'CoverageEntry' object in the 'tests' list has a file path of 'src/foo.py', indicating that it covers lines 1-5.</li>
                                        <li>The 'html' variable is set to include the source code of 'src/foo.py'.</li>
                                        <li>The assertion checks if the string '5 lines' is present in the rendered HTML. If not, it will fail and trigger a test failure.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">52 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-129, 131-132, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_llm_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the LLM annotation is included in the rendered HTML report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the LLM annotation is not displayed correctly, potentially allowing for authentication bypass.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The string "Tests login flow" should be present in the rendered HTML report.</li>
                                        <li>The string "Prevents auth bypass" should be present in the rendered HTML report.</li>
                                        <li>The LLM annotation nodeid should match with "test::foo" in the test result.</li>
                                        <li>The outcome of the test should be 'passed' according to the summary.</li>
                                        <li>The total number of tests passed should be 1.</li>
                                        <li>The number of tests that failed should be 0.</li>
                                        <li>The number of tests that were skipped should be 0.</li>
                                        <li>The number of tests with errors should be 0.</li>
                                        <li>The report root should have a valid end_time.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-134, 136-137, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_source_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the inclusion of source coverage summary in the rendered HTML.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the source coverage is not displayed correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'Source Coverage' section should be present in the rendered HTML.</li>
                                        <li>The 'src/foo.py' file path should be included in the 'Source Coverage' section.</li>
                                        <li>The percentage of covered code (80.0%) should be displayed in the 'Source Coverage' section.</li>
                                        <li>The ranges of missed code ('1-4, 6-8') should be included in the 'Source Coverage' section.</li>
                                        <li>The ranges of missed code ('5, 9-10') should not be included in the 'Source Coverage' section.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">63 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-132, 141-142, 145-153, 158-164, 166-172, 177, 192, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_xpass_summary</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the 'XFailed' and 'XPassed' summary entries are included in the rendered HTML.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the 'XFailed' and 'XPassed' summary entries are not displayed correctly due to missing fallback HTML.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'XFailed' summary entry should be present in the rendered HTML.</li>
                                        <li>The 'XPassed' summary entry should be present in the rendered HTML.</li>
                                        <li>Both 'XFailed' and 'XPassed' summary entries should be included in the rendered HTML, without any missing or incorrect information.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">50 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-132, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestComputeSha256::test_different_content</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'different_content' verifies that the same input produces different hashes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where two inputs with the same content but different encoding produce the same output hash.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>hash1 != hash2</li>
                                        <li>assert isinstance(hash1, int) and isinstance(hash2, int)</li>
                                        <li>assert len(hash1) == len(hash2)</li>
                                        <li>assert all(isinstance(c, str) for c in hash1) and all(isinstance(c, str) for c in hash2)</li>
                                        <li>assert all(c in hash1 for c in 'abcdefghijklmnopqrstuvwxyz') and all(c in hash2 for c in 'abcdefghijklmnopqrstuvwxyz')</li>
                                        <li>assert all(c in hash1 for c in '0123456789abcdefABCDEF') and all(c in hash2 for c in '0123456789abcdefABCDEF')</li>
                                        <li>assert len(hash1) == 32 and len(hash2) == 32</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 55)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestComputeSha256::test_empty_bytes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_empty_bytes': Verifies that an empty byte string produces consistent hash.</p>
                                <p><strong>Why Needed:</strong> To prevent a potential bug where the test fails due to incorrect expected hash values for empty byte strings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `compute_sha256(b'')` should produce a consistent hash value.</li>
                                        <li>The length of the resulting hash value should be equal to 64 bytes (the SHA256 hex length).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 55)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_run_meta</span>
                        <div class="test-meta">
                            <span>9ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `build_run_meta` method of ReportWriter to ensure it includes version info.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the report writer does not include version information in the run metadata.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The duration of the run should be 60 seconds.</li>
                                        <li>The pytest version should have a value.</li>
                                        <li>The plugin version should be '0.1.0'.</li>
                                        <li>The Python version should also be present.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_summary_all_outcomes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the `build_summary` method counts all outcome types correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the summary does not accurately count all outcome types, potentially leading to incorrect reporting of test results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>asserts that the total count of outcomes is equal to 6 (all outcome types: passed, failed, skipped, xfailed, xpassed, error)</li>
                                        <li>asserts that the number of passed outcomes is equal to 1</li>
                                        <li>asserts that the number of failed outcomes is equal to 1</li>
                                        <li>asserts that the number of skipped outcomes is equal to 1</li>
                                        <li>asserts that the number of xfailed outcomes is equal to 1</li>
                                        <li>asserts that the number of xpassed outcomes is equal to 1</li>
                                        <li>asserts that the number of error outcomes is equal to 1</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 156-158, 312, 314-315, 317-328, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_summary_counts</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `test_build_summary_counts` method correctly counts the total number of tests, passed tests, failed tests, and skipped tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in cases where the output of the `_build_summary` method changes unexpectedly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>total should be equal to 4 (number of tests)</li>
                                        <li>passed should be equal to 2 (number of passed tests)</li>
                                        <li>failed should be equal to 1 (number of failed tests)</li>
                                        <li>skipped should be equal to 1 (number of skipped tests)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">13 lines (ranges: 156-158, 312, 314-315, 317-322, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_create_writer</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that a new `ReportWriter` instance is created with the provided configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `ReportWriter` instance does not initialize correctly with the provided configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config` attribute of the `writer` object should be set to the provided `Config` instance.</li>
                                        <li>The `warnings` list of the `writer` object should be empty.</li>
                                        <li>The `artifacts` list of the `writer` object should be empty.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 156-158)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_assembles_tests</span>
                        <div class="test-meta">
                            <span>9ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test writes report with assembled tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the report does not include all tests and only shows failed ones.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the report.tests list should be equal to 2.</li>
                                        <li>The value of report.summary.total should be equal to 2.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">93 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_includes_coverage_percent</span>
                        <div class="test-meta">
                            <span>10ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `ReportWriter` class writes a report with a total coverage percentage.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the reported coverage percentage is not accurate due to missing or incomplete data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `coverage_total_percent` attribute of the `report.summary` object should be equal to the provided `coverage_percent` value.</li>
                                        <li>The `report.summary.coverage_total_percent` attribute should be a floating point number between 0 and 100.</li>
                                        <li>The `report.summary.coverage_total_percent` attribute should not exceed the maximum possible coverage percentage (100%).</li>
                                        <li>The `report.summary.coverage_total_percent` attribute should be greater than or equal to the minimum required coverage percentage (90%).</li>
                                        <li>The `report.summary.coverage_total_percent` attribute should be a non-negative number.</li>
                                        <li>The `report.summary.coverage_total_percent` attribute should not be zero.</li>
                                        <li>The `report.summary.coverage_total_percent` attribute should be an integer value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">93 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-199, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_includes_source_coverage</span>
                        <div class="test-meta">
                            <span>9ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test ReportWriter::test_write_report_includes_source_coverage verifies that the report includes source coverage summary.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the report accurately reflects source coverage.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of `report.source_coverage` should be 1.</li>
                                        <li>The file path of the first element in `report.source_coverage` should match `file_path = "src/foo.py"`.</li>
                                        <li>All elements in `report.source_coverage` should have a valid `file_path`, `statements`, `missed`, `covered`, `coverage_percent`, and `covered_ranges` value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">92 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_merges_coverage</span>
                        <div class="test-meta">
                            <span>9ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test ReportWriter::test_write_report_merges_coverage verifies that the report writer merges coverage into tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the coverage is not properly merged into the test results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The number of coverage entries in the first test should be 1.</li>
                                        <li>The file path of the first coverage entry should match 'src/foo.py'.</li>
                                        <li>Each coverage entry's file path should have a line range of '1-5' and a line count of 5.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">94 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186-189, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_atomic_write_fallback</span>
                        <div class="test-meta">
                            <span>10ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the ReportWriter falls back to direct write if atomic write fails.</p>
                                <p><strong>Why Needed:</strong> To prevent a regression where the atomic write operation fails and the fallback to direct write is not executed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The report.json file should exist at the specified path.</li>
                                        <li>Any warnings with code W203 should be present in the report.json file.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">125 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506-507, 509-512, 515-516)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_creates_directory_if_missing</span>
                        <div class="test-meta">
                            <span>11ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `ReportWriter` creates a directory if it does not exist.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the report writer fails to create an output directory when the input JSON file is missing.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output directory should be created at the specified path.</li>
                                        <li>The `ReportWriter` should raise an exception if the input JSON file does not exist.</li>
                                        <li>The test should fail with a meaningful error message indicating that the report writer failed to create the output directory.</li>
                                        <li>The test should verify that the output directory is correctly created and contains the expected files.</li>
                                        <li>The test should check that the `ReportWriter` raises an exception when the input JSON file does not exist.</li>
                                        <li>The test should verify that the error message indicates a failure to create the output directory.</li>
                                        <li>The test should check that the error message includes information about the missing input JSON file.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">84 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">123 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-477, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_ensure_dir_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the `test_ensure_dir_failure` test case ensures a directory creation failure is captured by the report writer.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the report writer does not capture directory creation failures, potentially leading to silent errors or incorrect reporting of such issues.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `writer.warnings` list contains at least one warning with code 'W201' (indicating a permission denied error).</li>
                                        <li>The `writer.warnings` list is not empty.</li>
                                        <li>The `writer.warnings` list does not contain any warnings with code other than 'W201'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 156-158, 470-473, 480-484)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_git_info_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_git_info_failure' verifies that the test_report_writer.py module can handle git command failures gracefully.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the test_report_writer.py module does not properly handle git command failures, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'get_git_info()' function should return None for both sha and dirty values when git is not found.</li>
                                        <li>The 'sha' variable should be None after calling 'get_git_info()'.</li>
                                        <li>The 'dirty' variable should also be None after calling 'get_git_info()'.</li>
                                        <li>An exception of type Exception should be raised when calling 'subprocess.check_output' with a side effect.</li>
                                        <li>The exception should have the message 'Git not found'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 67-73, 85-86)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_creates_file</span>
                        <div class="test-meta">
                            <span>36ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `ReportWriter` class creates an HTML file with expected content.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the report writer does not create an HTML file, potentially causing issues with report generation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The report.html file should exist at the specified path.</li>
                                        <li>The report.html file should contain the expected content as described in the assertions.</li>
                                        <li>All nodes in the report should be marked as either 'PASSED', 'FAILED', 'Skipped', 'XFailed', or 'XPassed'.</li>
                                        <li>Errors and warnings should not be present in the report.</li>
                                        <li>Test1 should be reported as passed, Test2 should be reported as failed with an error message.</li>
                                        <li>All test nodes should have a nodeid of either 'test1' or 'test2'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">115 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_includes_xfail_summary</span>
                        <div class="test-meta">
                            <span>37ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the xfail outcomes are included in the HTML summary.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the xfail outcomes are not displayed in the HTML summary.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'XFAILED' and 'XFailed' tags should be present in the HTML report.</li>
                                        <li>The 'XPASSED' and 'XPassed' tags should also be present in the HTML report.</li>
                                        <li>All xfail outcomes should be included in the HTML summary, regardless of their outcome status.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-326, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_json_creates_file</span>
                        <div class="test-meta">
                            <span>10ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that a JSON file is created with the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the report writer does not create a JSON file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'report.json' file should be created in the specified path.</li>
                                        <li>At least one artifact should be tracked for the report.</li>
                                        <li>The length of the artifacts list should be greater than zero.</li>
                                        <li>The file 'report.json' exists at the specified path.</li>
                                        <li>The 'report.json' file contains the expected JSON data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">78 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">117 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_creates_file</span>
                        <div class="test-meta">
                            <span>39ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the `write_pdf` method creates a PDF file when Playwright is available.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the `write_pdf` method does not create a PDF file even though Playwright is available.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `writer.write_report(tests)` call should successfully write the report to the specified PDF file.</li>
                                        <li>Any artifacts created by the report writer should have paths that match the path of the PDF file.</li>
                                        <li>The `exists()` method on the PDF file path should return True.</li>
                                        <li>The `any()` function should find any artifact with a path equal to the PDF file path.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">125 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 401, 410, 412, 414-423, 434-435, 437-443, 448, 453, 455, 458-462, 470-471)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_missing_playwright_warns</span>
                        <div class="test-meta">
                            <span>10ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that a warning is raised when the 'report_pdf' configuration option is set to a file path without a Playwright installation.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the report writer does not warn users about missing Playwright for PDF output, potentially leading to confusion or errors in their workflow.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'report_pdf' configuration option is set to a file path without a Playwright installation.</li>
                                        <li>The 'code' attribute of each warning object matches WarningCode.W204_PDF_PLAYWRIGHT_MISSING.value.</li>
                                        <li>Any warnings raised by the report writer do not have an empty 'code' attribute.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">98 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 401-405, 408)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer_coverage_v2.py::test_report_writer_ensure_dir_creation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test ensures directory creation of report writer output.</p>
                                <p><strong>Why Needed:</strong> Prevents a directory creation warning when writing reports with the `report_html` parameter.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test verifies that the `tmp_test_dir` exists after creating it.</li>
                                        <li>The test verifies that any warnings from the report writer are equal to 'W202'.</li>
                                        <li>The test verifies that the `tmp_test_dir` is deleted after writing the reports.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 156-158, 470-477)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer_coverage_v2.py::test_report_writer_metadata_skips</span>
                        <div class="test-meta">
                            <span>18ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the report_writer_metadata_skips test correctly skips metadata when reports are disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where metadata is included in reports even when they are disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'start_time' key should be present in the metadata.</li>
                                        <li>The 'llm_model' key should not be present in the metadata if the report is disabled.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_schemas.py::TestAnnotationSchema::test_from_dict_full</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `AnnotationSchema.from_dict` can create a valid annotation from a dictionary with all required fields.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of missing or invalid input data, ensuring the application behaves correctly when receiving unstructured input.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert schema.scenario == 'Verify login'</li>
                                        <li>assert schema.why_needed == 'Catch auth bugs'</li>
                                        <li>assert schema.key_assertions == ['assert 200', 'assert token']</li>
                                        <li>assert schema.confidence == 0.95</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_schemas.py::TestAnnotationSchema::test_to_dict_full</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> test_to_dict_full verifies that the full annotation to dictionary conversion is successful.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the full annotation to dictionary conversion correctly captures all necessary fields, including scenario and why needed information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert data['scenario'] == 'Verify login',</li>
                                        <li>assert data['why_needed'] == 'Catch auth bugs',</li>
                                        <li>assert data['key_assertions'] == ['assert 200', 'assert token'],</li>
                                        <li>assert data['confidence'] == 0.95</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 90-92, 94-98)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_report_created</span>
                        <div class="test-meta">
                            <span>88ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The HTML report is generated successfully.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the report does not exist or has incorrect content.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file path of the report should be created.</li>
                                        <li>The report should contain the expected string '<html'.</li>
                                        <li>The report should contain the expected string 'test_simple'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">101 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_summary_counts_all_statuses</span>
                        <div class="test-meta">
                            <span>125ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> test_html_summary_counts_all_statuses verifies that the HTML summary counts include all statuses.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the report does not display the total number of tests, failed tests, skipped tests, and errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'Total Tests' card should contain the correct count.</li>
                                        <li>The 'Passed' card should contain the correct count.</li>
                                        <li>The 'Failed' card should contain the correct count.</li>
                                        <li>The 'Skipped' card should contain the correct count.</li>
                                        <li>The 'XFailed' and 'XPassed' cards should contain the correct counts.</li>
                                        <li>The 'Errors' card should contain the correct count.</li>
                                        <li>All labels in the list passed should match the expected count.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 78-79, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212-214, 216, 227-228, 230-236, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">111 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-328, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_json_report_created</span>
                        <div class="test-meta">
                            <span>74ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The JSON report is created successfully.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the report generation fails due to an incorrect or missing schema version.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'schema_version' key in the report data should be set to '1.0'.</li>
                                        <li>The 'summary' dictionary should contain the correct keys ('total', 'passed', and 'failed') with the expected values.</li>
                                        <li>The number of 'passed' and 'failed' counts in the summary should match the total count in the report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">51 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-118, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">107 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_annotations_in_report</span>
                        <div class="test-meta">
                            <span>82ms</span>
                            <span title="Covered file count">üõ°Ô∏è 13</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that LLM annotations are included in the report when a provider is enabled.</p>
                                <p><strong>Why Needed:</strong> Prevents regression by ensuring LLM annotations are present in reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'scenario' key in the report contains 'Checks the happy path'.</li>
                                        <li>The 'why_needed' key in the report indicates that LLM annotations prevent regressions.</li>
                                        <li>The 'llm_annotation' key in the report is present and contains the expected data.</li>
                                        <li>The 'scenario' value matches the provided string 'Checks the happy path'.</li>
                                        <li>The 'key_assertions' list includes the expected assertions for the 'llm_annotation' key.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 52-53, 72, 75, 80, 107, 110-111, 128, 136, 147, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">23 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69-70, 94-95, 97)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">94 lines (ranges: 104-107, 109-111, 113, 115, 161-165, 167, 169, 171, 173, 176, 178-180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407-419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">186 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-340, 343, 345, 348-352, 355, 357-362, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_error_is_reported</span>
                        <div class="test-meta">
                            <span>6.09s</span>
                            <span title="Covered file count">üõ°Ô∏è 12</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM errors are surfaced in HTML output.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where LLM errors are not reported correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'LLM error' keyword is present in the report content.</li>
                                        <li>The 'boom' string is present in the report content.</li>
                                        <li>The 'LLM error' and 'boom' keywords are both found in the report content.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 39-41, 53, 55-56, 86, 88, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">73 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137-139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198-201, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 52-53, 72, 75, 80, 107, 110-111, 128, 136, 147, 165, 167, 175, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 78-79, 81-82, 84-85, 94-95, 97)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">186 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-340, 343, 345, 348-353, 357-362, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">101 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestMarkers::test_llm_opt_out_marker</span>
                        <div class="test-meta">
                            <span>59ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the LLM opt-out marker.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where the LLM opt-out marker is not recorded correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test verifies that the `llm_opt_out` marker is applied to the `test_opt_out` function.</li>
                                        <li>The test verifies that the `llm_opt_out` marker is set to `True` for the `test_opt_out` function.</li>
                                        <li>The test verifies that only one test is recorded in the report.</li>
                                        <li>The test verifies that the `llm_opt_out` marker is correctly applied to the `test_opt_out` function.</li>
                                        <li>The test verifies that the `llm_opt_out` marker is not set for other functions.</li>
                                        <li>The test verifies that the `llm_opt_out` marker is recorded in the report correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181-182, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180-182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestMarkers::test_requirement_marker</span>
                        <div class="test-meta">
                            <span>58ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the requirement marker to ensure it records the correct requirements.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the requirement marker is not recorded correctly, potentially leading to incorrect reporting of tests that require specific dependencies.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest.mark.requirement` decorator should be applied to the test function with the required requirements.</li>
                                        <li>The `test_with_req()` function should have the correct requirements specified using the `@pytest.mark.requirement()` decorator.</li>
                                        <li>The report generated by pytest should contain the correct requirements for each test.</li>
                                        <li>The `reqs` list in the `tests` dictionary should contain the expected requirements.</li>
                                        <li>The 'REQ-001' and 'REQ-002' strings should be present in the `reqs` list.</li>
                                        <li>The `test_with_req()` function should have a valid requirement string for each test.</li>
                                        <li>The report path should contain the correct file name and path to the report JSON.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-200, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188-190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_multiple_xfail_outcomes</span>
                        <div class="test-meta">
                            <span>65ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'test_multiple_xfail_outcomes' test verifies that multiple xfailed tests are recorded in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that all xfailed tests are properly reported and counted in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The number of xfailed tests is exactly 2 as per the expected output.</li>
                                        <li>Each xfailed test has a corresponding outcome ('xfailed') in the 'outcomes' list.</li>
                                        <li>All xfailed tests are included in the 'tests' list with their respective outcomes.</li>
                                        <li>The 'summary' dictionary contains an 'xfailed' key with a value of 2, indicating that there were exactly 2 xfailed tests.</li>
                                        <li>Each test is marked as xfail using the '@pytest.mark.xfail' decorator.</li>
                                        <li>The 'makepyfile' function creates two test functions ('test_xfail_one' and 'test_xfail_two') with the '@pytest.mark.xfail' decorator.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">108 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-324, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_skip_outcome</span>
                        <div class="test-meta">
                            <span>66ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the test skip outcome is correctly recorded in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a false positive 'all tests passed' message when a test is skipped.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The number of skipped tests should be 1 according to the report.</li>
                                        <li>The report path contains the expected file name for skipped tests (report.json).</li>
                                        <li>The data dictionary in the report should contain the correct key 'skipped' with value 1.</li>
                                        <li>The assertion checks if the 'summary' key exists and has a value of 1.</li>
                                        <li>The assertion checks if the 'skipped' count is equal to 1.</li>
                                        <li>The assertion checks if the file name contains the expected string (report.json).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">43 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 106-107, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">107 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321-322, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_xfail_outcome</span>
                        <div class="test-meta">
                            <span>62ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the test 'test_xfail' is marked as Xfailed and its outcome is recorded in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where an Xfailed test does not get recorded in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test 'test_xfail' should be marked as Xfailed by the pytester.</li>
                                        <li>The test 'test_xfail' should have an outcome of Xfailed in the report.</li>
                                        <li>The number of Xfailed tests should match the expected value (1) in the report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">108 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-324, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestParametrization::test_parametrized_tests</span>
                        <div class="test-meta">
                            <span>64ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that parameterized tests are recorded separately and their results are correctly reported in the report.json file.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where parameterized tests might not be recorded or reported correctly, leading to incorrect reporting of test results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Parameterized tests are recorded separately</li>
                                        <li>The total count of passed tests is correct (3)</li>
                                        <li>The number of passed tests matches the expected value (3)</li>
                                        <li>The report.json file contains the correct summary data</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167, 169-171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_help_contains_examples</span>
                        <div class="test-meta">
                            <span>51ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_markers_registered</span>
                        <div class="test-meta">
                            <span>46ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that LLM markers are registered and correctly displayed in the pytest output.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where LLM markers are not properly registered, potentially causing unexpected behavior or errors in downstream tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm_opt_out*' marker should be present in the pytest output.</li>
                                        <li>The 'llm_context*' marker should be present in the pytest output.</li>
                                        <li>The 'requirement*' marker should be present in the pytest output.</li>
                                        <li>The 'llm_opt_out*' marker should match the expected pattern in the pytest output.</li>
                                        <li>The 'llm_context*' marker should match the expected pattern in the pytest output.</li>
                                        <li>The 'requirement*' marker should match the expected pattern in the pytest output.</li>
                                        <li>All markers should be present in the pytest output, without any missing or incorrect matches.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_plugin_registered</span>
                        <div class="test-meta">
                            <span>53ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The plugin is successfully registered and its help message is displayed.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the plugin registration fails or is not properly reported by pytest11.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `--help` option should display the help message of the plugin.</li>
                                        <li>The output should contain the line '*--llm-report*'.</li>
                                        <li>The plugin's name and description should be displayed in the help message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestSpecialCharacters::test_special_chars_in_nodeid</span>
                        <div class="test-meta">
                            <span>85ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'Special characters in nodeid' verifies that the test does not crash when special characters are present in node IDs.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where special characters in node IDs could cause the test to crash or produce invalid results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The report path should exist and contain '<html>' as part of its contents.</li>
                                        <li>The HTML file should be valid and not contain any syntax errors.</li>
                                        <li>Special characters should not be present in the node ID, causing the test to fail.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">101 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_boundary_one_minute</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function with a boundary of exactly one minute.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in cases where the input duration is less than or equal to one minute, as it would incorrectly format it as '1m 0.0s'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result should be '1m 0.0s' when the input duration is exactly 60 seconds.</li>
                                        <li>The unit prefix 'm' should not be included in the output if the input duration is less than or equal to one minute.</li>
                                        <li>The total number of seconds should match the input duration.</li>
                                        <li>Any non-numeric characters (e.g., 's') should be ignored when formatting the result.</li>
                                        <li>The function should raise a `ValueError` for invalid input durations (e.g., negative numbers, zero).</li>
                                        <li>The function should handle cases where the input duration is exactly one minute without any additional units.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_microseconds_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the function `format_duration` correctly formats sub-millisecond durations as microseconds.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not format durations to include microsecond units.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of calling `format_duration(0.0005)` should contain the string 'Œºs'.</li>
                                        <li>The formatted output should be equal to '500Œºs'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 39-40)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_milliseconds_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `format_duration` function correctly formats sub-second durations as milliseconds.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not format the duration to the correct number of decimal places when it is less than one second.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result should contain 'ms' in its string representation.</li>
                                        <li>The value of `result` should be equal to '500.0ms'.</li>
                                        <li>The function should handle durations less than one second correctly and format them as milliseconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 39, 41-42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_minutes_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the 'minutes' key assertion for durations over a minute.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when formatting durations to show minutes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'result' string should contain the word 'm'.</li>
                                        <li>The 'result' string should be in the format '1m X.Xs'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_multiple_minutes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies the correct formatting of multiple minutes in a duration.</p>
                                <p><strong>Why Needed:</strong> Prevents incorrect formatting of durations with multiple minutes, which can lead to misleading output.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result should be '3m 5.0s' (three minutes and five seconds).</li>
                                        <li>The minute part should be displayed as '3'.</li>
                                        <li>The second part of the minute should be displayed as '5.0'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_one_second</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the function formats a single second into '1.00s' as expected.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in formatting time intervals to seconds.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return '1.00s' when given an argument of exactly 1.0.</li>
                                        <li>The function should handle cases where the input is not a number (e.g., strings, None).</li>
                                        <li>The function should correctly format negative time intervals (e.g., '-1.0s').</li>
                                        <li>The function should return '1.00' when given an argument of exactly 1.0 and no units.</li>
                                        <li>The function should handle cases where the input is a very large or small number (e.g., floats, integers).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 39, 41, 43-44)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_seconds_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the function correctly formats seconds under a minute.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not format seconds correctly when they are less than one minute.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of `format_duration(5.5)` should contain the string 's' to indicate that it's in seconds.</li>
                                        <li>The result of `format_duration(5.5)` should be equal to '5.50s' to verify its correctness.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 39, 41, 43-44)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_small_milliseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function with a duration of 1 millisecond.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the function incorrectly returns '1ms' for durations less than 1 millisecond.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return '1.0ms' for a duration of 1 millisecond.</li>
                                        <li>The function should not return '1ms' or any other value for a duration less than 1 millisecond.</li>
                                        <li>The function should handle durations in the range [1, 1000000] correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 39, 41-42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_very_small_microseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function with a very small duration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly formats durations as milliseconds instead of microseconds when they are extremely small.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should correctly format the given duration as microseconds.</li>
                                        <li>The function should return '1Œºs' for a duration of 1 microsecond.</li>
                                        <li>The function should handle very small durations (less than 0.001 seconds) without truncation or incorrect formatting.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 39-40)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_datetime_with_utc</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `iso_format` function with a datetime object representing UTC time.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues where a datetime object is not correctly formatted as UTC.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output of the `iso_format` function should be in the format 'YYYY-MM-DDTHH:MM:SS+HH:MM:SS', which represents UTC time.</li>
                                        <li>The `tzinfo` parameter should be set to `UTC` for the datetime object.</li>
                                        <li>Any errors or exceptions thrown by the `iso_format` function should be properly handled and reported.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_naive_datetime</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the naive datetime format by verifying it matches the expected output without timezone.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the naive datetime format is not correctly handled, potentially leading to incorrect date representation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `iso_format(dt)` returns the correct ISO formatted string for the given datetime object.</li>
                                        <li>The resulting string does not include any timezone information.</li>
                                        <li>The resulting string is in the correct format according to the ISO 8601 standard.</li>
                                        <li>The function handles edge cases where the input datetime is invalid or outside the expected range.</li>
                                        <li>The function correctly formats the naive datetime without considering daylight saving time (DST) rules.</li>
                                        <li>The function does not introduce any timezone-related bugs or inconsistencies.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_with_microseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `iso_format` function with a datetime object that includes microseconds.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `iso_format` function does not correctly handle dates with microseconds.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'result' variable should contain the string '123456'.</li>
                                        <li>The 'result' variable should be equal to '123456'.</li>
                                        <li>The 'result' variable should include the substring '123456' in its value.</li>
                                        <li>The `iso_format` function is correctly handling microseconds by including them in the output.</li>
                                        <li>The `iso_format` function is not returning an empty string or None when given a datetime object with microseconds.</li>
                                        <li>The `iso_format` function is correctly formatting the date and time to include microseconds.</li>
                                        <li>The `iso_format` function is preserving the original timezone of the input datetime object.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestUtcNow::test_has_utc_timezone</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `utc_now()` function returns a datetime object with an established UTC timezone.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the test may fail if the system's default timezone is not set to UTC.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>result.tzinfo is not None</li>
                                        <li>result.tzinfo == UTC</li>
                                        <li>result.tzinfo is a valid timezone object (e.g., 'UTC')</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestUtcNow::test_is_current_time</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `utc_now()` function returns a current time within a specified tolerance.</p>
                                <p><strong>Why Needed:</strong> Prevents incorrect or outdated times from being reported as current.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The returned time is within the UTC time range (before <= result <= after).</li>
                                        <li>The difference between before and after is less than or equal to the specified tolerance.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestUtcNow::test_returns_datetime</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `utc_now()` function should return a valid datetime object.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function returns an incorrect or missing datetime value.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert isinstance(result, datetime)</li>
                                        <li>assert result is not None</li>
                                        <li>assert isinstance(result, datetime) and result.year == 1970</li>
                                        <li>assert isinstance(result, datetime) and result.month in [1, 2, 3, 4, 5]</li>
                                        <li>assert isinstance(result, datetime) and result.day in [1, 2, 3, 4, 5]</li>
                                        <li>assert isinstance(result, datetime) and result.hour == 0</li>
                                        <li>assert isinstance(result, datetime) and result.minute == 0</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
        </div>

        <section class="source-coverage">
            <h2>Source Coverage</h2>
            <div class="source-coverage-table">
                <div class="source-coverage-header">
                    <span>File</span>
                    <span>Stmts</span>
                    <span>Miss</span>
                    <span>Cover</span>
                    <span>%</span>
                    <span>Covered Lines</span>
                    <span>Missed Lines</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/_git_info.py</span>
                    <span>2</span>
                    <span>0</span>
                    <span>2</span>
                    <span>100.0%</span>
                    <span class="source-lines">2-3</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/aggregation.py</span>
                    <span>116</span>
                    <span>5</span>
                    <span>111</span>
                    <span>95.69%</span>
                    <span class="source-lines">13, 15-19, 21, 35, 38, 44, 46, 52-53, 55-57, 59, 61-64, 69, 73-74, 77-80, 84, 87-89, 93, 103, 109-111, 113-117, 119-120, 125, 127-128, 130-131, 134-135, 141-144, 146, 148, 162, 164, 168, 170, 172, 182, 184-188, 190-191, 194, 196, 205, 217, 219-233, 235, 237, 245-246, 248-249, 251, 253-255, 259, 262-263, 265-266, 269-271, 273, 275-276, 280</span>
                    <span class="source-lines">66, 90-91, 192, 203</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/cache.py</span>
                    <span>47</span>
                    <span>3</span>
                    <span>44</span>
                    <span>93.62%</span>
                    <span class="source-lines">13, 15-19, 21, 27, 33, 39-41, 43, 53, 55-56, 58, 60-62, 68-69, 78, 86, 88, 90, 92, 94, 97, 103, 107, 118-119, 121, 123, 129, 132-136, 141, 144, 153</span>
                    <span class="source-lines">64-65, 130</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/collector.py</span>
                    <span>111</span>
                    <span>2</span>
                    <span>109</span>
                    <span>98.2%</span>
                    <span class="source-lines">19, 21-22, 24, 26-27, 33-34, 45-50, 52, 58, 60-62, 69, 78-79, 81, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127-128, 130, 132-133, 135-137, 140, 143, 155, 163-164, 167-169, 171, 173, 181-182, 185-189, 191, 198-200, 202, 209-210, 212-214, 216, 218, 227-228, 230-236, 238, 241, 250-252, 254, 261, 264-265, 268-269, 271, 277, 279, 285</span>
                    <span class="source-lines">141, 239</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/coverage_map.py</span>
                    <span>135</span>
                    <span>10</span>
                    <span>125</span>
                    <span>92.59%</span>
                    <span class="source-lines">13, 15-17, 19-22, 30, 38, 44-45, 47, 58-60, 64, 72-73, 83, 86, 88-90, 92, 94-96, 98, 101-104, 106-108, 114, 116, 118, 121-122, 127, 131-135, 137-140, 144-146, 148, 150, 152-153, 156, 160-162, 165, 167-168, 173, 176, 178-184, 187-189, 191, 196, 199-200, 202, 204, 216-217, 220, 224-225, 228-234, 236, 239, 241, 243-244, 246-248, 250, 252-254, 259-260, 263-264, 271, 273, 276-279, 281-283, 285, 299-300, 302, 308</span>
                    <span class="source-lines">62, 123, 125, 128, 157, 221, 249, 251, 257, 274</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/errors.py</span>
                    <span>35</span>
                    <span>0</span>
                    <span>35</span>
                    <span>100.0%</span>
                    <span class="source-lines">8-9, 12, 25-28, 31-36, 39-42, 45-46, 49-51, 54-55, 64-66, 68, 70, 74-76, 80, 129, 139</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/__init__.py</span>
                    <span>3</span>
                    <span>0</span>
                    <span>3</span>
                    <span>100.0%</span>
                    <span class="source-lines">4-5, 7</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/annotator.py</span>
                    <span>110</span>
                    <span>0</span>
                    <span>110</span>
                    <span>100.0%</span>
                    <span class="source-lines">4, 6-10, 12-15, 21-22, 25-28, 31, 45-46, 48-50, 54, 56-57, 59, 61-62, 64, 66-68, 71-72, 74-82, 87, 97-98, 100, 102, 104-105, 115, 127, 129-132, 137-139, 142, 165-168, 170-171, 176, 178, 180-183, 185-190, 192-193, 198-201, 203, 206, 229-232, 234, 236-237, 239-240, 245-246, 248-253, 255-256, 261-264, 266</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/base.py</span>
                    <span>78</span>
                    <span>0</span>
                    <span>78</span>
                    <span>100.0%</span>
                    <span class="source-lines">13, 15-18, 26, 40, 46, 52-53, 55, 72, 75-76, 78, 80, 101, 107-108, 110-111, 122, 128, 130, 136, 138, 147, 149, 165, 167-173, 175, 177, 186-187, 190-192, 194-195, 198-200, 203-208, 212, 214, 220-221, 224-225, 228-230, 233, 245, 247, 249-250, 252-253, 255, 257-258, 260, 262-263, 265, 267</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/gemini.py</span>
                    <span>275</span>
                    <span>18</span>
                    <span>257</span>
                    <span>93.45%</span>
                    <span class="source-lines">7, 9-13, 15-16, 23-27, 30-34, 37-42, 44-46, 48-50, 52, 57-63, 65-70, 72-73, 75-78, 80-85, 87-88, 91-97, 99-103, 105, 107-114, 121-122, 125, 128, 134, 136-139, 141-142, 144, 160-161, 167-169, 171-172, 174, 176-184, 186-188, 190-191, 193, 196, 200-208, 210-211, 213-215, 217-223, 225-227, 233-234, 238-239, 242-243, 245-248, 252-253, 260, 266-267, 269, 273-277, 279-283, 286-287, 292-293, 300-301, 303, 315, 317-318, 322, 327, 330-332, 335-343, 345-346, 348, 352-355, 357, 360-366, 368-374, 380-382, 384-387, 389, 391-392, 396-402, 405, 408-410, 412-414, 416-421, 427-428, 430-434, 437-440, 442-443, 445-447</span>
                    <span class="source-lines">89, 104, 106, 115-117, 199, 230-231, 235-237, 244, 250, 256, 367, 441, 444</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/litellm_provider.py</span>
                    <span>32</span>
                    <span>1</span>
                    <span>31</span>
                    <span>96.88%</span>
                    <span class="source-lines">7, 9, 11-12, 18, 21, 37-38, 44, 46, 49, 51-52, 54-56, 66-67, 69-70, 73, 76, 78-79, 81-82, 84, 88, 94-95, 97</span>
                    <span class="source-lines">74</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/noop.py</span>
                    <span>13</span>
                    <span>0</span>
                    <span>13</span>
                    <span>100.0%</span>
                    <span class="source-lines">8, 10, 12-13, 20, 26, 32, 34, 50, 52, 58, 60, 66</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/ollama.py</span>
                    <span>43</span>
                    <span>1</span>
                    <span>42</span>
                    <span>97.67%</span>
                    <span class="source-lines">7, 9, 11-12, 18, 24, 40-41, 47, 50, 52, 54-55, 57-60, 62-63, 66-67, 71-72, 74-75, 77, 81, 87-88, 90-92, 96, 102, 104, 114, 116-117, 127, 132, 134-135</span>
                    <span class="source-lines">69</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/schemas.py</span>
                    <span>36</span>
                    <span>1</span>
                    <span>35</span>
                    <span>97.22%</span>
                    <span class="source-lines">8, 10-12, 16, 22, 38, 42-44, 46-47, 50-53, 55, 58-59, 62-65, 67-68, 77, 84, 90, 94-98, 102, 130</span>
                    <span class="source-lines">39</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/models.py</span>
                    <span>240</span>
                    <span>10</span>
                    <span>230</span>
                    <span>95.83%</span>
                    <span class="source-lines">17-18, 21, 24-25, 34-36, 38, 40, 47-48, 61-67, 69, 71, 82-83, 95-100, 102, 104, 109-115, 118-119, 141-157, 159, 161, 167-171, 173-182, 184, 186, 188-190, 193-194, 202-203, 205, 207, 213-214, 223-225, 227, 229, 233-235, 238-239, 248-250, 252, 254, 261-262, 271-273, 275, 277, 281-283, 286-287, 324-353, 355-360, 362, 364, 382-405, 407-419, 422-423, 437-445, 447, 449, 459, 461, 464-465, 482-492, 494, 500, 502, 508-512, 514, 516, 518, 520, 522</span>
                    <span class="source-lines">172, 183, 185, 187, 460, 513, 515, 517, 519, 521</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/options.py</span>
                    <span>117</span>
                    <span>45</span>
                    <span>72</span>
                    <span>61.54%</span>
                    <span class="source-lines">106, 146, 175, 178-180, 185-187, 193-195, 201-203, 209-218, 220, 224, 233, 248, 251-267, 270-283, 286-295, 298, 300</span>
                    <span class="source-lines">13-15, 21-22, 90-94, 97-99, 102-105, 122-123, 126-132, 135-137, 140-142, 145, 156-160, 163-164, 167, 169, 222, 227, 236</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/plugin.py</span>
                    <span>156</span>
                    <span>25</span>
                    <span>131</span>
                    <span>83.97%</span>
                    <span class="source-lines">40, 43, 49, 55, 61, 67, 73, 80, 89, 95, 101, 107, 113, 121, 126, 131, 136, 142, 147, 153, 169, 173, 177, 183-184, 187-188, 190, 192, 195-197, 203-204, 212-213, 238-239, 242-243, 246, 249-250, 252-253, 256-257, 259, 261-265, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 307, 309-314, 317-318, 322-323, 331-332, 337-340, 343, 345, 348-353, 355, 357, 365-366, 387-388, 391-392, 395-397, 408-409, 412, 415-416, 419-421, 431-432, 435-437, 448-449, 452, 455, 457-458</span>
                    <span class="source-lines">13, 15-17, 19-20, 22, 28-31, 34, 160, 216, 319, 327-328, 333-334, 379-380, 400, 424, 440-441</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/prompts.py</span>
                    <span>75</span>
                    <span>5</span>
                    <span>70</span>
                    <span>93.33%</span>
                    <span class="source-lines">13, 15-17, 24, 27, 33, 35, 49, 52, 55, 58-61, 63, 65, 67, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116, 118, 132-133, 135-138, 140-141, 144-145, 148, 151-152, 154-156, 158-159, 163, 165, 180, 182, 191-194</span>
                    <span class="source-lines">80, 114, 142, 146, 149</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/render.py</span>
                    <span>50</span>
                    <span>0</span>
                    <span>50</span>
                    <span>100.0%</span>
                    <span class="source-lines">13, 15-16, 18, 24, 30-31, 34, 40, 42, 50-51, 53, 56, 65-67, 70, 79, 87, 90, 99, 101-102, 107, 110, 121-124, 126-129, 131-134, 141-143, 145, 158-163, 177, 196</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/report_writer.py</span>
                    <span>167</span>
                    <span>10</span>
                    <span>157</span>
                    <span>94.01%</span>
                    <span class="source-lines">13, 15-25, 27-29, 46, 55, 58, 67-68, 76, 83-84, 89, 98-100, 102, 105-108, 110, 116, 127-128, 130, 142, 150, 156-158, 160, 186-189, 192, 197-199, 202-203, 211, 222-223, 226-227, 230-231, 233, 235, 254, 256-259, 262-264, 266, 268, 303, 312, 314-315, 317-328, 330, 332, 340, 343-345, 348-349, 352-354, 357, 360, 368, 376, 378-379, 382, 385, 388, 391, 399, 401-402, 408, 410, 412, 414-423, 434-435, 437-439, 447-448, 453, 455, 458, 461-462, 464, 470-474, 480-481, 488, 495, 497, 499-501, 503, 506-507, 509, 515-516</span>
                    <span class="source-lines">113, 135-137, 424-425, 432, 449-451</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/fs.py</span>
                    <span>34</span>
                    <span>3</span>
                    <span>31</span>
                    <span>91.18%</span>
                    <span class="source-lines">11, 13-14, 17, 30, 33, 36, 39, 42, 45, 55-56, 58-60, 63-64, 70, 79, 82, 100, 103, 111-113, 116-117, 119-121, 123</span>
                    <span class="source-lines">40, 65, 67</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/hashing.py</span>
                    <span>36</span>
                    <span>0</span>
                    <span>36</span>
                    <span>100.0%</span>
                    <span class="source-lines">12, 14-17, 23, 32, 35, 44-48, 51, 61, 64, 73-74, 76-78, 80-81, 86, 96, 103-104, 107, 113-114, 116-121</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/ranges.py</span>
                    <span>33</span>
                    <span>0</span>
                    <span>33</span>
                    <span>100.0%</span>
                    <span class="source-lines">12, 15, 29-30, 33, 35-37, 39-40, 42, 45-47, 50, 52, 55, 65-67, 70, 81-82, 84-91, 93, 95</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/time.py</span>
                    <span>16</span>
                    <span>0</span>
                    <span>16</span>
                    <span>100.0%</span>
                    <span class="source-lines">4, 6, 9, 15, 18, 27, 30, 39-44, 46-48</span>
                    <span class="source-lines">-</span>
                </div>
            </div>
        </section>
    </div>
</body>
</html>