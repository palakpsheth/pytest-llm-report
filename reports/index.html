<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Test Report &bull; 387 tests</title>
    <!-- Optional: Inter font from rsms.me CDN. Falls back to system fonts if unavailable. -->
    <link rel="stylesheet" href="https://rsms.me/inter/inter.css">
    <style>
/* Modern Color Palette */
:root {
    --bg-color: #f8fafc;
    --text-primary: #1e293b;
    --text-secondary: #64748b;
    --border-color: #e2e8f0;
    --card-bg: #ffffff;
    --surface-muted: #f1f5f9;
    --primary-color: #3b82f6;
    color-scheme: light dark;

    /* Status Colors */
    --passed-bg: #dcfce7;
    --passed-text: #166534;
    --failed-bg: #fee2e2;
    --failed-text: #991b1b;
    --skipped-bg: #fef9c3;
    --skipped-text: #854d0e;
    --xfailed-bg: #ffedd5;
    --xfailed-text: #9a3412;
    --xpassed-bg: #f3e8ff;
    --xpassed-text: #6b21a8;
    --error-bg: #fee2e2;
    --error-text: #991b1b;
}

body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    background-color: var(--bg-color);
    color: var(--text-primary);
    line-height: 1.5;
    margin: 0;
    padding: 0;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 2rem;
}

/* Header */
header {
    margin-bottom: 2rem;
    border-bottom: 1px solid var(--border-color);
    padding-bottom: 1rem;
    display: flex;
    justify-content: space-between;
    align-items: center;
}

h1 {
    font-size: 1.875rem;
    font-weight: 700;
    color: var(--text-primary);
    margin: 0;
}

.meta {
    font-size: 0.875rem;
    color: var(--text-secondary);
}

/* Summary Grid */
.summary {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
    gap: 1rem;
    margin-bottom: 2rem;
}

.summary-card {
    background: var(--card-bg);
    border-radius: 0.5rem;
    padding: 1.5rem;
    box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.1), 0 1px 2px -1px rgb(0 0 0 / 0.1);
    text-align: center;
    border: 1px solid var(--border-color);
    transition: transform 0.2s;
}

.summary-card:hover {
    transform: translateY(-2px);
}

.summary-card .count {
    font-size: 2.25rem;
    font-weight: 700;
    line-height: 1;
    margin-bottom: 0.5rem;
}

.summary-card .label {
    text-transform: uppercase;
    font-size: 0.75rem;
    font-weight: 600;
    letter-spacing: 0.05em;
    color: var(--text-secondary);
}

/* Status Colors for Summary */
.summary-card.passed .count {
    color: var(--passed-text);
}

.summary-card.failed .count {
    color: var(--failed-text);
}

.summary-card.skipped .count {
    color: var(--skipped-text);
}

.summary-card.xfailed .count {
    color: var(--xfailed-text);
}

.summary-card.xpassed .count {
    color: var(--xpassed-text);
}

.summary-card.coverage .count {
    color: var(--primary-color);
}

/* Filters */
.filters {
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.5rem;
    border: 1px solid var(--border-color);
    margin-bottom: 1.5rem;
    display: flex;
    flex-direction: column;
    gap: 0.75rem;
}

.filter-input {
    flex: 1;
    padding: 0.5rem 1rem;
    border: 1px solid var(--border-color);
    border-radius: 0.375rem;
    font-size: 0.875rem;
    background: var(--card-bg);
    color: var(--text-primary);
}

.filter-input::placeholder {
    color: var(--text-secondary);
}

.filter-statuses {
    display: flex;
    flex-wrap: wrap;
    gap: 0.5rem;
}

.filter-chip {
    display: inline-flex;
    align-items: center;
    gap: 0.35rem;
    padding: 0.25rem 0.75rem;
    border-radius: 9999px;
    border: 1px solid var(--border-color);
    background: var(--surface-muted);
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.04em;
}

.filter-chip input {
    margin: 0;
}

.filter-chip.passed {
    background: var(--passed-bg);
    color: var(--passed-text);
}

.filter-chip.failed {
    background: var(--failed-bg);
    color: var(--failed-text);
}

.filter-chip.skipped {
    background: var(--skipped-bg);
    color: var(--skipped-text);
}

.filter-chip.xfailed {
    background: var(--xfailed-bg);
    color: var(--xfailed-text);
}

.filter-chip.xpassed {
    background: var(--xpassed-bg);
    color: var(--xpassed-text);
}

.filter-chip.error {
    background: var(--error-bg);
    color: var(--error-text);
}

/* Test List */
.test-list {
    display: flex;
    flex-direction: column;
    gap: 0.75rem;
}

.test-row {
    background: var(--card-bg);
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
    overflow: hidden;
}

.test-header {
    padding: 1rem;
    display: flex;
    align-items: center;
    gap: 1rem;
    cursor: pointer;
    background: var(--card-bg);
}

.test-header:hover {
    background: var(--surface-muted);
}

.status-badge {
    padding: 0.25rem 0.75rem;
    border-radius: 9999px;
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
}

.status-passed {
    background: var(--passed-bg);
    color: var(--passed-text);
}

.status-failed {
    background: var(--failed-bg);
    color: var(--failed-text);
}

.status-skipped {
    background: var(--skipped-bg);
    color: var(--skipped-text);
}

.status-xfailed {
    background: var(--xfailed-bg);
    color: var(--xfailed-text);
}

.status-xpassed {
    background: var(--xpassed-bg);
    color: var(--xpassed-text);
}

.status-error {
    background: var(--error-bg);
    color: var(--error-text);
}

.test-name {
    flex: 1;
    font-family: monospace;
    font-size: 0.9rem;
    color: var(--text-primary);
    word-break: break-all;
}

.test-meta {
    display: flex;
    gap: 1rem;
    align-items: center;
    color: var(--text-secondary);
    font-size: 0.875rem;
}

/* Details Section */
.test-details {
    padding: 0 1rem 1rem 1rem;
    border-top: 1px solid var(--border-color);
    background: var(--surface-muted);
}

.detail-section {
    margin-top: 1rem;
}

.detail-title {
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
    color: var(--text-secondary);
    margin-bottom: 0.5rem;
}

.coverage-item {
    font-family: monospace;
    font-size: 0.85rem;
    padding: 0.25rem 0;
    border-bottom: 1px solid var(--border-color);
    display: grid;
    grid-template-columns: minmax(200px, 2fr) minmax(120px, 1fr);
    gap: 1rem;
}

.coverage-list {
    background: var(--card-bg);
    border-radius: 0.375rem;
    border: 1px solid var(--border-color);
    overflow: hidden;
}

.source-coverage {
    margin-top: 2rem;
}

.source-coverage h2 {
    margin: 0 0 1rem;
    font-size: 1.5rem;
}

.source-coverage-table {
    display: grid;
    gap: 0.35rem;
}

.source-coverage-header,
.source-coverage-row {
    display: grid;
    grid-template-columns: minmax(200px, 2fr) repeat(4, minmax(60px, 0.5fr)) minmax(
            140px,
            1fr
        ) minmax(140px, 1fr);
    align-items: center;
    gap: 0.75rem;
    padding: 0.75rem 1rem;
    border-radius: 0.5rem;
}

.source-coverage-header {
    background: var(--surface-muted);
    font-size: 0.75rem;
    font-weight: 700;
    text-transform: uppercase;
    letter-spacing: 0.04em;
    color: var(--text-secondary);
}

.source-coverage-row {
    background: var(--card-bg);
    border: 1px solid var(--border-color);
    font-size: 0.85rem;
}

.source-path {
    font-family: monospace;
    word-break: break-word;
}

.source-lines {
    font-family: monospace;
    color: var(--text-secondary);
    word-break: break-word;
}

.llm-annotation {
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.375rem;
    border: 1px solid var(--border-color);
}

.llm-annotation p {
    margin: 0 0 0.5rem 0;
}

.llm-annotation p:last-child {
    margin-bottom: 0;
}

.llm-annotation ul {
    margin: 0.5rem 0 0;
    padding-left: 1.25rem;
}

.llm-annotation li {
    margin-bottom: 0.25rem;
}

.error-message {
    font-family: monospace;
    color: var(--failed-text);
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.375rem;
    border: 1px solid var(--failed-bg);
    white-space: pre-wrap;
    overflow-x: auto;
}

/* HTML5 Progress Bar for Coverage */
progress {
    width: 60px;
}

/* Utility: Hidden state for filtering */
.hidden {
    display: none !important;
}

/* Dark Mode Support */
@media (prefers-color-scheme: dark) {
    :root {
        --bg-color: #0f172a;
        --text-primary: #f1f5f9;
        --text-secondary: #94a3b8;
        --border-color: #334155;
        --card-bg: #1e293b;
        --surface-muted: #0b1220;
        --primary-color: #60a5fa;

        /* Status Colors - Adjusted for dark mode */
        --passed-bg: #14532d;
        --passed-text: #86efac;
        --failed-bg: #7f1d1d;
        --failed-text: #fca5a5;
        --skipped-bg: #713f12;
        --skipped-text: #fde047;
        --xfailed-bg: #7c2d12;
        --xfailed-text: #fdba74;
        --error-bg: #7f1d1d;
        --error-text: #fca5a5;
    }

    /* Adjust box shadows for dark mode */
    .summary-card {
        box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.3), 0 1px 2px -1px rgb(0 0 0 / 0.3);
    }
}

@media print {
    body {
        background: #ffffff;
        color: #0f172a;
    }

    .container {
        max-width: none;
        padding: 1rem 1.5rem;
    }

    header {
        border-bottom: 2px solid var(--border-color);
    }

    .filters {
        display: none;
    }

    .summary-card,
    .test-row {
        box-shadow: none;
    }

    .test-header {
        background: #ffffff;
    }

    .test-row {
        page-break-inside: avoid;
        break-inside: avoid;
    }

    .test-details {
        background: #ffffff;
    }

    .llm-annotation {
        background: var(--surface-muted);
    }

    progress {
        width: 80px;
    }
}

body.pdf-mode .filters {
    display: none;
}

body.pdf-mode .test-row {
    page-break-inside: avoid;
    break-inside: avoid;
}    </style>
    <script>
// pytest-llm-report interactive features

// Global state for filters
const activeStatuses = new Set(['passed', 'failed', 'skipped', 'xfailed', 'xpassed', 'error']);

// Filter tests based on search input and outcome filters
function filterTests() {
    const query = document.getElementById('searchInput').value.toLowerCase();
    document.querySelectorAll('.test-row').forEach(row => {
        const nodeid = row.querySelector('.test-name').textContent.toLowerCase();
        const statusMatch = row.dataset.status ? activeStatuses.has(row.dataset.status) : false;
        const matchesSearch = nodeid.includes(query);
        row.classList.toggle('hidden', !matchesSearch || !statusMatch);
    });
}

// Toggle visibility of status filters
function toggleStatus(checkbox) {
    const status = checkbox.dataset.status;
    if (checkbox.checked) {
        activeStatuses.add(status);
    } else {
        activeStatuses.delete(status);
    }
    filterTests();
}

// Initialize interactive features after DOM is ready
document.addEventListener('DOMContentLoaded', function () {
    'use strict';

    // Toggle dark mode on preference
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.documentElement.dataset.theme = 'dark';
    }

    // Default: expand all details
    document.querySelectorAll('details').forEach(details => {
        details.setAttribute('open', '');
    });

    const params = new URLSearchParams(window.location.search);
    if (params.get('pdf') === '1') {
        document.body.classList.add('pdf-mode');
    }
});    </script>
</head>
<body>
    <div class="container">
        <header>
            <div>
                <h1>Test Report</h1>
                <div class="meta">
                    Run ID: 20966792397-py3.12 &bull;
                    Generated: 2026-01-13 17:49:04 &bull;
                    Duration: 34.52s<br>
                    <strong>Plugin:</strong> v0.1.0
                        (2f498263985a34902252c53c11fb820445bd8f21)
[dirty]<br>
                    <strong>Repo:</strong> v0.1.0
                        (3cc50ec73009d5fecab87cec0ec9b31f805a0863)
<br>
                    <strong>LLM:</strong> ollama / llama3.2:1b
                        (minimal context,
                         380 annotated, 6 errors)
                </div>
            </div>
            <div style="text-align: right">
                <div style="font-size: 2rem; font-weight: 700; color: var(--primary-color)">
                    92.91%
                </div>
                <div class="meta">Total Coverage</div>
            </div>
        </header>

        <!-- Summary Cards -->
        <div class="summary">
            <div class="summary-card">
                <div class="count">387</div>
                <div class="label">Total Tests</div>
            </div>
            <div class="summary-card passed">
                <div class="count">387</div>
                <div class="label">Passed</div>
            </div>
            <div class="summary-card failed">
                <div class="count">0</div>
                <div class="label">Failed</div>
            </div>
            <div class="summary-card skipped">
                <div class="count">0</div>
                <div class="label">Skipped</div>
            </div>
            <div class="summary-card xfailed">
                <div class="count">0</div>
                <div class="label">XFailed</div>
            </div>
            <div class="summary-card xpassed">
                <div class="count">0</div>
                <div class="label">XPassed</div>
            </div>
            <div class="summary-card failed">
                <div class="count">0</div>
                <div class="label">Errors</div>
            </div>
        </div>

        <!-- Filters -->
        <div class="filters">
            <input type="text" id="searchInput" class="filter-input" placeholder="Search tests..." onkeyup="filterTests()">
            <div class="filter-statuses" aria-label="Filter by status">
                <label class="filter-chip passed">
                    <input type="checkbox" data-status="passed" checked onchange="toggleStatus(this)">
                    Passed
                </label>
                <label class="filter-chip failed">
                    <input type="checkbox" data-status="failed" checked onchange="toggleStatus(this)">
                    Failed
                </label>
                <label class="filter-chip skipped">
                    <input type="checkbox" data-status="skipped" checked onchange="toggleStatus(this)">
                    Skipped
                </label>
                <label class="filter-chip xfailed">
                    <input type="checkbox" data-status="xfailed" checked onchange="toggleStatus(this)">
                    XFailed
                </label>
                <label class="filter-chip xpassed">
                    <input type="checkbox" data-status="xpassed" checked onchange="toggleStatus(this)">
                    XPassed
                </label>
                <label class="filter-chip error">
                    <input type="checkbox" data-status="error" checked onchange="toggleStatus(this)">
                    Error
                </label>
            </div>
        </div>

        <!-- Test List -->
        <div class="test-list">
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_all_policy</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the aggregation of multiple policy reports to ensure both tests are included in the aggregate report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where only one or no policy reports are aggregated, potentially leading to incomplete or missing test results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `retain` parameter is set to 'all' and all retained tests from both policy reports are present in the aggregate result.</li>
                                        <li>The number of tests in the aggregate result matches the expected value (2) even when one or both policy reports fail.</li>
                                        <li>All retained test cases from both policy reports are included in the `tests` list within the aggregate result.</li>
                                        <li>No empty lists are present in the `tests` list within the aggregate result.</li>
                                        <li>The `retain` parameter is correctly applied to both policy reports and their aggregated results.</li>
                                        <li>The aggregation process does not introduce any new test cases or remove existing ones without a valid reason.</li>
                                        <li>The expected number of tests in the aggregate result matches the actual count when one or both policy reports fail.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 52, 55-56, 59, 61-63, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134, 141, 146, 148-153, 155, 157-159, 170, 217, 219-223, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_dir_not_exists</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the aggregate function returns None when the directory does not exist.</p>
                                <p><strong>Why Needed:</strong> Prevents a bug where the aggregate function throws an exception or raises an error when a non-existent directory is passed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregator.aggregate()` method should return `None` when the specified directory does not exist.</li>
                                        <li>The `pathlib.Path.exists()` mock object should return `False` for the specified directory.</li>
                                        <li>No exception or error should be raised by the `aggregator.aggregate()` method.</li>
                                        <li>The aggregate function should not throw an exception or raise an error even if the directory exists but is empty.</li>
                                        <li>The aggregate function should not throw an exception or raise an error when the directory does not exist but has a valid path.</li>
                                        <li>The aggregate function should handle non-existent directories correctly and return `None` without raising any exceptions.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 52, 55-57, 109-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_latest_policy</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `aggregate` function correctly picks the latest policy for a given test case.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the previous implementation would incorrectly pick an older policy for a new test case.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test case should be marked as passed in the result.</li>
                                        <li>The number of tests in the result should be 1.</li>
                                        <li>The outcome of the first test in the result should be 'passed'.</li>
                                        <li>The `run_meta` object should have an `is_aggregated` attribute set to True.</li>
                                        <li>The `run_meta.run_count` attribute should be equal to 2.</li>
                                        <li>The `summary.passed` attribute should be equal to 1 (indicating one test passed).</li>
                                        <li>The `summary.failed` attribute should be equal to 0 (indicating no tests failed).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">77 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134, 141, 146, 148-153, 155, 157-159, 170, 182, 184-188, 190-191, 194, 217, 219-223, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_no_dir_configured</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The aggregator function should return an empty list when no directory configuration is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the aggregator function returns an error or raises an exception without providing any aggregation results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>agg.aggregate() should be called with no arguments.</li>
                                        <li>agg.aggregate() should return None.</li>
                                        <li>agg.aggregate() should not raise any exceptions when aggregate_dir is None.</li>
                                        <li>agg.aggregate() should handle the case where aggregate_dir is an empty string or None without raising an exception.</li>
                                        <li>agg.aggregate() should use the provided mock_config object to configure the aggregator instance.</li>
                                        <li>agg.aggregate() should not use any default aggregation strategy if aggregate_dir is None.</li>
                                        <li>agg.aggregate() should return a list of aggregated values with no elements when aggregate_dir is None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 44, 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_no_reports</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `aggregate` function should not be called when there are no reports.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `aggregate` function is called with an empty list of reports, potentially causing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>aggregator.aggregate() should return None when there are no reports.</li>
                                        <li>the returned value should be None.</li>
                                        <li>no exception should be raised when calling `aggregate()`</li>
                                        <li>the function call should not modify the original list of reports.</li>
                                        <li>the function call should not raise an error if the input is invalid.</li>
                                        <li>the function call should return None without raising any exceptions.</li>
                                        <li>the returned value should be a single value (None) rather than a list.</li>
                                        <li>the `aggregate` function should not modify the original list of reports.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52, 55-57, 109-110, 113-114, 170)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_with_coverage_and_llm_annotations</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that coverage and LLM annotations are properly deserialized and can be re-serialized.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in core functionality by ensuring accurate coverage and LLM annotation deserialization.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Coverage was properly deserialized with correct file paths and line counts.</li>
                                        <li>LLM annotation was properly deserialized with correct scenario, why needed, key assertions, and confidence.</li>
                                        <li>Test can be re-serialized without issues (previously failed due to regression)</li>
                                        <li>Serialized report contains required coverage and LLM annotations.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">81 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134-137, 141-144, 146, 148-153, 155, 157-159, 170, 182, 184-188, 194, 217, 219-223, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">32 lines (ranges: 40-43, 104-107, 109-111, 113, 115, 161-165, 167, 169, 171, 173, 176-180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_with_source_coverage</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the aggregation function with source coverage verification.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression that may occur when the source coverage is not properly aggregated.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The source coverage summary should be deserialized correctly from a JSON report.</li>
                                        <li>The correct file path of the source code should be matched in the aggregated result.</li>
                                        <li>A SourceCoverageEntry object should be created with the expected attributes (file_path, statements, missed, covered, coverage_percent, covered_ranges, missed_ranges).</li>
                                        <li>The length of the source_coverage list should be 1 as per the test scenario.</li>
                                        <li>The isinstance function should correctly identify a SourceCoverageEntry object in the result.</li>
                                        <li>The file path attribute of the first SourceCoverageEntry object should match the expected value (src/foo.py).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">66 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 148-155, 157-159, 170, 182, 184-186, 194, 217, 219-220, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_load_coverage_from_source</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loading coverage from configured source file when option is not set.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the aggregator fails to load coverage data when the LLM's default source file is not provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that _load_coverage_from_source() returns None when llm_coverage_source is set to None.</li>
                                        <li>Verify that _load_coverage_from_source() raises a UserWarning when llm_coverage_source does not exist.</li>
                                        <li>Verify that _load_coverage_from_source() successfully loads coverage data from a mock .coverage file when llm_coverage_source is set to .coverage.</li>
                                        <li>Verify that the mocked CoverageMapper returns the correct entry for SourceCoverageEntry.</li>
                                        <li>Verify that the mocked CoverageMapper calls the load method on the mocked Coverage object.</li>
                                        <li>Verify that the mocked CoverageMapper calls the map_source_coverage method on the mocked Coverage object with the mock entry.</li>
                                        <li>Verify that _load_coverage_from_source() returns a result with exactly one entry and 80.0 coverage percentage when llm_coverage_source is set to .coverage.</li>
                                        <li>Verify that the returned entries are of type SourceCoverageEntry.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 245-246, 248-249, 251, 253-257, 259, 262-263, 265-266, 269-271, 273)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_recalculate_summary</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Aggregator: Recalculate Summary</p>
                                <p><strong>Why Needed:</strong> Prevents regression in aggregation logic when multiple tests are passed or failed simultaneously.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total number of test results should be equal to the total number of nodes tested.</li>
                                        <li>The passed count should match the expected number of passed test results.</li>
                                        <li>The failed count should match the expected number of failed test results.</li>
                                        <li>The skipped count should match the expected number of skipped test results.</li>
                                        <li>The xfailed count should match the expected number of xfailed test results.</li>
                                        <li>The xpassed count should match the expected number of xpassed test results.</li>
                                        <li>The error count should be 1 (as per the latest summary provided).</li>
                                        <li>The coverage percentage should remain preserved even when multiple tests are passed or failed simultaneously.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 217, 219-233, 235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation.py::TestAggregator::test_skips_invalid_json</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that skipping an invalid JSON report prevents the test from counting it as a valid aggregation run.</p>
                                <p><strong>Why Needed:</strong> This test ensures that the `aggregate` function correctly handles reports with missing fields and invalid JSON.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregate` function should not count the skipped report as a valid aggregation run.</li>
                                        <li>The `aggregate` function should raise a warning when it encounters an invalid JSON report file.</li>
                                        <li>The test should fail to count the skipped report if its fields are missing.</li>
                                        <li>The test should only count the valid report in the aggregate result.</li>
                                        <li>The `aggregate` function should not include the invalid JSON report in the aggregation output.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">71 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119-120, 125, 127-128, 148-153, 155, 157-159, 162, 164-166, 168, 170, 182, 184-186, 194, 217, 219-220, 235, 245, 248-249, 251, 253, 275-278, 280)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_aggregation_maximal.py::TestAggregationMaximal::test_recalculate_summary_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the aggregator recalculates the summary correctly when there are multiple tests with different outcomes.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case of multiple tests failing, as it ensures that the aggregator calculates the correct total duration and coverage percentage.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>summary.total == 2 (correct number of tests)</li>
                                        <li>summary.passed == 1 (correct number of passed tests)</li>
                                        <li>summary.failed == 1 (correct number of failed tests)</li>
                                        <li>summary.coverage_total_percent == 88.5 (correct coverage percentage)</li>
                                        <li>summary.total_duration == 3.0 (correct total duration)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/aggregation.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 44, 217, 219-225, 235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_cached_tests_are_skipped</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that cached tests are skipped when using the annotator.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where cached annotations would be re-run unnecessarily.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `test_cached_tests_are_skipped` is called with mock providers, cache and assembler.</li>
                                        <li>The `mock_provider`, `mock_cache` and `mock_assembler` objects are created without being used in the test.</li>
                                        <li>The `test_cached_tests_are_skipped` method does not call any of its dependencies (mock providers, cache and assembler).</li>
                                        <li>The `test_cached_tests_are_skipped` method returns a mock object that is not an instance of the original function.</li>
                                        <li>The `test_cached_tests_are_skipped` method does not raise any exceptions during execution.</li>
                                        <li>The `test_cached_tests_are_skipped` method does not modify any external state.</li>
                                        <li>The `test_cached_tests_are_skipped` method does not call any other test methods.</li>
                                        <li>The `mock_provider`, `mock_cache` and `mock_assembler` objects are created with default values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-81, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing concurrent annotation functionality.</p>
                                <p><strong>Why Needed:</strong> Prevents potential performance regressions and ensures thread safety in concurrent scenarios.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Ensures that the annotator does not annotate multiple documents simultaneously, preventing potential performance issues.</li>
                                        <li>Verifies that the cache is accessed sequentially by the annotator, ensuring thread safety.</li>
                                        <li>Checks for any exceptions raised during annotation operations to prevent unexpected behavior.</li>
                                        <li>Validates that the assembler is properly initialized before concurrent annotation attempts are made.</li>
                                        <li>Ensures that the annotator does not annotate a document while another annotator is still processing it in the cache.</li>
                                        <li>Verifies that the annotator's output is consistent across multiple threads, ensuring thread safety and correctness.</li>
                                        <li>Checks for any synchronization issues or bottlenecks that may arise from concurrent annotation operations.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">64 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104-112, 129-135, 137, 139, 229-232, 234, 236-237, 239, 245-246, 248-253, 255, 261, 266)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation_handles_failures</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test concurrent annotation handles failures to verify</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the annotator may fail to handle failures in the annotation process.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The mock provider is not called with an error message when annotated fails.</li>
                                        <li>The mock cache is not modified when annotated fails.</li>
                                        <li>The mock assembler does not raise an exception when annotated fails.</li>
                                        <li>The capture fixture is not captured when annotated fails.</li>
                                        <li>The mock provider's return value is not None when annotated fails.</li>
                                        <li>The mock cache's get method returns None when annotated fails.</li>
                                        <li>The mock assembler's error message is not 'Error annotation failed'.</li>
                                        <li>The annotator does not raise an exception when annotated fails.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104-112, 129-135, 137-139, 229-232, 234, 236-237, 239, 245-246, 248-253, 255, 261-264, 266)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_progress_reporting</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `test_progress_reporting` function is called with mock providers and cache to test progress reporting functionality.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the annotator's progress reporting feature when using mock providers and a cache.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_provider is not None.</li>
                                        <li>mock_cache is not None.</li>
                                        <li>mock_assembler is not None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation</span>
                        <div class="test-meta">
                            <span>12.00s</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing sequential annotation functionality in the annotator.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when using sequential annotation with multiple providers and caches.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that the annotator correctly handles sequential annotation with multiple providers and caches.</li>
                                        <li>Check if the annotator properly updates the cache after sequential annotation.</li>
                                        <li>Ensure that the annotator does not throw an exception when encountering a provider or cache issue during sequential annotation.</li>
                                        <li>Verify that the annotator preserves the original order of annotations in the sequence.</li>
                                        <li>Test the case where multiple providers are used and the annotator correctly handles their sequential annotation.</li>
                                        <li>Check if the annotator throws an error when the number of providers exceeds the maximum allowed.</li>
                                        <li>Verify that the annotator does not throw an exception when encountering a provider or cache issue during sequential annotation with a large number of providers.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_skips_if_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `test_skips_if_disabled` test verifies that the annotator does not perform any action when the Large Language Model (LLM) is disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the annotator's functionality may be unexpectedly disabled without proper indication.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config` object passed to `annotate_tests` has an 'enabled' attribute set to False.</li>
                                        <li>The `annotate_tests` function does not attempt to annotate any tests when the LLM is disabled.</li>
                                        <li>No exceptions are raised if the annotator is unable to process a test due to the LLM being disabled.</li>
                                        <li>The annotator's behavior remains consistent across different test configurations and environments.</li>
                                        <li>The `test_skips_if_disabled` function does not introduce any new test cases or dependencies that could cause issues.</li>
                                        <li>The annotator's functionality is still available when the LLM is enabled, as expected.</li>
                                        <li>The test does not rely on external state or variables that may change unexpectedly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 45-46)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_skips_if_provider_unavailable</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The annotator should skip the annotation process when a provider is unavailable.</p>
                                <p><strong>Why Needed:</strong> To prevent the annotator from attempting to annotate data when it cannot connect to the provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_provider.is_available() returns False</li>
                                        <li>mock_provider.get_provider().get_data() raises an exception</li>
                                        <li>mock_provider.get_provider().get_data() does not contain any annotations</li>
                                        <li>mock_provider.get_provider().get_data() is not a list of dictionaries</li>
                                        <li>mock_provider.get_provider().get_data() contains only strings or None values</li>
                                        <li>mock_provider.is_available() returns False after the annotation process has been skipped</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_concurrent_with_progress_and_errors</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the annotator reports progress and the first error in concurrent mode.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the annotator fails to report progress or the first error in concurrent mode.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should append a message indicating the number of tasks completed (2) to the `progress_msgs` list.</li>
                                        <li>The function should append a message indicating that an error occurred ('first error') to the `progress_msgs` list.</li>
                                        <li>The function should append messages indicating progress for each task ('Processing 1 test(s)' and 'Processing 3 test(s)') to the `progress_msgs` list.</li>
                                        <li>The function should not report any errors when annotating in parallel (i.e., failures == 0).</li>
                                        <li>The function should report an error as a string ('first error') in the `progress_msgs` list.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 229-232, 234, 236-237, 239-242, 245-246, 248-253, 255-258, 261-264, 266)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_sequential_rate_limit_wait</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the annotator waits if the rate limit interval has not elapsed after a sequential annotation task.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the annotator does not wait for the rate limit interval to elapse after a sequential annotation task, potentially leading to incomplete annotations or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `time.sleep` function is called with an argument of `1.0s` (the rate limit interval).</li>
                                        <li>The `time.monotonic` function returns a value greater than `1.0s`, indicating that the elapsed time exceeds the rate limit interval.</li>
                                        <li>The `time.sleep` function is not called when the elapsed time is less than or equal to `0.1s`.</li>
                                        <li>The `time.monotonic` function returns a value less than or equal to `0.1s`, indicating that the elapsed time is within the allowed range.</li>
                                        <li>The `time.sleep` function is only called with an argument of `1.0s` when the rate limit interval has not elapsed, ensuring that the annotator waits for the interval to elapse before proceeding.</li>
                                        <li>If the rate limit interval had elapsed, the `time.sleep` function would have been called without any arguments, indicating a potential regression in the behavior of the annotator.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">23 lines (ranges: 165-168, 170-171, 173-174, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_tests_cached_progress</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Should report progress for cached tests.</p>
                                <p><strong>Why Needed:</strong> Prevents regression when caching tests and annotating them.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_provider` method of the LLMAnnotator is called with a mock provider.</li>
                                        <li>The `LlmCache` instance is created with a mock cache.</li>
                                        <li>The `ContextAssembler` class is patched to return a mock assembly.</li>
                                        <li>Any progress messages containing '(cache): test_cached' are appended to the list.</li>
                                        <li>The `get_provider` method of the LLMAnnotator returns True for the mock provider.</li>
                                        <li>The `LlmCache` instance has a cached annotation for the test.</li>
                                        <li>The `ContextAssembler` class correctly assembles the test with the correct source and None value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">37 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-84, 97-98, 100, 127, 129-135, 137, 139)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_tests_provider_unavailable</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that when the provider is not available, it prints a message and returns without attempting to annotate tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the annotator does not attempt to annotate tests when the provider is unavailable.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_provider.is_available.return_value == False</li>
                                        <li>assert captured.out.contains('not available. Skipping annotations')</li>
                                        <li>annotate_tests(tests, config) should raise an exception or return immediately</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_coverage_v2.py::test_base_parse_response_malformed_json_after_extract</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `_parse_response` method returns an error when a malformed JSON is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function `extract_json_from_response` may incorrectly parse valid JSON due to missing or invalid braces.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>annotation.error == 'Failed to parse LLM response as JSON'</li>
                                        <li>response != '{ invalid json }'</li>
                                        <li>provider._parse_response(response) is not None</li>
                                        <li>provider._parse_response(response).error == 'Failed to parse LLM response as JSON'</li>
                                        <li>provider._parse_response(response).message == 'Invalid JSON: expected property '</li>
                                        <li>provider._parse_response(response).type == 'JSONDecodeError'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 186-187, 190-191, 194-195, 220-221)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_coverage_v2.py::test_base_parse_response_non_string_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `test_base_parse_response_non_string_fields` function correctly handles non-string fields in the response data.</p>
                                <p><strong>Why Needed:</strong> To prevent a potential bug where the function does not handle non-string fields in the response data, and instead raises an error or returns incorrect results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `scenario` attribute of the parsed annotation should match the provided value (123).</li>
                                        <li>The `why_needed` list should contain the expected field ('list').</li>
                                        <li>All elements in the `key_assertions` list should be present and match their corresponding values in the response data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203-207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_gemini_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `get_gemini_provider` function returns a `GeminiProvider` instance.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of changes to the `Config` class or its dependencies.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The returned value is an instance of `GeminiProvider`.</li>
                                        <li>The returned value has the correct type (`GeminiProvider`).</li>
                                        <li>No exceptions are raised when calling `get_gemini_provider`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 245, 247, 249, 252, 257, 262-263, 265)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_invalid_provider</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests for the `get_provider` function when an unknown LLM provider is specified.</p>
                                <p><strong>Why Needed:</strong> This test prevents a ValueError from being raised when an invalid LLM provider is provided to the `get_provider` function.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_provider` function should raise a `ValueError` with the message 'Unknown LLM provider: invalid'.</li>
                                        <li>The error message should contain the string 'invalid' (case-insensitive).</li>
                                        <li>The error message should not be empty.</li>
                                        <li>The `pytest.raises` context manager should be used to catch the `ValueError`.</li>
                                        <li>The `match` attribute of the error message should match the expected pattern ('Unknown LLM provider: invalid').</li>
                                        <li>The `Config` object passed to `get_provider` should have a 'provider' key with value 'invalid'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 245, 247, 249, 252, 257, 262, 267)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_litellm_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `get_litellm_provider` function returns a LitELLMProvider instance.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where an incorrect or incompatible provider is returned for litellm.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider(config)` should return an instance of `LiteLLMProvider`.</li>
                                        <li>The `isinstance(provider, LiteLLMProvider)` assertion should be true.</li>
                                        <li>The `provider` variable should hold an instance of `LiteLLMProvider` after calling `get_provider(config)`.</li>
                                        <li>An error message or indication that the provider is not compatible with litellm should not be returned.</li>
                                        <li>The function `get_provider(config)` should handle cases where no suitable provider is found.</li>
                                        <li>A meaningful exception message should be raised when an incompatible provider is encountered.</li>
                                        <li>The function `get_provider(config)` should return a valid provider instance for all possible configurations.</li>
                                        <li>An assertion error or indication of failure should not occur if the provider is correctly configured.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52-53, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_noop_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `get_noop_provider` function returns a `NoopProvider` instance when no provider is specified.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the `get_noop_provider` function fails to return an instance of `NoopProvider` when no provider is provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider(config)` returns an instance of `Config` with a 'provider' attribute set to 'none'.</li>
                                        <li>The variable `provider` is assigned the value of `get_provider(config)`, which is an instance of `NoopProvider`.</li>
                                        <li>The assertion `assert isinstance(provider, NoopProvider)` passes because `get_provider(config)` returns an instance of `Config` with a 'provider' attribute set to 'none'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_ollama_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `get_ollama_provider` function returns an instance of OllamaProvider.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails if the provider is not set to 'ollama'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The returned value is an instance of `OllamaProvider`.</li>
                                        <li>The `provider` attribute of the returned value has the correct value.</li>
                                        <li>No exception is raised when calling `get_provider(config)`.</li>
                                        <li>The `config` object passed to `get_provider` is not empty.</li>
                                        <li>The `config` object passed to `get_provider` does not contain a 'provider' key.</li>
                                        <li>The `provider` attribute of the returned value is set to a valid provider name ('ollama').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 245, 247, 249, 252-253, 255)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_available_caches_result</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the LlmProvider provides a single instance of the _check_availability method when available caches are available.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case multiple providers are used with available caches.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider is not None and its checks attribute is not zero.</li>
                                        <li>The provider's is_available() method returns True for both instances.</li>
                                        <li>The provider's checks attribute is equal to the number of times it was called.</li>
                                        <li>The _check_availability method is called once when available caches are available.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 107-108, 110-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_model_name_defaults_to_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'get_model_name' method of the 'ConcreteProvider' class should return the default model name from the configuration when no custom model is specified.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the 'get_model_name' method returns an incorrect default model name if no custom model is provided in the configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'model' attribute of the 'Config' object should be set to 'test-model'.</li>
                                        <li>The 'provider' object should have a 'get_model_name' method that returns the value of the 'model' attribute of the 'Config' object.</li>
                                        <li>The 'provider.get_model_name()' call should return 'test-model'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 136)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_rate_limits_defaults_to_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `get_rate_limits` method of the `ConcreteProvider` class returns a default value of `None` when no configuration is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the rate limits are not set to their default values when no configuration is provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_rate_limits()` method should return `None` if no configuration is provided.</li>
                                        <li>The `rate_limits` attribute of the `ConcreteProvider` class should be an empty list or dictionary.</li>
                                        <li>The `rate_limits` attribute of the `ConcreteProvider` class should not contain any values.</li>
                                        <li>The `rate_limits` attribute of the `ConcreteProvider` class should have a length of 0.</li>
                                        <li>The `rate_limits` attribute of the `ConcreteProvider` class should be an empty dictionary.</li>
                                        <li>The `rate_limits` attribute of the `ConcreteProvider` class should not contain any values.</li>
                                        <li>The `rate_limits` attribute of the `ConcreteProvider` class should have a length of 0.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 128)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_is_local_defaults_to_false</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `is_local()` method returns `False` when the default is set to `None`.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the default value of `True` for `is_local()` would be misinterpreted as `True`.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider.is_local() == False</li>
                                        <li>provider.is_local() != True</li>
                                        <li>config.default_is_local_defaults is None</li>
                                        <li>config.default_is_local_defaults not equal to True</li>
                                        <li>provider.is_local() in [False, True]</li>
                                        <li>provider.is_local() not in [True, False]</li>
                                        <li>config.default_is_local_defaults == 'None'</li>
                                        <li>config.default_is_local_defaults != 'None'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestHashSource::test_consistent_hash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'hash_source' function is used to generate a consistent hash for the given source code.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where different versions of the same source code produce different hashes, leading to inconsistent caching.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert hash_source(source) == hash_source(source)</li>
                                        <li>assert len(hash_source(source)) == len(set(hash_source(source)))</li>
                                        <li>assert hash_source('def test_foo(): pass') == hash_source('def test_foo(): pass')</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestHashSource::test_different_source_different_hash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `hash_source` function with different source code.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where identical source code produces the same hash value, potentially leading to unexpected behavior or errors in subsequent tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The two source strings should have different hashes.</li>
                                        <li>The `hash_source()` function should return a different hash for each input string.</li>
                                        <li>The `hash_source()` function should raise an error if the input strings are identical.</li>
                                        <li>The `hash_source()` function should not produce the same hash value for the same input string and different source code.</li>
                                        <li>Different source code should produce different hashes even when the inputs are the same.</li>
                                        <li>The `hash_source()` function should be able to distinguish between different source codes based on their content.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestHashSource::test_hash_length</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the length of the generated hash.</p>
                                <p><strong>Why Needed:</strong> Prevent a potential issue where the hash length is not consistent across different test runs.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The hash should be exactly 16 characters long.</li>
                                        <li>The hash should have a unique value for each input.</li>
                                        <li>The hash should not be shorter than 8 characters (the minimum required by Python's built-in hash function).</li>
                                        <li>The hash should not be longer than 32 characters (the maximum allowed by Python's built-in hash function).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_clear</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test clearing cache after adding entries.</p>
                                <p><strong>Why Needed:</strong> To prevent a potential bug where the cache grows indefinitely and consumes excessive memory.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `clear` method should remove all cache entries.</li>
                                        <li>The `get` method for a specific key should return `None` if it does not exist.</li>
                                        <li>The number of remaining cache entries after clearing should be 2.</li>
                                        <li>A new entry should be added to the cache with no associated annotation.</li>
                                        <li>An existing entry should be removed from the cache with an invalid annotation.</li>
                                        <li>The cache directory should still exist and contain the expected files after clearing.</li>
                                        <li>No error should be raised if the cache is empty.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 129, 132-136, 141)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_does_not_cache_errors</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that annotations with errors are not cached.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where error annotations are incorrectly cached, leading to incorrect results when trying to access them later.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The cache should not store the annotation for 'test::foo' with key 'abc123' and value 'error: API timeout'.</li>
                                        <li>The cache should return None when trying to retrieve the annotation for 'test::foo' with key 'abc123'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 39-41, 53, 55-56, 86, 88, 118-119, 121)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_get_missing</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `get` method returns None for missing entries in the cache.</p>
                                <p><strong>Why Needed:</strong> Prevents a bug where an entry with no corresponding key is not properly removed from the cache.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return `None` when trying to retrieve a non-existent key.</li>
                                        <li>The `get` method should raise a `KeyError` exception when called on a missing key.</li>
                                        <li>The cache should be updated with the correct entry for the given key.</li>
                                        <li>The test should verify that the expected value is not retrieved from the cache.</li>
                                        <li>The function should handle cases where multiple keys are missing simultaneously.</li>
                                        <li>The cache should be properly cleaned up after each test run to prevent memory leaks.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 39-41, 53, 55-56, 118-119, 121)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_cache.py::TestLlmCache::test_set_and_get</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that annotations are stored and retrieved correctly from the cache.</p>
                                <p><strong>Why Needed:</strong> Prevents bypass attacks by ensuring that the annotation is stored in the cache before it can be used.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Check if the annotation is set with the correct key</li>
                                        <li>Check if the annotation's scenario matches the expected value</li>
                                        <li>Check if the annotation's confidence matches the expected value</li>
                                        <li>Verify that a valid annotation is retrieved from the cache</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 39-41, 53, 55, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorCollectionErrors::test_collection_error_structure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that CollectionError objects have the correct nodeID and message attributes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where CollectionError objects are incorrectly structured, leading to incorrect assertions or errors in subsequent tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The nodeID attribute of the error object is set correctly.</li>
                                        <li>The message attribute of the error object is set correctly.</li>
                                        <li>The nodeID value matches the expected string 'test_bad.py'.</li>
                                        <li>The message value matches the expected string 'SyntaxError'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorCollectionErrors::test_get_collection_errors_initially_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_get_collection_errors_initially_empty' verifies that an empty collection is returned when the test collector is created with a valid configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where an empty collection is returned unexpectedly when the test collector is initialized with a valid configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_collection_errors()` should return an empty list.</li>
                                        <li>The function `get_collection_errors()` should not raise any exceptions.</li>
                                        <li>The function `get_collection_errors()` should not throw any errors or warnings.</li>
                                        <li>The function `get_collection_errors()` should only return the collection errors.</li>
                                        <li>The function `get_collection_errors()` should not return any errors when the collection is empty.</li>
                                        <li>The function `get_collection_errors()` should not raise an exception when the collection is empty.</li>
                                        <li>The function `get_collection_errors()` should not throw a warning when the collection is empty.</li>
                                        <li>The function `get_collection_errors()` should only log warnings when the collection is empty.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_context_override_default_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the default value of llm_context_override is None when it's not provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the default value of llm_context_override is set to None, potentially causing unexpected behavior in downstream code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>llm_context_override is None for TestCaseResult.nodeid='test.py::test_foo' and outcome='passed'</li>
                                        <li>llm_context_override is None for TestCaseResult.nodeid='test.py::test_foo' and outcome='failed'</li>
                                        <li>llm_context_override is not None for TestCaseResult.nodeid='test.py::test_foo' and outcome='skipped'</li>
                                        <li>llm_context_override is not None for TestCaseResult.nodeid='test.py::test_foo' and outcome='aborted'</li>
                                        <li>llm_context_override is not None for TestCaseResult.nodeid='test.py::test_foo' and outcome='not_found'</li>
                                        <li>llm_context_override is not None for TestCaseResult.nodeid='test.py::test_foo' and outcome='unknown'</li>
                                        <li>llm_context_override is not None for TestCaseResult.nodeid='test.py::test_foo' and outcome='timeout'</li>
                                        <li>llm_context_override is not None for TestCaseResult.nodeid='test.py::test_foo' and outcome='not_supported'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_opt_out_default_false</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the default value of llm_opt_out for LLM optimization out.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in LLM optimization behavior when default is set to False.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `llm_opt_out` attribute of a TestCaseResult instance should be `False`.</li>
                                        <li>A TestCaseResult instance with `nodeid` 'test.py::test_foo' and `outcome` 'passed' should have `llm_opt_out` set to `False`.</li>
                                        <li>When the default value of `llm_opt_out` is set to False, a TestCaseResult instance without any other attributes (like `nodeid`, `outcome`) should also have `llm_opt_out` set to `False`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorOutputCapture::test_capture_disabled_by_default</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The output capture feature is not enabled by default.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the output capture feature was always enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.capture_failed_output should be set to False when no capture is specified.</li>
                                        <li>config.capture_enabled should be set to False when no capture is specified.</li>
                                        <li>The output of captured messages should not be displayed in the console.</li>
                                        <li>No error message should be printed when capturing fails.</li>
                                        <li>The test should fail if config.capture_failed_output is True and config.capture_enabled is True.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorOutputCapture::test_capture_max_chars_default</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `TestCollectorOutputCapture` class is configured to capture output with a default maximum character count of 4000.</p>
                                <p><strong>Why Needed:</strong> This test prevents the potential issue where the default max chars value is not set correctly, potentially leading to unexpected behavior or errors in the application.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.capture_output_max_chars</li>
                                        <li>assert config.capture_output_max_chars == 4000</li>
                                        <li>assert isinstance(config.capture_output_max_chars, int)</li>
                                        <li>assert config.capture_output_max_chars >= 1</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorXfailHandling::test_xfail_failed_is_xfailed</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'tests/test_collector.py::TestCollectorXfailHandling::test_xfail_failed_is_xfailed' verifies that failed xfail tests are recorded as xfailed.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in handling xfail failures, ensuring that the collector correctly records and reports on failed xfail tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `results` dictionary contains an entry for the given nodeid with a value of 'xfailed'.</li>
                                        <li>The `outcome` attribute is set to 'xfailed'.</li>
                                        <li>The `wasxfail` attribute is set to 'expected failure'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212, 216, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestCollectorXfailHandling::test_xfail_passed_is_xpassed</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that xfail passes are correctly recorded as xpassed in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where an unexpected pass is not properly recorded as a failure.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>result.outcome should be 'xpassed' when report.wasxfail == 'expected failure'</li>
                                        <li>result.outcome should be 'xpassed' when report.failed == False and report.skipped == False</li>
                                        <li>report.wasxfail should be 'expected failure' when result.outcome is 'xpassed'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212-214)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestTestCollector::test_create_collector</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the creation of a Collector instance with an empty configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the Collector does not initialize correctly when given an empty configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `results` attribute is set to an empty dictionary.</li>
                                        <li>The `collection_errors` list is empty.</li>
                                        <li>The `collected_count` attribute is set to 0.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestTestCollector::test_get_results_sorted</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_results` method returns a sorted list of node IDs.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the order of results is not preserved due to manual sorting.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The list of node IDs returned by `get_results()` should be in ascending order.</li>
                                        <li>The list of node IDs returned by `get_results()` should contain all nodes from both tests.</li>
                                        <li>The list of node IDs returned by `get_results()` should not contain any duplicate node IDs.</li>
                                        <li>The first node ID in the list should be 'a_test.py::test_a'.</li>
                                        <li>The second node ID in the list should be 'z_test.py::test_z'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector.py::TestTestCollector::test_handle_collection_finish</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `handle_collection_finish` method of TestCollector to ensure it correctly tracks collected and deselected items.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the collector might incorrectly report the number of collected or deselected items when the collection is finished.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `collected_count` attribute should be updated with the correct count after calling `handle_collection_finish`.</li>
                                        <li>The `deselected_count` attribute should be updated with the correct count after calling `handle_collection_finish`.</li>
                                        <li>The number of collected items should match the expected value (3) before and after calling `handle_collection_finish`.</li>
                                        <li>The number of deselected items should match the expected value (1) before and after calling `handle_collection_finish`.</li>
                                        <li>The total number of collected and deselected items should be equal to the sum of their counts before and after calling `handle_collection_finish`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 78-79, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_disabled_via_handle_report</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the collector does not capture output when config is disabled and handle_report integration is used.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the collector captures output even if the `capture_failed_output` configuration flag is set to False.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `collector.handle_runtest_logreport(report)` call should not modify the `result.captured_stdout` attribute.</li>
                                        <li>The `results` dictionary in the test case `t` should still contain a key named 'output' with value 'failed'.</li>
                                        <li>The `collector.results` dictionary in the test case `t` should have an empty list for the key 'skipped'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stderr</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `test_capture_output_stderr` function captures stderr correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the captured stderr is not properly recorded.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `captured_stderr` attribute of the `TestCaseResult` object should be set to the expected value.</li>
                                        <li>The `report.capstderr` method should return the expected error message.</li>
                                        <li>The `collector._capture_output(result, report)` function should record the captured stderr correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264, 268-269)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stdout</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `test_capture_output_stdout` function captures stdout correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the captured stdout is not properly recorded.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `captured_stdout` attribute of the `TestCaseResult` object should contain the expected output.</li>
                                        <li>The `report.capstdout` method should be called with the correct value.</li>
                                        <li>The `report.capstderr` method should be called with an empty string.</li>
                                        <li>The captured stdout should not be empty after calling `_capture_output`.</li>
                                        <li>The captured stderr should be empty after calling `_capture_output`.</li>
                                        <li>The `result.captured_stdout` attribute should contain the expected output.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_truncated</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `test_capture_output_truncated` function truncates output exceeding the specified maximum characters.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the collector fails to truncate output exceeding the max chars limit, leading to incorrect results or unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The captured stdout length is less than or equal to the `capture_output_max_chars` value.</li>
                                        <li>The captured stderr length is zero.</li>
                                        <li>The captured stdout contains only characters up to the specified maximum length.</li>
                                        <li>The captured stderr does not contain any characters.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_create_result_with_item_markers</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test `test_create_result_with_item_markers` verifies that the collector extracts item markers correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the collector might not extract all required item markers.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>item.callspec.id should be set to 'param1' when calling get_closest_marker('llm_opt_out')</li>
                                        <li>item.get_closest_marker('llm_context_override') should return a mock object with 'complete' as argument</li>
                                        <li>result.param_id should match the value of `item.callspec.id`</li>
                                        <li>result.llm_opt_out should be set to True based on the marker name</li>
                                        <li>result.llm_context_override should match the expected value from get_closest_marker('llm_context_override')</li>
                                        <li>result.requirements should contain 'REQ-1' and 'REQ-2' as strings</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">35 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163-164, 167-169, 171, 181-182, 185-189, 198-200, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_repr_crash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `_extract_error` method to handle ReprFileLocation behavior.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential crash when handling ReprFileLocation in error reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report.longrepr.__str__.return_value` is set to 'Crash report'.</li>
                                        <li>The `_extract_error` method returns the expected string 'Crash report'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_string</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `_extract_error` method of `TestCollector` to verify it returns a string longrepr when an error occurs.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of unexpected error during report extraction.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_error` method should return 'Some error occurred' as the extracted error string.</li>
                                        <li>The `report.longrepr` attribute should be set to 'Some error occurred'.</li>
                                        <li>The `collector._extract_error(report)` expression should evaluate to 'Some error occurred'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_fallback</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `_extract_skip_reason` method returns `None` when no longrepr is provided.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the method does not handle cases where `longrepr` is `None`.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_skip_reason` method should return `None` for an empty or `None` `report.longrepr`.</li>
                                        <li>The `_extract_skip_reason` method should raise a `ValueError` when `report.longrepr` is `None`.</li>
                                        <li>The `_extract_skip_reason` method should not attempt to extract the skip reason from an empty report.</li>
                                        <li>The `_extract_skip_reason` method should return `None` for a `report` object with no attributes.</li>
                                        <li>The `_extract_skip_reason` method should raise an error when called on a non-report object.</li>
                                        <li>The `_extract_skip_reason` method should not attempt to extract the skip reason from a report that does not have a `longrepr` attribute.</li>
                                        <li>The `_extract_skip_reason` method should return `None` for a report with a `longrepr` attribute set to an empty string.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250, 252)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_string</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test `test_extract_skip_reason_string` verifies that the `_extract_skip_reason` method returns a string as expected when given a `report` object.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `longrepr` attribute of the `report` object is not correctly extracted and returned by the `_extract_skip_reason` method.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `report.longrepr` should be 'Just skipped'.</li>
                                        <li>The string 'Just skipped' should be returned as the skip reason.</li>
                                        <li>If the report does not have a `longrepr` attribute, an error should be raised with a meaningful message.</li>
                                        <li>If the `report` object is not a `MagicMock` instance, the test should fail with a clear error message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_tuple</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `collectors._extract_skip_reason` extracts skip message from a tuple containing file, line and message.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where a test fails due to incorrect handling of tuples with multiple values.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report.longrepr` attribute is not converted to a string before being passed to `_extract_skip_reason`.</li>
                                        <li>The `collectors._extract_skip_reason(report)` function should correctly extract the skip message from the tuple containing file, line and message.</li>
                                        <li>The expected output of `_extract_skip_reason` is correct for all cases where it's applicable.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_collection_report_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `handle_collection_report` method correctly records a collection error in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `handle_collection_report` method does not record errors when they occur during collection.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `collection_errors` list should contain exactly one item with `nodeid='test_broken.py'` and `message='SyntaxError'`.</li>
                                        <li>The error message should be 'SyntaxError'.</li>
                                        <li>The node ID of the error should match 'test_broken.py'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 58, 60-65, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_rerun</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'Should handle rerun attribute.' verifies that the collector correctly handles rerun attributes in runtest logs.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the collector might not handle rerun attributes correctly, leading to incorrect results or unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>res.rerun_count should be equal to 1 (one rerun)</li>
                                        <li>res.final_outcome should be 'failed' (the final outcome of the runtest)</li>
                                        <li>collector.results['t::r'] should contain the expected result (in this case, a dictionary with 'rerun_count' and 'final_outcome')</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_setup_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `handle_runtest_setup_failure` method records a setup error in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where a setup failure is not properly recorded in the report, potentially leading to incorrect analysis or reporting.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>res.outcome == 'error'</li>
                                        <li>res.phase == 'setup'</li>
                                        <li>res.error_message == 'Setup failed'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99-103, 109-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_teardown_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test should record error if teardown fails after pass.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the collector does not record errors when the teardown function fails after passing the runtest.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert res.outcome == 'error'</li>
                                        <li>assert res.phase == 'teardown'</li>
                                        <li>assert res.error_message == 'Cleanup failed'</li>
                                        <li>assert call_report.failed is True</li>
                                        <li>assert teardown_report.failed is True</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">38 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-115, 124, 127-128, 130, 132-133, 135-137, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_model_parsing_edge_cases</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the GeminiProvider's parsing of preferred models for edge cases, including an empty list and a specific model.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case the 'GeminiProvider' class is modified to handle edge cases where the preferred models are not provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `provider._parse_preferred_models()` returns a list of strings containing the specified models.</li>
                                        <li>The function `provider._parse_preferred_models()` returns an empty list when no preferred models are provided.</li>
                                        <li>The function `provider._parse_preferred_models()` returns a list containing 'All' when the model is set to 'All'.</li>
                                        <li>The function `provider._parse_preferred_models()` does not return any values when the model is None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 134, 136-139, 141-142, 385, 387, 417-424)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_rate_limiter_edge_math</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter prevents over and under token limits when recording tokens but not requests.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the rate limiter allows excessive token usage without triggering an error, potentially leading to performance issues or security vulnerabilities.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert limiter.next_available_in(60) > 0</li>
                                        <li>assert limiter.next_available_in(10) == 0</li>
                                        <li>assert limiter.record_tokens(50) is None</li>
                                        <li>assert len(limiter.tokens) < 100</li>
                                        <li>assert len(limiter.requests) < 10</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">35 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_models_to_dict_variants</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `models_to_dict` method returns accurate coverage percentages for SourceCoverageEntry instances with varying numbers of statements, covered and missed lines.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in coverage calculation when dealing with SourceCoverageEntry objects with different numbers of statements, covered, or missed lines.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `coverage_percent` attribute is correctly set to the specified value for each SourceCoverageEntry instance.</li>
                                        <li>The `covered_ranges` and `missed_ranges` attributes are correctly formatted as strings within their respective keys in the assertion.</li>
                                        <li>The `duration` attribute is correctly set to 1.0 seconds for each RunMeta instance, regardless of its start time or duration.</li>
                                        <li>For LlmAnnotation instances with an error message 'timeout', the corresponding `error` key should contain 'timeout' as expected.</li>
                                        <li>When creating a new SourceCoverageEntry object with incorrect data (e.g., missing statements), the `to_dict()` method should raise an AssertionError indicating that the coverage percentage is not 50.0%.</li>
                                        <li>The `to_dict()` method should correctly handle cases where the number of covered or missed lines exceeds the total number of lines in the file.</li>
                                        <li>For RunMeta instances with a start time before the current time, the corresponding `duration` key should be set to 1.0 seconds as expected.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 71-78, 104-107, 109, 111-113, 115, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_create_mapper</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the creation of a CoverageMapper instance with an empty configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where a CoverageMapper created with an invalid or missing configuration would not throw an error, but instead silently initialize and return a non-configured object.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config` attribute of the `CoverageMapper` instance is set to the provided `Config` object.</li>
                                        <li>The `warnings` attribute of the `CoverageMapper` instance is initialized with an empty list.</li>
                                        <li>An assertion error would be raised if the `config` attribute was not set or was missing in the `CoverageMapper` instance.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 44-45)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_get_warnings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the `get_warnings` method returns a list of warnings.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function does not return a list of warnings as expected.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_warnings()` method should return a list of warnings.</li>
                                        <li>The `get_warnings()` method should be able to handle any configuration or error scenarios.</li>
                                        <li>The `get_warnings()` method should raise an exception if no warnings are found, as per the test requirements.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 44-45, 308)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_map_coverage_no_coverage_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the `map_coverage` method returns an empty dictionary when no coverage file exists.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to missing coverage data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `Path.exists` mock should return False for the given input.</li>
                                        <li>The `glob.glob` mock should return an empty list for the given input.</li>
                                        <li>The `map_coverage` method should return an empty dictionary.</li>
                                        <li>At least one warning should be emitted by the test.</li>
                                        <li>The number of warnings emitted should be greater than zero.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_all_phases</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `CoverageMapper` extracts all phases when `include_phase=all`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the coverage map might not include all phases if `include_phase=all` is used.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_extract_nodeid` method should return the correct node ID for each phase.</li>
                                        <li>The `_extract_nodeid` method should not return any node IDs when `include_phase=all`.</li>
                                        <li>The `_extract_nodeid` method should include all phases in the coverage map.</li>
                                        <li>The coverage map should have all phases included if `include_phase=all` is used.</li>
                                        <li>The coverage map should exclude no phases if `include_phase=all` is used.</li>
                                        <li>The node IDs returned by `_extract_nodeid` should match the expected node IDs for each phase.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_empty_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 44-45, 216-217)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_filters_setup</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `test_extract_nodeid_filters_setup` test case extracts node IDs correctly when `include_phase=run`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where setup phase nodes are incorrectly filtered out during coverage analysis.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `_extract_nodeid` in the `CoverageMapper` class should extract node IDs from the given string.</li>
                                        <li>The extracted node ID should be `None` when the input string contains 'setup'.</li>
                                        <li>The `include_phase` parameter in the `Config` object should match the expected phase ('run') for this test case.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 44-45, 216, 220, 224-225, 228-230)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_with_run_phase</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `extract_nodeid` method extracts the correct node ID from a run phase context.</p>
                                <p><strong>Why Needed:</strong> This test prevents coverage regression by ensuring that the `extract_nodeid` method correctly identifies nodes in the run phase of the code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The extracted node ID matches the expected value (`test.py::test_foo`).</li>
                                        <li>The node ID is present in the context string (`test.py::test_foo|run`).</li>
                                        <li>The node ID is not empty or null.</li>
                                        <li>The node ID does not contain any whitespace characters.</li>
                                        <li>The node ID does not start with a digit.</li>
                                        <li>The node ID does not contain any special characters (e.g., !, @, #, $, etc.).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_full_logic</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test should extract all contexts for full logic coverage.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in coverage reporting when the test is run with `omit_tests_from_coverage=True`.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert 'test_app.py::test_one' in result</li>
                                        <li>assert 'test_app.py::test_two' in result</li>
                                        <li>assert len(one_cov) == 1 and one_cov[0].line_count == 2</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">57 lines (ranges: 44-45, 118, 121-122, 127, 131-135, 137-140, 144, 148, 150, 152-153, 156, 160-163, 165, 167-168, 173, 176, 178-184, 187-189, 191-194, 196, 199-200, 202, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">13 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65-67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_no_contexts</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `CoverageMapper` correctly handles data with no test contexts by returning an empty dictionary.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the `CoverageMapper` returns incorrect results when given data without any test contexts.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_data.contexts_by_lineno.return_value == {}</li>
                                        <li>mock_data.measured_files.return_value == ['app.py']</li>
                                        <li>result == {}</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 44-45, 118, 121-122, 127, 131-135, 144-146)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_nodeid_variants</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test extracts node ID variants for different phases.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in coverage analysis by ensuring the correct extraction of node IDs based on phase.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `_extract_nodeid` correctly identifies the target node ID when a phase is specified.</li>
                                        <li>The function `_extract_nodeid` ignores the target node ID when no phase is specified.</li>
                                        <li>The function `_extract_nodeid` handles cases without phases (e.g., `test.py::test_no_phase`) correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231-234, 236, 239)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_no_files</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the function `test_load_coverage_data_no_files` raises a warning when no coverage files exist.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not raise an error or warning for scenarios with no coverage data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `_load_coverage_data()` returns `None` when no coverage files are found.</li>
                                        <li>The list of warnings is expected to contain exactly one entry with code 'W001'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 44-45, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_read_error</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case verifies that the test_load_coverage_data_read_error function handles errors reading coverage files correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the CoverageMapper class fails to handle errors when loading coverage data from corrupted or invalid files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return None when attempting to load coverage data from a corrupt .coverage file.</li>
                                        <li>Any warnings generated by the mapper should contain the message 'Failed to read coverage data'.</li>
                                        <li>The function should not attempt to load coverage data from a valid .coverage file.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 44-45, 72-73, 83, 86, 88, 92, 94-96, 107-111, 114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_with_parallel_files</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test should handle parallel coverage files from xdist and verify that the CoverageMapper correctly updates its internal data structures.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the CoverageMapper does not update its internal data structures when loading coverage data with parallel files from xdist.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `update` method of the `CoverageData` class should be called at least twice when loading coverage data with parallel files.</li>
                                        <li>The `update` method of the `CoverageData` class should not be called zero times when loading coverage data with parallel files.</li>
                                        <li>The `update` method of the `CoverageData` class should call the `__call__` method on the mock instances correctly when loading coverage data with parallel files.</li>
                                        <li>The `update` method of the `CoverageData` class should update its internal state correctly when loading coverage data with parallel files.</li>
                                        <li>The `update` method of the `CoverageData` class should not raise an exception when loading coverage data with parallel files.</li>
                                        <li>The mock instances returned by the `patch` function should be different from each other when loading coverage data with parallel files.</li>
                                        <li>The mock instance for `mock_parallel_data1` should have a call count greater than 0 when loading coverage data with parallel files.</li>
                                        <li>The mock instance for `mock_parallel_data2` should not have a call count greater than 0 when loading coverage data with parallel files.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 44-45, 72-73, 83, 86, 88, 92, 94, 98, 101-104, 106)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_coverage_no_data</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `map_coverage` method returns an empty dictionary when _load_coverage_data returns None.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails with an unexpected error or incorrect result when there is no coverage data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_load_coverage_data` function should return `None` when called with no arguments.</li>
                                        <li>The `map_coverage` method should return an empty dictionary when passed an empty dictionary as input.</li>
                                        <li>The `map_coverage` method should not raise any exceptions when called with a non-empty dictionary as input.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 44-45, 58-60)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_analysis_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the coverage map does not include files with analysis errors.</p>
                                <p><strong>Why Needed:</strong> To prevent coverage maps from including error-prone code, which can indicate a deeper issue in the application.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_cov.analysis2.assert_called_once_with(mock_data)</li>
                                        <li>mock_data.measured_files.return_value == ['app.py']</li>
                                        <li>entries is an empty list after skipping files with errors</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 44-45, 243-244, 246-248, 250, 252-254, 259, 261, 263-268, 271, 299-300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_comprehensive</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'Should exercise all paths in map_source_coverage' verifies that the test covers all possible source code files.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the coverage mapper exercises all possible source code files, even if they are not directly analyzed by analysis2.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `map_source_coverage` returns a list of entries where each entry contains information about a file's coverage.</li>
                                        <li>Each entry in the returned list has the following properties: `file_path`, `statements`, `covered`, `missed`, and `coverage_percent`.</li>
                                        <li>The `coverage_percent` property is calculated by dividing the number of statements covered by the total number of statements in the file, then multiplying by 100.</li>
                                        <li>The test verifies that each entry has a unique `file_path`, `statements`, `covered`, `missed`, and `coverage_percent`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">32 lines (ranges: 44-45, 243-244, 246-248, 250, 252, 259-261, 273, 276-279, 281-283, 285-293, 295, 299-300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors.py::test_make_warning</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `make_warning` factory function with a valid warning code and message.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential warning that occurs when no .coverage file is found for a test.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function returns an instance of WarningCode.W001_NO_COVERAGE with the correct code.</li>
                                        <li>The `message` attribute contains the expected string 'No .coverage file found'.</li>
                                        <li>The `detail` attribute matches the provided value 'test-detail'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors.py::test_warning_code_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests that warning codes have correct values.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the warning code values are incorrect, potentially leading to unexpected behavior or errors in the application.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>{'name': 'Correct value for W001_NO_COVERAGE', 'expected_value': 'W001'}</li>
                                        <li>{'name': 'Correct value for W101_LLM_ENABLED', 'expected_value': 'W101'}</li>
                                        <li>{'name': 'Correct value for W201_OUTPUT_PATH_INVALID', 'expected_value': 'W201'}</li>
                                        <li>{'name': 'Correct value for W301_INVALID_CONFIG', 'expected_value': 'W301'}</li>
                                        <li>{'name': 'Correct value for W401_AGGREGATE_DIR_MISSING', 'expected_value': 'W401'}</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors.py::test_warning_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Warning.to_dict() method to ensure it returns a valid dictionary with required keys.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues where the Warning.to_dict() method does not return a dictionary with all required keys (code, message, detail).</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'code' key should be present and have the correct value ('W001').</li>
                                        <li>The 'message' key should be present and have the correct value ('No coverage').</li>
                                        <li>The 'detail' key should be present and have the correct value ('some/path').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 70-72, 74-76)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_known_code</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test makes sure a warning with standard message is created when known code is used.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where unknown code may cause unexpected warnings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>w.code == WarningCode.W101_LLM_ENABLED</li>
                                        <li>w.message == WARNING_MESSAGES[WarningCode.W101_LLM_ENABLED]</li>
                                        <li>w.detail is None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_unknown_code</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Make Warning: Unknown Code</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the fallback message for unknown code is not used.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `make_warning` does not raise an exception when given an unknown warning code.</li>
                                        <li>The function `make_warning` uses the correct fallback message for unknown code.</li>
                                        <li>The function `make_warning` restores the original message after using it to make a warning.</li>
                                        <li>The function `make_warning` correctly handles missing warning codes by returning the old message.</li>
                                        <li>The function `make_warning` does not raise an exception when given an invalid warning code.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_with_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Make Warning with Detail: Verifies that a warning is created with the correct code and detail.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where a warning is not properly created when an invalid configuration value is provided, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `make_warning` returns an instance of `WarningCode.W301_INVALID_CONFIG` with the specified detail.</li>
                                        <li>The attribute `detail` of the returned warning instance matches the expected value 'Bad value'.</li>
                                        <li>The code attribute of the returned warning instance matches the expected value WarningCode.W301_INVALID_CONFIG.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestWarningCodes::test_codes_are_strings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that all enum values are strings and start with 'W' to prevent WarningsCodes from being used as non-string codes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where WarningCode enum values could be misused as non-string codes, leading to incorrect warnings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert isinstance(code.value, str) checks if the value of each code is indeed a string.</li>
                                        <li>assert code.value.startswith('W') checks if all enum values start with 'W' and are therefore valid warning codes.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_no_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the Warning class can be serialized to a dictionary without including detailed information.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the warning message is not properly represented in the serialized data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'code' key should contain the warning code.</li>
                                        <li>The 'message' key should contain the warning message.</li>
                                        <li>Both keys should match the expected values ('W001' and 'No coverage', respectively).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 70-72, 74, 76)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_with_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the warning_to_dict method with detailed information.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where warnings are not properly serialized to dictionaries with detail.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `to_dict()` method of Warning objects returns a dictionary with the correct keys ('code', 'message', and 'detail').</li>
                                        <li>The 'detail' key in the returned dictionary contains the expected string value ('Check setup').</li>
                                        <li>The 'message' key in the returned dictionary contains the expected string value ('No coverage').</li>
                                        <li>The 'code' key in the returned dictionary contains the correct string value ('W001').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 70-72, 74-76)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestIsPythonFile::test_non_python_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `is_python_file` function correctly identifies non-python files.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function incorrectly returns True for non-python files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return False when given a file with a non-.py extension (e.g. 'foo/bar.txt')</li>
                                        <li>The function should return False when given a file with a non-pyc extension (e.g. 'foo/bar.pyc')</li>
                                        <li>The function should not incorrectly report True for files with non-python extensions (e.g. 'foo/bar.py')</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 79)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestIsPythonFile::test_python_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `is_python_file` function correctly identifies .py files.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function incorrectly identifies non-py files as Python files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return True for files with names starting with '.py'.</li>
                                        <li>The function should raise an error or return False for files without names starting with '.py'.</li>
                                        <li>The function should handle file extensions other than '.py' correctly.</li>
                                        <li>The function should not incorrectly identify non-existent .py files as Python files.</li>
                                        <li>The function should be able to handle files with different casing in their names (e.g., 'Foo.py', 'foo.py').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 79)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestMakeRelative::test_makes_path_relative</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test makes absolute path relative to test directory.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails if the test directory is not in the same Python package as the test file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `make_relative` function should return the expected file path.</li>
                                        <li>The file path returned by `make_relative` should be an absolute path relative to the test directory.</li>
                                        <li>The parent directory of the original file path should be created if it does not exist.</li>
                                        <li>The `touch` method should not raise an exception if the file already exists.</li>
                                        <li>The `mkdir` method should create the parent directories if they do not exist.</li>
                                        <li>The `file_path.parent.mkdir(parents=True, exist_ok=True)` line should work correctly even if the test directory is not in the same package as the test file.</li>
                                        <li>The `make_relative` function should be able to handle cases where the original file path is a relative path.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> test returns normalized with no base</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the function does not normalize paths correctly without a base.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `make_relative` should return the original path when there is no base.</li>
                                        <li>The function `make_relative` should handle cases where the input is an absolute path.</li>
                                        <li>The function `make_relative` should correctly normalize the resulting relative path.</li>
                                        <li>The function `make_relative` should not modify the original path.</li>
                                        <li>The function `make_relative` should raise a meaningful error when given invalid input.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 30, 33, 36, 39, 42, 55-56)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestNormalizePath::test_already_normalized</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests that the `normalize_path` function correctly handles already-normalized paths.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `normalize_path` function incorrectly normalizes paths that are already in their normalized form.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output of `normalize_path('foo/bar')` should be `'foo/bar'`.</li>
                                        <li>The input path 'foo/bar' is already normalized, so it should not be modified by the `normalize_path` function.</li>
                                        <li>If the input path is already in its normalized form, then the output should also be in its normalized form.</li>
                                        <li>If the input path has leading or trailing whitespace, then the output should have the same leading/trailing whitespace.</li>
                                        <li>If the input path contains any invalid characters (e.g. `/./`, `:`, etc.), then the output should not contain any such characters.</li>
                                        <li>The function should handle cases where the input path is a relative path (i.e. it does not start with a slash).</li>
                                        <li>The function should handle cases where the input path is an absolute path (i.e. it starts with a slash).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestNormalizePath::test_forward_slashes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `normalize_path` function with a scenario where forward slashes are used in the path.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when handling paths that contain forward slashes, which is necessary for Unix-based systems.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `normalize_path` function correctly converts backslashes to forward slashes in the input path.</li>
                                        <li>The resulting normalized path does not contain any backslashes.</li>
                                        <li>The function handles paths with multiple consecutive forward slashes correctly.</li>
                                        <li>The function preserves the original directory structure of the input path.</li>
                                        <li>The function is case-insensitive when handling paths that contain forward slashes.</li>
                                        <li>The function correctly handles paths with leading or trailing forward slashes.</li>
                                        <li>The function does not modify the original input path.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies the normalization of a path with a trailing slash.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where a file path with a trailing slash is not correctly normalized to remove it.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `normalize_path` removes any leading or trailing slashes from the input path.</li>
                                        <li>If the input path does not start or end with a slash, the function should still return the original path.</li>
                                        <li>The function handles paths with multiple consecutive slashes correctly.</li>
                                        <li>It is not necessary to strip the trailing slash if it is already present in the input path.</li>
                                        <li>The function preserves the relative path information when stripping leading/trailing slashes.</li>
                                        <li>If the input path starts with a slash, the function should return the same path without stripping any additional slashes.</li>
                                        <li>The function does not modify the original path but returns a new normalized path.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_custom_exclude_patterns</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies whether a path matches custom exclusion patterns.</p>
                                <p><strong>Why Needed:</strong> This test prevents the test module from skipping paths that match custom patterns, ensuring consistent testing behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'tests/conftest.py' file should be skipped if it contains any of the excluded patterns ('test*').</li>
                                        <li>The 'src/module.py' file should not be skipped if it does not contain any of the excluded patterns ('test*').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116-117, 119-121, 123)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_normal_path</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `should_skip_path` function returns `False` for a normal path.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function incorrectly skips normal paths without checking their contents.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return `True` when given a normal path (`src/module.py`).</li>
                                        <li>The function should not return `False` when given a normal path (`src/module.py`).</li>
                                        <li>The function should raise an exception or return a specific value indicating that the path is skipped.</li>
                                        <li>The function should check the file contents before making a decision about skipping it.</li>
                                        <li>The function should handle cases where the path does not exist or is not accessible.</li>
                                        <li>The function should provide clear and consistent error messages for different scenarios.</li>
                                        <li>The function should be able to skip paths that are intended to be skipped (e.g., `__pycache__` directories).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_git</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_fs.py::TestShouldSkipPath::test_skips_git verifies whether a .git directory should be skipped.</p>
                                <p><strong>Why Needed:</strong> This test prevents the test from skipping non-.git directories, which could lead to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert should_skip_path('.git/objects/foo') is True</li>
                                        <li>should not skip '.git' (expected False)</li>
                                        <li>should not skip '.git/objects/bar' (expected False)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_pycache</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `should_skip_path` function correctly skips `__pycache__` directories.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the `should_skip_path` function incorrectly includes `__pycache__` directories in the list of paths to skip.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The path `foo/__pycache__/bar.pyc` is not included in the list of paths to skip.</li>
                                        <li>The path `foo/__pycache__/bar.pyc` does not contain any executable code.</li>
                                        <li>The path `foo/__pycache__/bar.pyc` has a file extension that indicates it's a cache file (e.g. `.pyc`, `.pyc`).</li>
                                        <li>The `should_skip_path` function is correctly handling the case where the directory contains a non-executable file.</li>
                                        <li>The `should_skip_path` function is not including directories with non-ASCII characters in the list of paths to skip.</li>
                                        <li>The `should_skip_path` function is correctly skipping directories that are not Python modules (e.g. directories containing other types of files).</li>
                                        <li>The `should_skip_path` function is not incorrectly including directories with a file extension that indicates it's a cache file (e.g. `.pyc`, `.o`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_venv</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> test_skips_venv verifies whether the 'venv' directory is skipped by the function `should_skip_path`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the function `should_skip_path` incorrectly identifies 'venv' directories as being to be skipped, potentially leading to incorrect behavior in subsequent tests or code.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert should_skip_path('venv/lib/python/site.py') is True</li>
                                        <li>assert should_skip_path('.venv/lib/python/site.py') is True</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/fs.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_pruning</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that pruning clears the request and token usage lists after a past request.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the rate limiter does not clear the request and token usage lists when a request is made in the past.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of _request_times should be 0 after pruning.</li>
                                        <li>The length of _token_usage should be 0 after pruning.</li>
                                        <li>The _request_times list should contain only one element (the time of the past request).</li>
                                        <li>_request_times list should not contain any requests made in the future.</li>
                                        <li>The _token_usage list should contain two elements (the time and value of the past request) before pruning.</li>
                                        <li>The _token_usage list should be empty after pruning.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 39-42, 81-85, 87-88)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_rpm_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter prevents requests from exceeding the specified limit.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where multiple requests are made in quick succession, potentially overwhelming the server.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `next_available_in` method should return a value greater than 0.</li>
                                        <li>The `next_available_in` method should be less than or equal to 60.0 seconds.</li>
                                        <li>The limiter should not be unavailable after recording a request.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95, 97, 100-102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_tpm_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the rate limiter prevents a regression when there are not enough tokens available.</p>
                                <p><strong>Why Needed:</strong> This test verifies that the rate limiter does not allow too many requests to be made in a short period of time, preventing potential abuse.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The next_available_in method returns a non-negative value (0) if there are no tokens left for the given number of requests.</li>
                                        <li>The _token_usage list is updated correctly after recording tokens and waiting for an available slot.</li>
                                        <li>The length of the _token_usage list remains unchanged at 2 even after multiple records and waits.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">33 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-94, 100-101, 103, 105, 107-108, 110-114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_wait_for_slot</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `wait_for_slot` method of `_GeminiRateLimiter` sleeps for a specified amount of time.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the rate limiter does not sleep when it should, potentially leading to unexpected behavior or errors in other parts of the application.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `wait_for_slot` method is called with the correct argument (1)</li>
                                        <li>The `time.sleep` mock object is called with the correct argument (1)</li>
                                        <li>The `time.sleep` mock object is not called before the first call to `wait_for_slot`</li>
                                        <li>The `time.sleep` mock object is called after the last call to `wait_for_slot`</li>
                                        <li>The `time.sleep` mock object is called within a reasonable amount of time (1 second)</li>
                                        <li>The rate limiter does not sleep when it should, potentially leading to unexpected behavior or errors in other parts of the application</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">31 lines (ranges: 39-42, 45-46, 48, 52-54, 58-59, 61-63, 73, 76-78, 81-82, 84, 87-88, 92-93, 95, 97, 100-102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_record_zero_tokens</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter records zero tokens when no tokens are available.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the rate limiter does not record tokens for an extended period, potentially leading to unexpected behavior or errors in downstream applications.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_token_usage` attribute of the `limiter` object should be empty after calling `record_tokens(0)`.</li>
                                        <li>The `len(limiter._token_usage)` should be equal to 0 after calling `record_tokens(0)`.</li>
                                        <li>If no tokens are available, the rate limiter should not attempt to record any additional tokens.</li>
                                        <li>If an error occurs while recording tokens, it should not propagate to downstream applications.</li>
                                        <li>The rate limiter's `_token_usage` attribute should be reset to its initial state after calling `record_tokens(0)`.</li>
                                        <li>Calling `record_tokens(n)` with a non-zero value should not have any effect on the rate limiter's behavior.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39-42, 66-67)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_requests_per_day_exhaustion</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the test prevents a rate limit exceeded error when exceeding daily requests.</p>
                                <p><strong>Why Needed:</strong> This test ensures that the Gemini Rate Limiter does not exceed the daily request limit, preventing an error from being raised.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The limiter raises `_GeminiRateLimitExceeded` with the message `requests_per_day` when the daily limit is exceeded.</li>
                                        <li>The limiter records a request before raising the rate limit exceeded error.</li>
                                        <li>The limiter waits for a slot of at least 10 requests to be available after exceeding the daily limit.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">24 lines (ranges: 32-34, 39-42, 45-46, 48-50, 58-60, 73, 76-78, 81-82, 84, 87-88)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_tpm_fallback_wait</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the TPM fallback wait time is sufficient to prevent rate limiting when there are insufficient tokens available.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential rate limiting issue where the TPM would fall back to waiting for an extended period of time if there were not enough tokens available.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `wait` variable should be greater than 0.</li>
                                        <li>The sum of `tokens_used` and `request_tokens` should be less than or equal to the rate limit (`10` tokens per minute).</li>
                                        <li>If `token_usage` is empty, then `limiter._seconds_until_tpm_available(now, 5)` should return a non-zero value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">24 lines (ranges: 39-42, 66, 68-70, 81-82, 84, 87-88, 100-101, 103, 105, 107-108, 110-114)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_provider_rpm_cooldown</span>
                        <div class="test-meta">
                            <span>593ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that RPM rate limit cooldown handling is correctly implemented.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the RPM rate limit cooldown is not properly handled, leading to incorrect behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `models/gemini-pro` model should be present in the `_cooldowns` dictionary.</li>
                                        <li>The value of `models/gemini-pro` in the `_cooldowns` dictionary should be greater than 1000.0.</li>
                                        <li>The RPM rate limit cooldown handling should correctly handle the first call to `_GeminiRateLimitExceeded` with a retry after 0.1 seconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">23 lines (ranges: 52-53, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">117 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-215, 217-223, 225-227, 233-234, 238-240, 242-243, 274-277, 280, 282-290, 292-295, 297-298, 300-301, 346, 348-350, 352-353, 381-382, 385-386)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_rate_limit_retry</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the annotate method retries when rate limiting occurs and returns a valid annotation for the recovered scenario.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case of rate limiting, ensuring that the annotate method correctly handles retry attempts.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation returned by _annotate_internal is 'Recovered Scenario'.</li>
                                        <li>The mock_post call count is 2, indicating two successful retries.</li>
                                        <li>The scenario assertion passes for the recovered scenario.</li>
                                        <li>The mock_parse call count is 1, indicating only one parsing attempt.</li>
                                        <li>The mock_get and mock_post return status codes of 429 and 200 respectively, as expected.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">181 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-215, 217-222, 225-227, 233-234, 238-240, 242-243, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330-333, 335-341, 343, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_success</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that _annotate_internal returns LlmAnnotation correctly when _parse_response is called with the expected format.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where _parse_response might expect an incorrect format, leading to incorrect annotations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation returned by _annotate_internal has the correct scenario.</li>
                                        <li>The annotation does not have any errors.</li>
                                        <li>_parse_response returns the expected response structure when called with the correct scenario and error.</li>
                                        <li>The annotation's scenario is correctly set to 'Success Scenario'.</li>
                                        <li>The annotation does not contain any error information.</li>
                                        <li>_build_prompt is mocked to avoid complex dependency, ensuring the test environment remains consistent.</li>
                                        <li>_annotate_internal calls _parse_response where it expects a specific format, allowing for correct annotations.</li>
                                        <li>Mocking _parse_response ensures that the test can focus on verifying the correctness of _annotate_internal without worrying about incorrect response formats.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">173 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-223, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-343, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_availability</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the availability check fails when environment variables are not set</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the availability check returns False even if it's supposed to be available.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider._check_availability() is False</li>
                                        <li>provider._check_availability() is True</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 134, 136-139, 141-142, 266-267, 269)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpd_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter does not exceed the daily limit when no requests are made in a given time period.</p>
                                <p><strong>Why Needed:</strong> The test prevents a potential bug where the rate limiter exceeds the daily limit and causes an error or unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>limiter.record_request() should not be called before checking if there is an available slot for the next request.</li>
                                        <li>limiter.next_available_in(100) should return None when no requests are made in a given time period.</li>
                                        <li>The rate limiter's daily limit should not be exceeded even after multiple requests are made within the same time period.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 39-42, 45-46, 48-50, 73, 76-78, 81-82, 84, 87-88)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpm_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the rate limiter does not block requests when there are available slots.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the rate limiter blocks all requests for a short period, causing unexpected behavior or errors in downstream applications.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The next_available_in method should return 0.0 after the first two requests.</li>
                                        <li>The next_available_in method should return 0.0 after the third request and before the wait time.</li>
                                        <li>The wait time should be between 0 and 60 seconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">27 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-97, 100-102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeConfigHash::test_different_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that different configuration providers produce different hashes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential hash collision bug where two different configurations with the same provider could have the same hash.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `compute_config_hash` function should return a different hash for `config1` and `config2`.</li>
                                        <li>The `compute_config_hash` function should not return the same hash when both `config1` and `config2` are from different providers.</li>
                                        <li>The `compute_config_hash` function should raise an exception if both `config1` and `config2` are from the same provider.</li>
                                        <li>The `compute_config_hash` function should be able to distinguish between `config1` and `config2` based on their provider.</li>
                                        <li>If `config1` is from provider 'ollama' and `config2` is from provider 'none', they should have different hashes.</li>
                                        <li>If both `config1` and `config2` are from provider 'none', the hash should be the same.</li>
                                        <li>The order of configuration providers does not affect the hash.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 96-101, 103-104)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeConfigHash::test_returns_short_hash</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests that the computed hash is of length 16.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where the hash might be too long for efficient storage or comparison purposes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the computed hash should be exactly 16 characters.</li>
                                        <li>The hash value should not exceed 16 bytes (128 bits) in length.</li>
                                        <li>A hash with more than 16 characters should raise an error or indicate an issue.</li>
                                        <li>The hash is a valid SHA-256 hash according to the hashlib library's documentation.</li>
                                        <li>The computed hash does not contain any null bytes (NULs).</li>
                                        <li>The computed hash contains only hexadecimal digits (0-9, A-F, a-f).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 96-101, 103-104)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeFileSha256::test_consistent_with_bytes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the computed SHA-256 hash of a file matches its content hash when the same file is used for both computations.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the hash computation and content hash are not consistent due to differences in the way Python's `hashlib` library handles file paths and byte arrays.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The computed SHA-256 hash of the file should match its content hash.</li>
                                        <li>The path to the file should be a valid absolute or relative path.</li>
                                        <li>The file should not be empty.</li>
                                        <li>The file should have a valid UTF-8 encoding.</li>
                                        <li>The file should not contain any binary data other than the expected content.</li>
                                        <li>The file's mode (e.g., 'w', 'r', etc.) should be compatible with Python's `hashlib` library.</li>
                                        <li>The computed hash should be identical to the provided content hash even if the file is modified or deleted.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 32, 44-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeFileSha256::test_hashes_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `compute_file_sha256` function correctly hashes a file.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the hash is not generated correctly due to incorrect file contents or encoding.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the computed hash should be exactly 64 bytes.</li>
                                        <li>The hash should be generated from the correct file contents (i.e., 'hello world' in this case).</li>
                                        <li>The hash should be generated without any errors or exceptions.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 44-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeHmac::test_different_key</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_different_key' verifies that different keys produce different signatures.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where two different keys could produce the same signature, which would be unexpected behavior for an HMAC-based system.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Verify that the computed signature is different from the expected signature when using different keys.</li>
                                        <li>Check if the computed signature has a different value than the expected signature.</li>
                                        <li>Ensure the computed signature does not match the expected signature even if they are identical.</li>
                                        <li>Test if the difference in signatures is due to the actual key used for computation.</li>
                                        <li>Verify that the computed signature is unique when using different keys.</li>
                                        <li>Check if the expected signature matches the computed signature when using different keys.</li>
                                        <li>Ensure the computed signature has a different value than the expected signature even if they are identical.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 61)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeHmac::test_with_key</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies the production of an HMAC signature with a given key.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where the HMAC length is not correctly calculated when using a secret key.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the generated HMAC should be exactly 64 bytes.</li>
                                        <li>The HMAC should be generated using the provided secret key.</li>
                                        <li>No other parameters (such as message and digest) should be used to generate the signature.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 61)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeSha256::test_consistent</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The function `compute_sha256` is called with the same input data (b'test') and produces the same output hash.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where different inputs to the `compute_sha256` function produce different hashes, potentially leading to inconsistent results in downstream applications.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The two calls to `compute_sha256(b'...')` should return the same hash object.</li>
                                        <li>The two calls to `compute_sha256(b'...')` should return the same bytes object.</li>
                                        <li>The output hash of `compute_sha256(b'...')` should be equal to the input hash.</li>
                                        <li>The output hash of `compute_sha256(b'...')` should not change even if the input is modified (e.g., by appending a new character).</li>
                                        <li>The output hash of `compute_sha256(b'...')` should produce the same hash when run multiple times with the same input data.</li>
                                        <li>The function should raise an error or return a specific value when given invalid input data (e.g., non-string, non-byte string).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestComputeSha256::test_length</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the length of the computed SHA-256 hash.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where the hash length is not consistent across different test runs or environments.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the output should be exactly 64 characters.</li>
                                        <li>The hash should have 64 hexadecimal digits.</li>
                                        <li>No leading zeros are present in the hash.</li>
                                        <li>No trailing zeros are present in the hash.</li>
                                        <li>All hexadecimal digits are present and distinct.</li>
                                        <li>No duplicate hexadecimal digits are present.</li>
                                        <li>The hash is not empty.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestGetDependencySnapshot::test_includes_pytest</span>
                        <div class="test-meta">
                            <span>70ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `get_dependency_snapshot()` function includes the 'pytest' package.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the 'pytest' package is not included in the dependency snapshot, potentially causing issues with downstream dependencies.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'pytest' package should be present in the `get_dependency_snapshot()` output.</li>
                                        <li>The 'pytest' package should have been detected by the function.</li>
                                        <li>The presence of 'pytest' in the snapshot indicates that it is a required dependency.</li>
                                        <li>Without 'pytest', the test would fail due to missing dependencies.</li>
                                        <li>Including 'pytest' ensures compatibility with pytest testing framework.</li>
                                        <li>This test helps ensure that the `get_dependency_snapshot()` function works correctly for pytest packages.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 113-114, 116-121)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestGetDependencySnapshot::test_returns_dict</span>
                        <div class="test-meta">
                            <span>72ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_dependency_snapshot()` function returns a dictionary.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function might return an incorrect data type (e.g., list instead of dict).</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>snapshot is an instance of dict</li>
                                        <li>snapshot has no attributes other than __dict__</li>
                                        <li>snapshot does not contain any non-essential keys</li>
                                        <li>snapshot contains only essential package information</li>
                                        <li>snapshot has the correct number of packages</li>
                                        <li>snapshot includes all required dependencies</li>
                                        <li>snapshot is a dictionary with the expected structure</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 113-114, 116-121)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_loads_key</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `load_hmac_key` function with a loaded HMAC key.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `load_hmac_key` function fails to load an HMAC key from a file due to incorrect file path or permissions.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should successfully load the HMAC key from the specified file.</li>
                                        <li>The loaded HMAC key should match the expected value of 'my-secret-key'.</li>
                                        <li>The `load_hmac_key` function should not throw any exceptions when loading an HMAC key from a valid file.</li>
                                        <li>The `load_hmac_key` function should return the correct HMAC key for the given configuration.</li>
                                        <li>The `load_hmac_key` function should handle cases where the file is missing or cannot be read due to permissions issues.</li>
                                        <li>The `load_hmac_key` function should not throw any exceptions when loading an HMAC key from a non-existent file.</li>
                                        <li>The `load_hmac_key` function should correctly handle cases where the file path is incorrect or malformed.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 73, 76-77, 80-81)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_missing_key_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the function returns None when a missing key file is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function would return a non-None value if a key file does not exist.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should raise a ValueError or return None when a key file does not exist.</li>
                                        <li>The function should not attempt to load the HMAC key from the provided key file.</li>
                                        <li>The function should return an error message indicating that the key file is missing.</li>
                                        <li>The function should not throw any exceptions, but instead return a meaningful result.</li>
                                        <li>The function's behavior should be consistent across different Python versions and environments.</li>
                                        <li>The function's documentation should clearly indicate what happens when a key file does not exist.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 73, 76-78)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_no_key_file</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `load_hmac_key` function returns `None` when no key file is specified.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function does not handle cases without a key file configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `load_hmac_key` function should return `None` if no key file is provided.</li>
                                        <li>No exception should be raised when no key file is specified.</li>
                                        <li>The function should correctly handle the absence of a key file configuration.</li>
                                        <li>The function should not throw an error or raise an exception when no key file is present.</li>
                                        <li>The function's behavior should align with the expected requirements for HMAC key loading.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/hashing.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 73-74)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_aggregation_defaults</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that aggregation defaults have sensible values.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where aggregation defaults are not set to sensible values, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.aggregate_dir is None</li>
                                        <li>config.aggregate_policy == 'latest'</li>
                                        <li>config.aggregate_include_history is False</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_capture_failed_output_default_false</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the capture failed output default setting is set to False.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the default capture failed output setting would be enabled by default, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.capture_failed_output</li>
                                        <li>assert config.capture_failed_output == False</li>
                                        <li>The capture failed output is not set to True.</li>
                                        <li>The capture failed output is not set to False by default.</li>
                                        <li>The capture failed output setting is not explicitly enabled in the configuration.</li>
                                        <li>The capture failed output setting is not disabled by default.</li>
                                        <li>The capture failed output setting is not properly validated or checked for in the test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_context_mode_default_minimal</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the context mode is set to 'minimal' by default.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the context mode is not set to 'minimal' by default.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.llm_context_mode is equal to 'minimal'</li>
                                        <li>config.llm_context_mode is 'minimal'</li>
                                        <li>config.llm_context_mode is not 'default'</li>
                                        <li>config.llm_context_mode is not 'minimal'</li>
                                        <li>get_default_config() returns a configuration with llm_context_mode set to 'minimal'</li>
                                        <li>the value of config.llm_context_mode is not 'minimal'</li>
                                        <li>the value of config.llm_context_mode is 'minimal'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_llm_not_enabled_by_default</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that LLM is not enabled by default in the configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where LLM is enabled by default without a clear reason or context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.is_llm_enabled() == False</li>
                                        <li>config.get_llm_enabled_value() == False</li>
                                        <li>get_default_config().llm_enabled() == False</li>
                                        <li>not config.is_llm_enabled() in [True, None]</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 107, 147, 224, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_omit_tests_default_true</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `TestConfigDefaults` class has a default setting to omit tests from coverage.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the `TestConfigDefaults` class does not correctly set the `omit_tests_from_coverage` flag by default.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config.omit_tests_from_coverage` attribute is set to True.</li>
                                        <li>The `get_default_config()` function returns an instance of `TestConfigDefaults` with a `omit_tests_from_coverage` attribute set to True.</li>
                                        <li>The `TestConfigDefaults` class has a default setting for the `omit_tests_from_coverage` flag.</li>
                                        <li>The `TestConfigDefaults` class correctly sets the `omit_tests_from_coverage` flag by default.</li>
                                        <li>The `get_default_config()` function returns an instance of `TestConfigDefaults` with the correct default value for `omit_tests_from_coverage`.</li>
                                        <li>The `config.omit_tests_from_coverage` attribute is a boolean value indicating whether to omit tests from coverage.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_provider_default_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the default provider setting when it is set to None.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the provider is not set to 'none' in case of privacy requirements.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The config object has a 'provider' attribute with value 'none'.</li>
                                        <li>The 'provider' attribute in the config object matches the expected default value when it's None.</li>
                                        <li>The provider setting does not override any other settings, ensuring consistent behavior.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_secret_exclude_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that secret files are excluded by default from the LLM context.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where sensitive information like secret files might be inadvertently included in the LLM context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'secret' keyword is present in the list of excluded globs.</li>
                                        <li>The '.env' file is not present in the list of excluded globs.</li>
                                        <li>No other keywords or patterns are present in the excluded globs.</li>
                                        <li>All secret files and directories should be excluded from the LLM context by default.</li>
                                        <li>Any sensitive information like secret files should be handled properly to prevent data exposure.</li>
                                        <li>The test ensures that only specific files and directories are excluded from the LLM context.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_deterministic_output</span>
                        <div class="test-meta">
                            <span>6ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the output of the full pipeline is deterministic by comparing it to a sorted list.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the order of reported tests changes due to external factors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>nodeids should be in ascending order</li>
                                        <li>nodeids should not contain duplicates</li>
                                        <li>nodeid 'z_test.py::test_z' is present in the sorted list</li>
                                        <li>nodeid 'a_test.py::test_a' is present in the sorted list</li>
                                        <li>nodeid 'm_test.py::test_m' is present in the sorted list</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">78 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">117 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_empty_test_suite</span>
                        <div class="test-meta">
                            <span>6ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that an empty test suite produces a valid report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in cases where the test suite is empty, ensuring the report contains no invalid or incomplete information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total count of tests in the report should be zero.</li>
                                        <li>The summary section of the report should contain an 'empty' key with a value of zero.</li>
                                        <li>There should be no missing or invalid test data in the report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_html_report_generation</span>
                        <div class="test-meta">
                            <span>31ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The full pipeline generates an HTML report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the HTML report is not generated correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The HTML report should exist at the specified path.</li>
                                        <li>The HTML report contains the expected content.</li>
                                        <li>The 'test_pass' string is present in the HTML report content.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">113 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_json_report_generation</span>
                        <div class="test-meta">
                            <span>54ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the full pipeline generates a valid JSON report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the JSON report is not generated correctly due to missing or incorrect configuration settings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'report_json' attribute of the config object should be set to the path of the generated report.</li>
                                        <li>The 'report_html' attribute of the config object should be set to the path of the generated report.</li>
                                        <li>The JSON output from the report writer should contain a 'schema_version' key with the correct value.</li>
                                        <li>The total count in the summary section should match the number of tests run.</li>
                                        <li>The passed count should match the number of tests that were successfully executed.</li>
                                        <li>The failed count should match the number of tests that encountered an error during execution.</li>
                                        <li>The skipped count should match the number of tests that were intentionally skipped by the test runner.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/_git_info.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 2-3)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">78 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">133 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-322, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_report_root_has_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the ReportRoot has required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where a report root is missing required fields, which could lead to incorrect reporting or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'schema_version' field should be present in the data.</li>
                                        <li>The 'run_meta' field should be present in the data.</li>
                                        <li>The 'summary' field should be present in the data.</li>
                                        <li>The 'tests' field should be present in the data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_aggregation_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that `RunMeta` has aggregation fields.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the schema compatibility test fails due to missing aggregation fields.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>is_aggregated is present in the meta data</li>
                                        <li>run_count is present in the meta data</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_status_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'RunMeta has run status fields' verifies that the RunMeta object contains the required status fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the RunMeta object is missing certain status fields, potentially causing issues with the integration gate.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'exit_code' field should be present in the data.</li>
                                        <li>The 'interrupted' field should be present in the data.</li>
                                        <li>The 'collect_only' field should be present in the data.</li>
                                        <li>The 'collected_count' field should be present in the data.</li>
                                        <li>The 'selected_count' field should be present in the data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_schema_version_defined</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests that the schema version is defined and matches a semver-like format.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when using outdated or incompatible versions of the schema.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The schema version should be defined.</li>
                                        <li>The schema version should match a semver-like format (e.g., '1.2.3').</li>
                                        <li>The schema version should contain at least one dot (.) character.</li>
                                        <li>The schema version should not start with a zero (0).</li>
                                        <li>The schema version should not end with a zero (0).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_test_case_has_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `test_test_case_has_required_fields` test verifies that the `TestCaseResult` object has the required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the `TestCaseResult` object is missing required fields, potentially causing unexpected behavior or errors in downstream processing.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `nodeid` field should be present in the `data` dictionary.</li>
                                        <li>The `outcome` field should be present in the `data` dictionary.</li>
                                        <li>The `duration` field should be present in the `data` dictionary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_gemini_returns_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_provider` function returns an instance of `GeminiProvider` when the `provider` parameter is set to `'gemini'`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the correct provider is not returned due to a mismatch in the expected class name.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The method `__class__.__name__` of the `provider` object should return 'GeminiProvider'.</li>
                                        <li>The attribute `provider` should have an instance of `GeminiProvider` as its value.</li>
                                        <li>The correct provider should be returned for a valid configuration.</li>
                                        <li>An error message or exception should not be raised when calling `get_provider` with the expected provider.</li>
                                        <li>The test should fail if the incorrect class name is used instead of 'GeminiProvider'.</li>
                                        <li>A custom error message should not be printed by the `get_provider` function.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 245, 247, 249, 252, 257, 262-263, 265)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_litellm_returns_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `get_provider` function returns an instance of LiteLLMProvider when the configuration is set to 'litellm'.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `get_provider` function does not return an instance of LiteLLMProvider when the configuration is set to 'litellm'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider.__class__ == 'LiteLLMProvider'</li>
                                        <li>provider.model == 'gpt-3.5-turbo'</li>
                                        <li>provider.name == 'liteellm'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52-53, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_none_returns_noop</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> test_get_provider_with_none_provider returns NoopProvider.</p>
                                <p><strong>Why Needed:</strong> The test prevents a potential bug where the LLM is not properly initialized with a None provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider should be None</li>
                                        <li>NoopProvider should be returned by get_provider() when config.provider='none'</li>
                                        <li>get_provider() should raise an exception when config.provider is 'none'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_ollama_returns_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that when using 'ollama' as a provider, OllamaProvider is returned.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the correct provider type is not detected for 'ollama'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider.__class__.__name__ should be equal to 'OllamaProvider'.</li>
                                        <li>The provider instance has an attribute named 'model' with value 'llama3.2'.</li>
                                        <li>The provider instance has a method named 'get_http_url' that returns the correct URL.</li>
                                        <li>The provider instance does not raise any exceptions when getting the model.</li>
                                        <li>The provider instance is of type OllamaProvider, not another class.</li>
                                        <li>The provider instance's get_http_url method returns the correct URL for 'ollama'.</li>
                                        <li>The provider instance has a method named 'get_model' that returns the correct model name.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 245, 247, 249, 252-253, 255)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestGetProvider::test_unknown_raises</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case: Unknown provider raises ValueError when trying to get a provider.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where unknown providers are used without raising an error.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider` should raise a `ValueError` with the message 'unknown' when called with an unknown provider.</li>
                                        <li>The exception message should contain the string 'unknown'.</li>
                                        <li>The exception message should be case-insensitive (e.g., 'Unknown Provider').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 245, 247, 249, 252, 257, 262, 267)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestLlmProviderContract::test_noop_implements_interface</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that NoopProvider implements LlmProvider interface correctly.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the NoopProvider is not implemented as required by the LlmProvider contract.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider should have all required methods.</li>
                                        <li>The provider should have 'annotate' method.</li>
                                        <li>The provider should have 'is_available' method.</li>
                                        <li>The provider should have 'get_model_name' method.</li>
                                        <li>The provider should have 'config' attribute.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestNoopProvider::test_annotate_returns_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the annotate method of NoopProvider returns an empty annotation when no node is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the annotate method fails to return an annotation for a non-existent node.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>annotation should be an instance of LlmAnnotation</li>
                                        <li>annotation scenario should be an empty string</li>
                                        <li>annotation why needed should be an empty string</li>
                                        <li>annotation key assertions should be an empty list</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestNoopProvider::test_get_model_name_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'test_get_model_name_empty' test verifies that the 'NoopProvider' class returns an empty string when given no configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the model name is not returned correctly if no configuration is provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert provider.get_model_name() == ''</li>
                                        <li>assert provider.get_model_name() != 'noop' (to account for edge case)</li>
                                        <li>assert provider.get_model_name() != 'default' (to account for edge case)</li>
                                        <li>assert provider.get_model_name() != 'unknown' (to account for edge case)</li>
                                        <li>assert isinstance(provider.get_model_name(), str) (to ensure it's a string)</li>
                                        <li>assert len(provider.get_model_name()) == 0 (to verify the length is 0)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 66)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm.py::TestNoopProvider::test_is_available</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the NoopProvider instance is accessible and returns True for is_available method.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the provider might not be available due to configuration or initialization issues.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `is_available()` method of the `NoopProvider` class should return `True`.</li>
                                        <li>The `is_available()` method of the `NoopProvider` class should always be called before attempting to use it.</li>
                                        <li>The `Config` instance passed to the `NoopProvider` constructor should not prevent the provider from being available.</li>
                                        <li>The `noop` function defined in the `NoopProvider` class should not return any value when called without arguments.</li>
                                        <li>The `is_available()` method of the `NoopProvider` class should be able to handle cases where it is not explicitly called before use.</li>
                                        <li>The `Config` instance passed to the `NoopProvider` constructor should have a valid configuration that allows the provider to function correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 107, 110-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 58)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_emits_summary</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test annotation summary is printed when annotations run.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the annotation summary is not printed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider` of `llm.annotator` returns a `FakeProvider` instance.</li>
                                        <li>The `provider` attribute of the `TestCaseResult` object is set to `FakeProvider` instance.</li>
                                        <li>The `captured.out` variable contains the expected string 'Annotated 1 test(s) via litellm'.</li>
                                        <li>The `get_provider` function is called with a valid configuration.</li>
                                        <li>The `provider` attribute of the `TestCaseResult` object is set to a valid provider.</li>
                                        <li>The `FakeProvider` instance is used in the annotation process.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_reports_progress</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the progress report is generated for annotating tests with LLM.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the progress report is not generated correctly, potentially leading to incorrect reporting of LLM annotation status.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test case should be able to generate a correct progress report.</li>
                                        <li>The progress report should include the name of the test being annotated and its annotation ID.</li>
                                        <li>The progress report should indicate that the annotation is in progress.</li>
                                        <li>The progress report should not contain any missing or empty lines.</li>
                                        <li>The progress report should only contain messages related to LLM annotations.</li>
                                        <li>The progress report should not contain any other types of messages (e.g. test results, errors).</li>
                                        <li>The progress report should be generated for all tests annotated with LLM.</li>
                                        <li>The progress report should not be empty for the first test annotated with LLM.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_respects_opt_out_and_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM annotations respect opt-out and limit settings.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring LLM annotations do not skip opt-out tests or exceed the maximum number of tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'tests/test_a.py::test_a' node should be called when running LLM annotations with opt-out enabled.</li>
                                        <li>The first test case should have an annotation result without LLM optimisation.</li>
                                        <li>The second and third test cases should not have any annotation results.</li>
                                        <li>The number of LLM annotations generated should not exceed the maximum limit set by config.</li>
                                        <li>The 'tests/test_a.py::test_a' node should be called when running LLM annotations with a maximum of 1 tests.</li>
                                        <li>The 'tests/test_b.py::test_b' and 'tests/test_c.py::test_c' nodes should not have any annotation results.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 45, 48-49, 56-57, 59, 61-62, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_respects_rate_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that LLM annotations respect the requests-per-minute rate limit.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where LLM annotations may not respect the rate limit, potentially leading to inaccurate results or performance issues.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider.calls should contain all test nodes with the expected outcome.</li>
                                        <li>sleep_calls should contain the expected sleep duration.</li>
                                        <li>The time.sleep function is called at least twice for each test node.</li>
                                        <li>Each test node has been annotated once within the rate limit.</li>
                                        <li>No more than 30 requests per minute are made to the LLM annotation service.</li>
                                        <li>The number of calls to get_provider is equal to the number of tests passed.</li>
                                        <li>The sleep duration between calls to time.sleep is at least 2 seconds for each test node.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-173, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_skips_unavailable_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the annotation function skips unavailable providers with a suitable message.</p>
                                <p><strong>Why Needed:</strong> To prevent the annotation process from failing when an unavailable provider is detected, and provide a clear indication of what went wrong.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test verifies that the annotation function correctly skips annotation when an unavailable provider is present.</li>
                                        <li>The test verifies that the message provided by the annotation function is accurate and informative.</li>
                                        <li>The test verifies that the annotation process does not fail unexpectedly due to an unavailable provider.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_uses_cache</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests that annotations are cached between runs and that the annotation function calls a provider when needed.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the annotation function is called without a previous run, potentially causing issues with cache consistency.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider.calls` assertion checks if the `get_provider` method of the `FakeProvider` instance was called before and after calling `annotate_tests`.</li>
                                        <li>The `test.llm_annotation` assertion checks if it is not `None` after calling `annotate_tests`.</li>
                                        <li>The `test.llm_annotation.scenario` assertion checks that the scenario used by `test_llm_annotation` matches 'cached'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">30 lines (ranges: 39-41, 53, 55-56, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-81, 87-92, 97-98, 100, 102, 104, 115-122, 127, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `test_required_fields` test verifies that the schema has a 'required' field.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the schema is not properly validated without the required fields.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert 'scenario' in required</li>
                                        <li>assert 'why_needed' in required</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_from_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the AnnotationSchema can correctly parse a dictionary containing required keys.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues where the AnnotationSchema is not properly configured or validated against expected input data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>checks password</li>
                                        <li>checks username</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_empty</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_empty</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the annotation schema does not handle empty input correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>schema.scenario == ""</li>
                                        <li>schema.why_needed == ""</li>
                                        <li>schema.scenario == ""</li>
                                        <li>schema.why_needed == ""</li>
                                        <li>schema.scenario == ""</li>
                                        <li>schema.why_needed == ""</li>
                                        <li>schema.scenario == ""</li>
                                        <li>schema.why_needed == ""</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_partial</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the AnnotationSchema handles partial inputs correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the AnnotationSchema does not handle partial input correctly, potentially leading to incorrect results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The schema's scenario attribute is set to 'Partial only'.</li>
                                        <li>The schema's why_needed attribute is empty. This indicates that there are no specific requirements for handling partial inputs.</li>
                                        <li>The assertion checks if the schema's scenario and why_needed attributes match the expected values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_has_required_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the schema has required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the schema is not properly defined with required fields, potentially leading to errors or inconsistencies.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert 'scenario' in ANNOTATION_JSON_SCHEMA['properties']</li>
                                        <li>assert 'why_needed' in ANNOTATION_JSON_SCHEMA['properties']</li>
                                        <li>assert 'key_assertions' in ANNOTATION_JSON_SCHEMA['properties']</li>
                                        <li>assert isinstance(ANNOTATION_JSON_SCHEMA, dict)</li>
                                        <li>assert ANNOTATION_JSON_SCHEMA is not None</li>
                                        <li>assert type(ANNOTATION_JSON_SCHEMA) == dict</li>
                                        <li>assert len(ANNOTATION_JSON_SCHEMA) > 0</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `AnnotationSchema` instance correctly serializes to a dictionary.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring the correct serialization of the schema to a dictionary.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assertion 1: The value of 'scenario' in the resulting dictionary matches the specified scenario.</li>
                                        <li>assertion 2: The value of 'why_needed' in the resulting dictionary matches the specified why_needed.</li>
                                        <li>assertion 3: The key 'key_assertions' is present in the resulting dictionary and contains the expected list of assertions.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 90-92, 94-96, 98)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_from_factory</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the factory method to return a NoopProvider when the provider is set to 'none'.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the factory returns an incorrect provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `get_provider(config)` should return a NoopProvider instance for the given configuration.</li>
                                        <li>The `isinstance(provider, NoopProvider)` assertion should pass when the returned provider is indeed a NoopProvider.</li>
                                        <li>The test should fail when the provider is set to 'none' as expected.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_is_llm_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `test_noop_is_llm_provider` test verifies that the `NoopProvider` class correctly inherits from `LlmProvider`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the `NoopProvider` class is incorrectly implemented as an LLM provider instead of a NoOp provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` variable should be an instance of `LlmProvider`.</li>
                                        <li>The `provider` variable should have a type hint of `LlmProvider`.</li>
                                        <li>The `provider` variable should not contain any LLM-related code.</li>
                                        <li>The `provider` variable should only contain NoOp-related code.</li>
                                        <li>The `provider` variable should be able to be instantiated with the correct configuration.</li>
                                        <li>The `NoopProvider` class should correctly inherit from `LlmProvider` without any issues.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_returns_empty_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> NoopProvider returns empty annotation when no function is annotated with @noop.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the NoopProvider does not return an annotation for functions that are not annotated with @noop.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation returned by the NoopProvider should be empty.</li>
                                        <li>The scenario of this test should be an empty string.</li>
                                        <li>The why_needed message should indicate that the NoopProvider should return an annotation for functions without @noop annotations.</li>
                                        <li>The key_assertions should include a check for an empty annotation.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_annotate_returns_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `annotate` method of the `ProviderContract` returns an instance of `TestCaseResult` with the expected attributes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `annotate` method does not return an instance of `TestCaseResult`, potentially causing issues downstream in the testing pipeline.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result has a 'scenario' attribute</li>
                                        <li>The result has a 'why_needed' attribute</li>
                                        <li>The result has a 'key_assertions' attribute</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_handles_empty_code</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the ProviderContract handles an empty code node gracefully.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where an empty code node would cause the contract to fail or produce incorrect results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>...</li>
                                        <li>...</li>
                                        <li>...</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_handles_none_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `provider.annotate` method should handle a `None` context for the `code` field without raising an error or returning an empty value.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential issues where the `provider.annotate` method returns an incorrect result when given a `None` context for the `code` field, potentially causing downstream errors in the LLM contract.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `result` variable is not `None` after calling `provider.annotate(test, 'code', None)`.</li>
                                        <li>The `result` variable contains valid data (e.g., a dictionary or an object) for the `code` field.</li>
                                        <li>The `result` variable has the correct type (e.g., `dict`, `object`) for the `code` field.</li>
                                        <li>The `provider.annotate` method is called with the correct arguments (`test`, `'code'`, `None`).</li>
                                        <li>The `provider.annotate` method returns a value that can be used to update the test result correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_has_annotate_method</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> All providers should have an annotate method.</p>
                                <p><strong>Why Needed:</strong> The test prevents a potential bug where the annotate method is missing for some providers.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider_name in ['none', 'ollama', 'litellm', 'gemini']</li>
                                        <li>hasattr(provider, 'annotate')</li>
                                        <li>callable(provider.annotate)</li>
                                        <li>for provider_name in ['none', 'ollama', 'litellm', 'gemini']: has_attr(provider, 'annotate')</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 52-53, 245, 247, 249-250, 252-253, 255, 257-258, 260, 262-263, 265)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/noop.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_handles_context_too_large</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `annotate` method handles large contexts correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the `annotate` method fails with an error when dealing with very large contexts.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The context is annotated successfully without raising any exceptions.</li>
                                        <li>The annotation does not exceed the maximum allowed size.</li>
                                        <li>The `annotate` method returns an empty list for very large contexts.</li>
                                        <li>No errors are raised when annotating a very large context.</li>
                                        <li>The context is properly cleaned up after annotation.</li>
                                        <li>The `annotate` method handles very large contexts without significant performance impact.</li>
                                        <li>The test passes even with the most extreme values of the context dimension.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 52-53, 72, 75-76, 80, 165, 167, 175)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">155 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-221, 233, 245-248, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417-418, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_dependency</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the LiteLLMProvider annotates missing dependencies correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider does not report missing dependencies, potentially leading to silent failures or incorrect results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation error message is correct and includes the name of the missing dependency.</li>
                                        <li>The annotation error message includes the correct installation command for the missing dependency.</li>
                                        <li>The annotation error message includes the full path to the missing dependency.</li>
                                        <li>The annotation error message does not include any misleading or incomplete information about the missing dependency.</li>
                                        <li>The annotation error message is consistent across different environments and test runs.</li>
                                        <li>The annotation error message is clear and easy to understand, even for users who are not familiar with pip or dependencies.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 134, 136-139, 141-142, 160-164)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_token</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the GeminiProvider annotates a missing token in the configuration.</p>
                                <p><strong>Why Needed:</strong> To prevent a bug where a missing API token causes the provider to fail.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `GEMINI_API_TOKEN` environment variable is not set before calling `annotate()`</li>
                                        <li>The `GEMINI_API_TOKEN` environment variable is set but empty (i.e., an error) after calling `annotate()`</li>
                                        <li>The `GEMINI_API_TOKEN` environment variable is set and has a non-empty value after calling `annotate()`</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 134, 136-139, 141-142, 160-161, 167-169)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_records_tokens</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that tokens are recorded and rate limits logic runs correctly.</p>
                                <p><strong>Why Needed:</strong> Prevents regressions due to incorrect token usage or missing rate limit checks.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `annotate` method of the `GeminiProvider` instance records a token on the limiter.</li>
                                        <li>The `annotate` method of the `GeminiProvider` instance checks for and reports the total number of tokens used by the provider.</li>
                                        <li>The rate limits logic is executed without errors or exceptions.</li>
                                        <li>The number of tokens recorded matches the expected value (123) in the response metadata.</li>
                                        <li>The limiter's token usage list contains only one entry with a count of 123.</li>
                                        <li>The `annotate` method does not raise an exception when called.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">183 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-223, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-343, 346, 348-350, 352-355, 360-366, 368, 370-371, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_retries_on_rate_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the LLM provider annotates retries on rate limits correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the provider does not retry when rate limiting occurs.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The provider should retry after a certain time (e.g. 1 minute) if the API call exceeds the rate limit.</li>
                                        <li>The provider should retry with a different set of parameters (e.g. query, model) to avoid rate limiting.</li>
                                        <li>The provider should not retry immediately after the rate limit is exceeded.</li>
                                        <li>The provider should retry within a certain time window (e.g. 5 minutes) if the API call exceeds the rate limit.</li>
                                        <li>The provider should retry with a different set of parameters (e.g. query, model) to avoid rate limiting and ensure consistent results.</li>
                                        <li>The provider should not retry immediately after the rate limit is exceeded and the previous retry failed.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">181 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 233-234, 238-240, 242-243, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330-333, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-369, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_rotates_models_on_daily_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `annotate` method of the `GeminiProvider` class rotates models on the daily limit when called with a large number of annotations.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the LLM provider's behavior, ensuring that it correctly handles high annotation loads without exceeding the daily limit.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `annotate` method calls `self._rotate_models_on_daily_limit()` after annotating a large number of models.</li>
                                        <li>The total number of annotations annotated does not exceed the daily limit (1000).</li>
                                        <li>The time taken to annotate a large number of models is within a reasonable range (less than 10 seconds).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">177 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419-420, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_skips_on_daily_limit</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The `test_annotate_skips_on_daily_limit` test verifies that the annotation skips when exceeding daily limit.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the annotation does not skip when exceeding the daily limit, potentially causing issues with downstream processing.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `annotation` attribute is set to `True` before exceeding the daily limit.</li>
                                        <li>The `annotation` attribute is reset to `False` after exceeding the daily limit.</li>
                                        <li>The `skips_on_daily_limit` attribute is not updated when exceeding the daily limit.</li>
                                        <li>The `annotation` attribute is only set to `True` if the `skips_on_daily_limit` attribute is `False` before exceeding the daily limit.</li>
                                        <li>Exceeding the daily limit does not cause the annotation to be skipped.</li>
                                        <li>The test passes even when the `skips_on_daily_limit` attribute is `True` but the `annotation` attribute is still set to `True` after exceeding the daily limit.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">184 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-208, 210-211, 213-214, 217-222, 225-227, 252-254, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_success_with_mock_response</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LiteLLM provider annotates a valid response correctly.</p>
                                <p><strong>Why Needed:</strong> Prevents regressions caused by incorrect annotation of invalid responses.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>status ok</li>
                                        <li>redirect</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">177 lines (ranges: 39-42, 45-46, 48-49, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_exhausted_model_recovers_after_24h</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the exhausted model recovers after 24 hours.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the model does not recover from exhaustion.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The recovered model should have the same accuracy as before exhaustion.</li>
                                        <li>The recovered model should have the same number of parameters as before exhaustion.</li>
                                        <li>The recovered model's training time should be less than or equal to 24 hours.</li>
                                        <li>The recovered model's inference time should be within a reasonable range (e.g., < 10 seconds).</li>
                                        <li>The recovered model's memory usage should not increase significantly after 24 hours.</li>
                                        <li>The recovered model's performance metrics (e.g., F1 score, mean squared error) should remain stable or improve slightly after exhaustion.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">190 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-188, 190-191, 193-194, 196, 200-208, 210-211, 213-214, 217-222, 225-227, 252-254, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_fetch_available_models_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `fetch_available_models` method returns an error when no models are available.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the `fetch_available_models` method might return an error even when there are models available in the cache.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assertRaises(SystemExit) with pytest.raises(SystemExit)</li>
                                        <li>assertRaises(SystemExit) with mock.patch.object(monkeypatch, 'raise')</li>
                                        <li>the returned value is a SystemExit exception</li>
                                        <li>the error message contains 'no models found'</li>
                                        <li>the error message does not contain any other information</li>
                                        <li>the error message does not indicate that the cache is exhausted</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 134, 136-139, 141-142, 280, 282-283, 286-290, 292-295, 297-298, 300-301, 346, 348-350, 352-355, 360-363, 374-377, 385, 387, 391-392, 396-402, 405, 408-410, 412-414, 417-418, 428, 430-432, 435-436)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_model_list_refreshes_after_interval</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The model list should refresh after an interval of 5 seconds.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the model list does not update after a short period of time.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `refresh_interval` attribute is set to 5000 (5 seconds) before each test call.</li>
                                        <li>The `refreshed_model_list` attribute is an empty list at the start of each test call.</li>
                                        <li>After calling `refresh_model_list()`, the `refreshed_model_list` attribute contains a non-empty list of models.</li>
                                        <li>The length of the `refreshed_model_list` attribute increases by 1 after each test call.</li>
                                        <li>The `refresh_interval` attribute is not set to 0 when the test function is called multiple times in quick succession.</li>
                                        <li>The `refresh_model_list()` method does not modify the original model list but returns a new one instead.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/gemini.py</span>
                                    <span style="color: var(--text-secondary)">169 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-369, 374-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_handles_completion_error</span>
                        <div class="test-meta">
                            <span>6.00s</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the LiteLLMProvider annotates completion errors correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the LLM provider does not surface completion errors during annotation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation should contain an error message indicating a completion error.</li>
                                        <li>The error message should be 'boom'.</li>
                                        <li>The annotation should have an error attribute that contains the string 'boom'.</li>
                                        <li>The annotation should indicate that the test case failed due to a completion error.</li>
                                        <li>The annotation should not return any value (i.e., it should be a CaseResult with outcome='passed').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 78-79, 81-82, 84-85)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_invalid_key_assertions</span>
                        <div class="test-meta">
                            <span>6.00s</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LiteLLMProvider rejects invalid key_assertions payloads.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the provider incorrectly accepts or ignores invalid key_assertions payloads.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Invalid response: key_assertions must be a list</li>
                                        <li>Key assertion 'oops' is not a valid JSON object</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207-209)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69, 73, 76, 81-82, 84-85)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_missing_dependency</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The LiteLLMProvider annotates a missing dependency in the provided test case.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the LiteLLM provider incorrectly reports an error when a required library is not installed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation message includes the correct error message for installing the 'litellm' library.</li>
                                        <li>The annotation message does not include any misleading or incorrect information about the installation process.</li>
                                        <li>The annotation message provides a clear and concise way to inform users of the required dependency.</li>
                                        <li>The test case is able to pass successfully even if the 'litellm' library is not installed.</li>
                                        <li>The test case can be relied upon to provide accurate and helpful error messages for missing dependencies.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 37-41)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_success_with_mock_response</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the LiteLLM provider annotates a successful response with the correct key assertions and confidence level.</p>
                                <p><strong>Why Needed:</strong> This test prevents regressions by ensuring that the provider correctly annotates responses as 'status ok' and redirects to the expected URL.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation should contain the correct scenario.</li>
                                        <li>The annotation should contain the correct why needed message.</li>
                                        <li>The annotation should include the key assertions in its confidence level.</li>
                                        <li>The annotation should capture the model used by the provider.</li>
                                        <li>The annotation should capture the messages sent to the user during the login process.</li>
                                        <li>The annotation should include the expected URL redirection in its content.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69-70)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_is_available_with_module</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the LiteLLM provider detects installed 'litellm' module.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the provider does not detect the 'litellm' module if it is not installed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'litellm' module should be present in sys.modules.</li>
                                        <li>The 'litellm' module should have been imported successfully.</li>
                                        <li>The 'litellm' module should be available as a provider for the LiteLLMProvider.</li>
                                        <li>The 'litellm' module should not cause any import errors when trying to instantiate it.</li>
                                        <li>The 'litellm' module should have its __name__ attribute set correctly.</li>
                                        <li>The 'litellm' module should have been registered in sys.modules with the correct key.</li>
                                        <li>The provider for 'litellm' should be available and functional.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 107, 110-111)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 94-95, 97)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_fallbacks_on_context_length_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the annotate function handles context length errors correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the annotate function fails to handle context length errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>It should not raise an exception when the context length is too large.</li>
                                        <li>It should return an error message indicating that the context length is too large.</li>
                                        <li>The returned error message should include information about the maximum allowed context length.</li>
                                        <li>The function should handle cases where the input is a list or tuple with more elements than the maximum allowed context length.</li>
                                        <li>The function should not raise an exception when the input is a string with more characters than the maximum allowed context length.</li>
                                        <li>The returned error message should be informative and easy to understand for users of the annotate function.</li>
                                        <li>The test should pass even if the input is a list or tuple that exceeds the maximum allowed context length by one element.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">33 lines (ranges: 52-53, 72, 75-76, 78, 165, 167-173, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">15 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-60, 62-63, 66-67)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_handles_call_error</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the Ollama provider correctly annotates a call to _call_ollama with an error message when it fails.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in handling call errors, ensuring that the annotation is accurate and informative even when the underlying function raises an exception.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation should include the string 'Failed after 3 retries. Last error: boom'.</li>
                                        <li>The annotation should indicate that the call failed with a RuntimeError.</li>
                                        <li>The annotation should specify the exact error message 'boom' to avoid confusion with other potential errors.</li>
                                        <li>The annotation should not include any additional information about the underlying system prompt.</li>
                                        <li>The annotation should only include the error message, without any context or details about the call.</li>
                                        <li>The annotation should be consistent across all test cases that use the Ollama provider.</li>
                                        <li>The annotation should not interfere with other tests that may also annotate calls to _call_ollama with different error messages.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-59, 71-72, 74-75, 77-78)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_missing_httpx</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The OllamaProvider should report an error when annotating a function without the required httpx dependency.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider incorrectly reports missing dependencies, potentially leading to incorrect or misleading error messages.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The annotation message should include the correct error message indicating that httpx is not installed.</li>
                                        <li>The annotation message should be specific about which dependency is required (httpx) and how to install it (pip install httpx).</li>
                                        <li>The test case should pass with an error message that indicates the missing dependency and provides a clear solution for installation.</li>
                                        <li>The provider's behavior should change when the test_case function is annotated, indicating that the annotation was successful despite the missing dependency.</li>
                                        <li>The test result should be marked as passed even if the annotation fails, to reflect the correct outcome of the test.</li>
                                        <li>The error message should not include any misleading information or assumptions about the user's environment.</li>
                                        <li>The provider should correctly report the version of httpx required for the test_case function.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 40-44)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_success_full_flow</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the full annotation flow of an Ollama provider with mocked HTTP responses.</p>
                                <p><strong>Why Needed:</strong> Prevents authentication bugs by ensuring that the correct response is returned from the API.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Check status to ensure the API returns a successful response</li>
                                        <li>Validate token to ensure it's properly authenticated</li>
                                        <li>Verify the response contains the expected JSON structure</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-60, 62-63, 114, 116-123, 127-130, 132, 134-135)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_success</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Ollama provider makes correct API call.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in Ollama provider's API call functionality.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The response from the Ollama model is 'test response'.</li>
                                        <li>The URL of the API call is set to `http://localhost:11434/api/generate`.</li>
                                        <li>The model used by the Ollama provider is correctly identified as `llama3.2:1b`.</li>
                                        <li>The prompt used for the API call is set to `test prompt`.</li>
                                        <li>The system prompt used for the API call is set to `system prompt`.</li>
                                        <li>The stream parameter of the response is set to `False`.</li>
                                        <li>The timeout of the API call is correctly set to 60 seconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 114, 116-123, 127-130, 132, 134-135)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_uses_default_model</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the Ollama provider uses the default model when not specified.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the default model is not used by the provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'model' key in the captured response should be set to 'llama3.2'.</li>
                                        <li>The 'model' key in the captured response should contain only strings or None.</li>
                                        <li>The 'model' key in the captured response should not be empty.</li>
                                        <li>The default model 'llama3.2' is present in the captured response.</li>
                                        <li>The absence of a specified model is handled correctly and the default model is used.</li>
                                        <li>The provider's _call_ollama method does not raise an error when no model is provided.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 114, 116-123, 127-130, 132, 134-135)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the Ollama provider returns False when the server is unavailable.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the Ollama provider fails to return an error when the server is not running.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The method _check_availability() of the OllamaProvider instance should raise a ConnectionError exception when the server is unavailable.</li>
                                        <li>The method _check_availability() of the OllamaProvider instance should set the 'is_available' attribute to False.</li>
                                        <li>The method _check_availability() of the OllamaProvider instance should not return any value (i.e., it should be a no-op).</li>
                                        <li>The method _check_availability() of the OllamaProvider instance should raise an exception when the server is unavailable.</li>
                                        <li>The 'is_available' attribute of the provider instance should be set to False after calling _check_availability().</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 87-88, 90-91, 93-94)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_non_200</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the Ollama provider returns False for non-200 status codes when checking availability.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the provider incorrectly returns True for non-200 status codes, leading to incorrect usage of the API.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The method _check_availability() is called on the OllamaProvider instance.</li>
                                        <li>The status code returned by the _check_availability() method is not 200.</li>
                                        <li>The provider's availability is set to False.</li>
                                        <li>The provider does not return True for non-200 status codes.</li>
                                        <li>The provider raises an exception when checking availability with a non-200 status code.</li>
                                        <li>The provider returns a response with a status code greater than 200.</li>
                                        <li>The provider sets the 'Content-Type' header of the response to 'application/json'.</li>
                                        <li>The provider does not set the 'Content-Type' header of the response to 'application/json' for non-200 status codes.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 87-88, 90-92)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_success</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the Ollama provider checks availability via /api/tags endpoint and returns True when successful.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider does not check for availability before returning, potentially leading to unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The '/api/tags' URL is present in the provided URL.</li>
                                        <li>The response status code is 200.</li>
                                        <li>The provider returns True when checking availability.</li>
                                        <li>The provider's _check_availability method is called with no arguments.</li>
                                        <li>The provider does not raise an exception if it cannot connect to the Ollama host.</li>
                                        <li>The provider does not log any errors or warnings.</li>
                                        <li>The provider's configuration is valid and matches the expected values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 87-88, 90-92)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_is_local_returns_true</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The Ollama provider should always return `is_local=True`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the provider might incorrectly return `False` for local configurations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>provider.is_local() == True</li>
                                        <li>provider.config.provider == 'ollama'</li>
                                        <li>provider.config is not None</li>
                                        <li>is_local() is True</li>
                                        <li>not is_local() is False</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/ollama.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 102)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_json</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `OllamaProvider` class throws an error when parsing a response with invalid JSON.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the Ollama provider incorrectly reports valid responses as invalid JSON.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>annotation.error == 'Failed to parse LLM response as JSON'</li>
                                        <li>provider._parse_response('not-json') is not None</li>
                                        <li>provider._parse_response('not-json').error != 'Invalid JSON'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 52-53, 186-187, 190-192)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-52, 55)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_key_assertions</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the Ollama provider rejects invalid key_assertions payloads in its _parse_response method.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the Ollama provider incorrectly accepts or ignores invalid key_assertions payloads.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>the response data should be a dictionary with a 'key_assertions' key</li>
                                        <li>the 'key_assertions' value should be a list of strings</li>
                                        <li>the error message should indicate that the 'key_assertions' field is required and must be a list</li>
                                        <li>the provider should reject any invalid or missing 'key_assertions' payloads</li>
                                        <li>the provider should raise an exception when encountering an invalid 'key_assertions' payload</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207-209)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The provided test verifies that the Ollama provider correctly parses a JSON response from a markdown code fence.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential bugs where the provider fails to extract JSON from markdown code fences, potentially leading to incorrect or incomplete annotations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The extracted JSON is a valid JSON object.</li>
                                        <li>The JSON object contains only strings and numbers.</li>
                                        <li>The JSON object does not contain any arrays or objects with nested keys.</li>
                                        <li>The JSON object has the correct indentation (4 spaces).</li>
                                        <li>No null values are present in the JSON object.</li>
                                        <li>The JSON object does not contain any undefined variables or functions.</li>
                                        <li>All string values in the JSON object are enclosed in double quotes.</li>
                                        <li>The JSON object is a valid JSON string.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 38, 42-44, 46-47)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_plain_fence</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The provided test verifies that the Ollama provider can extract JSON from a plain markdown fence without any language specified.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case the provider encounters an issue when parsing responses with no language specified.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The response is not empty.</li>
                                        <li>The response starts with `"`.</li>
                                        <li>The response ends with `""`.</li>
                                        <li>Each line of the response contains only two characters (either `"` or `""`).</li>
                                        <li>There are no special markdown syntaxes in the response.</li>
                                        <li>The provider correctly extracts JSON from the response without any language specified.</li>
                                        <li>The extracted JSON is a valid Python dictionary.</li>
                                        <li>The extracted JSON does not contain any invalid characters.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 38, 42-44, 46-47)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_success</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the Ollama provider correctly parses valid JSON responses.</p>
                                <p><strong>Why Needed:</strong> Prevents bugs in the LLM providers by ensuring they return expected results for successful responses.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert a is not None</li>
                                        <li>assert b is not None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestArtifactEntry::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `CoverageEntry` class correctly serializes its attributes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the serialized data does not match the expected format.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `file_path` attribute is set to 'src/foo.py'.</li>
                                        <li>The `line_ranges` attribute is set to '1-3, 5, 10-15'.</li>
                                        <li>The `line_count` attribute is set to 10.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 254-257)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestCollectionError::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `CoverageEntry.to_dict()` correctly serializes the test collection.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the serialized test collection is incorrect or incomplete.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key in the dictionary should match the expected value.</li>
                                        <li>The 'line_ranges' key in the dictionary should match the expected value.</li>
                                        <li>The 'line_count' key in the dictionary should match the expected value.</li>
                                        <li>All required keys ('file_path', 'line_ranges', and 'line_count') are present in the dictionary.</li>
                                        <li>The values of the 'file_path', 'line_ranges', and 'line_count' keys are correct and follow the expected format.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 207-209)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestCoverageEntry::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test CoverageEntry serialization correctness.</p>
                                <p><strong>Why Needed:</strong> To prevent a bug where the coverage entry's line ranges are not correctly serialized.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key should be present and contain the expected value.</li>
                                        <li>The 'line_ranges' key should be present and contain the correct string representation.</li>
                                        <li>The 'line_count' key should be present and contain the expected value.</li>
                                        <li>The line ranges in the 'line_ranges' key should be correctly formatted (e.g., '1-3, 5, 10-15')</li>
                                        <li>Each range in the 'line_ranges' key should be a valid Python range object (e.g., [1, 3], [5, 7])</li>
                                        <li>The line count should be an integer value (0 or positive)</li>
                                        <li>If the file path is empty, the 'file_path' key should still contain an empty string.</li>
                                        <li>If the line ranges are not correctly formatted, they should raise a ValueError.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 40-43)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_empty_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> An empty annotation should be created with default values.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where an empty annotation would cause the model to fail or produce incorrect results without any specified parameters.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>annotation.scenario == "" (empty string)</li>
                                        <li>annotation.why_needed == "" (empty string) (to prevent model failure)</li>
                                        <li>annotation.key_assertions == [] (to ensure no specific keys are set)</li>
                                        <li>assert annotation.confidence is None (to prevent incorrect confidence values)</li>
                                        <li>assert annotation.error is None (to prevent incorrect error messages)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_to_dict_minimal</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `LlmAnnotation` object's `to_dict()` method returns a dictionary with required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the minimal annotation is correctly serialized without any optional fields.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'scenario' key should be present in the dictionary.</li>
                                        <li>The 'why_needed' key should be present in the dictionary.</li>
                                        <li>The 'key_assertions' key should be present in the dictionary.</li>
                                        <li>The 'confidence' field should not be included in the dictionary when it is None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 104-107, 109, 111, 113, 115)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_to_dict_with_all_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the full annotation is included in the dictionary.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where only partial fields are included in the dictionary.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'scenario' field should be present and have the correct value.</li>
                                        <li>The 'confidence' field should have a value greater than or equal to 0.95.</li>
                                        <li>The 'context_summary' field should contain the expected keys ('mode' and 'bytes') with correct values.</li>
                                        <li>All other required fields (error, token_present) should be present and have correct values.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 104-107, 109-111, 113-115)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_default_report</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Default Report</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the report does not include required schema version and collection error information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'schema_version' key should be present in the dictionary with value equal to SCHEMA_VERSION.</li>
                                        <li>The 'tests' key should be an empty list.</li>
                                        <li>The 'warnings' key should not be included in the dictionary (excluding the empty lists).</li>
                                        <li>The 'collection_errors' key should also not be included in the dictionary (excluding the empty lists).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_report_with_collection_errors</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Report with Collection Errors should be verified by checking the presence of collection errors in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where a report might not include all collection errors, potentially leading to incorrect reporting or missed issues.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'collection_errors' key is present in the report dictionary and contains exactly one error.</li>
                                        <li>The value of 'nodeid' for the first error is set to 'test_bad.py'.</li>
                                        <li>All collection errors are included in the report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">58 lines (ranges: 207-209, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508-510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_report_with_warnings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that a ReportRoot instance with warnings is properly reported.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the report does not include all warnings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of `d['warnings']` should be equal to 1.</li>
                                        <li>The value of `d['warnings'][0]['code']` should be 'W001'.</li>
                                        <li>All warnings in the report should have a non-empty code.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">60 lines (ranges: 229-231, 233, 235, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportRoot::test_tests_sorted_by_nodeid</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests should be sorted by nodeid in output.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the order of tests is not guaranteed by their nodeids.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The list of nodeids returned by `to_dict()` matches the expected ordering.</li>
                                        <li>Each nodeid appears once in the list.</li>
                                        <li>All nodeids appear before any other test results.</li>
                                        <li>Nodeids are present in the order they were defined (a_test.py::test_a, m_test.py::test_m, z_test.py::test_z).</li>
                                        <li>The nodeids are sorted alphabetically (z_test.py::test_z comes first).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">71 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportWarning::test_to_dict_with_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test `test_to_dict_with_detail` verifies that a `ReportWarning` instance is converted to a dictionary with the correct 'detail' key.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the 'detail' field in the warning dictionary is not correctly populated for certain types of warnings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of the 'detail' key in the warning dictionary should be '/path/to/file'.</li>
                                        <li>The 'detail' key should contain the path to the file that triggered the warning.</li>
                                        <li>The 'detail' field should not be missing from the warning dictionary.</li>
                                        <li>The 'detail' field should have a non-empty value for warnings with a code of 'W001'.</li>
                                        <li>The 'detail' field should include the message and code of the warning. In this case, it includes only the message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 229-231, 233-235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestReportWarning::test_to_dict_without_detail</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `to_dict()` method of `ReportWarning` returns a dictionary with only 'code' and 'message' keys.</p>
                                <p><strong>Why Needed:</strong> This test prevents a warning from being reported when a `ReportWarning` object is created without specifying any details.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The expected output should be a dictionary with 'code' and 'message' keys.</li>
                                        <li>The 'detail' key should not be present in the dictionary.</li>
                                        <li>The value of 'code' should be exactly 'W001'.</li>
                                        <li>The value of 'message' should be exactly 'No coverage'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 229-231, 233, 235)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_aggregation_fields_present</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test RunMeta to ensure it has aggregation fields.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where RunMeta is missing necessary aggregation fields.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'run_id' key should contain the value 'run-123'.</li>
                                        <li>The 'run_group_id' key should contain the value 'group-456'.</li>
                                        <li>The 'is_aggregated' key should be True.</li>
                                        <li>The 'aggregation_policy' key should be set to 'merge'.</li>
                                        <li>The 'run_count' key should have a value of 3.</li>
                                        <li>The length of 'source_reports' should be equal to 2.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 277-279, 281-283, 364-380, 382, 385, 387, 390, 393, 395, 397, 399-405, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_llm_fields_excluded_when_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests that LLM fields are excluded when annotations are disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the LLM fields are included even when annotations are disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm_annotations_enabled' key is not present in the data.</li>
                                        <li>The 'llm_provider' key is not present in the data.</li>
                                        <li>The 'llm_model' key is not present in the data.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_llm_traceability_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that LLM traceability fields are included when enabled.</p>
                                <p><strong>Why Needed:</strong> Prevent regression in model tracing functionality by ensuring all required fields are present.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>data['llm_annotations_enabled'] is True</li>
                                        <li>data['llm_provider'] == 'ollama'</li>
                                        <li>data['llm_model'] == 'llama3.2:1b'</li>
                                        <li>data['llm_context_mode'] == 'complete'</li>
                                        <li>data['llm_annotations_count'] == 10</li>
                                        <li>data['llm_annotations_errors'] == 2</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407-419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_non_aggregated_excludes_source_reports</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'Non-aggregated report should not include source_reports' verifies that a non-aggregated run does not include source reports.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the source reports are included in non-aggregated runs, which can be misleading and cause confusion.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'source_reports' key is present in the dictionary.</li>
                                        <li>The value of 'is_aggregated' is set to False.</li>
                                        <li>The presence of 'source_reports' in the dictionary indicates an aggregated report.</li>
                                        <li>A non-aggregated run should not include source reports.</li>
                                        <li>The absence of 'source_reports' in the dictionary suggests that a non-aggregated run does not contain source reports.</li>
                                        <li>Including source reports in a non-aggregated run can be misleading and cause confusion.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_run_meta_to_dict_full</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test RunMeta to dict with all optional fields.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of missing or outdated plugin version, which could lead to incorrect aggregation policy.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'git_sha' field is set to 'abc1234'.</li>
                                        <li>The 'git_dirty' field is True.</li>
                                        <li>The 'repo_version' field is set to '1.0.0'.</li>
                                        <li>The 'repo_git_sha' field is set to 'abc1234'.</li>
                                        <li>The 'repo_git_dirty' field is True.</li>
                                        <li>The 'plugin_git_sha' field is set to 'def5678'.</li>
                                        <li>The 'plugin_git_dirty' field is False.</li>
                                        <li>The 'config_hash' field is set to 'def5678'.</li>
                                        <li>The length of the source reports list is 1.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">49 lines (ranges: 277-279, 281-283, 364-380, 382-405, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestRunMeta::test_run_status_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test RunMeta includes run status fields.</p>
                                <p><strong>Why Needed:</strong> The test prevents a potential bug where the 'RunMeta' object is missing certain required fields for accurate representation of its run status.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert d['exit_code'] == 1</li>
                                        <li>assert d['interrupted'] is True</li>
                                        <li>assert d['collect_only'] is True</li>
                                        <li>assert d['collected_count'] == 10</li>
                                        <li>assert d['selected_count'] == 8</li>
                                        <li>assert d['deselected_count'] == 2</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSchemaVersion::test_schema_version_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies the schema version is formatted as a semver string.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the schema version is not in a valid semver format.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The schema version should be split into three parts (major, minor, patch).</li>
                                        <li>Each part of the schema version should be a digit.</li>
                                        <li>All digits in the schema version should be non-zero.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSchemaVersion::test_schema_version_in_report_root</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `ReportRoot` class includes the correct `schema_version`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the schema version is not included in the report root.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report.schema_version` attribute should be set to `SCHEMA_VERSION`.</li>
                                        <li>The `to_dict()` method of `ReportRoot` should return an object with a `schema_version` key set to `SCHEMA_VERSION`.</li>
                                        <li>The value of the `schema_version` key in the `to_dict()` output should match `SCHEMA_VERSION`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSourceCoverageEntry::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test coverage entry serialization.</p>
                                <p><strong>Why Needed:</strong> CoverageEntry should be able to serialize correctly without any errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key is present and contains the expected value.</li>
                                        <li>The 'line_ranges' key is present and contains the expected values.</li>
                                        <li>The 'line_count' key is present and contains the expected value.</li>
                                        <li>Each assertion passes, indicating coverage entry serialization is correct.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 71-78)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSourceReport::test_to_dict_minimal</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `to_dict` method of `LlmAnnotation` returns a dictionary with required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the minimal annotation format includes all necessary information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'scenario' key should be present in the dictionary.</li>
                                        <li>The 'why_needed' key should be present in the dictionary.</li>
                                        <li>The 'key_assertions' key should be present in the dictionary.</li>
                                        <li>The 'confidence' key should not be present in the dictionary when it is None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 277-279, 281, 283)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSourceReport::test_to_dict_with_run_id</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `to_dict()` method of `SourceReport` with a `run_id`.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where a `SourceReport` instance is created without a `run_id`.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `run_id` key in the resulting dictionary should be 'run-1'.</li>
                                        <li>The value of the `run_id` key should match the provided `run_id`.</li>
                                        <li>If no `run_id` is provided, the `run_id` key should still be present but empty ('').</li>
                                        <li>If an invalid `run_id` (e.g., not a string) is provided, it should raise an error.</li>
                                        <li>The `to_dict()` method should return a dictionary with the required keys (`run_id`) and value.</li>
                                        <li>The `to_dict()` method should handle cases where the `SourceReport` instance has no `run_id` or an empty string for `run_id`.</li>
                                        <li>If the `run_id` is not present in the resulting dictionary, it should be ignored.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 277-279, 281-283)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestSummary::test_to_dict</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests coverage-related functionality.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the `CoverageEntry` class does not correctly serialize to JSON.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'file_path' key in the dictionary should be equal to the expected value.</li>
                                        <li>The 'line_ranges' key in the dictionary should be equal to the expected value.</li>
                                        <li>The 'line_count' key in the dictionary should be equal to the expected value.</li>
                                        <li>The 'CoverageEntry' class correctly serializes to JSON when creating a `CoverageEntry` object.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 449-457, 459, 461)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_minimal_result</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that a minimal result has the required fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in cases where a minimal result is expected but not provided.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'nodeid' field of the minimal result should be set to 'test_foo.py::test_bar'.</li>
                                        <li>The 'outcome' field of the minimal result should be set to 'passed'.</li>
                                        <li>The 'duration' field of the minimal result should be set to 0.0.</li>
                                        <li>The 'phase' field of the minimal result should be set to 'call'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the `TestCaseResult` includes a coverage list.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the `CoverageEntry` is included in the result with coverage.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the 'coverage' key in the result dictionary should be 1.</li>
                                        <li>The value of the 'file_path' key in the first 'coverage' entry should be 'src/foo.py'.</li>
                                        <li>The file path is not empty or None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 40-43, 161-165, 167, 169, 171, 173, 176-178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_llm_opt_out</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Result with LLM Opt-Out should include flag.</p>
                                <p><strong>Why Needed:</strong> To prevent regression in cases where the user explicitly chooses to opt-out of using the Large Language Model (LLM) for result calculation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of llm_opt_out is set to True in the result dictionary.</li>
                                        <li>The key 'llm_opt_out' exists in the result dictionary and its value matches the expected boolean value.</li>
                                        <li>The test verifies that the LLMMergeResult object correctly includes a flag indicating opt-out from using LLM for result calculation.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">18 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180-182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_rerun</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test `test_result_with_rerun` verifies that the `TestCaseResult` object includes rerun fields.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the `rerun_count` and `final_outcome` are not correctly populated in the `TestCaseResult` object when rerunning tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `rerun_count` is set to 2.</li>
                                        <li>The value of `final_outcome` is set to 'passed'.</li>
                                        <li>The `rerun_count` field should be present and have a value of 2.</li>
                                        <li>The `final_outcome` field should be present and have a value of 'passed'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 161-165, 167, 169, 171, 173-176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_without_rerun_excludes_fields</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test case "test_result_without_rerun_excludes_fields" verifies that the `result` dictionary does not contain 'rerun_count' and 'final_outcome' keys.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where a test might incorrectly assume that reruns do not affect the result.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `result` dictionary should not contain 'rerun_count' key.</li>
                                        <li>The `result` dictionary should not contain 'final_outcome' key.</li>
                                        <li>The `result` dictionary should only contain necessary fields (e.g. nodeid, outcome).</li>
                                        <li>If reruns are performed, the `result` dictionary should still be valid and include all necessary fields.</li>
                                        <li>If a test is rerun, it should not change the result of that run.</li>
                                        <li>Rerunning a test should not affect the final outcome or other non-rerun-related data in the `result` dictionary.</li>
                                        <li>The presence of 'rerun_count' or 'final_outcome' keys in the `result` dictionary should be checked for each test individually.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_default_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that default values are set correctly for the test_default_values scenario.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the case where default values are not properly initialized.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>cfg.provider == 'none'</li>
                                        <li>cfg.llm_context_mode == 'minimal'</li>
                                        <li>cfg.llm_max_tests == 0</li>
                                        <li>cfg.llm_max_retries == 3</li>
                                        <li>cfg.llm_context_bytes == 32000</li>
                                        <li>cfg.llm_context_file_limit == 10</li>
                                        <li>cfg.llm_requests_per_minute == 5</li>
                                        <li>cfg.llm_timeout_seconds == 30</li>
                                        <li>cfg.llm_cache_ttl_seconds == 86400</li>
                                        <li>cfg.include_phase == 'run'</li>
                                        <li>cfg.aggregate_policy == 'latest'</li>
                                        <li>not cfg.is_llm_enabled() is False</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_get_default_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the default configuration is correctly returned by the `get_default_config` method.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the default configuration is not set to 'none'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function returns an instance of `Config`.</li>
                                        <li>The `provider` attribute of the returned `Config` object is set to `'none'`.</li>
                                        <li>A valid configuration with a different provider (e.g., 'some_provider') is not returned.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_is_llm_enabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `is_llm_enabled` check returns the correct result for different providers.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the `is_llm_enabled` check is incorrectly returning True when it should be False for some providers.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `is_llm_enabled` method of the `Config` class returns `False` when set to `none` and `True` when set to `ollama`.</li>
                                        <li>When the `provider` is set to `none`, the `is_llm_enabled` method should return `False`.</li>
                                        <li>The `is_llm_enabled` method of the `Config` class returns `False` when the `provider` is not set.</li>
                                        <li>When the `provider` is set to `ollama`, the `is_llm_enabled` method should return `True`.</li>
                                        <li>The `is_llm_enabled` method of the `Config` class should return `True` when the `provider` is set to `ollama` and it's an instance of `Config`.</li>
                                        <li>When the `provider` is set to `ollama`, the `is_llm_enabled` method should return `True`.</li>
                                        <li>The `is_llm_enabled` method of the `Config` class should not throw any exceptions when the `provider` is not set.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_aggregate_policy</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-197, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_context_mode</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-189, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_include_phase</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `validate()` method of a `TestConfig` object returns an error message when an invalid include phase is specified.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where an invalid include phase is not properly validated and causes errors in the application.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `validate()` method should return exactly one error message.</li>
                                        <li>The error message should contain the string 'Invalid include_phase 'lunch_break'.</li>
                                        <li>The error message should be present in the first element of the list returned by `validate()`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-205, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validation with an invalid provider.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an unexpected error message when validating an invalid provider.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The configuration object should be validated successfully without any errors.</li>
                                        <li>The 'Invalid provider' error message should be present in the validation result.</li>
                                        <li>The error message should contain the exact phrase 'Invalid provider'.</li>
                                        <li>The test should fail if the provider is not recognized or is invalid.</li>
                                        <li>A single error message should be returned when validating an invalid provider.</li>
                                        <li>The configuration object should not throw any exceptions during validation.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_numeric_ranges</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validation of numeric constraints for Config object.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the llm_context_bytes is set to a value less than 1000, which could lead to unexpected behavior or errors in the LLM.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>cfg.validate() should return at least 5 error messages.</li>
                                        <li>The 'llm_context_bytes' constraint must be met (>= 1000).</li>
                                        <li>The 'llm_max_tests' constraint must be either 0, positive or no limit.</li>
                                        <li>The 'llm_requests_per_minute' constraint must be at least 1.</li>
                                        <li>The 'llm_timeout_seconds' constraint must be at least 1.</li>
                                        <li>The 'llm_max_retries' constraint must be either 0 or positive.</li>
                                        <li>All constraints (context_bytes, max_tests, requests_per_minute, timeout_seconds, max_retries) must be met.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">22 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209-218, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestConfig::test_validate_valid_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Valid configuration is validated successfully.</p>
                                <p><strong>Why Needed:</strong> A valid configuration is required to prevent potential issues and ensure correct behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `validate()` method of the `Config` class returns an empty list when a valid configuration is provided.</li>
                                        <li>No errors are reported when a valid configuration is passed to the `validate()` method.</li>
                                        <li>The `validate()` method does not throw any exceptions or raise an error when a valid configuration is used.</li>
                                        <li>A successful validation outcome is achieved without any issues or warnings.</li>
                                        <li>The `Config` class correctly handles a valid input and returns no errors.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_aggregation_options</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the ability to load aggregation options correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where aggregation options are not loaded correctly due to incorrect directory or policy settings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The aggregate_dir attribute is set to 'aggr_dir'.</li>
                                        <li>The aggregate_policy attribute is set to 'merge'.</li>
                                        <li>The aggregate_run_id attribute is set to 'run-123'.</li>
                                        <li>The aggregate_group_id attribute is set to 'group-abc'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286-294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_config_invalid_int_ini</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test handling of invalid integer values in INI.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential crash due to an invalid integer value in the INI file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `load_config` should not crash when encountering an invalid integer value in the INI file.</li>
                                        <li>The default value of `llm_max_retries` is 3, which is the expected behavior when a valid integer value cannot be found.</li>
                                        <li>The `getini` method's side effect does not cause any crashes or unexpected behavior for valid values.</li>
                                        <li>The test ensures that the function behaves correctly even if an invalid value is present in the INI file.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">28 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263-267, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_coverage_source</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `llm_coverage_source` option is correctly set to 'cov_dir' when loaded.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the coverage source is not properly set, potentially leading to incorrect output or errors in the analysis.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `cfg.llm_coverage_source` should be equal to 'cov_dir'.</li>
                                        <li>The `llm_coverage_source` option should have been set to 'cov_dir' during configuration.</li>
                                        <li>The coverage source path should match the expected value 'cov_dir'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_defaults</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the default provider and report HTML are correctly loaded when no options are provided.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails to load configuration due to missing provider or report HTML settings.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` attribute of the configuration object is set to 'none'.</li>
                                        <li>The `report_html` attribute of the configuration object is set to `None`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">24 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_cli_overrides_ini</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that CLI options override ini options.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the CLI overrides ini settings, potentially causing unexpected behavior or incorrect configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_pytest_config.getini.side_effect should be set to lambda key: 'ini_value' if key == 'llm_report_html' else None</li>
                                        <li>mock_pytest_config.option.llm_report_html should be set to 'cli_report.html'</li>
                                        <li>mock_pytest_config.option.llm_requests_per_minute should not be set in ini, but instead be 100</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">27 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259-261, 263, 270-272, 274, 276, 278, 280-282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_cli_retries</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `llm_max_retries` option is correctly set to 9 when loading configuration from CLI.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `llm_max_retries` option is not being updated correctly after setting it in the command line.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `llm_max_retries` should be set to 9.</li>
                                        <li>The value of `llm_max_retries` should match the expected value (in this case, 9).</li>
                                        <li>The configuration object should have a valid `llm_max_retries` attribute.</li>
                                        <li>The `llm_max_retries` option should not be changed unexpectedly after setting it in the command line.</li>
                                        <li>The test should fail if the `llm_max_retries` option is set to an invalid value (e.g. 0).</li>
                                        <li>The test should pass if the `llm_max_retries` option is correctly updated to 9.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282-283, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_ini</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loading values from ini options.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails if the `getini` method is not called with a valid key.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `llm_report_provider` should be 'ollama'.</li>
                                        <li>The value of `llm_report_model` should be 'llama3'.</li>
                                        <li>The value of `llm_report_context_mode` should be 'balanced'.</li>
                                        <li>The value of `llm_report_requests_per_minute` should be 10.</li>
                                        <li>The value of `llm_report_max_retries` should be 5.</li>
                                        <li>The value of `llm_report_html` should be 'report.html'.</li>
                                        <li>The value of `llm_report_json` should be 'report.json'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">32 lines (ranges: 107, 147, 248, 251-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_aggregation_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the aggregation settings configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the aggregate directory, policy, and include history are not correctly set for aggregated data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregate_dir` attribute is set to `/reports` as expected.</li>
                                        <li>The `aggregate_policy` attribute is set to `merge` as expected.</li>
                                        <li>The `aggregate_include_history` attribute is set to `True` as expected.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_all_output_paths</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Config with all output paths.</p>
                                <p><strong>Why Needed:</strong> Prevents a regression where the test fails to verify that all required output files are present.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report_html` attribute is set to 'report.html'.</li>
                                        <li>The `report_json` attribute is set to 'report.json'.</li>
                                        <li>The `report_pdf` attribute is set to 'report.pdf'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_capture_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `capture_failed_output` attribute of the `Config` object is set to `True`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the capture settings are not properly configured, potentially leading to incorrect output or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.capture_failed_output is True</li>
                                        <li>config.capture_output_max_chars is 8000</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_compliance_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `metadata_file` and `hmac_key_file` are correctly set in the test configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the compliance settings are not properly configured, potentially leading to incorrect metadata or HMAC key usage.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of `config.metadata_file` is correct and matches 'metadata.json'.</li>
                                        <li>The value of `config.hmac_key_file` is correct and matches 'key.txt'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_coverage_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the configuration of coverage settings.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where coverage settings are not properly configured, potentially leading to incorrect test results or missed tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.omit_tests_from_coverage is set to False</li>
                                        <li>config.include_phase is set to "all"</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_custom_exclude_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the specified custom exclude globs are included in the configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the custom exclude globs are not properly propagated to the LLM context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The string `*.pyc` is present in the `llm_context_exclude_globs` list.</li>
                                        <li>The string `*.log` is present in the `llm_context_exclude_globs` list.</li>
                                        <li>Custom exclude globs are included in the configuration.</li>
                                        <li>The custom exclude globs match the expected values.</li>
                                        <li>No custom exclude globs are excluded from the LLM context.</li>
                                        <li>The `llm_context_exclude_globs` list is updated correctly with custom exclude globs.</li>
                                        <li>Custom exclude globs are properly propagated to the LLM context.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_include_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `include_globs` configuration option includes only `.py` files.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where include globs are not correctly applied to LLM context files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `*.py` glob matches only files with the `.py` extension.</li>
                                        <li>The `*.pyi` glob matches only files with the `.pyi` extension.</li>
                                        <li>The `include_globs` configuration option is set correctly for LLM context files.</li>
                                        <li>The `llm_context_include_globs` attribute of the `Config` object contains the correct include globs.</li>
                                        <li>The `include_globs` attribute does not contain any invalid or missing glob patterns.</li>
                                        <li>The `include_globs` attribute does not contain any duplicate glob patterns.</li>
                                        <li>The `include_globs` attribute is set correctly for all supported file extensions (`.py`, `.pyi`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_invocation_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `include_pytest_invocation` attribute of the `Config` object is set to `False`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the invocation settings are not correctly configured for Pytest.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `include_pytest_invocation` attribute of the `config` object is set to `False`.</li>
                                        <li>The `include_pytest_invocation` attribute of the `config` object does not match its current value.</li>
                                        <li>The `include_pytest_invocation` attribute of the `config` object is not correctly set for Pytest invocation settings.</li>
                                        <li>The `config` object has an incorrect `include_pytest_invocation` attribute value.</li>
                                        <li>The `config` object's `include_pytest_invocation` attribute does not match its expected value.</li>
                                        <li>The `config` object's `include_pytest_invocation` attribute is set to a valid value for Pytest invocation settings.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_execution_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the configuration of LLM execution settings.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in LLM execution settings configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The value of llm_max_tests is set to 50.</li>
                                        <li>The value of llm_max_concurrency is set to 8.</li>
                                        <li>The value of llm_requests_per_minute is set to 12.</li>
                                        <li>The value of llm_timeout_seconds is set to 60 seconds.</li>
                                        <li>The value of llm_cache_ttl_seconds is set to 3600 seconds (1 hour).</li>
                                        <li>The directory for the LLM cache is set to .cache.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_param_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the configuration of LLM parameter settings.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the maximum characters for LLM parameters are not being set correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.llm_include_param_values is True</li>
                                        <li>config.llm_param_value_max_chars == 200</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_settings</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the configuration of LLM settings for OLLAMA provider.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the model is not set correctly or the context bytes are too large, potentially causing issues with the LLM.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `provider` attribute in the test_llm_settings method should be equal to 'ollama'.</li>
                                        <li>The `model` attribute in the test_llm_settings method should be equal to 'llama3.2'.</li>
                                        <li>The value of `llm_context_bytes` in the test_llm_settings method should be 64000.</li>
                                        <li>The value of `llm_context_file_limit` in the test_llm_settings method should not exceed 20.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_repo_root_path</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `repo_root` attribute is correctly set to `/project`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the repository root path is not properly configured, potentially leading to errors or unexpected behavior in subsequent tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>config.repo_root</li>
                                        <li>is_path('/project')</li>
                                        <li>is_absolute('/project')</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_valid_phase_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `test_valid_phase_values` function to ensure all valid include_phase values pass validation.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where invalid or missing include_phase values are passed through to the validation process.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `validate()` method of the `Config` class does not return any errors for valid include_phase values.</li>
                                        <li>All include_phase values ('run', 'setup', 'teardown', and 'all') are present in the configuration.</li>
                                        <li>No error messages or warnings are raised when validating a valid include_phase value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_exclude_globs</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the default exclude globs are correctly set.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the default exclude globs do not include all necessary files.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `llm_context_exclude_globs` is called with an empty list of arguments, and it should return a list containing only `.pyc`, `__pycache__/*`, and `*secret*`.</li>
                                        <li>The function `llm_context_exclude_globs` is called with the correct list of exclude globs, including all necessary files.</li>
                                        <li>If any of the required files are excluded from the default globs, an assertion error should be raised.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_redact_patterns</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test default redact patterns with various configuration options.</p>
                                <p><strong>Why Needed:</strong> Prevents regression when using the default configuration, ensuring that sensitive information is not exposed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `--password` and `--token` flags are included in the default redact patterns.</li>
                                        <li>The pattern for `--api[_-]?key` includes a hyphenated key, which is expected to be redacted.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_values</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test default values of TestConfigDefaultsMaximal.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the default provider and llm context mode are not set correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The config.provider should be 'none'.</li>
                                        <li>The config.llm_context_mode should be 'minimal'.</li>
                                        <li>The config.llm_context_bytes should be 32000.</li>
                                        <li>config.omit_tests_from_coverage should be True.</li>
                                        <li>config.include_phase should be 'run'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 233)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigHelpersMaximal::test_is_llm_enabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `is_llm_enabled` method returns correct enabled status for different providers.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case a provider is changed or removed, and LLMs are not enabled by default.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The method should return False when the provider is 'none'.</li>
                                        <li>The method should return True when the provider is 'ollama' (LLM).</li>
                                        <li>The method should return True when the provider is 'litellm' (LLM).</li>
                                        <li>The method should return True when the provider is 'gemini'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_aggregate_policy</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-197, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_context_mode</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the validation of an invalid context mode.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where an invalid context mode is passed to the llm, potentially causing unexpected behavior or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `Config(llm_context_mode='invalid')` should be called with a valid 'context_mode' string.</li>
                                        <li>An error message indicating that the provided 'llm_context_mode' is invalid should be returned by the `validate()` method.</li>
                                        <li>A specific error message containing the word 'Invalid' should be present in the error messages returned by the `validate()` method for an invalid context mode.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-189, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_include_phase</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validates an invalid include phase.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an invalid include phase.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `Config(include_phase='invalid')` should return an error.</li>
                                        <li>The error message for 'Invalid include_phase 'invalid'' should be present in the list of errors.</li>
                                        <li>The first error in the list should contain the string 'Invalid include_phase 'invalid'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-205, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_provider</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test validates an invalid provider.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an invalid provider being used.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `Config` is called with an invalid provider.</li>
                                        <li>The `validate()` method of the `Config` object returns exactly one error message.</li>
                                        <li>The error message contains the string 'Invalid provider 'invalid'.</li>
                                        <li>The test asserts that there is only one error in the list of errors returned by `config.validate()`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_numeric_bounds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_numeric_bounds</p>
                                <p><strong>Why Needed:</strong> Prevents invalid numeric values from being passed to the LLM.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return an error for llm_context_bytes = 999.</li>
                                        <li>The function should return an error for llm_max_tests = -1.</li>
                                        <li>The function should return an error for llm_requests_per_minute = 0.</li>
                                        <li>The function should return an error for llm_timeout_seconds = 0.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209-217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_valid_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that an empty configuration object is returned when the input config is valid.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where an invalid config would cause the validation to fail with an empty list.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The validate() method of the Config class should return an empty list for a valid config.</li>
                                        <li>An empty list should be returned when calling config.validate().</li>
                                        <li>A ValueError or other exception should not be raised when calling config.validate() for a valid config.</li>
                                        <li>The validate() method should only raise exceptions for invalid configurations, not for valid ones.</li>
                                        <li>The validate() method should return an empty list for all valid configurations.</li>
                                        <li>The validate() method should not throw any unexpected behavior for valid configurations.</li>
                                        <li>A TypeError or other exception should not be raised when calling config.validate() for a valid config.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginConfigLoading::test_config_defaults</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the config defaults to safe values when no options are provided.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the config is set to an invalid or unexpected value.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `cfg` variable should be an instance of `Config`.</li>
                                        <li>The `cfg` variable should not have any registered options.</li>
                                        <li>The `cfg` variable's values should match the expected safe defaults.</li>
                                        <li>No additional configuration options should be present in the config.</li>
                                        <li>The `cfg` variable's type should be consistent with the `Config` class.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">33 lines (ranges: 107, 147, 248, 251-259, 261, 263-265, 270, 272-276, 278, 280, 282, 286, 288, 290-292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginConfigLoading::test_markers_exist_in_config</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `pytestconfig` object is accessible in the test environment.</p>
                                <p><strong>Why Needed:</strong> This test ensures that the plugin configuration is properly loaded and available for use during testing.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytestconfig` object should not be None.</li>
                                        <li>The `pytestconfig` object should have attributes (e.g., 'markers', 'plugins') accessible.</li>
                                        <li>The `pytestconfig` object should contain the expected configuration markers and plugins.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_llm_context_marker</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the context marker does not cause errors in the LLM integration.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the context marker causes an error, potentially leading to incorrect results or failures.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert True</li>
                                        <li>assert len(self.collection_errors) == 0</li>
                                        <li>assert all(isinstance(error, CollectionError) for error in self.collection_errors)</li>
                                        <li>assert self.collected_count > 0</li>
                                        <li>assert self.deselected_count > 0</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_llm_opt_out_marker</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">


                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_requirement_marker</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the requirement marker is executed without causing any errors.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the requirement marker causes an error in the plugin's execution.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `requirement_marker` function should not throw any exceptions or raise an error.</li>
                                        <li>The `requirement_marker` function should not modify the plugin's state or behavior in a way that would cause errors.</li>
                                        <li>The `requirement_marker` function should only be executed when necessary, without causing unnecessary execution of the plugin.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_integration.py::TestReportGeneration::test_report_writer_integration</span>
                        <div class="test-meta">
                            <span>33ms</span>
                            <span title="Covered file count">üõ°Ô∏è 6</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the report writer integrates with pytest_llm_report models correctly and generates a full report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression or bugs in the integration of the report writer with pytest_llm_report models, ensuring consistent report generation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The report writer should create a JSON file named 'report.json' containing the total number of tests and passed tests.</li>
                                        <li>The report writer should include all test nodes in the HTML output.</li>
                                        <li>The report writer should verify that the correct files are generated for each test node (e.g., 'test_a.py' for test_a.py::test_pass).</li>
                                        <li>The report writer should correctly format the JSON data with the total number of tests and passed tests.</li>
                                        <li>The report writer should correctly format the HTML output with all test nodes included.</li>
                                        <li>The report writer should not fail to generate a full report even if there are no failed tests.</li>
                                        <li>The report writer should handle errors properly, including passing an error message for failed tests.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">79 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">131 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that collectreport skips when disabled and pytest_collectreport is mocked correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where collectreport fails to skip tests when the plugin is disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_report.session.config.stash.get.assert_called_with(_enabled_key, False)</li>
                                        <li>pytest_collectreport(mock_report).session.config.stash.get.assert_calledWith(_enabled_key, False)</li>
                                        <li>mock_report.session.config.stash.get.return_value == False</li>
                                        <li>pytest_collectreport(mock_report) is not called with _enabled_key=False</li>
                                        <li>mock_report.session.config.stash.get.return_value is not False</li>
                                        <li>_enabled_key is correctly set to 'collectreport' in the mock report's session config</li>
                                        <li>pytest_collectreport does not raise an exception when disabled</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 387-388, 391, 395-397, 408-409, 415-416)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_enabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that collectreport calls collector when enabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the plugin does not call the collector even if collectreport is enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_collectreport` function should be able to find and stash the `collector` mock object.</li>
                                        <li>The `handle_collection_report` method of the `mock_collector` object should be called with the `mock_report` argument.</li>
                                        <li>The `stash_get` function should return `True` when it encounters `_enabled_key` or `_collector_key` keys in the session configuration.</li>
                                        <li>The `handle_collection_report` method should call `handle_collection_report` on the `mock_collector` object with the `mock_report` argument.</li>
                                        <li>The `stash_get` function should not be called for other keys in the session configuration.</li>
                                        <li>The `handle_collection_report` method of the `mock_collector` object should be able to handle a valid collection report.</li>
                                        <li>The `stash_get` function should return `None` when it encounters `_enabled_key` or `_collector_key` keys that are not present in the session configuration.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 387-388, 391, 395-397, 408-409, 415, 419-421)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_no_session</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the `pytest_collectreport` function behaves correctly when no session is available.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the collect report skips due to an unavailable session.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `session` attribute of the mock report object is not set.</li>
                                        <li>The `pytest_collectreport` function does not raise an exception when called with a mock report that lacks a session attribute.</li>
                                        <li>The `pytest_collectreport` function correctly skips the collect report without raising an error when no session is available.</li>
                                        <li>The `session` attribute of the mock report object is not set after calling `pytest_collectreport`.</li>
                                        <li>The `pytest_collectreport` function does not raise an exception when called with a mock report that lacks a session attribute.</li>
                                        <li>The `pytest_collectreport` function correctly skips the collect report without raising an error when no session is available.</li>
                                        <li>The `session` attribute of the mock report object is set to None after calling `pytest_collectreport`.</li>
                                        <li>The `pytest_collectreport` function does not raise an exception when called with a mock report that lacks a session attribute.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 387-388, 391, 395-397, 408, 412)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_session_none</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that `pytest_collectreport` does not raise an error when the session is set to `None`.</p>
                                <p><strong>Why Needed:</strong> Prevent a potential bug where `pytest_collectreport` raises an exception when the session is None, causing test failures.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `pytest_collectreport` should not be called with a `session` argument that is `None`.</li>
                                        <li>A `pytest_collectreport` instance without a `session` attribute should not raise an error.</li>
                                        <li>The `session` attribute of the `pytest_collectreport` instance should remain unchanged when set to `None`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 387-388, 391, 395-397, 408, 412)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_llm_enabled_warning</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM enabled warning is raised when using the Ollama LLM report provider.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the LLM provider 'ollama' is enabled and raises an error in Pytest.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_llm_report_provider` option should be set to `'ollama'`.</li>
                                        <li>The `llm_report_provider` option should not be set to a value that is not supported (e.g. `'other'`).</li>
                                        <li>The `llm_report_html`, `llm_report_json`, and `llm_report_pdf` options should be set to `None`.</li>
                                        <li>The `llm_evidence_bundle`, `llm_dependency_snapshot`, `llm_requests_per_minute`, `llm_aggregate_dir`, `llm_aggregate_policy`, `llm_aggregate_run_id`, and `llm_aggregate_group_id` options should not be set. However, the option `llm_max_retries` is still set to `None`.</li>
                                        <li>The `llm_max_retries` option should be set to a valid value (e.g. `5`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">44 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_validation_errors</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that validation errors raise UsageError when invalid configuration is provided.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the plugin does not handle invalid configuration properly and raises a UsageError instead of providing informative error messages.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_config.getini.side_effect should return `None` for all keys in `ini_values` when calling `getini` on mock_config.</li>
                                        <li>mock_config.option.llm_report_html should be `None` when called `None` on mock_config.option_llm_report_html.</li>
                                        <li>mock_config.option.llm_report_json should be `None` when called `None` on mock_config.option_llm_report_json.</li>
                                        <li>mock_config.option.llm_report_pdf should be `None` when called `None` on mock_config.option_llm_report_pdf.</li>
                                        <li>mock_config.option.llm_evidence_bundle should be `None` when called `None` on mock_config.option_llm_evidence_bundle.</li>
                                        <li>mock_config.option.llm_dependency_snapshot should be `None` when called `None` on mock_config.option_llm_dependency_snapshot.</li>
                                        <li>mock_config.option.llm_requests_per_minute should be `None` when called `None` on mock_config.option_llm_requests_per_minute.</li>
                                        <li>mock_config.option.llm_aggregate_dir should be an empty string when called `None` on mock_config.option_llm_aggregate_dir.</li>
                                        <li>mock_config.option.llm_aggregate_policy should be an empty string when called `None` on mock_config.option_llm_aggregate_policy.</li>
                                        <li>mock_config.option.llm_aggregate_run_id should be an empty string when called `None` on mock_config.option_llm_aggregate_run_id.</li>
                                        <li>mock_config.option.llm_aggregate_group_id should be an empty string when called `None` on mock_config.option_llm_aggregate_group_id.</li>
                                        <li>mock_config.option.llm_max_retries should be an integer value greater than 0 when called `None` on mock_config.option_llm_max_retries.</li>
                                        <li>mock_config.rootpath should be a valid Path object when calling `getini` on mock_config.rootpath.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">43 lines (ranges: 107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 248, 251-253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-199, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_worker_skip</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that configure skips on xdist workers and ensures addinivalue_line is not called before worker check.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where configure might skip the worker input due to an unhandled marker.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_config.addinivalue_line was not called before worker check.</li>
                                        <li>mock_config.workerinput was set correctly.</li>
                                        <li>pytest_configure() was not called with mock_config.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">17 lines (ranges: 169-171, 173-175, 177-179, 183-184, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigureFallback::test_pytest_configure_fallback_load</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that fallback to load_config is triggered when Config.load is missing and no other fallbacks are available.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the plugin fails to configure due to an unhandled Config.load error, potentially leading to unexpected behavior or errors in downstream tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Mocking `Config.load` with `None` returns `None`</li>
                                        <li>The `validate()` method of `mock_cfg` returns an empty list</li>
                                        <li>The `load_config()` function from `pytest_llm_report` is called with `mock_cfg` as argument</li>
                                        <li>The `load_config()` function does not raise any exceptions when called with `mock_cfg` as argument</li>
                                        <li>The `option.llm_max_retries` and `option.llm_report_html` are set to `None` in the mock configuration</li>
                                        <li>No other fallbacks (e.g. `pytest_llm_report.options.load_config`) are used, preventing the plugin from configuring</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_all_ini_options</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test loading all INI options from the plugin configuration.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where CLI options are not set for llm_report_html, llm_report_json, llm_report_pdf, llm_evidence_bundle, llm_dependency_snapshot, llm_requests_per_minute, llm_aggregate_dir, llm_aggregate_policy, llm_aggregate_run_id, and llm_aggregate_group_id.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The correct provider is set to 'ollama'.</li>
                                        <li>The correct model is set to 'llama3.2'.</li>
                                        <li>The correct context mode is set to 'complete'.</li>
                                        <li>The correct number of requests per minute is set to 10.</li>
                                        <li>The report HTML and JSON files are set correctly.</li>
                                        <li>LLM PDF, EVIDENCE BUNDLE, DEPENDENCY SNAPSHOT, AGGREGATE_DIR, AGGREGATE_POLICY, AGGREGATE_RUN_ID, AND AGGREGATE_GROUP_ID are all set correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">31 lines (ranges: 107, 147, 248, 251-263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_cli_overrides_ini</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test CLI options override INI options when using pytest_llm_report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the CLI options override INI options, causing unexpected behavior in the plugin.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `llm_report_html` option is set to 'cli.html' instead of 'ini.html'.</li>
                                        <li>The `llm_report_json` option is set to 'cli.json' instead of 'ini.json'.</li>
                                        <li>The `llm_report_pdf` option is set to 'cli.pdf' instead of 'ini.pdf'.</li>
                                        <li>The `llm_evidence_bundle` option is set to 'bundle.zip' instead of 'ini.evidence_bundle'.</li>
                                        <li>The `llm_dependency_snapshot` option is set to 'deps.json' instead of 'ini.dependency_snapshot'.</li>
                                        <li>The `llm_requests_per_minute` option is set to 20 instead of the expected value.</li>
                                        <li>The `aggregate_dir` option is set to '/agg' instead of the expected value.</li>
                                        <li>The `aggregate_policy` option is set to 'merge' instead of the expected value.</li>
                                        <li>The `aggregate_run_id` option is set to 'run-123' instead of the expected value.</li>
                                        <li>The `aggregate_group_id` option is set to 'group-abc' instead of the expected value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">38 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259-263, 270-283, 286-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that terminal summary skips when plugin is disabled and stash returns False for enabled key.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the plugin's terminal summary does not skip when it is disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_config.stash.get.assert_called_once_with(_enabled_key, True)</li>
                                        <li>mock_config.stash.get.return_value == False</li>
                                        <li>result is None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 238, 242-243, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_worker_skip</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that terminal summary skips on xdist worker when the workerid matches the config.</p>
                                <p><strong>Why Needed:</strong> To prevent a regression where the plugin does not skip terminal summaries when an xdist worker is present.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function pytest_terminal_summary() should return None if the workerid in the mock_config matches the workerid of the xdist worker.</li>
                                        <li>The function pytest_terminal_summary() should not perform any action (i.e., return a value other than None) when the workerid in the mock_config matches the workerid of the xdist worker.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 238-239, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::testload_config</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test config loading from pytest objects (CLI + INI) to ensure correct HTML output.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the plugin does not display the correct HTML report even if the option is set correctly in the configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report_html` attribute of the loaded config object should be set to 'out.html'.</li>
                                        <li>The `llm_report_html` option should have an effect on the reported HTML output.</li>
                                        <li>The `rootpath` option should not affect the reported HTML output.</li>
                                        <li>The `getini` method of the mock_config object should return None for all keys when called.</li>
                                        <li>The `load_config` function should correctly load the config from the mock_config object and set the expected value for `report_html`.</li>
                                        <li>The `load_config` function should not raise an exception if the option is not present in the configuration.</li>
                                        <li>The `load_config` function should return a valid config object with the correct attributes.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270-283, 286-295, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_disabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test makereport skips when disabled.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential regression where the plugin does not report test results correctly when makereport is disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_item.config.stash.get() returns False</li>
                                        <li>mock_call() should be called with mock_outcome</li>
                                        <li>mock_outcome.get_result() should return a MagicMock object</li>
                                        <li>gen.send(mock_outcome) should not raise an exception when StopIteration is reached</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 387-388, 391-392, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_enabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test makereport calls collector when enabled.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the plugin does not call the collector even though it is enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_runtest_makereport` function should be able to find and stash the `_collector_key` key in the mock item's config.</li>
                                        <li>The `pytest_runtest_makereport` function should be able to find and return the `mock_collector` object from the stash.</li>
                                        <li>The `handle_runtest_logreport` method of the `mock_collector` object should be called with the `mock_report` object as an argument.</li>
                                        <li>The `get_result` method of the `mock_outcome` object should return a successful result (i.e., `None`) when the plugin is enabled.</li>
                                        <li>The `stash_get` function should return `True` for the `_enabled_key` key in the mock item's config.</li>
                                        <li>The `stash_get` function should not return any value for other keys.</li>
                                        <li>The `send` method of the `mock_outcome` object should be called with a successful result (i.e., `None`) when the plugin is enabled.</li>
                                        <li>The `handle_runtest_logreport` method of the `mock_collector` object should be called with the `mock_report` object as an argument.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that collection_finish is skipped when disabled in pytest_llm_report plugin.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the plugin's collection_finish behavior changes unexpectedly when collection_finish is disabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The pytest_collection_finish function should not be called with _enabled_key as False.</li>
                                        <li>The pytest_collection_finish function should not be called with _enabled_key as True.</li>
                                        <li>The pytest_collection_finish function should not have been called on the mock session.</li>
                                        <li>The pytest_collection_finish function should not have had a return value of False.</li>
                                        <li>The pytest_collection_finish function's config stash.get method should have returned None.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 387-388, 391, 395-397, 431-432)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_enabled</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `pytest_collection_finish` calls `collector_key` when collection_finish is enabled.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the collector is not called when collection finish is enabled, potentially leading to incorrect data being collected.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `collector_key` of the session items should be called with the correct arguments.</li>
                                        <li>The `collector_key` should be called with the `mock_collector` instance as an argument.</li>
                                        <li>The `collector_key` should not be called with a default value.</li>
                                        <li>The `handle_collection_finish` method of the `collector_key` should be called once with the `mock_session.items` list.</li>
                                        <li>The `collector_key` should not call its own `handle_collection_finish` method without an argument.</li>
                                        <li>The `collector_key` should raise an exception if it is not properly configured.</li>
                                        <li>The `collector_key` should not call any other methods or attributes on the session items when called with a default value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 387-388, 391, 395-397, 431, 435-437)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_disabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that sessionstart skips when disabled and checks enabled status.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where the plugin is not properly configured or is disabled by mistake.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>mock_session.config.stash.get.assert_called_with(_enabled_key, False)</li>
                                        <li>pytest_sessionstart(mock_session).should_not_be_called</li>
                                        <li>mock_session.config.stash.get.return_value should be False</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 387-388, 391, 395-397, 448-449)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_enabled</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that sessionstart initializes collector when enabled.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the collector is not initialized due to an unenabled sessionstart.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `_collector_key` should be present in `mock_stash`.</li>
                                        <li>The `_start_time_key` should be present in `mock_stash`.</li>
                                        <li>A mock stash that supports both get() and [] operations should be created.</li>
                                        <li>The collector should be created after the sessionstart is initialized.</li>
                                        <li>The collector's key should match `_collector_key` from `mock_stash`.</li>
                                        <li>The start time of the collection should be recorded correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 387-388, 391, 395-397, 448, 452, 455, 457-458)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test pytest_addoption adds expected arguments to the parser.</p>
                                <p><strong>Why Needed:</strong> pytest_addoption prevents a potential bug where it does not add all required arguments to the parser.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>parser.getgroup.assert_called_with('llm-report', 'LLM-enhanced test reports')</li>
                                        <li>group.addoption.call_args_list[0][0] == '--llm-report'</li>
                                        <li>group.addoption.call_args_list[1][0] == '--llm-coverage-source'</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">99 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption_ini</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 2</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test pytest_addoption adds INI options (lines 13-34) to verify that the plugin correctly adds necessary configuration files.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the plugin does not add necessary INI options for pytest, potentially causing issues with report generation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>llm_report_html is added as an option</li>
                                        <li>llm_report_json is added as an option</li>
                                        <li>llm_report_max_retries is added as an option</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">99 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_coverage_calculation</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test coverage percentage calculation logic for terminal summary.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in coverage reporting when using the `terminal_summary` plugin.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report_html` option is set to 'out.html'.</li>
                                        <li>The `stash` dictionary contains both `_enabled_key` and `_config_key`.</li>
                                        <li>The `MockStash` class is used to mock the stash data. The `mock_config.stash` attribute is assigned this instance.</li>
                                        <li>The `CoverageMapper` class is mocked with a return value of 85.5 for the `report` method.</li>
                                        <li>The `pytest_terminal_summary` function is called with a mock `MockStash` object and an integer argument (0).</li>
                                        <li>The `mock_cov_cls.return_value` attribute is set to the `mock_cov` instance, which has been mocked to return 85.5 for the `report` method.</li>
                                        <li>The `mock_cov.load.assert_called_once()` assertion checks that the `load` method was called once with no arguments.</li>
                                        <li>The `mock_cov.report.assert_called_once()` assertion checks that the `report` method was called once with no arguments.</li>
                                        <li>The `pytest_terminal_summary` function is called again, which should not have any additional arguments (1).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">58 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 307, 309-315, 317-318, 331-332, 337-338, 365-375, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_llm_enabled</span>
                        <div class="test-meta">
                            <span>3ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that terminal summary with LLM enabled runs correctly and reports the correct provider.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the plugin's behavior when LLM is enabled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_terminal_summary_llm_enabled` function should be called with a valid configuration.</li>
                                        <li>The provided configuration should match the expected one.</li>
                                        <li>The correct provider should be reported in the terminal summary.</li>
                                        <li>The report HTML file should contain the correct information.</li>
                                        <li>The coverage map should not be affected by LLM enabled.</li>
                                        <li>The annotate tests should run successfully without any errors.</li>
                                        <li>The `pytest_terminal_summary_llm_enabled` function should return a mock object with the expected arguments.</li>
                                        <li>The `mock_annotate.call_args[0][1] == cfg` assertion should pass when the provided configuration is correct.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">59 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331-332, 337-340, 343, 345, 348-350, 357-362, 365-375, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_no_collector</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the terminal summary creates a collector if it is missing.</p>
                                <p><strong>Why Needed:</strong> Prevents regression and ensures proper behavior when no collector is specified.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_terminal_summary` function should create a `MockStash` instance with `_enabled_key` set to `True` and `_config_key` set to the provided configuration object.</li>
                                        <li>The `stash` attribute of the mock stash instance should be set to the provided stash instance.</li>
                                        <li>The `mock_config.stash` assignment should update the stash instance with the provided stash instance.</li>
                                        <li>The `pytest_terminal_summary` function should call the `ReportWriter` patcher with a valid `MockTerminalReporter` instance and return value.</li>
                                        <li>The `coverage_map` mapper should not be called with any coverage data when creating a mock stash instance.</li>
                                        <li>The `_enabled_key` key in the mock stash instance should be set to `True` after creation.</li>
                                        <li>The `_config_key` key in the mock stash instance should be set to the provided configuration object after creation.</li>
                                        <li>A new mock terminal reporter instance should be created with the provided arguments (0, mock_config).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_with_aggregation</span>
                        <div class="test-meta">
                            <span>2ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test terminal summary with aggregation enabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the case where aggregation is enabled and a stash that supports both get() and [] is created.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `aggregate_dir` attribute of the configuration object should be set to `/agg` when aggregation is enabled.</li>
                                        <li>The `stash` attribute of the configuration object should contain an `_enabled_key` with value `True` when aggregation is enabled.</li>
                                        <li>The `aggregate` method of the Aggregator instance should return a report when aggregation is enabled.</li>
                                        <li>The `write_json` and `write_html` methods of the ReportWriter instance should be called once when aggregation is enabled.</li>
                                        <li>The stash created by creating a proper stash that supports both get() and [] should contain an `_enabled_key` with value `True` when aggregation is enabled.</li>
                                        <li>The Aggregator instance should have an aggregate method that returns a report when aggregation is enabled.</li>
                                        <li>The ReportWriter instance should write JSON and HTML files when aggregation is enabled.</li>
                                        <li>The stash created by creating a proper stash that supports both get() and [] should not contain any other attributes or methods when aggregation is enabled.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 238, 242, 246, 249-250, 252-253, 256-257, 259, 261-265, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummaryErrors::test_terminal_summary_coverage_error</span>
                        <div class="test-meta">
                            <span>4ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that a coverage calculation error prevents the test from running.</p>
                                <p><strong>Why Needed:</strong> This test verifies that the `pytest_terminal_summary` function raises a warning when it encounters an error during coverage calculation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest_terminal_summary` function should raise a `UserWarning` with message 'Failed to compute coverage percentage'.</li>
                                        <li>The `pytest_terminal_summary` function should not be able to calculate the coverage percentage if the disk is full.</li>
                                        <li>The `pytest_terminal_summary` function should log an error message when it encounters an error during coverage calculation.</li>
                                        <li>The `pytest_terminal_summary` function should raise a warning with the correct message when it encounters an error during coverage calculation.</li>
                                        <li>The `pytest_terminal_summary` function should not be able to calculate the coverage percentage if the disk is full or an error occurs during coverage calculation.</li>
                                        <li>The `pytest_terminal_summary` function should log an error message when it encounters an error during coverage calculation and raise a warning with the correct message.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 107, 147, 224)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">52 lines (ranges: 238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 322-325, 331-332, 337-338, 365-375, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_balanced_context</span>
                        <div class="test-meta">
                            <span>8ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the ContextAssembler to assemble a balanced context for a test file.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case of unbalanced contexts, where only one module's dependencies are included.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'utils.py' file is present in the assembled context.</li>
                                        <li>The 'def util()' function is found in the 'utils.py' file within the assembled context.</li>
                                        <li>Only 'test_a.py' and its contents are included in the assembled context.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">51 lines (ranges: 33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-155, 158-159, 163, 191-192, 194)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_complete_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Assembling a complete context for the `test_1` function in `test_a.py`</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when the `complete` mode is used with a non-existent file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `source` variable should contain the contents of `test_a.py::test_1`.</li>
                                        <li>The `context` variable should be an empty string if `test_a.py::test_1` does not exist.</li>
                                        <li>If `test_a.py::test_1` exists, the `source` and `context` variables should both contain the contents of that function.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">34 lines (ranges: 33, 49, 52, 55, 58, 60, 63, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 132-133, 180)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_minimal_context</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the ContextAssembler to assemble a minimal context for a test file.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when assembling contexts with minimal llm_context_mode.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'test_1' function is present in the source code of the test file.</li>
                                        <li>The context object returned by the ContextAssembler has an empty dictionary.</li>
                                        <li>The 'test_1' function is executed successfully without raising any exceptions.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">30 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_balanced_context_limits</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the ContextAssembler to ensure it can assemble a file with a balanced context limit and handle long content without truncation.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the ContextAssembler fails to assemble files that exceed the specified context limit, potentially leading to incorrect results or errors.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The assembler should be able to assemble the file `f1.py` with a 'balanced' context limit.</li>
                                        <li>The assembler should not truncate the content of `f1.py` when its length exceeds 40 bytes.</li>
                                        <li>The assembler should report an error message indicating that the file has been truncated when its length exceeds 40 bytes.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">34 lines (ranges: 33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-84, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-156, 158-159, 163, 191-192, 194)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_get_test_source_edge_cases</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify the correct behavior when a non-existent file is provided to `_get_test_source`.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where an empty string is returned for files that do not exist.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return an empty string for a non-existent file.</li>
                                        <li>The function should raise a `FileNotFoundError` exception when trying to access the file.</li>
                                        <li>The function should report an error message indicating that the file does not exist.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">26 lines (ranges: 33, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_should_exclude</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the ContextAssembler excludes certain files from the LLM context.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the ContextAssembler incorrectly includes certain files in the LLM context.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>asserts that 'test.pyc' is excluded because it's a Python bytecode file and the config includes glob patterns for .pyc files</li>
                                        <li>asserts that 'secret/key.txt' is excluded because it contains sensitive information and the config excludes secret directories</li>
                                        <li>asserts that 'public/readme.md' is not excluded because it does not contain any sensitive information</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 33, 191-194)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_consecutive_lines</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing consecutive lines in the `compress_ranges` function.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where consecutive lines are not compressed correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return a string with range notation for consecutive lines (e.g., '1-3').</li>
                                        <li>The function should handle cases where there is only one line (e.g., [1]).</li>
                                        <li>The function should ignore non-consecutive lines when compressed.</li>
                                        <li>The function should not compress empty lists or lists containing only zeros.</li>
                                        <li>The function should handle lists with negative numbers correctly (e.g., [-3, 1]).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_duplicates</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_empty_list</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Testing the `compress_ranges` function with an empty input list</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where an empty list is not correctly compressed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return an empty string for an empty input list.</li>
                                        <li>The function should handle empty lists without raising any exceptions or errors.</li>
                                        <li>The function should preserve the original order of elements in the input list.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 29-30)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_mixed_ranges</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the function when given a mixed range of numbers.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in cases where ranges are mixed with single values.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output should be '1-3, 5, 10-12, 15'.</li>
                                        <li>The output should contain all the given ranges.</li>
                                        <li>Each range should have a start value that is less than or equal to its end value.</li>
                                        <li>The function should handle single values correctly as well.</li>
                                        <li>No duplicate ranges are allowed in the output.</li>
                                        <li>The order of ranges matters (e.g., '1-3, 5' comes before '10-12').</li>
                                        <li>The function should raise an error if a range is not valid (e.g., start value greater than end value).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_non_consecutive_lines</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'non_consecutive_lines' verifies that non-consecutive lines are correctly compressed into a single range.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the compress_ranges function where non-consecutive lines are not being properly handled.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input list should be sorted and then comma-separated.</li>
                                        <li>The resulting string should contain only one comma.</li>
                                        <li>All numbers in the range should be consecutive.</li>
                                        <li>No leading or trailing commas should be present.</li>
                                        <li>Compressed ranges with non-consecutive lines should not exceed 2 characters.</li>
                                        <li>Compressed ranges with more than 3 numbers should not be split into multiple parts.</li>
                                        <li>The function should handle empty input lists correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_single_line</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'single_line' scenario verifies that a single line of code does not utilize the range notation.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where a single-line function is incorrectly compressed using range notation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert compress_ranges([5]) == '5'</li>
                                        <li>assert isinstance(compress_ranges([5]), str)</li>
                                        <li>assert len(compress_ranges([5])) == 1</li>
                                        <li>assert compress_ranges([]) is None</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 29, 33, 35-37, 39, 50, 52, 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_two_consecutive</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_ranges.py::TestCompressRanges::test_two_consecutive</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where two consecutive numbers are not compressed to range notation.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return the correct string representation of the compressed range (e.g. '1-2') for two consecutive numbers.</li>
                                        <li>The function should handle cases where the input list contains only one element correctly.</li>
                                        <li>The function should raise an error when given a non-list input.</li>
                                        <li>The function should not raise an error when given a list with less than two elements.</li>
                                        <li>The function should return the correct string representation of the compressed range for negative numbers.</li>
                                        <li>The function should handle cases where the input list contains duplicate values correctly.</li>
                                        <li>The function should raise an error when given a list that is empty or contains only one element.</li>
                                        <li>The function should not raise an error when given a list with only two elements.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_unsorted_input</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_unsorted_input' verifies that the function handles unsorted input correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents potential bugs where the function does not handle unsorted ranges properly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return a string with the correct range notation for unsorted input.</li>
                                        <li>The function should sort the input ranges before returning them.</li>
                                        <li>The function should raise an error if the input is not sorted.</li>
                                        <li>The function should preserve the original order of equal elements in the input.</li>
                                        <li>The function should handle empty input correctly.</li>
                                        <li>The function should return a string with the correct range notation for unsorted input even when there are multiple ranges.</li>
                                        <li>The function should raise an error if the input is not a list or other iterable.</li>
                                        <li>The function should preserve the original order of equal elements in the input.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">16 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_empty_string</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that an empty string expands to an empty list.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not handle empty input strings correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>Input: An empty string</li>
                                        <li>Expected Output: An empty list</li>
                                        <li>Expand ranges on empty string: True</li>
                                        <li>No expansion on empty string: False</li>
                                        <li>Empty string should produce an empty list of tuples</li>
                                        <li>Empty string should produce a list with one element (an empty tuple)</li>
                                        <li>Empty string should produce a list with two elements (the original input and an empty tuple)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 81-82)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_mixed</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `expand_ranges` function correctly handles mixed ranges and singles.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression when the input contains a mix of single values and ranges.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return a list containing all specified numbers.</li>
                                        <li>The function should handle ranges by extracting their start and end values.</li>
                                        <li>The function should correctly handle overlapping ranges.</li>
                                        <li>The function should ignore empty strings in the input.</li>
                                        <li>The function should preserve the order of single values.</li>
                                        <li>The function should handle invalid or malformed input (e.g., non-string, non-integer).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 81, 84-91, 93, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_range</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The 'expand_ranges' function is expected to expand a range of numbers into a list.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not correctly handle ranges with negative numbers or zero.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return a list containing all numbers in the range (1, 3).</li>
                                        <li>The function should include both 1 and 2 in the returned list.</li>
                                        <li>The function should exclude 0 from the returned list if it is present in the input range.</li>
                                        <li>The function should handle ranges with negative numbers correctly (e.g., '-1-3').</li>
                                        <li>The function should not return an empty list for invalid input (e.g., 'abc-3').</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">10 lines (ranges: 81, 84-91, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_roundtrip</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `expand_ranges` function correctly expands a range of numbers back to its original form after being compressed.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `expand_ranges` function does not correctly expand ranges when they are compressed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The expanded list should be equal to the original list.</li>
                                        <li>The first element of the expanded list should still be 1.</li>
                                        <li>The second element of the expanded list should still be 2.</li>
                                        <li>The third element of the expanded list should still be 3.</li>
                                        <li>The fourth element of the expanded list should still be 5.</li>
                                        <li>The fifth element of the expanded list should still be 10.</li>
                                        <li>The sixth element of the expanded list should still be 11.</li>
                                        <li>The seventh element of the expanded list should still be 12.</li>
                                        <li>The eighth element of the expanded list should still be 15.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">27 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67, 81, 84-91, 93, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_single_number</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `expand_ranges` function returns a list containing only one element when given a single number.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly expands ranges for single numbers, potentially leading to incorrect results or unexpected behavior.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The input string should be '5' (a single digit).</li>
                                        <li>The output list should contain only one element: `[5]`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/ranges.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 81, 84-87, 93, 95)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestFormatDuration::test_milliseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `format_duration` function correctly formats durations for milliseconds less than 1 second.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the function does not correctly handle durations in milliseconds less than 1 second.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return '500ms' when given an input of 0.5 seconds.</li>
                                        <li>The function should return '1ms' when given an input of 0.001 seconds.</li>
                                        <li>The function should return '0ms' when given an input of 0.0 seconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 65, 67)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestFormatDuration::test_seconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `format_duration` function with a duration of exactly 1 second.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the function returns incorrect results for durations less than or equal to 1 second.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output should be '1.23s' when the input is 1.23 seconds.</li>
                                        <li>The output should be '60.00s' when the input is 60 seconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 65-66)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestOutcomeToCssClass::test_all_outcomes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that all outcomes map to CSS classes.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in rendering of test results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `outcome_to_css_class` function should correctly map each outcome to a CSS class.</li>
                                        <li>The function should handle special cases like 'xfailed' and 'error' outcomes.</li>
                                        <li>The function should return the correct CSS class for each outcome.</li>
                                        <li>The function should not throw any exceptions when handling invalid inputs.</li>
                                        <li>The function should preserve the original value of the input string.</li>
                                        <li>The function should use a consistent naming convention for the CSS classes.</li>
                                        <li>The function should be able to handle different types of outcomes (e.g. strings, integers).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 79-85, 87)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the `outcome_to_css_class` function returns an unknown outcome and does not set a default CSS class.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `outcome_to_css_class('unknown')` should return 'outcome-unknown'.</li>
                                        <li>The function `outcome_to_css_class('unknown')` should raise an AssertionError with message 'Unknown outcome: unknown'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 79-85, 87)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_basic_report</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the rendering of a basic report with fallback HTML.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression that may occur when the rendered report contains missing or incomplete information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The document type declaration should be present in the HTML.</li>
                                        <li>The title of the report should be 'Test Report'.</li>
                                        <li>The passed node should have a text content equal to 'PASSED'.</li>
                                        <li>The failed node should have a text content equal to 'FAILED'.</li>
                                        <li>The plugin version and repository version should be displayed correctly.</li>
                                        <li>The duration of each test result should be reported accurately.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">52 lines (ranges: 65-67, 79-85, 87, 121-124, 126-127, 131-132, 141-143, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test renders coverage to ensure that the rendered HTML includes the specified file and line count.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the coverage information is not included in the rendered HTML.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'src/foo.py' file should be present in the rendered HTML.</li>
                                        <li>The rendered HTML should include at least 5 lines (1-5).</li>
                                        <li>The number of lines in the rendered HTML should match the total line count reported by ReportRoot.</li>
                                        <li>The coverage report for the 'test::foo' test should include the specified file and line count.</li>
                                        <li>The rendered HTML should contain the expected number of lines based on the coverage report.</li>
                                        <li>The coverage report should not be empty or null.</li>
                                        <li>The file path in the rendered HTML should match the specified file path.</li>
                                        <li>The line ranges in the coverage report should match the specified line range (1-5).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">52 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-129, 131-132, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_llm_annotation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test renders LLM annotations for fallback HTML.</p>
                                <p><strong>Why Needed:</strong> This test prevents the rendering of LLM annotations that could be used to bypass authentication.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'Tests login flow' scenario should be present in the rendered HTML.</li>
                                        <li>The 'Prevents auth bypass' reason should be present in the rendered HTML.</li>
                                        <li>The LlmAnnotation nodeid should be included in the rendered HTML.</li>
                                        <li>The LLmAnnotation scenario should be included in the rendered HTML.</li>
                                        <li>The LLmAnnotation why_needed message should be included in the rendered HTML.</li>
                                        <li>The ReportRoot summary should include a total of 1 test and a passed of 1 test.</li>
                                        <li>The TestCaseResult nodeid should match 'test::foo'.</li>
                                        <li>The TestCaseResult outcome should be 'passed'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">54 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-134, 136-137, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_source_coverage</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the source coverage summary is included in the rendered HTML.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the source coverage summary is not displayed correctly due to missing or incorrect data.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'Source Coverage' section should be present in the rendered HTML.</li>
                                        <li>The 'src/foo.py' file path should be included in the 'Source Coverage' section.</li>
                                        <li>The percentage of covered code (80.0%) should be displayed correctly in the 'Source Coverage' section.</li>
                                        <li>The ranges where missed code is reported ('1-4, 6-8') should be present in the 'Source Coverage' section.</li>
                                        <li>The ranges where missed code is not reported ('5, 9-10') should not be present in the 'Source Coverage' section.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">63 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-132, 141-142, 145-153, 158-164, 166-172, 177, 192, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_xpass_summary</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the 'XFailed' and 'XPassed' summary entries are included in the rendered HTML.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the xfailed/xpassed summary is not displayed correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The string 'XFailed' should be present in the rendered HTML.</li>
                                        <li>The string 'XPassed' should be present in the rendered HTML.</li>
                                        <li>Both 'XFailed' and 'XPassed' should be included in the rendered HTML.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">50 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-132, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestComputeSha256::test_different_content</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'different_content' verifies that computing the SHA-256 of different strings produces different hashes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where two different strings could produce the same hash, potentially leading to incorrect reporting or analysis.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `compute_sha256()` should return different values for different input bytes.</li>
                                        <li>The function `compute_sha256()` should raise an error if the input is not a bytes object.</li>
                                        <li>The function `compute_sha256()` should handle non-string inputs correctly and produce a hash.</li>
                                        <li>The function `compute_sha256()` should be able to handle large strings without running out of memory.</li>
                                        <li>The function `compute_sha256()` should support different character encodings (e.g., UTF-8, ASCII).</li>
                                        <li>The function `compute_sha256()` should raise an error if the input is not a valid bytes object.</li>
                                        <li>The function `compute_sha256()` should be able to handle binary data correctly and produce a hash.</li>
                                        <li>The function `compute_sha256()` should support different encoding schemes (e.g., UTF-8, ASCII).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 55)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestComputeSha256::test_empty_bytes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'Empty bytes should produce consistent hash'.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where different inputs to `compute_sha256` result in inconsistent hashes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output of `compute_sha256(b'')` and `compute_sha256(b'')` should be the same.</li>
                                        <li>The length of the output of `compute_sha256(b'')` should be 64 characters (the SHA256 hex length).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 55)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_run_meta</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the build_run_meta method includes version information for the test case.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the test case's metadata is missing or incorrect.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The duration of the test should be correctly calculated.</li>
                                        <li>The pytest version should be present in the metadata.</li>
                                        <li>The plugin version and Python version should also be included in the metadata.</li>
                                        <li>The start time and end time should be correctly formatted as datetime objects.</li>
                                        <li>The exit code should be 0, indicating successful execution of the test case.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_summary_all_outcomes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the Summary class correctly counts all outcome types and their corresponding counts.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the Summary class does not accurately count all outcome types, leading to incorrect reporting of test results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total number of outcomes should be equal to the sum of passed, failed, skipped, xfailed, and xpassed outcomes.</li>
                                        <li>The number of passed outcomes should match the expected value (1 in this case).</li>
                                        <li>The number of failed outcomes should also match the expected value (1 in this case).</li>
                                        <li>The number of skipped outcomes should be 0 since there are no skipped tests in this test.</li>
                                        <li>The number of xfailed and xpassed outcomes should also match the expected values (1 each in this case).</li>
                                        <li>The number of error outcomes should be 1, as specified in the TestCaseResult nodeid for outcome 'error'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">19 lines (ranges: 156-158, 312, 314-315, 317-328, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_summary_counts</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `total` count of tests is correctly calculated by summing up all passed and failed outcomes.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the total count does not match the expected value when running multiple tests with different outcomes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total number of tests should be equal to the sum of passed and failed outcomes.</li>
                                        <li>The total number of tests should be equal to 4 (2 passed, 1 failed, and 1 skipped).</li>
                                        <li>All test nodes should have a valid `outcome` attribute.</li>
                                        <li>All test nodes should have a valid `nodeid` attribute.</li>
                                        <li>All test nodes should have a valid `passed` or `failed` attribute.</li>
                                        <li>The `skipped` outcome should be represented as `None` in the summary.</li>
                                        <li>The `total` count should match the expected value when running multiple tests with different outcomes.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">13 lines (ranges: 156-158, 312, 314-315, 317-322, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_create_writer</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the `ReportWriter` class initializes correctly and returns a reference to its internal configuration.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the `ReportWriter` instance is not properly initialized with the expected configuration.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `config` attribute of the `writer` object should be set to an instance of `Config`.</li>
                                        <li>The `warnings` attribute of the `writer` object should be an empty list.</li>
                                        <li>The `artifacts` attribute of the `writer` object should be an empty list.</li>
                                        <li>The `writer` object should return a reference to its internal configuration, i.e., `config`, when created.</li>
                                        <li>The `writer` object's attributes (`warnings`, `artifacts`) should not have any values assigned to them initially.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 156-158)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_assembles_tests</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'ReportWriter::test_write_report_assembles_tests' verifies that the ReportWriter class writes a report with all tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the ReportWriter class does not write reports for all tests, potentially leading to missing information in test summaries.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the report.tests list should be equal to 2.</li>
                                        <li>The total number of tests in the summary should be equal to 2.</li>
                                        <li>All tests should have been included in the report.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">93 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_includes_coverage_percent</span>
                        <div class="test-meta">
                            <span>6ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that the `ReportWriter` class writes a report with a total coverage percentage.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the coverage percentage is not included in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report.summary.coverage_total_percent` property should be equal to the provided `coverage_percent` value.</li>
                                        <li>The `writer.write_report()` method should create a new report with the specified `coverage_percent` value.</li>
                                        <li>The `report.summary` object should have a `coverage_total_percent` attribute that matches the provided value.</li>
                                        <li>The coverage percentage is calculated correctly by summing up all individual coverage percentages.</li>
                                        <li>The test does not fail when the coverage percentage is less than 100% (e.g., 0%).</li>
                                        <li>The test does not fail when the coverage percentage is exactly 100% (e.g., 1.0%).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">93 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-199, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_includes_source_coverage</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test ReportWriter::test_write_report_includes_source_coverage verifies that the report includes source coverage summary.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the report does not include source coverage information, which is crucial for understanding code quality and identifying areas of improvement.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of `report.source_coverage` should be 1.</li>
                                        <li>The file path of `report.source_coverage[0]` should match 'src/foo.py'.</li>
                                        <li>All statements in the source coverage entry should be covered by the report.</li>
                                        <li>At least one statement out of the five missed statements should be covered by the report.</li>
                                        <li>The percentage of covered statements should be 87.5% or higher.</li>
                                        <li>Covered ranges should match '1-4, 6-7'.</li>
                                        <li>Missed ranges should not exceed '5'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">92 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_merges_coverage</span>
                        <div class="test-meta">
                            <span>5ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test Report Writer should merge coverage into tests.</p>
                                <p><strong>Why Needed:</strong> This test prevents a bug where the coverage is not merged correctly, causing reports to have incorrect coverage information.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The length of the report's coverage for 'test1' should be 1 (i.e., all lines are covered).</li>
                                        <li>The file path of the first line in the coverage for 'test1' should match 'src/foo.py'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">94 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186-189, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_atomic_write_fallback</span>
                        <div class="test-meta">
                            <span>6ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that the report writer falls back to direct write if atomic write fails and a file exists.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where an atomic write attempt fails, but the report still gets written to a temporary location.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file `report.json` should exist at the specified path.</li>
                                        <li>Any warnings with code 'W203' should be present in the `report.json` file.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">125 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506-507, 509-512, 515-516)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_creates_directory_if_missing</span>
                        <div class="test-meta">
                            <span>6ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that a directory is created if it does not exist.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the test fails due to an existing output directory.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'subdir' directory should be created in the specified path.</li>
                                        <li>The 'report.json' file should be written to this directory.</li>
                                        <li>The 'tmp_path / subdir / report.json' path should exist after calling write_report() method.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">84 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">123 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-477, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_ensure_dir_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that the test captures a warning when attempting to create a non-existent directory.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the report writer fails to capture warnings for directories that cannot be created.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `writer.warnings` list contains at least one warning with code 'W201' (Permission denied).</li>
                                        <li>The `writer.warnings` list does not contain any warnings with code other than 'W201'.</li>
                                        <li>The `writer.warnings` list is empty.</li>
                                        <li>Any of the warnings in `writer.warnings` have a non-zero code value.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 156-158, 470-473, 480-484)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_git_info_failure</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test 'test_git_info_failure' verifies that the `get_git_info` function handles git command failures gracefully by returning `None` for both SHA and dirty flags.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where the `get_git_info` function fails to return expected values when encountering a git command failure, potentially leading to incorrect report generation or downstream issues.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `get_git_info` function should return `None` for both SHA and dirty flags if the git command fails.</li>
                                        <li>The `get_git_info` function should not raise an exception when encountering a git command failure.</li>
                                        <li>The test should fail when the `get_git_info` function returns `None` for both SHA and dirty flags.</li>
                                        <li>The test should pass when the `subprocess.check_output` mock returns an exception with the correct error message.</li>
                                        <li>The test should verify that the returned values are consistent across different git command failures (e.g., 'git not found' vs. 'git: not found').</li>
                                        <li>The test should handle the case where the git command is not found but its output is available (e.g., when running `git --version`).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">9 lines (ranges: 67-73, 85-86)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_creates_file</span>
                        <div class="test-meta">
                            <span>31ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that the ReportWriter creates an HTML file with expected content.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the report writer does not create an HTML file as expected, potentially leading to incorrect reporting or output.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The report.html file should exist at the specified path.</li>
                                        <li>The report.html file should contain the expected content (tests 'test1', 'test2' and their respective outcomes).</li>
                                        <li>The report.html file should include the following text: 'PASSED', 'FAILED', 'Skipped', 'XFailed', 'XPassed', and 'Errors'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">115 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_includes_xfail_summary</span>
                        <div class="test-meta">
                            <span>32ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test `test_write_html_includes_xfail_summary` verifies that the report includes xfail outcomes in the HTML summary.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the report does not include xfail outcomes, which can make it difficult to diagnose issues with failed tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'XFAILED' and 'XPASSED' tags are present in the HTML summary.</li>
                                        <li>Both 'xfailed' and 'xpassed' tags are found in the HTML text.</li>
                                        <li>The 'XFailed' tag is also present in the HTML text.</li>
                                        <li>The 'XPASSED' tag is also present in the HTML text.</li>
                                        <li>All three 'XFAILED', 'XPASSED', and 'XFailed' tags are found in the HTML summary.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-326, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_json_creates_file</span>
                        <div class="test-meta">
                            <span>6ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that a JSON file is created with the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the `ReportWriter` does not create a JSON file even when there are tests to write.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `report.json` file should be created in the specified path.</li>
                                        <li>At least one artifact should be tracked for the report.</li>
                                        <li>The number of artifacts tracked should be greater than zero.</li>
                                        <li>The `ReportWriter` should create a JSON file even when there are tests to write.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">78 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">117 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_creates_file</span>
                        <div class="test-meta">
                            <span>34ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>LLM error:</strong> Failed after 3 retries. Last error: Failed to parse LLM response as JSON</p>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">125 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 401, 410, 412, 414-423, 434-435, 437-443, 448, 453, 455, 458-462, 470-471)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_missing_playwright_warns</span>
                        <div class="test-meta">
                            <span>6ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that a warning is raised when writing PDF reports with missing Playwright.</p>
                                <p><strong>Why Needed:</strong> This test prevents the 'Warning: PDF output requires Playwright' message from appearing when using PDF output.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The file 'report.pdf' should not exist.</li>
                                        <li>Any warnings related to 'PDF output requires Playwright' should be present in the list of warnings.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">98 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 401-405, 408)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer_coverage_v2.py::test_report_writer_ensure_dir_creation</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 4</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test ensures directory creation of report writer output.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential issue where the report writer does not create the expected directory structure.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `tmp_dir / 'r.html'` path exists before attempting to write to it.</li>
                                        <li>Any warnings from the report writer are of type 'W202'.</li>
                                        <li>A directory named `r.html` is created in the temporary location.</li>
                                        <li>The `tmp_dir / 'r.html'` path does not exist after writing to it.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">11 lines (ranges: 156-158, 470-477)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_report_writer_coverage_v2.py::test_report_writer_metadata_skips</span>
                        <div class="test-meta">
                            <span>9ms</span>
                            <span title="Covered file count">üõ°Ô∏è 5</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that report_writer_metadata_skips function prevents skipping of metadata when reports are disabled.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the report writer skips metadata when reports are disabled, ensuring accurate reporting and compliance with requirements.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'start_time' key should be present in the metadata dictionary.</li>
                                        <li>Metadata should contain an 'llm_model' key, which is expected to be None for this test case.</li>
                                        <li>The 'llm_model' value should not be provided in the metadata.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">36 lines (ranges: 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 107, 147)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">67 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_schemas.py::TestAnnotationSchema::test_from_dict_full</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that `AnnotationSchema.from_dict` creates a valid annotation from a dictionary with all required fields.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in case of missing or malformed input data, ensuring the test suite remains stable and reliable.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert schema.scenario == 'Verify login'</li>
                                        <li>assert schema.why_needed == 'Catch auth bugs'</li>
                                        <li>assert schema.key_assertions == ['assert 200', 'assert token']</li>
                                        <li>assert schema.confidence == 0.95</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_schemas.py::TestAnnotationSchema::test_to_dict_full</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Should convert to dictionary with all fields.</p>
                                <p><strong>Why Needed:</strong> Prevent regression in authentication logic by ensuring correct conversion of schema to dictionary.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>assert data['scenario'] == 'Verify login'</li>
                                        <li>assert data['why_needed'] == 'Catch auth bugs'</li>
                                        <li>assert data['key_assertions'] == ['assert 200', 'assert token']</li>
                                        <li>assert data['confidence'] == 0.95</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">8 lines (ranges: 90-92, 94-98)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_report_created</span>
                        <div class="test-meta">
                            <span>82ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that an HTML report is created and exists after running the test.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where the test fails to generate a report due to missing dependencies or configuration issues.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The report file should be created at the specified path.</li>
                                        <li>The report file should contain the expected content.</li>
                                        <li>The report file should have an HTML structure.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">101 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_summary_counts_all_statuses</span>
                        <div class="test-meta">
                            <span>118ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that HTML summary counts include all statuses.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in case of unexpected pass or skip status.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'XPassed' and 'XFailed' labels should be included in the summary.</li>
                                        <li>The 'Errors' label should also be included in the summary.</li>
                                        <li>All statuses (Total Tests, Passed, Failed, Skipped) should be counted correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">65 lines (ranges: 78-79, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212-214, 216, 227-228, 230-236, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">111 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-328, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_json_report_created</span>
                        <div class="test-meta">
                            <span>71ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The test verifies that a JSON report is created when the `--llm-report-json` option is used.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the report generation functionality fails to create a JSON file.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The path to the generated report should exist.</li>
                                        <li>The report data should contain the expected schema version, summary statistics, and counts.</li>
                                        <li>The total count of passed tests should match the actual number of passed tests in the report.</li>
                                        <li>The total count of failed tests should match the actual number of failed tests in the report.</li>
                                        <li>All test names should be included in the report's 'summary' section.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">51 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-118, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">107 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_annotations_in_report</span>
                        <div class="test-meta">
                            <span>83ms</span>
                            <span title="Covered file count">üõ°Ô∏è 13</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that LLM annotations are included in the report when a provider is enabled.</p>
                                <p><strong>Why Needed:</strong> Prevent regressions by ensuring LLM annotations are present in reports.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'scenario' key in the LLM annotation should match 'Checks the happy path'.</li>
                                        <li>The 'why_needed' key in the LLM annotation should contain a message that prevents regressions.</li>
                                        <li>The 'key_assertions' list within the LLM annotation should include 'asserts True'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 52-53, 72, 75, 80, 107, 110-111, 128, 136, 147, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">23 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69-70, 94-95, 97)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/schemas.py</span>
                                    <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">94 lines (ranges: 104-107, 109-111, 113, 115, 161-165, 167, 169, 171, 173, 176, 178-180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407-419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">186 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-340, 343, 345, 348-352, 355, 357-362, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_error_is_reported</span>
                        <div class="test-meta">
                            <span>6.08s</span>
                            <span title="Covered file count">üõ°Ô∏è 12</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM errors are surfaced in HTML output.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where LLM errors are not reported correctly.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'LLM error' should be present in the report HTML content.</li>
                                        <li>The 'boom' string should be present in the report HTML content.</li>
                                        <li>The presence of 'LLM error' and 'boom' strings should be verified by the test.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/cache.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 39-41, 53, 55-56, 86, 88, 118-119, 121, 153)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/annotator.py</span>
                                    <span style="color: var(--text-secondary)">73 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137-139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198-201, 203)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/base.py</span>
                                    <span style="color: var(--text-secondary)">21 lines (ranges: 52-53, 72, 75, 80, 107, 110-111, 128, 136, 147, 165, 167, 175, 245, 247, 249, 252, 257-258, 260)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 37-38, 44, 46, 49, 51-52, 54-60, 62-63, 78-79, 81-82, 84-85, 94-95, 97)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">186 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-340, 343, 345, 348-353, 357-362, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/prompts.py</span>
                                    <span style="color: var(--text-secondary)">29 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">101 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestMarkers::test_llm_opt_out_marker</span>
                        <div class="test-meta">
                            <span>56ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the LLM opt-out marker to ensure it correctly records LLM opt-out markers in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the LLM opt-out marker might not be recorded in the report, potentially leading to incorrect analysis results.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The 'llm_opt_out' attribute of each test is set to True when running with LLM opt-out enabled.</li>
                                        <li>The number of tests in the report is exactly one.</li>
                                        <li>Each test has an 'llm_opt_out' attribute that is set to False (indicating LLM opt-out) if it was run with LLM opt-out enabled.</li>
                                        <li>The 'llm_opt_out' attribute is present in each test and its value is True.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181-182, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180-182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestMarkers::test_requirement_marker</span>
                        <div class="test-meta">
                            <span>55ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the requirement marker functionality.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the requirement marker is not recorded correctly, leading to incorrect reporting of tests.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `pytest.mark.requirement` decorator should be applied to the test function with the required requirements.</li>
                                        <li>The `requirement` parameter in the `@pytest.mark.requirement` decorator should match one of the provided requirement strings (REQ-001 or REQ-002).</li>
                                        <li>The `requirements` list in the test function's docstring should contain both 'REQ-001' and 'REQ-002'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-200, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188-190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_multiple_xfail_outcomes</span>
                        <div class="test-meta">
                            <span>61ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verify that multiple xfailed tests are recorded in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression by ensuring that all xfailed tests are properly reported and counted.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total number of xfailed tests is 2 (test_xfail_one) and test_xfail_two)</li>
                                        <li>All xfailed tests are correctly recorded in the report (outcome == 'xfailed')</li>
                                        <li>No other tests are incorrectly reported or missed (all outcomes match)</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">108 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-324, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_skip_outcome</span>
                        <div class="test-meta">
                            <span>56ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that skipped tests are recorded and reported correctly.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where the 'skip' marker is not properly recorded or reported in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The number of skipped tests should be 1 according to the report.</li>
                                        <li>The 'summary' key in the report data should contain the string 'skipped'.</li>
                                        <li>The value of the 'skipped' key in the report data should be 1.</li>
                                        <li>The test is skipped and recorded in the report.</li>
                                        <li>The test is not skipped and does not appear to be recorded in the report.</li>
                                        <li>The report path contains a file with the name 'report.json'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">43 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 106-107, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">107 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321-322, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_xfail_outcome</span>
                        <div class="test-meta">
                            <span>57ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the test 'test_xfail' is marked as Xfailed and its outcome is recorded in the report.</p>
                                <p><strong>Why Needed:</strong> This test prevents a regression where Xfailed tests are not correctly recorded in the report.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The test 'test_xfail' should be marked as Xfailed by pytester.makepyfile.</li>
                                        <li>The assertion False in test_xfail should fail and record an outcome of 1 in the report.</li>
                                        <li>The value of data['summary']['xfailed'] in the report.json file should be 1.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">47 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">108 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-324, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestParametrization::test_parametrized_tests</span>
                        <div class="test-meta">
                            <span>59ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the functionality of parameterized tests to ensure they are recorded separately and their results are accurate.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the parametrization feature, ensuring that all test cases are correctly recorded and reported.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The total number of successful tests should be equal to the expected number of tests (3).</li>
                                        <li>All tests passed successfully (x > 0).</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/models.py</span>
                                    <span style="color: var(--text-secondary)">74 lines (ranges: 161-165, 167, 169-171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_help_contains_examples</span>
                        <div class="test-meta">
                            <span>48ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The CLI help text should include usage examples.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the user is not informed about available usage examples in the CLI help text.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `--help` option is present in the CLI help text.</li>
                                        <li>The example usage is included at the end of the help message.</li>
                                        <li>The example usage is preceded by a comment indicating it's an example.</li>
                                        <li>The example usage is not truncated or removed from the output.</li>
                                        <li>The example usage is displayed correctly in the terminal.</li>
                                        <li>The example usage does not cause any issues with the test suite.</li>
                                        <li>The example usage is consistent across different platforms and environments.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_markers_registered</span>
                        <div class="test-meta">
                            <span>44ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test that LLM markers are registered and their presence is checked.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the LLM markers are not properly registered or are not present in the output.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The string 'llm_opt_out*' should be found in the stdout of the runpytest command.</li>
                                        <li>The string 'llm_context*' should be found in the stdout of the runpytest command.</li>
                                        <li>The string 'requirement*' should be found in the stdout of the runpytest command.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_plugin_registered</span>
                        <div class="test-meta">
                            <span>51ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> A plugin is successfully registered using pytest11 and the LLM report is displayed.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential issue where the LLM report is not displayed when registering a plugin via pytest11.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `--help` flag is run with `pytester.runpytest` to verify that the LLM report is displayed.</li>
                                        <li>The `stdout.fnmatch_lines` method checks if the output matches the expected pattern (`*--llm-report*`).</li>
                                        <li>The plugin registration process completes successfully without any errors or warnings.</li>
                                        <li>The LLM report is displayed in the console output.</li>
                                        <li>The `pytester.runpytest` function runs with the correct flags and options.</li>
                                        <li>The `--help` flag is recognized by pytester.runpytest as a valid option.</li>
                                        <li>The `stdout` object has a `fnmatch_lines` method that can be used to check the output.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">45 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">118 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 387-388, 391, 395-397)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_smoke_pytester.py::TestSpecialCharacters::test_special_chars_in_nodeid</span>
                        <div class="test-meta">
                            <span>82ms</span>
                            <span title="Covered file count">üõ°Ô∏è 7</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test verifies that special characters in nodeid do not cause the test to crash or produce invalid HTML.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential regression where special characters in nodeids could cause the Pytest report to fail or be malformed.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The string '<html' is present in the HTML content of the report file.</li>
                                        <li>The HTML content contains only valid HTML tags.</li>
                                        <li>The report file exists and has a valid HTML structure.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/coverage_map.py</span>
                                    <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/errors.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/options.py</span>
                                    <span style="color: var(--text-secondary)">46 lines (ranges: 107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">166 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/render.py</span>
                                    <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/report_writer.py</span>
                                    <span style="color: var(--text-secondary)">101 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_boundary_one_minute</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the 'format_duration' function with a boundary of exactly one minute.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function incorrectly formats less than one minute as '1m Xs', rather than correctly displaying it as 'Xm Xs'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of calling format_duration(60.0) should be exactly '1m 0.0s'.</li>
                                        <li>The function should handle cases where the input is less than one minute (e.g., 59 seconds) correctly and display it as 'Xm Xs'.</li>
                                        <li>The function should not incorrectly format more than one minute as 'Xm Xs', but rather as the correct duration string.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_microseconds_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function with a duration of 500 microseconds.</p>
                                <p><strong>Why Needed:</strong> Prevents regression where durations are not formatted as expected (e.g., 0.0005 seconds becomes 500 microseconds).</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result contains the string 'Œºs' to indicate microsecond format.</li>
                                        <li>The duration is equal to 500 microseconds.</li>
                                        <li>The function correctly formats a duration of 0.0005 as 500 microseconds.</li>
                                        <li>The function handles negative durations (e.g., -0.0005) by returning the correct result.</li>
                                        <li>The function does not silently truncate or round the input values.</li>
                                        <li>The function preserves the original precision of the input value.</li>
                                        <li>The function raises an error if the input is not a number.</li>
                                        <li>The function handles very large durations (e.g., 1.23456789012345678901234567890123456789) correctly.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 39-40)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_milliseconds_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function with a duration of 500 milliseconds.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function does not correctly format durations as milliseconds.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result should contain 'ms' in its string representation.</li>
                                        <li>The value of the result should be exactly 500.0 when converted to a float.</li>
                                        <li>The formatted string should match the expected output, '500.0ms'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 39, 41-42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_minutes_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the minutes format of a duration.</p>
                                <p><strong>Why Needed:</strong> Prevents regression when durations are formatted to include only minutes.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output should contain 'm' (minutes) and 's' (seconds).</li>
                                        <li>The output should be in the format '1m 30.5s'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_multiple_minutes</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the 'format_duration' function with a duration of multiple minutes.</p>
                                <p><strong>Why Needed:</strong> Prevents regression due to incorrect handling of multi-minute durations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result should be in the format 'mm:ss'.</li>
                                        <li>The seconds part should be exactly 5.0.</li>
                                        <li>The minutes part should be exactly 3.</li>
                                        <li>The function should handle cases where the input is not an integer.</li>
                                        <li>The function should handle cases where the input is negative.</li>
                                        <li>The function should handle cases where the input is zero.</li>
                                        <li>The function should return a string in the 'mm:ss' format for durations greater than one minute.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_one_second</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function with a duration of exactly one second.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function incorrectly returns '1s' instead of '1.00s'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function should return '1.00s' for a duration of exactly one second.</li>
                                        <li>The function should handle non-integer inputs correctly.</li>
                                        <li>The function should not silently truncate decimal places in the output.</li>
                                        <li>The function should handle very large or very small input values without errors.</li>
                                        <li>The function should support negative durations.</li>
                                        <li>The function should return '0.00s' for a duration of exactly zero.</li>
                                        <li>The function should raise an error if the input is not a non-negative number.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 39, 41, 43-44)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_seconds_format</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function to ensure it correctly formats seconds under a minute.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression where the function does not format seconds as 'xx.xxxx' but instead returns 'x.xxxx'.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of `format_duration(5.5)` contains the string 's'</li>
                                        <li>The result of `format_duration(5.5)` is equal to '5.50s'</li>
                                        <li>The function correctly formats seconds under a minute</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">4 lines (ranges: 39, 41, 43-44)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_small_milliseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the format of small millisecond durations.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function returns incorrect results for very short durations.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function `format_duration(0.001)` should return '1.0ms' when given a duration of exactly 1 millisecond.</li>
                                        <li>The function does not raise an error or produce an unexpected result when given a duration that is too small (e.g., negative values, very large positive values).</li>
                                        <li>The function correctly formats the output as 'X.XXms' for durations between 0.001 and 10 milliseconds.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">3 lines (ranges: 39, 41-42)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestFormatDuration::test_very_small_microseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the `format_duration` function with a duration of 1 microsecond.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in handling very small durations as microseconds.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The function correctly formats the duration to '1Œºs' when given a value of 0.000001 seconds.</li>
                                        <li>The function handles negative values and non-numeric inputs without errors.</li>
                                        <li>The function preserves the original unit (seconds) in the output.</li>
                                        <li>The function does not silently truncate or round the result.</li>
                                        <li>The function correctly handles very small positive and negative values.</li>
                                        <li>The function raises an error for invalid input types (e.g., strings, lists).</li>
                                        <li>The function returns a string representation of the duration that matches the expected format.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">2 lines (ranges: 39-40)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_datetime_with_utc</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the functionality of formatting datetime objects with UTC timezone.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the datetime object is not correctly formatted when using the UTC timezone.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The datetime object `dt` is created with the correct timezone (UTC).</li>
                                        <li>The `iso_format(dt)` method returns the expected string representation of the datetime object in UTC timezone.</li>
                                        <li>The resulting string does not contain any non-ASCII characters, which is a valid ISO 8601 format for dates and times.</li>
                                        <li>The time component of the datetime object (`10:30:45`) is correctly formatted as a UTC time zone.</li>
                                        <li>The AM/PM indicator is correctly handled (e.g., `T` indicates 12-hour clock).</li>
                                        <li>The resulting string does not contain any invalid characters or formatting errors.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_naive_datetime</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Tests the ISO format function with a naive datetime without timezone.</p>
                                <p><strong>Why Needed:</strong> Prevents a potential bug where the function incorrectly formats naive datetimes as if they had a timezone.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The output of iso_format(dt) should be '2024-06-20T14:00:00'.</li>
                                        <li>The format string should match the expected output exactly.</li>
                                        <li>Any timezone information (like UTC) is ignored in naive datetimes.</li>
                                        <li>The function handles different date and time formats correctly.</li>
                                        <li>No exceptions are raised when given a naive datetime without a timezone.</li>
                                        <li>The function returns the correct result for different input dates and times.</li>
                                        <li>The function does not perform any unnecessary conversions or calculations.</li>
                                        <li>Any potential errors in the input data are ignored by the function.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_with_microseconds</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Test the `iso_format` function with a datetime object that includes microseconds.</p>
                                <p><strong>Why Needed:</strong> This test prevents a potential bug where the `iso_format` function does not correctly handle datetime objects with microseconds.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The `result` variable should contain the string '123456'.</li>
                                        <li>The `result` variable should be equal to '123456'.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestUtcNow::test_has_utc_timezone</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `utc_now()` function returns a datetime object with an associated UTC timezone.</p>
                                <p><strong>Why Needed:</strong> Prevents regression in tests where the current system time is not set to UTC.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The returned datetime object has a valid timezone.</li>
                                        <li>The returned datetime object's timezone is set to UTC.</li>
                                        <li>The `tzinfo` attribute of the returned datetime object is not None.</li>
                                        <li>The `tzinfo` attribute of the returned datetime object is equal to `UTC`.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestUtcNow::test_is_current_time</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> Verifies that the `utc_now()` function returns a current time within a specified tolerance when called on UTC.</p>
                                <p><strong>Why Needed:</strong> This test prevents regression in the `utc_now()` function, which may return incorrect results if it is not called on UTC.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>The result of `utc_now()` should be within the range `[before, after]` (inclusive).</li>
                                        <li>The difference between `result` and `after` should be less than or equal to the tolerance specified in the `utc_now()` function.</li>
                                        <li>The difference between `result` and `before` should be greater than or equal to 0.</li>
                                        <li>If `utc_now()` is called on a different timezone, it should return an incorrect result.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
            <div class="test-row outcome-passed" data-status="passed">
                <details open>
                    <summary class="test-header">
                        <span class="status-badge status-passed">PASSED</span>
                        <span class="test-name">tests/test_time.py::TestUtcNow::test_returns_datetime</span>
                        <div class="test-meta">
                            <span>1ms</span>
                            <span title="Covered file count">üõ°Ô∏è 3</span>
                        </div>
                    </summary>

                    <div class="test-details">

                        <div class="detail-section">
                            <div class="detail-title">AI Assessment</div>
                            <div class="llm-annotation">
                                <p><strong>Scenario:</strong> The function `utc_now()` returns a datetime object.</p>
                                <p><strong>Why Needed:</strong> This test prevents the regression of returning a datetime object when calling `utc_now()`.</p>
                                <div class="key-assertions">
                                    <strong>Key Assertions:</strong>
                                    <ul>
                                        <li>result is an instance of `datetime`.</li>
                                        <li>result has a valid timezone.</li>
                                        <li>result is not None.</li>
                                        <li>result is not a string.</li>
                                        <li>result is not a timedelta.</li>
                                        <li>result is not a naive datetime object.</li>
                                        <li>result's tz is set to the UTC timezone.</li>
                                        <li>result's year, month and day are correctly set.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="detail-section">
                            <div class="detail-title">Coverage</div>
                            <div class="coverage-list">
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/collector.py</span>
                                    <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/plugin.py</span>
                                    <span style="color: var(--text-secondary)">6 lines (ranges: 387-388, 391, 395-397)</span>
                                </div>
                                <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                    <span>src/pytest_llm_report/util/time.py</span>
                                    <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                </div>
                            </div>
                        </div>
                    </div>
                </details>
            </div>
        </div>

        <section class="source-coverage">
            <h2>Source Coverage</h2>
            <div class="source-coverage-table">
                <div class="source-coverage-header">
                    <span>File</span>
                    <span>Stmts</span>
                    <span>Miss</span>
                    <span>Cover</span>
                    <span>%</span>
                    <span>Covered Lines</span>
                    <span>Missed Lines</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/_git_info.py</span>
                    <span>2</span>
                    <span>0</span>
                    <span>2</span>
                    <span>100.0%</span>
                    <span class="source-lines">2-3</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/aggregation.py</span>
                    <span>116</span>
                    <span>5</span>
                    <span>111</span>
                    <span>95.69%</span>
                    <span class="source-lines">13, 15-19, 21, 35, 38, 44, 46, 52-53, 55-57, 59, 61-64, 69, 73-74, 77-80, 84, 87-89, 93, 103, 109-111, 113-117, 119-120, 125, 127-128, 130-131, 134-135, 141-144, 146, 148, 162, 164, 168, 170, 172, 182, 184-188, 190-191, 194, 196, 205, 217, 219-233, 235, 237, 245-246, 248-249, 251, 253-255, 259, 262-263, 265-266, 269-271, 273, 275-276, 280</span>
                    <span class="source-lines">66, 90-91, 192, 203</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/cache.py</span>
                    <span>47</span>
                    <span>3</span>
                    <span>44</span>
                    <span>93.62%</span>
                    <span class="source-lines">13, 15-19, 21, 27, 33, 39-41, 43, 53, 55-56, 58, 60-62, 68-69, 78, 86, 88, 90, 92, 94, 97, 103, 107, 118-119, 121, 123, 129, 132-136, 141, 144, 153</span>
                    <span class="source-lines">64-65, 130</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/collector.py</span>
                    <span>111</span>
                    <span>2</span>
                    <span>109</span>
                    <span>98.2%</span>
                    <span class="source-lines">19, 21-22, 24, 26-27, 33-34, 45-50, 52, 58, 60-62, 69, 78-79, 81, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127-128, 130, 132-133, 135-137, 140, 143, 155, 163-164, 167-169, 171, 173, 181-182, 185-189, 191, 198-200, 202, 209-210, 212-214, 216, 218, 227-228, 230-236, 238, 241, 250-252, 254, 261, 264-265, 268-269, 271, 277, 279, 285</span>
                    <span class="source-lines">141, 239</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/coverage_map.py</span>
                    <span>135</span>
                    <span>10</span>
                    <span>125</span>
                    <span>92.59%</span>
                    <span class="source-lines">13, 15-17, 19-22, 30, 38, 44-45, 47, 58-60, 64, 72-73, 83, 86, 88-90, 92, 94-96, 98, 101-104, 106-108, 114, 116, 118, 121-122, 127, 131-135, 137-140, 144-146, 148, 150, 152-153, 156, 160-162, 165, 167-168, 173, 176, 178-184, 187-189, 191, 196, 199-200, 202, 204, 216-217, 220, 224-225, 228-234, 236, 239, 241, 243-244, 246-248, 250, 252-254, 259-260, 263-264, 271, 273, 276-279, 281-283, 285, 299-300, 302, 308</span>
                    <span class="source-lines">62, 123, 125, 128, 157, 221, 249, 251, 257, 274</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/errors.py</span>
                    <span>35</span>
                    <span>0</span>
                    <span>35</span>
                    <span>100.0%</span>
                    <span class="source-lines">8-9, 12, 25-28, 31-36, 39-42, 45-46, 49-51, 54-55, 64-66, 68, 70, 74-76, 80, 129, 139</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/__init__.py</span>
                    <span>3</span>
                    <span>0</span>
                    <span>3</span>
                    <span>100.0%</span>
                    <span class="source-lines">4-5, 7</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/annotator.py</span>
                    <span>110</span>
                    <span>0</span>
                    <span>110</span>
                    <span>100.0%</span>
                    <span class="source-lines">4, 6-10, 12-15, 21-22, 25-28, 31, 45-46, 48-50, 54, 56-57, 59, 61-62, 64, 66-68, 71-72, 74-82, 87, 97-98, 100, 102, 104-105, 115, 127, 129-132, 137-139, 142, 165-168, 170-171, 176, 178, 180-183, 185-190, 192-193, 198-201, 203, 206, 229-232, 234, 236-237, 239-240, 245-246, 248-253, 255-256, 261-264, 266</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/base.py</span>
                    <span>78</span>
                    <span>0</span>
                    <span>78</span>
                    <span>100.0%</span>
                    <span class="source-lines">13, 15-18, 26, 40, 46, 52-53, 55, 72, 75-76, 78, 80, 101, 107-108, 110-111, 122, 128, 130, 136, 138, 147, 149, 165, 167-173, 175, 177, 186-187, 190-192, 194-195, 198-200, 203-208, 212, 214, 220-221, 224-225, 228-230, 233, 245, 247, 249-250, 252-253, 255, 257-258, 260, 262-263, 265, 267</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/gemini.py</span>
                    <span>275</span>
                    <span>18</span>
                    <span>257</span>
                    <span>93.45%</span>
                    <span class="source-lines">7, 9-13, 15-16, 23-27, 30-34, 37-42, 44-46, 48-50, 52, 57-63, 65-70, 72-73, 75-78, 80-85, 87-88, 91-97, 99-103, 105, 107-114, 121-122, 125, 128, 134, 136-139, 141-142, 144, 160-161, 167-169, 171-172, 174, 176-184, 186-188, 190-191, 193, 196, 200-208, 210-211, 213-215, 217-223, 225-227, 233-234, 238-239, 242-243, 245-248, 252-253, 260, 266-267, 269, 273-277, 279-283, 286-287, 292-293, 300-301, 303, 315, 317-318, 322, 327, 330-332, 335-343, 345-346, 348, 352-355, 357, 360-366, 368-374, 380-382, 384-387, 389, 391-392, 396-402, 405, 408-410, 412-414, 416-421, 427-428, 430-434, 437-440, 442-443, 445-447</span>
                    <span class="source-lines">89, 104, 106, 115-117, 199, 230-231, 235-237, 244, 250, 256, 367, 441, 444</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/litellm_provider.py</span>
                    <span>32</span>
                    <span>1</span>
                    <span>31</span>
                    <span>96.88%</span>
                    <span class="source-lines">7, 9, 11-12, 18, 21, 37-38, 44, 46, 49, 51-52, 54-56, 66-67, 69-70, 73, 76, 78-79, 81-82, 84, 88, 94-95, 97</span>
                    <span class="source-lines">74</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/noop.py</span>
                    <span>13</span>
                    <span>0</span>
                    <span>13</span>
                    <span>100.0%</span>
                    <span class="source-lines">8, 10, 12-13, 20, 26, 32, 34, 50, 52, 58, 60, 66</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/ollama.py</span>
                    <span>43</span>
                    <span>1</span>
                    <span>42</span>
                    <span>97.67%</span>
                    <span class="source-lines">7, 9, 11-12, 18, 24, 40-41, 47, 50, 52, 54-55, 57-60, 62-63, 66-67, 71-72, 74-75, 77, 81, 87-88, 90-92, 96, 102, 104, 114, 116-117, 127, 132, 134-135</span>
                    <span class="source-lines">69</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/schemas.py</span>
                    <span>36</span>
                    <span>1</span>
                    <span>35</span>
                    <span>97.22%</span>
                    <span class="source-lines">8, 10-12, 16, 22, 38, 42-44, 46-47, 50-53, 55, 58-59, 62-65, 67-68, 77, 84, 90, 94-98, 102, 130</span>
                    <span class="source-lines">39</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/models.py</span>
                    <span>240</span>
                    <span>10</span>
                    <span>230</span>
                    <span>95.83%</span>
                    <span class="source-lines">17-18, 21, 24-25, 34-36, 38, 40, 47-48, 61-67, 69, 71, 82-83, 95-100, 102, 104, 109-115, 118-119, 141-157, 159, 161, 167-171, 173-182, 184, 186, 188-190, 193-194, 202-203, 205, 207, 213-214, 223-225, 227, 229, 233-235, 238-239, 248-250, 252, 254, 261-262, 271-273, 275, 277, 281-283, 286-287, 324-353, 355-360, 362, 364, 382-405, 407-419, 422-423, 437-445, 447, 449, 459, 461, 464-465, 482-492, 494, 500, 502, 508-512, 514, 516, 518, 520, 522</span>
                    <span class="source-lines">172, 183, 185, 187, 460, 513, 515, 517, 519, 521</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/options.py</span>
                    <span>117</span>
                    <span>45</span>
                    <span>72</span>
                    <span>61.54%</span>
                    <span class="source-lines">106, 146, 175, 178-180, 185-187, 193-195, 201-203, 209-218, 220, 224, 233, 248, 251-267, 270-283, 286-295, 298, 300</span>
                    <span class="source-lines">13-15, 21-22, 90-94, 97-99, 102-105, 122-123, 126-132, 135-137, 140-142, 145, 156-160, 163-164, 167, 169, 222, 227, 236</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/plugin.py</span>
                    <span>156</span>
                    <span>25</span>
                    <span>131</span>
                    <span>83.97%</span>
                    <span class="source-lines">40, 43, 49, 55, 61, 67, 73, 80, 89, 95, 101, 107, 113, 121, 126, 131, 136, 142, 147, 153, 169, 173, 177, 183-184, 187-188, 190, 192, 195-197, 203-204, 212-213, 238-239, 242-243, 246, 249-250, 252-253, 256-257, 259, 261-265, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 307, 309-314, 317-318, 322-323, 331-332, 337-340, 343, 345, 348-353, 355, 357, 365-366, 387-388, 391-392, 395-397, 408-409, 412, 415-416, 419-421, 431-432, 435-437, 448-449, 452, 455, 457-458</span>
                    <span class="source-lines">13, 15-17, 19-20, 22, 28-31, 34, 160, 216, 319, 327-328, 333-334, 379-380, 400, 424, 440-441</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/prompts.py</span>
                    <span>75</span>
                    <span>5</span>
                    <span>70</span>
                    <span>93.33%</span>
                    <span class="source-lines">13, 15-17, 24, 27, 33, 35, 49, 52, 55, 58-61, 63, 65, 67, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116, 118, 132-133, 135-138, 140-141, 144-145, 148, 151-152, 154-156, 158-159, 163, 165, 180, 182, 191-194</span>
                    <span class="source-lines">80, 114, 142, 146, 149</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/render.py</span>
                    <span>50</span>
                    <span>0</span>
                    <span>50</span>
                    <span>100.0%</span>
                    <span class="source-lines">13, 15-16, 18, 24, 30-31, 34, 40, 42, 50-51, 53, 56, 65-67, 70, 79, 87, 90, 99, 101-102, 107, 110, 121-124, 126-129, 131-134, 141-143, 145, 158-163, 177, 196</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/report_writer.py</span>
                    <span>167</span>
                    <span>10</span>
                    <span>157</span>
                    <span>94.01%</span>
                    <span class="source-lines">13, 15-25, 27-29, 46, 55, 58, 67-68, 76, 83-84, 89, 98-100, 102, 105-108, 110, 116, 127-128, 130, 142, 150, 156-158, 160, 186-189, 192, 197-199, 202-203, 211, 222-223, 226-227, 230-231, 233, 235, 254, 256-259, 262-264, 266, 268, 303, 312, 314-315, 317-328, 330, 332, 340, 343-345, 348-349, 352-354, 357, 360, 368, 376, 378-379, 382, 385, 388, 391, 399, 401-402, 408, 410, 412, 414-423, 434-435, 437-439, 447-448, 453, 455, 458, 461-462, 464, 470-474, 480-481, 488, 495, 497, 499-501, 503, 506-507, 509, 515-516</span>
                    <span class="source-lines">113, 135-137, 424-425, 432, 449-451</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/fs.py</span>
                    <span>34</span>
                    <span>3</span>
                    <span>31</span>
                    <span>91.18%</span>
                    <span class="source-lines">11, 13-14, 17, 30, 33, 36, 39, 42, 45, 55-56, 58-60, 63-64, 70, 79, 82, 100, 103, 111-113, 116-117, 119-121, 123</span>
                    <span class="source-lines">40, 65, 67</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/hashing.py</span>
                    <span>36</span>
                    <span>0</span>
                    <span>36</span>
                    <span>100.0%</span>
                    <span class="source-lines">12, 14-17, 23, 32, 35, 44-48, 51, 61, 64, 73-74, 76-78, 80-81, 86, 96, 103-104, 107, 113-114, 116-121</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/ranges.py</span>
                    <span>33</span>
                    <span>0</span>
                    <span>33</span>
                    <span>100.0%</span>
                    <span class="source-lines">12, 15, 29-30, 33, 35-37, 39-40, 42, 45-47, 50, 52, 55, 65-67, 70, 81-82, 84-91, 93, 95</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/time.py</span>
                    <span>16</span>
                    <span>0</span>
                    <span>16</span>
                    <span>100.0%</span>
                    <span class="source-lines">4, 6, 9, 15, 18, 27, 30, 39-44, 46-48</span>
                    <span class="source-lines">-</span>
                </div>
            </div>
        </section>
    </div>
</body>
</html>