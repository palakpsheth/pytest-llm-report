<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Test Report &bull; 532 tests</title>
    <!-- Optional: Inter font from rsms.me CDN. Falls back to system fonts if unavailable. -->
    <link rel="stylesheet" href="https://rsms.me/inter/inter.css">
    <style>
/* Modern Color Palette */
:root {
    --bg-color: #f8fafc;
    --text-primary: #1e293b;
    --text-secondary: #64748b;
    --border-color: #e2e8f0;
    --card-bg: #ffffff;
    --surface-muted: #f1f5f9;
    --primary-color: #3b82f6;
    color-scheme: light dark;

    /* Status Colors */
    --passed-bg: #dcfce7;
    --passed-text: #166534;
    --failed-bg: #fee2e2;
    --failed-text: #991b1b;
    --skipped-bg: #fef9c3;
    --skipped-text: #854d0e;
    --xfailed-bg: #ffedd5;
    --xfailed-text: #9a3412;
    --xpassed-bg: #f3e8ff;
    --xpassed-text: #6b21a8;
    --error-bg: #fee2e2;
    --error-text: #991b1b;
}

body {
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    background-color: var(--bg-color);
    color: var(--text-primary);
    line-height: 1.5;
    margin: 0;
    padding: 0;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 2rem;
}

/* Header */
header {
    margin-bottom: 2rem;
    border-bottom: 1px solid var(--border-color);
    padding-bottom: 1rem;
    display: flex;
    justify-content: space-between;
    align-items: center;
}

h1 {
    font-size: 1.875rem;
    font-weight: 700;
    color: var(--text-primary);
    margin: 0;
}

.meta {
    font-size: 0.875rem;
    color: var(--text-secondary);
}

/* Summary Grid */
.summary {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
    gap: 1rem;
    margin-bottom: 2rem;
}

.summary-card {
    background: var(--card-bg);
    border-radius: 0.5rem;
    padding: 1.5rem;
    box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.1), 0 1px 2px -1px rgb(0 0 0 / 0.1);
    text-align: center;
    border: 1px solid var(--border-color);
    transition: transform 0.2s;
}

.summary-card:hover {
    transform: translateY(-2px);
}

.summary-card .count {
    font-size: 2.25rem;
    font-weight: 700;
    line-height: 1;
    margin-bottom: 0.5rem;
}

.summary-card .label {
    text-transform: uppercase;
    font-size: 0.75rem;
    font-weight: 600;
    letter-spacing: 0.05em;
    color: var(--text-secondary);
}

/* Status Colors for Summary */
.summary-card.passed .count {
    color: var(--passed-text);
}

.summary-card.failed .count {
    color: var(--failed-text);
}

.summary-card.skipped .count {
    color: var(--skipped-text);
}

.summary-card.xfailed .count {
    color: var(--xfailed-text);
}

.summary-card.xpassed .count {
    color: var(--xpassed-text);
}

.summary-card.coverage .count {
    color: var(--primary-color);
}

/* Filters */
.filters {
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.5rem;
    border: 1px solid var(--border-color);
    margin-bottom: 1.5rem;
    display: flex;
    flex-direction: column;
    gap: 0.75rem;
}

.filter-input {
    flex: 1;
    padding: 0.5rem 1rem;
    border: 1px solid var(--border-color);
    border-radius: 0.375rem;
    font-size: 0.875rem;
    background: var(--card-bg);
    color: var(--text-primary);
}

.filter-input::placeholder {
    color: var(--text-secondary);
}

.filter-statuses {
    display: flex;
    flex-wrap: wrap;
    gap: 0.5rem;
}

.filter-chip {
    display: inline-flex;
    align-items: center;
    gap: 0.35rem;
    padding: 0.25rem 0.75rem;
    border-radius: 9999px;
    border: 1px solid var(--border-color);
    background: var(--surface-muted);
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
    letter-spacing: 0.04em;
}

.filter-chip input {
    margin: 0;
}

.filter-chip.passed {
    background: var(--passed-bg);
    color: var(--passed-text);
}

.filter-chip.failed {
    background: var(--failed-bg);
    color: var(--failed-text);
}

.filter-chip.skipped {
    background: var(--skipped-bg);
    color: var(--skipped-text);
}

.filter-chip.xfailed {
    background: var(--xfailed-bg);
    color: var(--xfailed-text);
}

.filter-chip.xpassed {
    background: var(--xpassed-bg);
    color: var(--xpassed-text);
}

.filter-chip.error {
    background: var(--error-bg);
    color: var(--error-text);
}

/* Test List */
.test-list {
    display: flex;
    flex-direction: column;
    gap: 0.75rem;
}

.test-row {
    background: var(--card-bg);
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
    overflow: hidden;
}

.test-header {
    padding: 1rem;
    display: flex;
    align-items: center;
    gap: 1rem;
    cursor: pointer;
    background: var(--card-bg);
}

.test-header:hover {
    background: var(--surface-muted);
}

.status-badge {
    padding: 0.25rem 0.75rem;
    border-radius: 9999px;
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
}

.status-passed {
    background: var(--passed-bg);
    color: var(--passed-text);
}

.status-failed {
    background: var(--failed-bg);
    color: var(--failed-text);
}

.status-skipped {
    background: var(--skipped-bg);
    color: var(--skipped-text);
}

.status-xfailed {
    background: var(--xfailed-bg);
    color: var(--xfailed-text);
}

.status-xpassed {
    background: var(--xpassed-bg);
    color: var(--xpassed-text);
}

.status-error {
    background: var(--error-bg);
    color: var(--error-text);
}

.test-name {
    flex: 1;
    font-family: monospace;
    font-size: 0.9rem;
    color: var(--text-primary);
    word-break: break-all;
}

.test-meta {
    display: flex;
    gap: 1rem;
    align-items: center;
    color: var(--text-secondary);
    font-size: 0.875rem;
}

/* Details Section */
.test-details {
    padding: 0 1rem 1rem 1rem;
    border-top: 1px solid var(--border-color);
    background: var(--surface-muted);
}

.detail-section {
    margin-top: 1rem;
}

.detail-title {
    font-size: 0.75rem;
    font-weight: 600;
    text-transform: uppercase;
    color: var(--text-secondary);
    margin-bottom: 0.5rem;
}

.coverage-item {
    font-family: monospace;
    font-size: 0.85rem;
    padding: 0.25rem 0;
    border-bottom: 1px solid var(--border-color);
    display: grid;
    grid-template-columns: minmax(200px, 2fr) minmax(120px, 1fr);
    gap: 1rem;
}

.coverage-list {
    background: var(--card-bg);
    border-radius: 0.375rem;
    border: 1px solid var(--border-color);
    overflow: hidden;
}

.source-coverage {
    margin-top: 2rem;
}

.source-coverage h2 {
    margin: 0 0 1rem;
    font-size: 1.5rem;
}

.source-coverage-table {
    display: grid;
    gap: 0.35rem;
}

.source-coverage-header,
.source-coverage-row {
    display: grid;
    grid-template-columns: minmax(200px, 2fr) repeat(4, minmax(60px, 0.5fr)) minmax(
            140px,
            1fr
        ) minmax(140px, 1fr);
    align-items: center;
    gap: 0.75rem;
    padding: 0.75rem 1rem;
    border-radius: 0.5rem;
}

.source-coverage-header {
    background: var(--surface-muted);
    font-size: 0.75rem;
    font-weight: 700;
    text-transform: uppercase;
    letter-spacing: 0.04em;
    color: var(--text-secondary);
}

.source-coverage-row {
    background: var(--card-bg);
    border: 1px solid var(--border-color);
    font-size: 0.85rem;
}

.source-path {
    font-family: monospace;
    word-break: break-word;
}

.source-lines {
    font-family: monospace;
    color: var(--text-secondary);
    word-break: break-word;
}

.llm-annotation {
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.375rem;
    border: 1px solid var(--border-color);
}

.llm-annotation p {
    margin: 0 0 0.5rem 0;
}

.llm-annotation p:last-child {
    margin-bottom: 0;
}

.llm-annotation ul {
    margin: 0.5rem 0 0;
    padding-left: 1.25rem;
}

.llm-annotation li {
    margin-bottom: 0.25rem;
}

.error-message {
    font-family: monospace;
    color: var(--failed-text);
    background: var(--card-bg);
    padding: 1rem;
    border-radius: 0.375rem;
    border: 1px solid var(--failed-bg);
    white-space: pre-wrap;
    overflow-x: auto;
}

/* HTML5 Progress Bar for Coverage */
progress {
    width: 60px;
}

/* Utility: Hidden state for filtering */
.hidden {
    display: none !important;
}

/* Dark Mode Support */
@media (prefers-color-scheme: dark) {
    :root {
        --bg-color: #0f172a;
        --text-primary: #f1f5f9;
        --text-secondary: #94a3b8;
        --border-color: #334155;
        --card-bg: #1e293b;
        --surface-muted: #0b1220;
        --primary-color: #60a5fa;

        /* Status Colors - Adjusted for dark mode */
        --passed-bg: #14532d;
        --passed-text: #86efac;
        --failed-bg: #7f1d1d;
        --failed-text: #fca5a5;
        --skipped-bg: #713f12;
        --skipped-text: #fde047;
        --xfailed-bg: #7c2d12;
        --xfailed-text: #fdba74;
        --error-bg: #7f1d1d;
        --error-text: #fca5a5;
    }

    /* Adjust box shadows for dark mode */
    .summary-card {
        box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.3), 0 1px 2px -1px rgb(0 0 0 / 0.3);
    }
}

@media print {
    body {
        background: #ffffff;
        color: #0f172a;
    }

    .container {
        max-width: none;
        padding: 1rem 1.5rem;
    }

    header {
        border-bottom: 2px solid var(--border-color);
    }

    .filters {
        display: none;
    }

    .summary-card,
    .test-row {
        box-shadow: none;
    }

    .test-header {
        background: #ffffff;
    }

    .test-row {
        page-break-inside: avoid;
        break-inside: avoid;
    }

    .test-details {
        background: #ffffff;
    }

    .llm-annotation {
        background: var(--surface-muted);
    }

    progress {
        width: 80px;
    }
}

body.pdf-mode .filters {
    display: none;
}

body.pdf-mode .test-row {
    page-break-inside: avoid;
    break-inside: avoid;
}

/* TOC Styling */
.toc {
    margin-bottom: 2rem;
    padding: 1rem;
    background: var(--card-bg);
    border: 1px solid var(--border-color);
    border-radius: 0.5rem;
}
.toc ul {
    list-style: none;
    padding: 0;
    margin: 0;
    display: flex;
    gap: 1.5rem;
    flex-wrap: wrap;
}
.toc a {
    color: var(--primary-color);
    text-decoration: none;
    font-weight: 600;
    cursor: pointer;
}
.toc a:hover {
    text-decoration: underline;
}

/* File Group Styling */
.test-file-group {
    margin-bottom: 2rem;
}
.test-file-header {
    font-size: 1.1rem;
    font-weight: 600;
    color: var(--text-primary);
    margin-bottom: 1rem;
    padding-bottom: 0.5rem;
    border-bottom: 2px solid var(--border-color);
    display: flex;
    justify-content: space-between;
    align-items: center;
}    </style>
    <script>
// pytest-llm-report interactive features

// Global state for filters
const activeStatuses = new Set(['passed', 'failed', 'skipped', 'xfailed', 'xpassed', 'error']);

// Filter tests based on search input and outcome filters
function filterTests() {
    const query = document.getElementById('searchInput').value.toLowerCase();
    document.querySelectorAll('.test-row').forEach(row => {
        const nodeid = row.querySelector('.test-name').textContent.toLowerCase();
        const statusMatch = row.dataset.status ? activeStatuses.has(row.dataset.status) : false;
        const matchesSearch = nodeid.includes(query);
        row.classList.toggle('hidden', !matchesSearch || !statusMatch);
    });
}

// Show only failures and scroll to list
function showFailuresOnly() {
    document.querySelectorAll('.filter-chip input').forEach(cb => {
        const s = cb.dataset.status;
        if (s === 'failed' || s === 'error') {
            cb.checked = true;
            activeStatuses.add(s);
        } else {
            cb.checked = false;
            activeStatuses.delete(s);
        }
    });
    filterTests();
    const testList = document.getElementById('test-list');
    if (testList) {
        testList.scrollIntoView({ behavior: 'smooth' });
    }
}

// Toggle visibility of status filters
function toggleStatus(checkbox) {
    const status = checkbox.dataset.status;
    if (checkbox.checked) {
        activeStatuses.add(status);
    } else {
        activeStatuses.delete(status);
    }
    filterTests();
}

// Initialize interactive features after DOM is ready
document.addEventListener('DOMContentLoaded', function () {
    'use strict';

    // Toggle dark mode on preference
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.documentElement.dataset.theme = 'dark';
    }

    // Default: expand all details
    document.querySelectorAll('details').forEach(details => {
        details.setAttribute('open', '');
    });

    const params = new URLSearchParams(window.location.search);
    if (params.get('pdf') === '1') {
        document.body.classList.add('pdf-mode');
    }
});    </script>
</head>
<body>
    <div class="container">
        <header>
            <div>
                <h1>Test Report</h1>
                <div class="meta">
                    Run ID: 21118757847-py3.12 &bull;
                    Generated: 2026-01-18 21:19:09 &bull;
                    Duration: 33.18s<br>
                    <strong>Plugin:</strong> v0.2.0
                        (b7a157f6cb9189cc50a17c846484c8454deeac61)
[dirty]<br>
                    <strong>Repo:</strong> v0.2.0
                        (64516b02bcb808571863229239cfd8a3ef690f50)
<br>
                    <strong>LLM:</strong> ollama / llama3.2:1b
                        (minimal context,
                         486 annotated, 45 errors)
                </div>
            </div>
            <div style="text-align: right">
                <div style="font-size: 2rem; font-weight: 700; color: var(--primary-color)">
                    92.96%
                </div>
                <div class="meta">Total Coverage</div>
            </div>
        </header>

        <!-- Summary Cards -->
        <div class="summary">
            <div class="summary-card">
                <div class="count">532</div>
                <div class="label">Total Tests</div>
            </div>
            <div class="summary-card passed">
                <div class="count">532</div>
                <div class="label">Passed</div>
            </div>
            <div class="summary-card failed">
                <div class="count">0</div>
                <div class="label">Failed</div>
            </div>
            <div class="summary-card skipped">
                <div class="count">0</div>
                <div class="label">Skipped</div>
            </div>
            <div class="summary-card xfailed">
                <div class="count">0</div>
                <div class="label">XFailed</div>
            </div>
            <div class="summary-card xpassed">
                <div class="count">0</div>
                <div class="label">XPassed</div>
            </div>
            <div class="summary-card failed">
                <div class="count">0</div>
                <div class="label">Errors</div>
            </div>
        </div>

        <!-- Table of Contents -->
        <nav class="toc">
            <ul>
                <li><a href="#source-coverage">Source Coverage</a></li>
                <li><a href="#test-list">Per Test Details</a></li>
                <li><a onclick="showFailuresOnly()">Failures Only</a></li>
            </ul>
        </nav>

        <section class="source-coverage" id="source-coverage">
            <h2>Source Coverage</h2>
            <div class="source-coverage-table">
                <div class="source-coverage-header">
                    <span>File</span>
                    <span>Stmts</span>
                    <span>Miss</span>
                    <span>Cover</span>
                    <span>%</span>
                    <span>Covered Lines</span>
                    <span>Missed Lines</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/_git_info.py</span>
                    <span>2</span>
                    <span>0</span>
                    <span>2</span>
                    <span>100.0%</span>
                    <span class="source-lines">2-3</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/aggregation.py</span>
                    <span>117</span>
                    <span>5</span>
                    <span>112</span>
                    <span>95.73%</span>
                    <span class="source-lines">13, 15-19, 21, 35, 38, 44, 46, 52-53, 55-57, 59, 61-64, 69, 73-74, 77-80, 84, 87-89, 93, 103, 109-111, 113-117, 119-120, 125, 127-128, 130-131, 134-135, 141-144, 147, 149, 151, 165, 167, 171, 173, 175, 185, 187-191, 193-194, 197, 199, 208, 220, 222-236, 238, 240, 248-249, 251-252, 254, 256-258, 262, 265-266, 268-269, 272, 274-275, 277, 279-280, 284</span>
                    <span class="source-lines">66, 90-91, 195, 206</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/cache.py</span>
                    <span>47</span>
                    <span>3</span>
                    <span>44</span>
                    <span>93.62%</span>
                    <span class="source-lines">13, 15-19, 21, 27, 33, 39-41, 43, 53, 55-56, 58, 60-62, 68-69, 78, 86, 88, 90, 92, 94, 97, 103, 107, 118-119, 121, 123, 129, 132-136, 141, 144, 153</span>
                    <span class="source-lines">64-65, 130</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/collector.py</span>
                    <span>111</span>
                    <span>2</span>
                    <span>109</span>
                    <span>98.2%</span>
                    <span class="source-lines">19, 21-22, 24, 26-27, 33-34, 45-50, 52, 58, 60-62, 69, 78-79, 81, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127-128, 130, 132-133, 135-137, 140, 143, 155, 163-164, 167-169, 171, 173, 181-182, 185-189, 191, 198-200, 202, 209-210, 212-214, 216, 218, 227-228, 230-236, 238, 241, 250-252, 254, 261, 264-265, 268-269, 271, 277, 279, 285</span>
                    <span class="source-lines">141, 239</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/coverage_map.py</span>
                    <span>135</span>
                    <span>6</span>
                    <span>129</span>
                    <span>95.56%</span>
                    <span class="source-lines">13, 15-17, 19-22, 30, 38, 44-45, 47, 58-60, 64, 72-73, 83, 86, 88-90, 92, 94-96, 98, 101-104, 106-108, 114, 116, 118, 121-122, 127-128, 131-135, 137-140, 144-146, 148, 150, 152-153, 156, 160-162, 165, 167-168, 173, 176, 178-184, 187-189, 191, 196, 199-200, 202, 204, 216-217, 220, 224-225, 228-234, 236, 239, 241, 243-244, 246-250, 252-254, 257, 259-260, 263-264, 271, 273-274, 276-279, 281-283, 285, 299-300, 302, 308</span>
                    <span class="source-lines">62, 123, 125, 157, 221, 251</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/errors.py</span>
                    <span>35</span>
                    <span>0</span>
                    <span>35</span>
                    <span>100.0%</span>
                    <span class="source-lines">8-9, 12, 25-28, 31-36, 39-42, 45-46, 49-51, 54-55, 64-66, 68, 70, 74-76, 80, 129, 139</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/__init__.py</span>
                    <span>3</span>
                    <span>0</span>
                    <span>3</span>
                    <span>100.0%</span>
                    <span class="source-lines">4-5, 7</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/annotator.py</span>
                    <span>110</span>
                    <span>0</span>
                    <span>110</span>
                    <span>100.0%</span>
                    <span class="source-lines">4, 6-10, 12-15, 21-22, 25-28, 31, 45-46, 48-50, 54, 56-57, 59, 61-62, 64, 66-68, 71-72, 74-82, 87, 97-98, 100, 102, 104-105, 115, 127, 129-132, 137-139, 142, 165-168, 170-171, 176, 178, 180-183, 185-190, 192-193, 198-201, 203, 206, 229-232, 234, 236-237, 239-240, 245-246, 248-253, 255-256, 261-264, 266</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/base.py</span>
                    <span>78</span>
                    <span>0</span>
                    <span>78</span>
                    <span>100.0%</span>
                    <span class="source-lines">13, 15-18, 26, 40, 46, 52-53, 55, 72, 75-76, 78, 80, 101, 107-108, 110-111, 122, 128, 130, 136, 138, 147, 149, 165, 167-173, 175, 177, 186-187, 190-192, 194-195, 198-200, 203-208, 212, 214, 220-221, 224-225, 228-230, 233, 245, 247, 249-250, 252-253, 255, 257-258, 260, 262-263, 265, 267</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/gemini.py</span>
                    <span>278</span>
                    <span>23</span>
                    <span>255</span>
                    <span>91.73%</span>
                    <span class="source-lines">7, 9-13, 15-16, 23-27, 30-34, 37-42, 44-46, 48-50, 52, 57-63, 65-70, 72-73, 75-78, 80-85, 87-88, 91-97, 99-103, 105, 107-114, 121-122, 125, 128, 134, 136-139, 141-142, 144, 160-161, 167-169, 171-172, 174, 176-184, 186-188, 190-191, 193, 196, 200-208, 210-211, 213-215, 217-223, 225-226, 235, 237-238, 242-243, 246-247, 249-250, 258-259, 266, 272-273, 275, 279-283, 285-289, 292-293, 298-299, 306-307, 309, 321, 323-324, 328, 333, 336-338, 341-349, 351-352, 354, 358-361, 363, 366-372, 374-380, 386-388, 390-393, 395, 397-398, 402-408, 411, 414-416, 418-420, 422-427, 433-434, 436-440, 443-446, 448-449, 451-453</span>
                    <span class="source-lines">89, 104, 106, 115-117, 199, 228-229, 233, 239-241, 248, 251-254, 256, 262, 373, 447, 450</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/litellm_provider.py</span>
                    <span>62</span>
                    <span>4</span>
                    <span>58</span>
                    <span>93.55%</span>
                    <span class="source-lines">8, 10, 12-13, 21, 31, 37-38, 41-42, 44, 51, 60-62, 64, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 108, 110, 114, 116, 118-120, 129, 131, 133, 135-136, 138, 142, 164, 175-176, 179-181, 183, 185-186, 188, 190, 195, 197, 199, 205-206, 208</span>
                    <span class="source-lines">123-124, 126, 191</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/noop.py</span>
                    <span>13</span>
                    <span>0</span>
                    <span>13</span>
                    <span>100.0%</span>
                    <span class="source-lines">8, 10, 12-13, 20, 26, 32, 34, 50, 52, 58, 60, 66</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/ollama.py</span>
                    <span>45</span>
                    <span>2</span>
                    <span>43</span>
                    <span>95.56%</span>
                    <span class="source-lines">7, 9, 11-12, 18, 24, 40-41, 47, 50, 52, 54-55, 57-60, 62, 64-65, 71, 73, 76-77, 79-80, 82, 86, 92-93, 95-97, 101, 107, 109, 119, 121-122, 132, 137, 139-140</span>
                    <span class="source-lines">69, 75</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/schemas.py</span>
                    <span>36</span>
                    <span>1</span>
                    <span>35</span>
                    <span>97.22%</span>
                    <span class="source-lines">8, 10-12, 16, 22, 38, 42-44, 46-47, 50-53, 55, 58-59, 62-65, 67-68, 77, 84, 90, 94-98, 102, 130</span>
                    <span class="source-lines">39</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/llm/token_refresh.py</span>
                    <span>71</span>
                    <span>0</span>
                    <span>71</span>
                    <span>100.0%</span>
                    <span class="source-lines">7, 9-14, 17, 20, 23-24, 36-39, 41-43, 47, 59-60, 63-66, 69-72, 74, 83, 85-88, 90-91, 93, 101-103, 107-109, 111, 113-116, 120, 132-136, 139-140, 143-145, 148-150, 153-156, 158, 160-162</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/models.py</span>
                    <span>243</span>
                    <span>0</span>
                    <span>243</span>
                    <span>100.0%</span>
                    <span class="source-lines">17-18, 21, 24-25, 34-36, 38, 40, 47-48, 61-67, 69, 71, 82-83, 95-100, 102, 104, 109-115, 118-119, 141-157, 159-160, 162, 164, 166, 173-196, 199-200, 208-209, 211, 213, 219-220, 229-231, 233, 235, 239-241, 244-245, 254-256, 258, 260, 267-268, 277-279, 281, 283, 287-289, 292-293, 330-359, 361-366, 368, 370, 388-411, 413-425, 428-429, 443-451, 453, 455, 465-467, 470-471, 488-498, 500, 506, 508, 514-528</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/options.py</span>
                    <span>197</span>
                    <span>51</span>
                    <span>146</span>
                    <span>74.11%</span>
                    <span class="source-lines">122, 162, 191, 194-196, 201-203, 209-211, 217-219, 225-227, 233-234, 237-246, 248, 252, 261, 276, 279-280, 288-293, 295, 300-305, 308-313, 316-317, 320-321, 324-325, 328-337, 340-343, 346-359, 362-363, 366-367, 370-375, 380-381, 384-385, 390-393, 398-401, 403, 405, 407, 414-415, 417-418, 420-421, 424-437, 440-449, 452, 454</span>
                    <span class="source-lines">13-15, 21-22, 98-102, 105-107, 110-115, 118-121, 138-139, 142-148, 151-153, 156-158, 161, 172-176, 179-180, 183, 185, 250, 255, 264</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/plugin.py</span>
                    <span>147</span>
                    <span>24</span>
                    <span>123</span>
                    <span>83.67%</span>
                    <span class="source-lines">40, 43, 49, 55, 61, 67, 73, 80, 89, 95, 101, 107, 113, 121, 127, 133, 150, 154, 158, 164-165, 168-169, 171, 173, 176-178, 184-185, 193-194, 221-222, 225-226, 229, 232-233, 235-236, 239-240, 242, 244-248, 251-252, 254, 256, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-288, 290, 292-295, 298-299, 307-308, 313-316, 319, 321, 324-329, 331, 333, 341-342, 363-364, 367-368, 371-373, 384-385, 388, 391-392, 395-397, 407-408, 411-413, 424-425, 428, 431, 433-434</span>
                    <span class="source-lines">13, 15-17, 19-20, 22, 28-31, 34, 141, 199, 303-304, 309-310, 355-356, 376, 400, 416-417</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/prompts.py</span>
                    <span>75</span>
                    <span>1</span>
                    <span>74</span>
                    <span>98.67%</span>
                    <span class="source-lines">13, 15-17, 24, 27, 33, 35, 49, 52, 55, 58-61, 63, 65, 67, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 114, 116, 118, 132-133, 135-138, 140-142, 144-146, 148-149, 151-152, 154-156, 158-159, 163, 165, 180, 182, 191-194</span>
                    <span class="source-lines">80</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/render.py</span>
                    <span>50</span>
                    <span>0</span>
                    <span>50</span>
                    <span>100.0%</span>
                    <span class="source-lines">13, 15-16, 18, 24, 30-31, 34, 40, 42, 50-51, 53, 56, 65-67, 70, 79, 87, 90, 99, 101-102, 107, 110, 121-124, 126-129, 131-134, 141-143, 145, 158-163, 177, 196</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/report_writer.py</span>
                    <span>167</span>
                    <span>3</span>
                    <span>164</span>
                    <span>98.2%</span>
                    <span class="source-lines">13, 15-25, 27-29, 46, 55, 58, 67-68, 76, 83-84, 89, 98-100, 102, 105-108, 110, 113, 116, 127-128, 130, 142, 150, 156-158, 160, 186-189, 192, 197-199, 202-203, 211, 222-223, 226-227, 230-231, 233, 235, 254, 256-259, 262-264, 266, 268, 303, 312, 314-315, 317-328, 330, 332, 340, 343-345, 348-349, 352-354, 357, 360, 368, 376, 378-379, 382, 385, 388, 391, 399, 401-402, 408, 410, 412, 414-425, 432, 434-435, 437-439, 447-451, 453, 455, 458, 461-462, 464, 470-474, 480-481, 488, 495, 497, 499-501, 503, 506-507, 509, 515-516</span>
                    <span class="source-lines">135-137</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/fs.py</span>
                    <span>34</span>
                    <span>1</span>
                    <span>33</span>
                    <span>97.06%</span>
                    <span class="source-lines">11, 13-14, 17, 30, 33, 36, 39, 42, 45, 55-56, 58-60, 63-65, 67, 70, 79, 82, 100, 103, 111-113, 116-117, 119-121, 123</span>
                    <span class="source-lines">40</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/hashing.py</span>
                    <span>36</span>
                    <span>0</span>
                    <span>36</span>
                    <span>100.0%</span>
                    <span class="source-lines">12, 14-17, 23, 32, 35, 44-48, 51, 61, 64, 73-74, 76-78, 80-81, 86, 96, 103-104, 107, 113-114, 116-121</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/ranges.py</span>
                    <span>33</span>
                    <span>0</span>
                    <span>33</span>
                    <span>100.0%</span>
                    <span class="source-lines">12, 15, 29-30, 33, 35-37, 39-40, 42, 45-47, 50, 52, 55, 65-67, 70, 81-82, 84-91, 93, 95</span>
                    <span class="source-lines">-</span>
                </div>
                <div class="source-coverage-row">
                    <span class="source-path">src/pytest_llm_report/util/time.py</span>
                    <span>16</span>
                    <span>0</span>
                    <span>16</span>
                    <span>100.0%</span>
                    <span class="source-lines">4, 6, 9, 15, 18, 27, 30, 39-44, 46-48</span>
                    <span class="source-lines">-</span>
                </div>
            </div>
        </section>

        <section class="per-test-details" id="test-list">
            <h2>Per Test Details</h2>

        <!-- Filters -->
        <div class="filters">
            <input type="text" id="searchInput" class="filter-input" placeholder="Search tests..." onkeyup="filterTests()">
            <div class="filter-statuses" aria-label="Filter by status">
                <label class="filter-chip passed">
                    <input type="checkbox" data-status="passed" checked onchange="toggleStatus(this)">
                    Passed
                </label>
                <label class="filter-chip failed">
                    <input type="checkbox" data-status="failed" checked onchange="toggleStatus(this)">
                    Failed
                </label>
                <label class="filter-chip skipped">
                    <input type="checkbox" data-status="skipped" checked onchange="toggleStatus(this)">
                    Skipped
                </label>
                <label class="filter-chip xfailed">
                    <input type="checkbox" data-status="xfailed" checked onchange="toggleStatus(this)">
                    XFailed
                </label>
                <label class="filter-chip xpassed">
                    <input type="checkbox" data-status="xpassed" checked onchange="toggleStatus(this)">
                    XPassed
                </label>
                <label class="filter-chip error">
                    <input type="checkbox" data-status="error" checked onchange="toggleStatus(this)">
                    Error
                </label>
            </div>
        </div>

        <!-- Test List -->
        <div class="test-list">
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_aggregation.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">10 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_all_policy</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test `test_aggregate_all_policy` verifies that the aggregate function correctly handles multiple reports from different nodes and phases.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the aggregate function might incorrectly retain tests for non-existent reports or failed tests.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The aggregated report should contain both retained tests.</li>
                                            <li>The length of the aggregated report's `tests` list should be equal to 2.</li>
                                            <li>Both retained tests should exist in the aggregated report.</li>
                                            <li>Failed tests should not be included in the aggregated report.</li>
                                            <li>Non-existent reports should not be included in the aggregated report.</li>
                                            <li>All tests from both nodes and phases should be included in the aggregated report.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/aggregation.py</span>
                                        <span style="color: var(--text-secondary)">70 lines (ranges: 52, 55-56, 59, 61-63, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134, 141, 147, 149, 151-156, 158, 160-162, 173, 220, 222-226, 238, 248, 251-252, 254, 256, 279-282, 284)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_dir_not_exists</span>
                            <div class="test-meta">
                                <span>4ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that the aggregate function returns None when the aggregation directory does not exist.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the aggregate function fails to aggregate data in cases where the aggregation directory does not exist.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `aggregator.aggregate()` method should return `None` when the specified `aggregate_dir` is not found or does not exist.</li>
                                            <li>The `pathlib.Path.exists()` mock returns `False` for the specified `aggregate_dir`.</li>
                                            <li>No exception is raised when the aggregation directory does not exist, preventing potential data loss.</li>
                                            <li>The aggregate function behaves as expected in this scenario, indicating a bug-free implementation.</li>
                                            <li>The test fails with an assertion error if the `aggregator.aggregate()` method returns a different value than `None` for a non-existent `aggregate_dir`.</li>
                                            <li>The `pathlib.Path.exists()` mock ensures that the `exists` method is called only when necessary, preventing unnecessary computations.</li>
                                            <li>The test case covers all possible scenarios where the aggregation directory does not exist or is not found, ensuring robustness and reliability.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/aggregation.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 52, 55-57, 109-111)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_latest_policy</span>
                            <div class="test-meta">
                                <span>3ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `aggregate` function correctly picks the latest policy for a given test case.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the `aggregate` function would incorrectly pick an older policy when multiple tests are run on different times.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `outcome` of the aggregated report should be 'passed' if it is the latest policy for the given test case.</li>
                                            <li>The number of tests in the aggregated report should be 1.</li>
                                            <li>The outcome of the first test in the aggregated report should be 'passed'.</li>
                                            <li>The `run_meta.is_aggregated` flag should be True.</li>
                                            <li>The `run_meta.run_count` should match the number of runs (2) on different times.</li>
                                            <li>The summary of the aggregated report should have exactly 1 passed test and 0 failed tests.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/aggregation.py</span>
                                        <span style="color: var(--text-secondary)">78 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134, 141, 147, 149, 151-156, 158, 160-162, 173, 185, 187-191, 193-194, 197, 220, 222-226, 238, 248, 251-252, 254, 256, 279-282, 284)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_no_dir_configured</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The aggregator function should not throw an exception when no directory configuration is provided.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the aggregator function throws an exception if no directory configuration is specified.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>agg.aggregate() should return None</li>
                                            <li>mock_config.aggregate_dir is set to None before calling aggregate()</li>
                                            <li>No exception is thrown when agg.aggregate() is called</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/aggregation.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 44, 52-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_no_reports</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that `aggregate` returns `None` when there are no reports.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in case of an empty report directory.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return `None` without attempting to aggregate any data.</li>
                                            <li>No reports should be found using the specified path.</li>
                                            <li>There should not be any aggregated data returned.</li>
                                            <li>The `aggregate` method should not raise an exception when called with no arguments.</li>
                                            <li>The `aggregate` method should not modify the original directory.</li>
                                            <li>The `aggregate` method should not return a report.</li>
                                            <li>No error should be raised if the directory is empty.</li>
                                            <li>The function should handle cases where the directory does not exist.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/aggregation.py</span>
                                        <span style="color: var(--text-secondary)">9 lines (ranges: 52, 55-57, 109-110, 113-114, 173)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_with_coverage_and_llm_annotations</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that coverage and LLM annotations are properly deserialized and can be re-serialized.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in core functionality by ensuring accurate coverage and LLM annotation deserialization.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Coverage was properly deserialized with correct file paths and line ranges.</li>
                                            <li>LLM annotation was properly deserialized with correct scenario, why needed, and key assertions.</li>
                                            <li>Can be re-serialized without any issues (this would fail before the fix).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/aggregation.py</span>
                                        <span style="color: var(--text-secondary)">82 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134-137, 141-144, 147, 149, 151-156, 158, 160-162, 173, 185, 187-191, 197, 220, 222-226, 238, 248, 251-252, 254, 256, 279-282, 284)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">34 lines (ranges: 40-43, 104-107, 109-111, 113, 115, 162, 166-171, 173, 175, 177, 179, 182-186, 188, 190, 192, 194, 196)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_aggregation.py::TestAggregator::test_aggregate_with_source_coverage</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `aggregate` method returns a single `SourceCoverageEntry` for each source file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where multiple source files are aggregated into one entry.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `source_coverage` list contains exactly one element with the expected structure.</li>
                                            <li>Each `SourceCoverageEntry` has a `file_path` attribute matching the expected value.</li>
                                            <li>All `SourceCoverageEntry` attributes match the expected values.</li>
                                            <li>The `coverage_percent` and `covered_ranges`/`missed_ranges` attributes are calculated correctly.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/aggregation.py</span>
                                        <span style="color: var(--text-secondary)">66 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 151-158, 160-162, 173, 185, 187-189, 197, 220, 222-223, 238, 248, 251-252, 254, 256, 279-282, 284)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_aggregation.py::TestAggregator::test_load_coverage_from_source</span>
                            <div class="test-meta">
                                <span>3ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test loading coverage from configured source file when option is not set.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a bug where the aggregator fails to load coverage data when the `llm_coverage_source` option is not provided or the specified file does not exist.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Verify that `_load_coverage_from_source()` returns `None` when `llm_coverage_source` is set to `None`.</li>
                                            <li>Verify that `_load_coverage_from_source()` raises a `UserWarning` with the message 'Coverage source not found' when the specified file does not exist.</li>
                                            <li>Verify that the mock coverage object returned by `_load_coverage_from_source()` has the correct attributes and methods when successfully loaded from a `.coverage` file.</li>
                                            <li>Verify that the mock mapper object returned by `_load_coverage_from_source()` has the correct `map_source_coverage` method call when the specified file does not exist.</li>
                                            <li>Verify that the coverage percentage is correctly reported as 80.0 when successfully loaded from a `.coverage` file.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/aggregation.py</span>
                                        <span style="color: var(--text-secondary)">19 lines (ranges: 248-249, 251-252, 254, 256-260, 262, 265-266, 268-269, 272, 274-275, 277)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_aggregation.py::TestAggregator::test_recalculate_summary</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `_recalculate_summary` method correctly recalculates the summary when new test results are added.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where a new test result would cause the summary to be outdated.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The total count of tests should match the provided number.</li>
                                            <li>The passed count should match the provided number.</li>
                                            <li>The failed count should match the provided number.</li>
                                            <li>The skipped count should match the provided number.</li>
                                            <li>The xfailed count should match the provided number.</li>
                                            <li>The xpassed count should match the provided number.</li>
                                            <li>The error count should match the provided number.</li>
                                            <li>The coverage percentage should be preserved.</li>
                                            <li>The total duration of all tests should remain unchanged.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/aggregation.py</span>
                                        <span style="color: var(--text-secondary)">17 lines (ranges: 220, 222-236, 238)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_aggregation.py::TestAggregator::test_skips_invalid_json</span>
                            <div class="test-meta">
                                <span>3ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test verifies that skipping an invalid JSON report prevents a regression.</p>
                                    <p><strong>Why Needed:</strong> This test ensures that the aggregation function correctly handles reports with missing or invalid fields.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'aggregate' function should not count any report when it encounters an invalid JSON file.</li>
                                            <li>The 'aggregate' function should raise a warning for skipping an invalid report file.</li>
                                            <li>The 'aggregate' function should only count the valid report in its run meta.</li>
                                            <li>The 'aggregate' function should ignore reports with missing fields.</li>
                                            <li>The 'aggregate' function should not include any skipped reports in its final result.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/aggregation.py</span>
                                        <span style="color: var(--text-secondary)">71 lines (ranges: 52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119-120, 125, 127-128, 151-156, 158, 160-162, 165, 167-169, 171, 173, 185, 187-189, 197, 220, 222-223, 238, 248, 251-252, 254, 256, 279-282, 284)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_aggregation_maximal.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">1 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_aggregation_maximal.py::TestAggregationMaximal::test_recalculate_summary_coverage</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the aggregator recalculates the summary correctly when given a set of tests with varying outcomes and durations.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in the aggregator's behavior when dealing with incomplete or inconsistent data.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The total duration of the summary should be equal to the sum of the individual test durations.</li>
                                            <li>The number of passed tests should match the expected outcome (1 passed, 1 failed).</li>
                                            <li>The coverage total percent should remain unchanged after recalculating the summary.</li>
                                            <li>The total duration of the summary should be greater than or equal to the longest test duration.</li>
                                            <li>The coverage total percent should still be within the specified range (88.5%).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/aggregation.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 44, 220, 222-228, 238)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_annotator.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">7 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_cached_tests_are_skipped</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that cached tests are skipped for the annotator test.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the annotator test is run multiple times with the same input data and the results are cached.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Mocking `mock_provider` to return an empty list or None when called.</li>
                                            <li>Mocking `mock_cache` to not store any results for the given input data.</li>
                                            <li>Mocking `mock_assembler` to not generate any output for the given input data.</li>
                                            <li>Verifying that the test does not run multiple times with the same input data and cached results.</li>
                                            <li>Checking that the test only runs once with unique input data and no cached results.</li>
                                            <li>Verifying that the cache is properly cleared after each test run.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-81, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation</span>
                            <div class="test-meta">
                                <span>3ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The annotator function is called concurrently with multiple providers and caches.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential performance regression where the annotator function may be called multiple times in quick succession without being properly synchronized.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>mock_provider.assert_called_once_with('provider1')</li>
                                            <li>mock_provider.assert_called_once_with('provider2')</li>
                                            <li>mock_cache.assert_called_once_with('cache1', 'annotation1')</li>
                                            <li>mock_assembler.assert_called_once_with('assembly1', 'annotation1')</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">64 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104-112, 129-135, 137, 139, 229-232, 234, 236-237, 239, 245-246, 248-253, 255, 261, 266)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation_handles_failures</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that concurrent annotation handling prevents failures.</p>
                                    <p><strong>Why Needed:</strong> This test ensures that annotators can handle failures in a concurrent manner without crashing or producing unexpected results.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Mocking the annotator, cache, and assembler objects to prevent any exceptions from being raised.</li>
                                            <li>Using capsys to capture the output of the annotation process.</li>
                                            <li>Verifying that the annotation process completes successfully even when there are failures.</li>
                                            <li>Checking for any errors or warnings produced by the annotation process.</li>
                                            <li>Ensuring that the annotator can handle failures without crashing or producing unexpected results.</li>
                                            <li>Testing that the cache is not affected by concurrent annotation handling failures.</li>
                                            <li>Verifying that the assembler does not produce any output when an error occurs during annotation.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104-112, 129-135, 137-139, 229-232, 234, 236-237, 239, 245-246, 248-253, 255, 261-264, 266)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_progress_reporting</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the progress reporting is implemented correctly in the test.</p>
                                    <p><strong>Why Needed:</strong> The current implementation may not accurately track progress or provide meaningful insights for annotators.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>mock_provider.progress_reporting() should return a mock object with the expected attributes</li>
                                            <li>mock_cache.progress_reporting() should be called before and after each annotation</li>
                                            <li>mock_assembler.progress_reporting() should be called before and after each annotation</li>
                                            <li>mock_provider.progress_reporting().get_progress() should return a valid progress value</li>
                                            <li>mock_cache.progress_reporting().get_progress() should return a valid progress value</li>
                                            <li>mock_assembler.progress_reporting().get_progress() should return a valid progress value</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation</span>
                            <div class="test-meta">
                                <span>12.00s</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies the sequential annotation of a test function.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential regression in the annotator's behavior when multiple tests are run sequentially.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `test_sequential_annotation` method should not throw an exception when called with multiple test functions.</li>
                                            <li>The `mock_provider`, `mock_cache`, and `mock_assembler` mocks should be created without any side effects.</li>
                                            <li>The `test_sequential_annotation` method should return a mock object that is not equal to the original function.</li>
                                            <li>The `mock_provider` mock should have been called with the correct arguments.</li>
                                            <li>The `mock_cache` mock should have been called with the correct arguments.</li>
                                            <li>The `mock_assembler` mock should have been called with the correct arguments.</li>
                                            <li>The `test_sequential_annotation` method should not raise an exception when called multiple times.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_skips_if_disabled</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies the annotator function behaves as expected when LLM is disabled.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the annotator function does not skip tests if LLM is disabled.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `Config` object with 'none' provider is created.</li>
                                            <li>An empty list of annotations is passed to the `annotate_tests` function.</li>
                                            <li>No annotation results are returned from the `annotate_tests` function.</li>
                                            <li>The annotator function does not skip any tests when LLM is disabled.</li>
                                            <li>A test case should be skipped if LLM is disabled.</li>
                                            <li>The `skip` method of the `Config` object with 'none' provider should return False.</li>
                                            <li>An error message or exception should be raised when trying to annotate tests with an empty list.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 45-46)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_annotator.py::TestAnnotateTests::test_skips_if_provider_unavailable</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The annotator should skip annotation if the provider is unavailable.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the annotator may continue to annotate without checking for provider availability.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Mocked provider is not available</li>
                                            <li>Annotation skipped due to provider unavailability</li>
                                            <li>No annotation performed when provider is unavailable</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_annotator_maximal.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">4 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_concurrent_with_progress_and_errors</span>
                            <div class="test-meta">
                                <span>3ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_concurrent_with_progress_and_errors</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the annotator does not report errors and progress messages concurrently, potentially leading to missing information about the annotation process.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should append 'first error' to the `progress_msgs` list when an error occurs.</li>
                                            <li>The function should check if any of the `progress_msgs` contain 'LLM annotation'.</li>
                                            <li>The function should report progress messages containing 'Processing X test(s)' for at least 2 tasks.</li>
                                            <li>The function should not return immediately after annotating all tasks without reporting errors or progress messages.</li>
                                            <li>The function should handle cases where there are no annotations (i.e., `failures == 0`).</li>
                                            <li>The function should report the first error found in the annotation process.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">28 lines (ranges: 229-232, 234, 236-237, 239-242, 245-246, 248-253, 255-258, 261-264, 266)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_sequential_rate_limit_wait</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> test_annotate_sequential_rate_limit_wait</p>
                                    <p><strong>Why Needed:</strong> Prevents regression when rate limit intervals are not yet reached.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The time.sleep() function was called before the annotation task completed.</li>
                                            <li>The time.sleep() function was called after the annotation task completed.</li>
                                            <li>The time.sleep() function was called with a delay of less than or equal to 1.0s.</li>
                                            <li>The time.sleep() function was called with a delay greater than 1.0s but less than or equal to 2.0s.</li>
                                            <li>The time.sleep() function was called with a delay greater than 2.0s.</li>
                                            <li>The time.sleep() function did not call before the annotation task completed.</li>
                                            <li>The time.sleep() function did not call after the annotation task completed.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">23 lines (ranges: 165-168, 170-171, 173-174, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_tests_cached_progress</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Should report progress for cached tests.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression when caching and annotating tests concurrently.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `get_provider` method of `LLMCache` should return a mock provider instance.</li>
                                            <li>The `assemble` method of the `ContextAssembler` class should be called with a tuple containing 'src' and None.</li>
                                            <li>Any progress messages in the `progress_msgs` list should contain '(cache): test_cached'.</li>
                                            <li>The `is_available` method of `mock_provider` should return True.</li>
                                            <li>The `get` method of `mock_cache` should return a mock annotation.</li>
                                            <li>The `append` method of `progress_msgs` should append a string containing '(cache): test_cached'.</li>
                                            <li>Any progress messages in the `progress_msgs` list should not contain any other strings.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">37 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-84, 97-98, 100, 127, 129-135, 137, 139)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_tests_provider_unavailable</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the annotator returns a message indicating that the provider is not available when it is not available.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the annotator fails to report an error when the provider is unavailable, potentially masking bugs or incorrect behavior.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>mock_provider.is_available.return_value == False</li>
                                            <li>assert captured.out.startswith('not available. Skipping annotations')</li>
                                            <li>mock_provider.get_provider() != mock_provider</li>
                                            <li>assert mock_provider.is_available.called_once_with() == True</li>
                                            <li>assert mock_provider.is_available.call_count == 1</li>
                                            <li>assert mock_provider.return_value.message == 'not available. Skipping annotations'</li>
                                            <li>assert mock_provider.return_value.status_code == 500</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_base_coverage_v2.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">2 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_base_coverage_v2.py::test_base_parse_response_malformed_json_after_extract</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 186-187, 190-191, 194-195, 220-221)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_base_coverage_v2.py::test_base_parse_response_non_string_fields</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `test_base_parse_response_non_string_fields` function handles non-string fields correctly.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not handle non-string fields in its response data.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert annotation.scenario == '123'</li>
                                            <li>assert annotation.why_needed == ['list']</li>
                                            <li>assert annotation.key_assertions == ['a']</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">22 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203-207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_base_maximal.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">9 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_gemini_provider</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `get_gemini_provider` function returns a `GeminiProvider` instance.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `get_gemini_provider` function might return an incorrect provider type if the configuration is invalid or missing.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `get_provider(config)` should return an instance of `GeminiProvider`.</li>
                                            <li>The returned `GeminiProvider` instance should have a valid `config` attribute.</li>
                                            <li>The `GeminiProvider` instance should be able to provide a `gemini_url` attribute.</li>
                                            <li>The `gemini_url` attribute of the `GeminiProvider` instance should be a string.</li>
                                            <li>The `get_provider(config)` function should not raise an exception if the configuration is invalid or missing.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 245, 247, 249, 252, 257, 262-263, 265)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_invalid_provider</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that a ValueError is raised when an unknown LLM provider is specified.</p>
                                    <p><strong>Why Needed:</strong> This test prevents the introduction of unknown LLM providers in the future.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `get_provider` raises a `ValueError` with the message 'Unknown LLM provider: invalid'.</li>
                                            <li>The error message includes the string 'invalid' which is the name of the unknown provider.</li>
                                            <li>The test verifies that the error message contains the exact phrase 'Unknown LLM provider: invalid'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 245, 247, 249, 252, 257, 262, 267)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_litellm_provider</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `get_litellm_provider` function returns a LitELLMProvider instance.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in case of an incorrect provider configuration.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The returned provider is indeed an instance of `LiteLLMProvider`.</li>
                                            <li>The provider is not None, indicating it was successfully retrieved.</li>
                                            <li>The provider has the correct type (LitELLMProvider).</li>
                                            <li>No exceptions were raised during the retrieval process.</li>
                                            <li>The provider's attributes match the expected values (e.g., name, description).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">9 lines (ranges: 52-53, 245, 247, 249, 252, 257-258, 260)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 37-38, 41)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_noop_provider</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that a NoopProvider instance is created when the 'provider' parameter is set to 'none'.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where a NoopProvider instance is not created if the 'provider' parameter is set to 'none'.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `get_provider` function returns an instance of `NoopProvider` when the 'provider' parameter is set to 'none'.</li>
                                            <li>The `config` object passed to `get_provider` has a valid `provider` attribute.</li>
                                            <li>The `provider` attribute of the returned `NoopProvider` instance is not None.</li>
                                            <li>A NoopProvider instance is created with no provider information.</li>
                                            <li>The `provider` method of the `NoopProvider` instance does not raise an exception when called.</li>
                                            <li>The `get_provider` function correctly handles a 'none' provider value.</li>
                                            <li>The test fails if the 'provider' parameter is set to anything other than 'none'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/noop.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_base_maximal.py::TestGetProvider::test_get_ollama_provider</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `get_ollama_provider` function returns an instance of OllamaProvider.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `OllamaProvider` is not correctly instantiated from the provided configuration.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `provider` parameter passed to `get_provider(config)` should be an instance of `ollama.OllamaProvider`.</li>
                                            <li>The returned `provider` should have the correct type and attributes.</li>
                                            <li>The `provider` attribute of the returned `provider` should be set correctly.</li>
                                            <li>The `OllamaProvider` class should be instantiated correctly from the provided configuration.</li>
                                            <li>The `get_provider` function should return an instance of `ollama.OllamaProvider` when given a valid configuration.</li>
                                            <li>The `get_provider` function should raise an error if it cannot instantiate the `OllamaProvider` class.</li>
                                            <li>The `provider` attribute of the returned `provider` should be set correctly after instantiation.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 245, 247, 249, 252-253, 255)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_available_caches_result</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the LlmProvider defaults implement _check_availability correctly.</p>
                                    <p><strong>Why Needed:</strong> The test prevents a potential regression where the default LlmProvider does not implement _check_availability.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The provider should be available.</li>
                                            <li>The provider should have one check.</li>
                                            <li>The checks counter should increment each time _check_availability is called.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 107-108, 110-111)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_model_name_defaults_to_config</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The `get_model_name()` method of the `ConcreteProvider` class should return the default model name specified in the configuration when no custom model is provided.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the default model name defaults to 'test-model' without any explicit configuration.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `get_model_name()` method of the `ConcreteProvider` class should return 'test-model'.</li>
                                            <li>The `get_model_name()` method of the `ConcreteProvider` class should not raise an exception when no custom model is provided.</li>
                                            <li>The default model name specified in the configuration should be 'test-model' if it exists, otherwise it should be a default value like 'default_model'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 136)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_rate_limits_defaults_to_none</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `get_rate_limits` method of a concrete LLM provider returns `None` when no default rate limits are specified.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the default rate limits for an LLM provider are not properly set, leading to unexpected behavior or errors in downstream applications.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The method `get_rate_limits()` is called on the concrete LLM provider.</li>
                                            <li>The result of calling `get_rate_limits()` is `None`.</li>
                                            <li>The absence of default rate limits is indicated by a value of `None` for this property.</li>
                                            <li>A concrete LLM provider instance is created with no specified default rate limits.</li>
                                            <li>The `get_rate_limits()` method is called on the non-defaulted LLM provider instance.</li>
                                            <li>The result of calling `get_rate_limits()` is still `None` even after a default rate limit has been set.</li>
                                            <li>A default rate limit is explicitly set for the concrete LLM provider instance before calling `get_rate_limits()`.</li>
                                            <li>The absence of a specified default rate limit indicates that this property should be set to `None`.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 128)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_base_maximal.py::TestLlmProviderDefaults::test_is_local_defaults_to_false</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that `is_local()` returns False for a non-local configuration.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in case the default configuration is not local.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `provider.is_local()` should return `False` for a valid configuration.</li>
                                            <li>A valid configuration should have a `local` key set to `True` or `False`.</li>
                                            <li>If `local` is not present, the function should raise an exception or return an error message.</li>
                                            <li>The value of `local` in the configuration dictionary should be either `True` or `False`.</li>
                                            <li>A non-local configuration should have a `local` key with a value that indicates it's not local.</li>
                                            <li>If the default configuration is not local, the function should raise an exception or return an error message.</li>
                                            <li>The function should handle cases where `local` is set to `None` in the configuration dictionary.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 52-53, 147)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_cache.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">7 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_cache.py::TestHashSource::test_consistent_hash</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The 'hash_source' function is called with the same source code, and it returns the expected hash value.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where different inputs to the 'hash_source' function produce different hashes due to caching issues.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>source = "def test_foo(): pass"</li>
                                            <li>assert hash_source(source) == hash_source(source)</li>
                                            <li>expected_hash_value = hash_source(source)</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_cache.py::TestHashSource::test_different_source_different_hash</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_cache.py::TestHashSource::test_hash_length</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify the length of the hash generated by `hash_source`.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where the hash length is not as expected due to caching or other external factors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The hash should be exactly 16 characters long.</li>
                                            <li>The hash should not be shorter than 16 characters.</li>
                                            <li>The hash should not be longer than 16 characters.</li>
                                            <li>The hash generated by `hash_source` is a string of only hexadecimal digits.</li>
                                            <li>The hash generated by `hash_source` has leading zeros (e.g. '0000...')</li>
                                            <li>The hash generated by `hash_source` does not contain any non-hexadecimal characters.</li>
                                            <li>The hash generated by `hash_source` meets the expected length and content requirements.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 153)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_cache.py::TestLlmCache::test_clear</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test clearing the LLM cache and verifying that it removes all entries.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a bug where the cache is not cleared correctly after adding some entries, causing incorrect assertions to be made about its contents.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Verify that the `clear` method returns the correct number of cache entries (2 in this case).</li>
                                            <li>Verify that the `get` method for a specific key returns `None` when called with that key after clearing the cache.</li>
                                            <li>Verify that all cache entries are removed from the cache after clearing it.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 129, 132-136, 141)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_cache.py::TestLlmCache::test_does_not_cache_errors</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that annotations with errors are not cached.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential regression where an error in the annotation causes it to be cached and then not removed even after the error has been resolved.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The cache should not contain the annotation for 'test::foo' with the key 'abc123' when the result of the get operation is None.</li>
                                            <li>The cache should remove the annotation for 'test::foo' with the key 'abc123' after the error has been resolved.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 39-41, 53, 55-56, 86, 88, 118-119, 121)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_cache.py::TestLlmCache::test_get_missing</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test 'get_missing' function of LlmCache class.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function returns `None` when trying to retrieve a non-existent key from the cache.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return None for a missing entry in the cache.</li>
                                            <li>The function should not raise any exceptions for missing entries.</li>
                                            <li>The function should handle the case of an empty cache correctly.</li>
                                            <li>The function should not throw an error when trying to retrieve a non-existent key.</li>
                                            <li>The function should be able to handle different types of keys (e.g., strings, integers).</li>
                                            <li>The function should be able to handle missing entries with different values (e.g., None, empty string).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">9 lines (ranges: 39-41, 53, 55-56, 118-119, 121)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_cache.py::TestLlmCache::test_set_and_get</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that annotations are stored and retrieved correctly from the cache.</p>
                                    <p><strong>Why Needed:</strong> Prevents bypass attacks by ensuring that LLMCache stores annotations in a secure manner.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Check that the annotation is set correctly with the given key and value.</li>
                                            <li>Check that the annotation's confidence level matches the expected value.</li>
                                            <li>Verify that the retrieved annotation has the same scenario, why_needed, and confidence as the original annotation.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">28 lines (ranges: 39-41, 53, 55, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_collector.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">11 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector.py::TestCollectorCollectionErrors::test_collection_error_structure</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that a `CollectionError` object has the correct 'nodeid' and 'message' attributes.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where a collection error is not properly structured, potentially leading to incorrect handling or reporting of errors in the code.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'nodeid' attribute should be set to the value provided in the error message.</li>
                                            <li>The 'message' attribute should contain the correct error message.</li>
                                            <li>The 'nodeid' and 'message' attributes should match the expected values.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector.py::TestCollectorCollectionErrors::test_get_collection_errors_initially_empty</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that an empty collection is returned when the `get_collection_errors` method is called on a newly created `TestCollector` instance with an empty configuration.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where an empty collection is returned unexpectedly, potentially causing issues downstream in the test suite.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `get_collection_errors` method returns an empty list when called on an empty configuration.</li>
                                            <li>A non-empty collection is not returned when called on an empty configuration.</li>
                                            <li>The `get_collection_errors` method raises a `ValueError` exception when called with an empty configuration.</li>
                                            <li>An error message is provided by the `get_collection_errors` method when called with an empty configuration.</li>
                                            <li>The `get_collection_errors` method does not raise any exceptions when called with an empty configuration.</li>
                                            <li>The `get_collection_errors` method returns a default value (None) when called with an empty configuration.</li>
                                            <li>A different error message is provided by the `get_collection_errors` method when called with an empty configuration.</li>
                                            <li>The `get_collection_errors` method raises a `TypeError` exception when called with an empty configuration.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">15 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_context_override_default_none</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the default value of llm_context_override in TestCollectorMarkerExtraction.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the default value of llm_context_override is not set correctly.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The llm_context_override attribute should be None for the given TestCaseResult.</li>
                                            <li>The llm_context_override attribute does not contain any string values.</li>
                                            <li>The llm_context_override attribute does not contain any boolean values.</li>
                                            <li>The llm_context_override attribute is set to a value other than None or an empty string.</li>
                                            <li>The llm_context_override attribute has the correct type (str).</li>
                                            <li>The llm_context_override attribute is not None for all TestCaseResult instances.</li>
                                            <li>The llm_context_override attribute is None for all TestCaseResult instances with a given nodeid and outcome.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_opt_out_default_false</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the default value of llm_opt_out is correctly set to False when not specified.</p>
                                    <p><strong>Why Needed:</strong> Prevents a bug where the default value of llm_opt_out is incorrectly set to True, potentially causing incorrect results in downstream tests or reports.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The llm_opt_out attribute is correctly initialized with False.</li>
                                            <li>The llm_opt_out attribute does not have any dependencies that could cause it to be set to True.</li>
                                            <li>The TestCaseResult class has a correct implementation of the llm_opt_out attribute.</li>
                                            <li>The test passes without any errors or warnings when running with default configuration.</li>
                                            <li>The test fails with an error or warning when running with specific configuration that sets llm_opt_out to True.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector.py::TestCollectorOutputCapture::test_capture_disabled_by_default</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The `capture` feature is not enabled by default.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the output capture feature is accidentally enabled by default.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>config.capture_failed_output should be set to False</li>
                                            <li>output_capture_enabled should be False</li>
                                            <li>no_output_capture_message should be displayed</li>
                                            <li>capture_output_path should not be provided</li>
                                            <li>log_level should not be set to DEBUG for output capture</li>
                                            <li>test_output_capture_enabled should be True</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector.py::TestCollectorOutputCapture::test_capture_max_chars_default</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The TestCollectorOutputCapture test verifies that the default value of `capture_output_max_chars` is correctly set to 4000.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the default max chars is not set to 4000, potentially leading to unexpected behavior or errors in the application.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `capture_output_max_chars` attribute of the TestCollectorOutputCapture class should be equal to 4000.</li>
                                            <li>The `capture_output_max_chars` attribute does not exceed 4000.</li>
                                            <li>The default value of `capture_output_max_chars` is correctly set to 4000.</li>
                                            <li>Increasing or decreasing the value of `capture_output_max_chars` will not affect the test outcome.</li>
                                            <li>The TestCollectorOutputCapture class is properly initialized with a valid configuration.</li>
                                            <li>The `capture_output_max_chars` attribute is updated correctly after initialization.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector.py::TestCollectorXfailHandling::test_xfail_failed_is_xfailed</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test 'xfail failures should be recorded as xfailed' is not being tested.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the collector incorrectly records failed tests as expected failure.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `results` dictionary contains an entry for `test_xfail.py::test_expected_fail` with a value of 'xfailed'.</li>
                                            <li>The `outcome` attribute is set to 'xfailed' in the `results` dictionary.</li>
                                            <li>The `wasxfail` attribute is set to 'expected failure' in the `results` dictionary.</li>
                                            <li>The `duration` and `longrepr` attributes are not relevant to this test, but they may be useful for debugging purposes.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212, 216, 227-228, 230-234, 238)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector.py::TestCollectorXfailHandling::test_xfail_passed_is_xpassed</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> xfail passes should be recorded as xpassed.</p>
                                    <p><strong>Why Needed:</strong> To prevent regression in case of unexpected failures during test runs.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `result.outcome` property is set to 'xpassed' when the report indicates an expected failure.</li>
                                            <li>The `collector.results[report.nodeid]` attribute returns a dictionary with the correct key ('xpassed') for the given node ID.</li>
                                            <li>The test verifies that the collector correctly marks xfail tests as passed after handling their log reports.</li>
                                            <li>The test ensures that the collector's results are consistent with the expected outcome of an unexpected failure.</li>
                                            <li>The test checks that the `result.outcome` property is set to 'xpassed' even when the report indicates a failed test.</li>
                                            <li>The test verifies that the collector correctly handles xfail tests and marks them as passed after handling their log reports.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212-214)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector.py::TestTestCollector::test_create_collector</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the `create_collector` method of `TestCollector` class.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `create_collector` method initializes with an empty results dictionary, potentially causing unexpected behavior or errors in subsequent operations.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `results` attribute of the `collector` object is set to an empty dictionary when created.</li>
                                            <li>The `collection_errors` list is empty when the `collector` object is created.</li>
                                            <li>The `collected_count` attribute is set to 0 when the `collector` object is created.</li>
                                            <li>The `results` attribute is not updated with any data when a new collection is added using `add()` method.</li>
                                            <li>The `collection_errors` list is not updated with any errors when an error occurs during collection.</li>
                                            <li>The `collected_count` attribute is not incremented correctly when the same collector is used multiple times.</li>
                                            <li>The `results` attribute is not checked for changes after a new collection is added using `add()` method.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector.py::TestTestCollector::test_get_results_sorted</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `get_results` method returns a list of node IDs sorted by their values.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the order of results is not maintained due to manual additions or modifications.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The list of node IDs returned by `get_results()` should be in ascending order based on their values.</li>
                                            <li>The list of node IDs returned by `get_results()` should contain only unique node IDs.</li>
                                            <li>The sorted list of node IDs should maintain the original order of results before any manual additions or modifications.</li>
                                            <li>No duplicate node IDs should be present in the sorted list.</li>
                                            <li>The first element of the sorted list should be 'a_test.py::test_a'.</li>
                                            <li>The second element of the sorted list should be 'z_test.py::test_z'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">15 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector.py::TestTestCollector::test_handle_collection_finish</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `handle_collection_finish` method correctly tracks collected and deselected counts.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the count of collected items is not updated correctly after the collection finish event.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `collected_count` attribute should be set to 3 (number of collected items)</li>
                                            <li>The `deselected_count` attribute should be set to 1 (number of deselected items)</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">16 lines (ranges: 78-79, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_collector_maximal.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">14 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_disabled_via_handle_report</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test should not capture output if config disabled (integration via handle_runtest_logreport).</p>
                                    <p><strong>Why Needed:</strong> To prevent capturing of output when the `capture_failed_output` configuration flag is set to False.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `collector.handle_runtest_logreport(report)` call should not modify the report's captured stdout.</li>
                                            <li>The `results['t']` attribute of the collector's results dictionary should contain a `captured_stdout` key with a value of `None`.</li>
                                            <li>The `collector.results` dictionary should still contain the original `outcome`, `when`, `passed`, and `failed` attributes.</li>
                                            <li>The `report` object should not have any additional attributes after calling `collector.handle_runtest_logreport(report)`.</li>
                                            <li>The `wasxfail` attribute of the report object should be deleted (if present).</li>
                                            <li>The `collector.results` dictionary should still contain all other attributes as before.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stderr</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the collector captures stderr and reports it correctly.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the collector does not capture stderr or reports incorrect stderr output.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `captured_stderr` attribute of the `TestCaseResult` object is set to 'Some error'.</li>
                                            <li>The `report.capstderr` method is called with an argument equal to 'Some error'.</li>
                                            <li>The `collector._capture_output(result, report)` function call passes a `report` object with `capstderr=True`.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264, 268-269)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stdout</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `capture_output` method captures stdout correctly.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the captured stdout is not properly recorded.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `captured_stdout` attribute of the `TestCaseResult` object should be set to 'Some output'.</li>
                                            <li>The `report.capstdout` method should have been called with the correct value ('Some output').</li>
                                            <li>The `collector._capture_output` function should have recorded the captured stdout correctly.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_truncated</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `test_capture_output_truncated` function truncates output exceeding max chars.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the collector does not truncate output exceeding the specified max characters, leading to unexpected behavior or errors in downstream processing.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The captured stdout should be truncated to 10 characters.</li>
                                            <li>The captured stderr is empty.</li>
                                            <li>The `captured_stdout` attribute of the result object contains only 5 characters (i.e., '1234567890').</li>
                                            <li>The `captured_stderr` attribute of the result object is empty.</li>
                                            <li>The collector does not truncate output exceeding the specified max chars when capturing stdout.</li>
                                            <li>The collector does not truncate output exceeding the specified max chars when capturing stderr.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">18 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_create_result_with_item_markers</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test creates a result with item markers.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in case of item markers being added to the collector without proper setup.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>item.get_closest_marker('llm_opt_out') returns MagicMock.</li>
                                            <li>item.get_closest_marker('llm_context') returns a MagicMock object with 'complete' as its argument.</li>
                                            <li>item.get_closest_marker('requirement') returns a MagicMock object with ['REQ-1', 'REQ-2'] as its arguments.</li>
                                            <li>result.param_id is set to 'param1'.</li>
                                            <li>result.llm_opt_out is set to True.</li>
                                            <li>result.llm_context_override is set to 'complete'.</li>
                                            <li>result.requirements contains ['REQ-1', 'REQ-2'].</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">35 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163-164, 167-169, 171, 181-182, 185-189, 198-200, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_repr_crash</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `collectors` module correctly handles ReprFileLocation when used in error representation.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential crash caused by using `str()` on a `ReprFileLocation` object.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `_extract_error` method returns the expected string 'Crash report' for `report.longrepr.__str__.return_value = 'Crash report'`.</li>
                                            <li>The `_extract_error` method does not crash when using `ReportError` with a `ReprFileLocation` object.</li>
                                            <li>The `_extract_error` method correctly handles cases where `str()` is used on a `ReprFileLocation` object.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">22 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_string</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `_extract_error` method returns a string longrepr when an error occurs.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails with a non-string result.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `report.longrepr` attribute is set to 'Some error occurred'.</li>
                                            <li>The value of `_extract_error(report)` is equal to 'Some error occurred'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">22 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_fallback</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the `_extract_skip_reason` method when `longrepr` is set to `None`.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails if `longrepr` is not provided in the report.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The method does not throw an exception or raise an error when `longrepr` is `None`.</li>
                                            <li>The `_extract_skip_reason` method returns `None` as expected when `longrepr` is `None`.</li>
                                            <li>The test asserts that `collector._extract_skip_reason(report)` returns `None` instead of raising an exception.</li>
                                            <li>The test checks if the `report.longrepr` attribute is set to `None` before calling `_extract_skip_reason`.</li>
                                            <li>If `longrepr` is not `None`, the method should return a specific value (e.g., `None`) or raise an error.</li>
                                            <li>The test verifies that the returned value is consistent across different scenarios.</li>
                                            <li>The test ensures that the `_extract_skip_reason` method behaves as expected in edge cases (e.g., when `longrepr` is not provided).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_string</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test `test_extract_skip_reason_string` verifies that the `_extract_skip_reason` method returns a string representing the skip reason.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the skip reason is not correctly extracted from the report.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The expected value of `report.longrepr` should be 'Just skipped'.</li>
                                            <li>The `_extract_skip_reason` method should return the correct string representation of the skip reason.</li>
                                            <li>If the test data changes, this test should still pass and not regress.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_tuple</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `extract_skip_reason_tuple` method correctly extracts a skip message from a tuple containing file, line, and message information.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a bug where the `extract_skip_reason_tuple` method does not extract the correct reason for skipping a test.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `report.longrepr` attribute is a tuple containing 'file', 'line', and 'message' values.</li>
                                            <li>The `report.longrepr` attribute contains the string 'Skipped for reason'.</li>
                                            <li>The extracted reason matches the expected reason in the `report.longrepr` attribute.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_collection_report_failure</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test 'handle_collection_report_failure' verifies that the TestCollector class correctly handles a collection report failure by recording an error and updating its collection errors list.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the TestCollector class does not handle collection reports failures correctly, potentially leading to incorrect or missing error messages in the collection errors list.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'collection_errors' attribute of the collector should be updated with an instance of MagicMock.</li>
                                            <li>The 'nodeid' attribute of the first element in 'collector.collection_errors' should match the value specified in the test report.</li>
                                            <li>The 'message' attribute of the first element in 'collector.collection_errors' should match the expected error message 'SyntaxError'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">21 lines (ranges: 58, 60-65, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_rerun</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test 'handle_runtest_rerun' verifies that the `rerun` attribute of a report is correctly set to 1 after a rerun.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where a rerun attribute is not correctly updated after a runtest rerun.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>res.rerun_count should be equal to 1</li>
                                            <li>res.final_outcome should be 'failed'</li>
                                            <li>report.rerun should have been set to 1</li>
                                            <li>report.wasxfail should have been deleted</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_setup_failure</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test verifies that the TestCollector handles a runtest setup failure correctly by recording an error report.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in the TestCollector's behavior when encountering a setup failure during a runtest.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'outcome' of the collector's results for the test 't::f' is set to 'error'.</li>
                                            <li>The phase of the collector's results for the test 't::f' is set to 'setup'.</li>
                                            <li>The error message in the collector's results for the test 't::f' is set to 'Setup failed'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">36 lines (ranges: 90, 93-94, 96, 99-103, 109-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_teardown_failure</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test verifies that the test handle_runtest_teardown_failure function correctly records an error when teardown fails after a pass.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the collector reports an error when teardown fails, which is necessary for cleanup to occur.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `teardown_report` should have been marked as failed.</li>
                                            <li>The `res.outcome` should be set to 'error'.</li>
                                            <li>The `res.phase` should be set to 'teardown'.</li>
                                            <li>The `res.error_message` should contain the string 'Cleanup failed'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">38 lines (ranges: 90, 93-94, 96, 99, 110-112, 114-115, 124, 127-128, 130, 132-133, 135-137, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_coverage_boosters.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">3 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_model_parsing_edge_cases</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that the GeminiProvider correctly handles edge cases when parsing preferred models.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider returns an empty list of models for an invalid model configuration.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `provider._parse_preferred_models()` should return a list containing 'm1' and 'm2'.</li>
                                            <li>The function `provider._parse_preferred_models()` should return an empty list when the model configuration is None.</li>
                                            <li>The function `provider._parse_preferred_models()` should return an empty list when the model configuration is set to 'All'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">17 lines (ranges: 134, 136-139, 141-142, 391, 393, 423-430)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_rate_limiter_edge_math</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the rate limiter prevents over and under token limits when tokens are recorded but not requests.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the rate limiter allows users to record excess tokens, potentially leading to abuse or unexpected behavior.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert limiter.next_available_in(60) > 0</li>
                                            <li>assert limiter.next_available_in(10) == 0</li>
                                            <li>assert limiter.record_tokens(50) <= requests_per_minute * tokens_per_minute</li>
                                            <li>assert limiter.record_tokens(110) > requests_per_minute * tokens_per_minute</li>
                                            <li>assert limiter.record_tokens(120) > requests_per_minute * tokens_per_minute</li>
                                            <li>assert limiter.record_tokens(130) > requests_per_minute * tokens_per_minute</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">35 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-114)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_boosters.py::TestCoverageBoosters::test_models_to_dict_variants</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `to_dict()` method of SourceCoverageEntry and LlmAnnotation correctly converts them to dictionaries with the expected values.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in coverage booster models where the coverage percentage is not accurately converted from source code coverage entries.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'coverage_percent' value in the resulting dictionary should be equal to the original `coverage_percent` value.</li>
                                            <li>The 'error' value in the resulting dictionary for LlmAnnotation should match the expected error message.</li>
                                            <li>The 'duration' value in the resulting dictionary for RunMeta should match the expected duration value.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">46 lines (ranges: 71-78, 104-107, 109, 111-113, 115, 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_coverage_map.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">7 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_create_mapper</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `CoverageMapper` class initializes correctly with a given configuration.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `CoverageMapper` instance does not have access to its configured settings.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert mapper.config is config</li>
                                            <li>assert mapper.warnings == []</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 44-45)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_get_warnings</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify the `get_warnings` method returns a list of warnings.</p>
                                    <p><strong>Why Needed:</strong> Prevents potential issues with incorrect or missing warning data.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `get_warnings()` method should return a list of warnings.</li>
                                            <li>The `get_warnings()` method should not raise an exception.</li>
                                            <li>All warnings in the list should be instances of the `Warning` class.</li>
                                            <li>No exceptions should be raised when calling `get_warnings()` on a valid configuration.</li>
                                            <li>The warning data is properly formatted and consistent across all tests.</li>
                                            <li>The test does not fail if there are no warnings for a particular configuration.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 44-45, 308)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map.py::TestCoverageMapper::test_map_coverage_no_coverage_file</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `map_coverage` method returns an empty dictionary when no coverage file is present.</p>
                                    <p><strong>Why Needed:</strong> Prevents a regression where the test fails to report coverage for files without a corresponding coverage file.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `mapper.map_coverage()` function should return an empty dictionary.</li>
                                            <li>The `mapper.warnings` attribute should have at least one warning.</li>
                                            <li>The `Path.exists` and `glob.glob` mock functions should be called with the correct arguments to return False and an empty list, respectively.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_all_phases</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `CoverageMapper` extracts node IDs for all phases when `include_phase=all`.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the coverage map might not include all phases if `include_phase=all`.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `_extract_nodeid()` method returns the correct node ID based on the phase.</li>
                                            <li>The `_extract_nodeid()` method does not return an empty string for any phase.</li>
                                            <li>The `_extract_nodeid()` method returns a node ID that matches the expected value for all phases.</li>
                                            <li>If `include_phase=all`, the coverage map should include all phases in the extracted node IDs.</li>
                                            <li>If `include_phase=all` and there are multiple nodes with the same name, only one of them should be included in the coverage map.</li>
                                            <li>The `_extract_nodeid()` method does not return a specific phase if it is not specified (e.g., `test.py::test_foo|run`).</li>
                                            <li>If `include_phase=all` and there are multiple nodes with different names, all of them should be included in the coverage map.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_empty_context</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 44-45, 216-217)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_filters_setup</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the ability to filter out setup phase when include_phase=run.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the test would incorrectly extract node IDs for tests in the setup phase.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function _extract_nodeid() should return None for the given input.</li>
                                            <li>The function _extract_nodeid() should not include any node IDs from the 'setup' phase.</li>
                                            <li>The mapper._extract_nodeid() method should correctly filter out node IDs from the setup phase.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">9 lines (ranges: 44-45, 216, 220, 224-225, 228-230)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_with_run_phase</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `extract_nodeid` method of the `CoverageMapper` class correctly extracts a node ID from the run phase context.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the node ID is not extracted correctly due to incorrect configuration or mismatched module paths.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The expected node ID matches the actual node ID.</li>
                                            <li>The `nodeid` variable holds the correct value.</li>
                                            <li>The `assert` statement passes if the node ID is as expected.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_coverage_map_coverage.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">17 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_coverage.py::TestExtractContexts::test_contexts_by_lineno_exception</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test Extracting Contexts when `contexts_by_lineno` raises an exception.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an unhandled exception in `contexts_by_lineno`.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `_extract_contexts` should not raise an exception when called with a mock data object that has already handled the exception.</li>
                                            <li>The function `_extract_contexts` should correctly handle the exception and return an empty dictionary.</li>
                                            <li>The function `_extract_contexts` should not throw any additional errors or warnings if the `contexts_by_lineno` raises an exception.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">29 lines (ranges: 44-45, 118, 121-122, 127, 131-135, 137-140, 144, 148, 150, 152, 156, 160-162, 167-170, 199, 202)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_coverage.py::TestExtractContexts::test_no_measured_files</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 44-45, 118, 121-122, 127-128)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_coverage.py::TestExtractContexts::test_skip_non_python_files</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that non-Python files are skipped.</p>
                                    <p><strong>Why Needed:</strong> Prevents a regression where non-python files are included in coverage reports.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>mocked mock_data.measured_files returns an empty list when the input is not a Python file.</li>
                                            <li>mocked mock_data.contexts_by_lineno returns an empty dictionary when the input is not a Python file.</li>
                                            <li>The test asserts that the result of _extract_contexts is an empty dictionary for non-Python files.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 44-45, 118, 121-122, 127, 131-135, 144-146)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_coverage.py::TestLoadCoverageData::test_coverage_not_installed</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that coverage.py is installed before attempting to load coverage data.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where coverage.py is missing, causing the _load_coverage_data method to fail and return None.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The mapper variable is not None after calling _load_coverage_data.</li>
                                            <li>The _load_coverage_data method does not raise an ImportError when coverage.py is installed.</li>
                                            <li>The _load_coverage_data method returns a Config object instead of None when coverage.py is installed.</li>
                                            <li>The mapper variable is not None after calling _load_coverage_data() and the config is successfully created.</li>
                                            <li>The mapper variable is not None after calling _load_coverage_data() and the config is successfully created, but without coverage data.</li>
                                            <li>The mapper variable is not None after calling _load_coverage_data() and the config is successfully created, with an empty coverage data set.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 44-45)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_coverage.py::TestLoadCoverageData::test_no_coverage_file</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test 'test_no_coverage_file' verifies that when no .coverage file exists, the function returns None and generates warnings.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the function would return an error or raise an exception when there is no coverage data.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `mapper._load_coverage_data()` should be called with no arguments.</li>
                                            <li>The result of `mapper._load_coverage_data()` should be None.</li>
                                            <li>Any warnings generated by the mapper should contain 'W001'.</li>
                                            <li>No error or exception should be raised when there is no coverage data.</li>
                                            <li>The function `Config` should have been successfully instantiated.</li>
                                            <li>The function `CoverageMapper` should have been successfully instantiated and created a mapper object.</li>
                                            <li>The function `mapper.warnings` should not contain any warnings for the test scenario.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">9 lines (ranges: 44-45, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_analysis_exception_handling</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the exception handling of analysis2 when it raises an exception.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where analysis2 raises an exception and coverage is not handled correctly.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `map_source_coverage` should return an empty list when `analysis2` raises an exception.</li>
                                            <li>A warning with the message 'COVERAGE_ANALYSIS_FAILED' should be added to the warnings list.</li>
                                            <li>All warnings in the map source coverage should have the message 'COVERAGE_ANALYSIS_FAILED'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">22 lines (ranges: 44-45, 243-244, 246-248, 250, 252-254, 259, 261, 263-268, 271, 299-300)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_empty_statements</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that an empty source file is handled correctly by the CoverageMapper.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where coverage reports are missing for files with no statements.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `map_source_coverage` should return an empty list when given a mock Covariance object and mock data.</li>
                                            <li>The `mock_data.measured_files.return_value` should be set to `['/project/src/empty.py']` before the test is run.</li>
                                            <li>The `mock_cov.get_data.return_value` should be set to the mock data object.</li>
                                            <li>The `mock_cov.analysis2.return_value` should contain an empty list of files, a list of empty strings, and a list of empty lists.</li>
                                            <li>The function `map_source_coverage` should not raise any exceptions when given a mock Covariance object and mock data.</li>
                                            <li>The test result should be an empty list.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">18 lines (ranges: 44-45, 243-244, 246-248, 250, 252-254, 259-261, 273-274, 299-300)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_include_test_files_when_not_configured</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that test files are included when omit_tests_from_coverage is False.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in case the configuration is not properly set, where missing test files would cause coverage to be incomplete or inaccurate.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The mapper returns exactly one result with a covered percentage of 2 out of 3 files.</li>
                                            <li>The mapper returns an empty list for all files.</li>
                                            <li>The mapper returns a single entry with the correct covered and missed counts.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">32 lines (ranges: 44-45, 243-244, 246-248, 250, 252, 259-261, 273, 276-279, 281-283, 285-293, 295, 299-300)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">17 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/ranges.py</span>
                                        <span style="color: var(--text-secondary)">13 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65-67)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_skip_non_python_files</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that non-Python files are skipped.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in coverage reporting when non-python files are present.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `mock_data.measured_files` method is called with the correct arguments.</li>
                                            <li>The `mock_data.contexts_by_lineno` method is called with the correct arguments.</li>
                                            <li>The `result` variable is set to an empty dictionary.</li>
                                            <li>The `assert` statement checks if the `result` is equal to the expected output.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 44-45, 243-244, 246-249, 299-300)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_skip_test_files_when_configured</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that test files are skipped when omit_tests_from_coverage is True.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the coverage map is not generated for test files even when omit_tests_from_coverage is enabled.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `map_source_coverage` method of `CoverageMapper` should return an empty list if `omit_tests_from_coverage` is True and there are no test files to cover.</li>
                                            <li>The `get_data` method of the mock `CoverageMapper` object should not have been called with a non-empty list of test files when `omit_tests_from_coverage` is True.</li>
                                            <li>The `mock_cov.get_data.return_value` call should return a mock data object that has no measured files.</li>
                                            <li>The `mock_data.measured_files.return_value` call should be empty, indicating no test files were covered.</li>
                                            <li>The `mock_cov.get_data.return_value` call should not have called any methods on the mock data object when `omit_tests_from_coverage` is True.</li>
                                            <li>The `map_source_coverage` method of `CoverageMapper` should return an empty list even if there are multiple test files in the repository.</li>
                                            <li>The `map_source_coverage` method of `CoverageMapper` should not have generated any coverage for test files when `omit_tests_from_coverage` is True.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">15 lines (ranges: 44-45, 243-244, 246-248, 250, 252-255, 257, 299-300)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_all_phase_config</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that all phases are accepted when configured.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in case of phase filtering configuration change.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The mapper should return the same nodeid for each phase.</li>
                                            <li>The mapper should accept any phase as input and return a matching nodeid.</li>
                                            <li>The mapper should handle phases with different module names correctly (e.g., 'test_foo.py::test_bar').</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_empty_string</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that an empty string does not return a node ID.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where an empty string could be returned as a node ID, potentially causing issues with coverage analysis.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert mapper._extract_nodeid('') == None</li>
                                            <li>assert mapper._extract_nodeid(None) == None</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 44-45, 216-217)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_none</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Tests that None input to _extract_nodeid method returns None.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the method does not handle None inputs correctly.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The method should return None when passed None as an argument.</li>
                                            <li>None is expected to be returned for the given input.</li>
                                            <li>The method's behavior is not tested with None inputs, which could lead to unexpected results or errors.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 44-45, 216-217)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_run_phase_default</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that run phase is the default filter.</p>
                                    <p><strong>Why Needed:</strong> Prevents a regression where nodeids are not extracted for phases other than 'run'.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>mapper._extract_nodeid('test_foo.py::test_bar|run') == 'test_foo.py::test_bar'</li>
                                            <li>mapper._extract_nodeid('test_foo.py::test_bar|setup') is None</li>
                                            <li>mapper._extract_nodeid('test_foo.py::test_bar|teardown') is None</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 216, 220, 224-225, 228-231, 233, 236)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_setup_phase_config</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that setup phase is correctly filtered when configured.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a bug where the setup phase is incorrectly filtered, potentially leading to false positives or negatives in coverage reports.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>mapper._extract_nodeid('test_foo.py::test_bar|setup') == 'test_foo.py::test_bar'</li>
                                            <li>mapper._extract_nodeid('test_foo.py::test_bar|run') is None</li>
                                            <li>mapper._extract_nodeid('test_foo.py::test_bar|teardown') is None</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231-233, 236)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_teardown_phase_config</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that teardown phase is correctly filtered when configured.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the test extract_nodeid function does not filter out nodeids from the teardown phase configuration.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return the expected nodeid when the phase matches.</li>
                                            <li>The function should return None when the phase doesn't match.</li>
                                            <li>The function should ignore nodes in the run and setup phases.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231, 233-234, 236)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_without_pipe</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `extract_nodeid` method returns the node ID without a phase delimiter when the context is in the correct format.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `extract_nodeid` method incorrectly handles contexts with phase delimiters.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The node ID should be returned unchanged (i.e., `test_foo.py::test_bar`) without any modifications.</li>
                                            <li>No additional context is required to extract the node ID (e.g., no pipe or delimiter in the file path).</li>
                                            <li>The method should not modify the original file path, only return the node ID.</li>
                                            <li>Any leading or trailing whitespace in the file path should be preserved.</li>
                                            <li>The `extract_nodeid` method should not add any additional information to the node ID (e.g., module name, filename).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 44-45, 216, 220, 224, 239)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_coverage_map_maximal.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">9 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_full_logic</span>
                            <div class="test-meta">
                                <span>15ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test should extract all contexts for full logic coverage.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in case of minimal coverage on app.py files.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function _extract_contexts returns 'test_app.py::test_one' and 'test_app.py::test_two' as expected.</li>
                                            <li>The line count for each context is correct (2 lines for both).</li>
                                            <li>The file paths of the contexts are correctly matched with app.py files.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">57 lines (ranges: 44-45, 118, 121-122, 127, 131-135, 137-140, 144, 148, 150, 152-153, 156, 160-163, 165, 167-168, 173, 176, 178-184, 187-189, 191-194, 196, 199-200, 202, 216, 220, 224-225, 228-229, 231, 233, 236)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">17 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/ranges.py</span>
                                        <span style="color: var(--text-secondary)">13 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65-67)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_no_contexts</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `extract_contexts` method correctly handles data with no test contexts.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in coverage analysis, as it ensures that all files are covered by context.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>mock_data.measured_files.return_value should be equal to `[]`</li>
                                            <li>mock_data.contexts_by_lineno.return_value should be an empty dictionary</li>
                                            <li>result should be equal to `None` or an empty dictionary</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 44-45, 118, 121-122, 127, 131-135, 144-146)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_nodeid_variants</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the `CoverageMapper` class with different phases and scenarios.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a bug where missing lines in the code are not correctly identified as covered by the coverage report.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `_extract_nodeid` method should return the expected node ID for each line of code.</li>
                                            <li>The `None` assertion is used when no lines match any phase, ensuring that the test covers all scenarios.</li>
                                            <li>The context without a pipe (`test.py::test_no_phase`) should also be correctly identified as covered by the coverage report.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 44-45, 216, 220, 224-225, 228-229, 231-234, 236, 239)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_no_files</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the function "test_load_coverage_data_no_files" to ensure it correctly handles cases where no coverage files exist.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug that would cause the function to incorrectly report warnings for non-existent .coverage files.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return None when no .coverage files are found.</li>
                                            <li>The number of warnings should be 1.</li>
                                            <li>The first warning message should be 'W001'.</li>
                                            <li>The current working directory should be changed to the temporary directory before calling _load_coverage_data().</li>
                                            <li>The function should not throw an exception or raise an error when no .coverage files are found.</li>
                                            <li>The function should return None immediately after checking the current working directory and calling _load_coverage_data().</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">9 lines (ranges: 44-45, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_read_error</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the test_load_coverage_data_read_error function to verify that it handles errors reading coverage files correctly.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the CoverageMapper class fails to handle errors when loading coverage data from corrupted files.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return None when an error occurs while reading coverage data.</li>
                                            <li>Any warnings with messages 'Failed to read coverage data' should be raised.</li>
                                            <li>The coverage file should be created in the temporary directory before attempting to load it.</li>
                                            <li>The CoverageMapper class's _load_coverage_data method should raise an Exception when a corrupt coverage file is encountered.</li>
                                            <li>The function should not attempt to load coverage data from a non-existent or corrupted file.</li>
                                            <li>The function should handle errors raised by the mock CoverageData object correctly.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">17 lines (ranges: 44-45, 72-73, 83, 86, 88, 92, 94-96, 107-111, 114)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_with_parallel_files</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test should handle parallel coverage files from xdist and verify that it correctly updates the CoverageData instances.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in handling parallel coverage files, ensuring that the CoverageMapper class can correctly update its internal state when dealing with such files.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>...</li>
                                            <li>...</li>
                                            <li>...</li>
                                            <li>...</li>
                                            <li>Mocking mock_parallel_data1 and mock_parallel_data2 to return different instances of CoverageData.</li>
                                            <li>The update method of CoverageData should be called at least twice during the load process for parallel coverage files.</li>
                                            <li>The assert statement should not raise an AssertionError when the update method is called multiple times.</li>
                                            <li>...</li>
                                            <li>...</li>
                                            <li>...</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">15 lines (ranges: 44-45, 72-73, 83, 86, 88, 92, 94, 98, 101-104, 106)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_coverage_no_data</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test verifies the behavior of `map_coverage` method when `_load_coverage_data` returns None.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the method does not handle cases with missing coverage data correctly.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The method should return an empty dictionary when no coverage data is loaded.</li>
                                            <li>No exception should be raised when no coverage data is loaded.</li>
                                            <li>The method should check if `_load_coverage_data` returns None before attempting to map coverage.</li>
                                            <li>The method should handle the case where `None` is returned from `_load_coverage_data` correctly.</li>
                                            <li>The method should not attempt to access any attributes or methods of the coverage data that are not present.</li>
                                            <li>The method should return a dictionary with an empty list as its value when no coverage data is loaded.</li>
                                            <li>The method should not raise an exception when no coverage data is loaded.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 44-45, 58-60)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_analysis_error</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test coverage map source coverage analysis error.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where an error during analysis causes the test to fail and report incorrect results.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `map_source_coverage` method should not return any entries when an error occurs during analysis.</li>
                                            <li>The error message from `analysis2` should be 'Analysis failed'.</li>
                                            <li>The number of returned entries should be 0, indicating no errors in the source files.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">22 lines (ranges: 44-45, 243-244, 246-248, 250, 252-254, 259, 261, 263-268, 271, 299-300)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_comprehensive</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Tests the coverage mapper's ability to map source code coverage across different analysis methods.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in coverage reporting when using multiple analysis methods (analysis2) with the same data.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `map_source_coverage` should return a list containing one entry with file path 'app.py', three statements, two covered lines, one missed line, and an accuracy of 66.67%.</li>
                                            <li>The assertions for each entry in the returned list should match the expected coverage statistics.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">32 lines (ranges: 44-45, 243-244, 246-248, 250, 252, 259-261, 273, 276-279, 281-283, 285-293, 295, 299-300)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">17 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/ranges.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_errors.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">3 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_errors.py::test_make_warning</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the `make_warning` function with a valid warning code and message.</p>
                                    <p><strong>Why Needed:</strong> To prevent a warning about missing coverage files for unknown warnings.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function returns an instance of WarningCode.W001_NO_COVERAGE with the specified code and message.</li>
                                            <li>The `detail` attribute is set to 'test-detail'.</li>
                                            <li>The `message` attribute contains the expected string 'Unknown warning.'</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_errors.py::test_warning_code_values</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that warning codes have correct values.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the warning code values are not correctly identified, potentially leading to incorrect handling of warnings in the code.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>{'name': 'assert WarningCode.W001_NO_COVERAGE.value == "W001"', 'description': 'Verify that the correct value is assigned to WarningCode.W001_NO_COVERAGE.'}</li>
                                            <li>{'name': 'assert WarningCode.W101_LLM_ENABLED.value == "W101"', 'description': 'Verify that the correct value is assigned to WarningCode.W101_LLM_ENABLED.'}</li>
                                            <li>{'name': 'assert WarningCode.W201_OUTPUT_PATH_INVALID.value == "W201"', 'description': 'Verify that the correct value is assigned to WarningCode.W201_OUTPUT_PATH_INVALID.'}</li>
                                            <li>{'name': 'assert WarningCode.W301_INVALID_CONFIG.value == "W301"', 'description': 'Verify that the correct value is assigned to WarningCode.W301_INVALID_CONFIG.'}</li>
                                            <li>{'name': 'assert WarningCode.W401_AGGREGATE_DIR_MISSING.value == "W401"', 'description': 'Verify that the correct value is assigned to WarningCode.W401_AGGREGATE_DIR_MISSING.'}</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_errors.py::test_warning_to_dict</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test Warning.to_dict() method.</p>
                                    <p><strong>Why Needed:</strong> Prevents a warning from being silently ignored when the 'detail' field is missing in a Warning object.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'code' field of the Warning object should be set to 'W001'.</li>
                                            <li>The 'message' field of the Warning object should be set to 'No coverage'.</li>
                                            <li>The 'detail' field of the Warning object should be set to 'some/path'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 70-72, 74-76)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_errors_maximal.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">6 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_known_code</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test verifies that a warning is created with the correct code and message for known code.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where warnings are not correctly generated for known code.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `make_warning` returns an instance of `WarningCode.W101_LLM_ENABLED` with the expected value.</li>
                                            <li>The warning message is set to `WARNING_MESSAGES[WarningCode.W101_LLM_ENABLED]` as expected.</li>
                                            <li>The detail attribute is not provided, which is incorrect for warnings related to known code.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_unknown_code</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Tests for the `make_warning` function to handle unknown code warnings.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where an unknown code is used in a warning message without being allowed by the enum.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `make_warning(missing_code)` should return a warning message that indicates 'Unknown warning.'</li>
                                            <li>The value of `WARNING_MESSAGES[missing_code]` should be set to the old message before restoring it.</li>
                                            <li>The test should fail when trying to make a warning with an unknown code.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_with_detail</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test 'test_make_warning_with_detail' verifies that a warning is created with the correct code and detail.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where a warning might not be created correctly when the detail parameter is set to an invalid configuration value.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `make_warning` returns a Warning object with the specified code and detail.</li>
                                            <li>The attribute `code` of the Warning object matches the expected WarningCode.W301_INVALID_CONFIG.</li>
                                            <li>The attribute `detail` of the Warning object matches the expected string 'Bad value'.</li>
                                            <li>The warning is created correctly even if the input configuration value is invalid.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_errors_maximal.py::TestWarningCodes::test_codes_are_strings</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that all WarningCode enum values are strings and start with 'W'.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential TypeError when trying to use non-string values as WarningCode enum members.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert isinstance(code.value, str) checks if the value of each WarningCode enum member is indeed a string.</li>
                                            <li>assert code.value.startswith('W') checks if all WarningCode enum members start with 'W'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_no_detail</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the warning to dictionary conversion without detail.</p>
                                    <p><strong>Why Needed:</strong> To prevent a potential bug where warnings are not properly serialized to JSON with detailed information.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `to_dict()` method of the Warning class returns a dictionary with the correct keys and values.</li>
                                            <li>The 'code' key in the returned dictionary contains the warning code.</li>
                                            <li>The 'message' key in the returned dictionary contains the warning message.</li>
                                            <li>The 'detail' field is missing from the returned dictionary.</li>
                                            <li>The 'level' field is missing from the returned dictionary.</li>
                                            <li>The 'category' field is missing from the returned dictionary.</li>
                                            <li>The 'filename' field is missing from the returned dictionary.</li>
                                            <li>The 'lineno' field is missing from the returned dictionary.</li>
                                            <li>The 'module' field is missing from the returned dictionary.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 70-72, 74, 76)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_with_detail</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the warning to dictionary conversion with detailed information.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where warnings are not properly serialized into dictionaries due to missing or incomplete detail.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `to_dict()` method of the `Warning` class returns a dictionary with the correct keys ('code', 'message', and 'detail').</li>
                                            <li>The value of each key in the returned dictionary matches the expected values (e.g., 'W001' for 'code', 'No coverage' for 'message', and 'Check setup' for 'detail').</li>
                                            <li>The presence of any missing or incomplete detail information is not detected by this test, as it relies on the `to_dict()` method to handle such cases.</li>
                                            <li>Without this test, warnings may be incorrectly serialized into dictionaries, potentially leading to unexpected behavior or errors in downstream applications.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 70-72, 74-76)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_fs.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">12 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs.py::TestIsPythonFile::test_non_python_file</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that the `is_python_file` function returns False for non-.py files.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the function incorrectly identifies non-python files as such.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert is_python_file('foo/bar.txt') is False</li>
                                            <li>assert is_python_file('foo/bar.pyc') is False</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 79)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs.py::TestIsPythonFile::test_python_file</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that the `is_python_file` function returns True for a `.py` file.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the function incorrectly identifies non-`.py` files as Python files.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return `True` when given a path to a `.py` file.</li>
                                            <li>The function should raise an error or return an appropriate value when given a non-`.py` file path.</li>
                                            <li>The function should correctly handle cases where the file name is not exactly `.py` but still contains Python code (e.g., `foo.py`)</li>
                                            <li>The function should not incorrectly identify files with non-Python extensions (e.g., `.txt`, `.js`) as Python files</li>
                                            <li>The function should raise an error when given a path to a non-existent file (e.g., `non_existent.py`)</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 79)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs.py::TestMakeRelative::test_makes_path_relative</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the function returns a normalized path when there is no base directory.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in case the `make_relative` function does not correctly handle cases where the input path has no base.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The output of `make_relative('foo/bar')` should be `'foo/bar''.</li>
                                            <li>The function should not raise an exception when given a non-existent directory.</li>
                                            <li>The function should return an empty string for paths with only one level of nesting.</li>
                                            <li>The function should handle cases where the input path is an absolute path.</li>
                                            <li>The function should handle cases where the input path is a relative path and does not have a base.</li>
                                            <li>The function should correctly normalize the output path even if it has multiple levels of nesting.</li>
                                            <li>The function should preserve the original directory structure when making relative paths.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 30, 33, 36, 39, 42, 55-56)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs.py::TestNormalizePath::test_already_normalized</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> tests/test_fs.py::TestNormalizePath::test_already_normalized</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the `normalize_path` function does not correctly handle already-normalized paths.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The input path is already normalized (i.e., no leading or trailing slashes).</li>
                                            <li>The normalized path remains unchanged (i.e., no changes are made to the original path).</li>
                                            <li>The function returns the original path as expected when given an already-normalized path.</li>
                                            <li>The function correctly handles paths with leading/trailing slashes.</li>
                                            <li>The function does not modify the input path in any way.</li>
                                            <li>The function preserves the original directory structure of the input path.</li>
                                            <li>The function raises a `ValueError` if the input path is not normalized.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs.py::TestNormalizePath::test_forward_slashes</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Tests the `normalize_path` function to remove trailing slashes from file paths.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where a file path with a trailing slash is returned unexpectedly.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return the original file path without any trailing slash.</li>
                                            <li>The function should not return a file path with a trailing slash if it already does not have one.</li>
                                            <li>The function should handle paths with multiple consecutive slashes correctly.</li>
                                            <li>The function should ignore leading or trailing whitespace when stripping slashes.</li>
                                            <li>The function should raise an error for invalid input (e.g., empty string, etc.).</li>
                                            <li>The function should preserve the original directory path in case of a trailing slash.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_custom_exclude_patterns</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `should_skip_path` function correctly skips paths matching custom patterns.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `should_skip_path` function incorrectly includes paths in the exclusion list, causing unexpected behavior or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert should_skip_path('tests/conftest.py', exclude_patterns=['test*']) is True</li>
                                            <li>assert should_skip_path('src/module.py', exclude_patterns=['test*']) is False</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">15 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116-117, 119-121, 123)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_normal_path</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The 'should_skip_path' function is called with a normal path.</p>
                                    <p><strong>Why Needed:</strong> This test prevents the 'should_skip_path' function from skipping normal file system paths.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert should_skip_path('src/module.py') is False</li>
                                            <li>assert not os.path.isdir('src/module.py')</li>
                                            <li>assert not os.path.exists('src/module.py')</li>
                                            <li>assert not os.path.isfile('src/module.py')</li>
                                            <li>assert not os.path.isabs('src/module.py')</li>
                                            <li>assert not should_skip_path(os.path.join('/path/to/directory', 'src/module.py'))</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_git</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `should_skip_path` function correctly identifies `.git` directories.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where the function incorrectly skips non-`.git` directories, potentially leading to unexpected behavior or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert should_skip_path('.git/objects/foo') is True</li>
                                            <li>assert not should_skip_path('non_git_directory') is False</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_pycache</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test verifies whether `should_skip_path` function correctly skips `__pycache__` directories.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not skip `__pycache__` directories as intended, potentially leading to incorrect behavior or security vulnerabilities.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return `True` when given a path that is located in `foo/__pycache__/bar.pyc`.</li>
                                            <li>The function should raise an exception with a meaningful error message when given a path that is not located in `foo/__pycache__/bar.pyc`.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs.py::TestShouldSkipPath::test_skips_venv</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> tests/test_fs.py::TestShouldSkipPath::test_skips_venv</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the function `should_skip_path` incorrectly identifies venv directories.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'venv' directory is not included in the list of paths to skip.</li>
                                            <li>The '.venv' directory is also not included in the list of paths to skip.</li>
                                            <li>If a path starts with 'venv', it should be skipped.</li>
                                            <li>If a path does not start with 'venv', it should not be skipped.</li>
                                            <li>This test ensures that the function correctly handles venv directories and other non-venv directories.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_fs_coverage.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">15 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs_coverage.py::TestIsPythonFile::test_is_python_file_false</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that a non-.py file does not match the expected criteria.</p>
                                    <p><strong>Why Needed:</strong> Prevents a false positive assertion for non-.py files, which could lead to incorrect results or unexpected behavior.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `is_python_file` should return `False` when given a non-.py file.</li>
                                            <li>The function `is_python_file` should not return `True` when given a non-.py file.</li>
                                            <li>The function `is_python_file` should handle files with extensions other than `.py` correctly.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 79)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs_coverage.py::TestIsPythonFile::test_is_python_file_true</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that a module file (.py) returns True.</p>
                                    <p><strong>Why Needed:</strong> Prevents False positives where non-python files are mistakenly considered Python files.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>is_python_file('module.py') == True</li>
                                            <li>is_python_file('path/to/module.py') == True</li>
                                            <li>is_python_file(Path('module.py')) == True</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 79)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs_coverage.py::TestMakeRelative::test_make_relative_path_not_under_base</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test makes a relative path not under the base directory.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression where make_relative fails when path is not relative to base.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return a normalized absolute path.</li>
                                            <li>The function should include the project name in the result.</li>
                                            <li>The function should include the file name in the result.</li>
                                            <li>path1 is not a subdirectory of path2.</li>
                                            <li>make_relative will fail when path1 is not relative to base.</li>
                                            <li>make_relative will return an absolute path for non-relative paths.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63, 65, 67)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs_coverage.py::TestMakeRelative::test_make_relative_success</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test makes relative path to file in test directory.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression when making a relative path to a file within the test directory.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should be able to successfully make a relative path to a file within the test directory.</li>
                                            <li>The function should not throw an error if the file does not exist in the test directory.</li>
                                            <li>The function should preserve the original file name and path.</li>
                                            <li>The function should handle cases where the parent directory of the test directory is empty.</li>
                                            <li>The function should handle cases where the test directory has been deleted or renamed.</li>
                                            <li>The function should not throw an error if the file is a subdirectory of the test directory.</li>
                                            <li>The function should preserve the original relative path when making a relative path to a file within the test directory.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 55, 58-60, 63-64)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs_coverage.py::TestMakeRelative::test_make_relative_with_none_base</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that the `make_relative` function correctly normalizes a path when the base is None.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the function returns an incorrect normalized path when the base is None.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return the original file name if the base is None.</li>
                                            <li>The function should not modify the original file name.</li>
                                            <li>The function should handle cases where the base directory does not exist.</li>
                                            <li>The function should raise an error for invalid input (e.g., non-string base)</li>
                                            <li>The function should preserve the relative path information (e.g., '..', '.').</li>
                                            <li>The function should return a normalized path with the correct file extension.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 30, 33, 36, 39, 42, 55-56)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_backslashes</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies the normalization of a path containing backslashes.</p>
                                    <p><strong>Why Needed:</strong> Prevents potential issues in file paths where backslashes are used instead of forward slashes.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The normalized path contains only forward slashes.</li>
                                            <li>Backslashes are converted to forward slashes.</li>
                                            <li>No leading or trailing backslashes are preserved.</li>
                                            <li>The resulting path is consistent with the original input.</li>
                                            <li>Backslash-escaped characters are handled correctly.</li>
                                            <li>Normalization of paths containing multiple consecutive backslashes is correct.</li>
                                            <li>Prevents issues in file paths where backslashes are used instead of forward slashes.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_path_object</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Normalizing a Path object input should return the original file path.</p>
                                    <p><strong>Why Needed:</strong> This test prevents potential issues where a custom Path object is passed to the `normalize_path` function, potentially causing unexpected behavior or incorrect results.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `normalize_path` function should return the original file path.</li>
                                            <li>The `normalize_path` function should not modify the input Path object.</li>
                                            <li>The `normalize_path` function should raise a `TypeError` if an unsupported type is passed as a Path object.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_trailing_slash</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies the normalization of a path with a trailing slash.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential issue where a directory is normalized to a file path, potentially causing unexpected behavior or errors in downstream applications.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `normalize_path` removes any trailing slashes from input paths.</li>
                                            <li>The resulting normalized path does not contain any trailing slashes.</li>
                                            <li>The function handles both Unix-style (with a slash) and Windows-style (without a slash) directory separators correctly.</li>
                                            <li>The normalization of a path with a trailing slash is deterministic, meaning the same input will always produce the same output.</li>
                                            <li>The function raises an error if the input path is empty or contains only whitespace characters.</li>
                                            <li>The function does not modify any system paths or environment variables.</li>
                                            <li>The test covers both cases where the input path starts and ends with a slash (Unix-style) and where it starts but not ends with a slash (Windows-style).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 30, 33, 36, 39, 42)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs_coverage.py::TestShouldSkipPath::test_should_not_skip_regular_path</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Testing that regular paths are not skipped.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the test might skip certain regular paths due to incorrect implementation of `should_skip_path` function.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `should_skip_path` function should return False for regular path inputs.</li>
                                            <li>The `should_skip_path` function should not throw an exception when given a regular path input.</li>
                                            <li>The test should verify that the function correctly handles different types of file paths (regular and non-regular).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_git</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies whether the `.git` directory is skipped by the `should_skip_path` function.</p>
                                    <p><strong>Why Needed:</strong> Prevents a regression where the test fails when running tests on a system with a `.git` directory.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The path to the `.git/hooks/pre-commit` file is checked and it is found in the list of paths that should be skipped.</li>
                                            <li>If the `should_skip_path` function returns False for the path to the `.git/hooks/pre-commit`, an assertion error will be raised.</li>
                                            <li>The test checks if the path to the `.git/hooks/pre-commit` file exists before asserting its value.</li>
                                            <li>If the path to the `.git/hooks/pre-commit` file does not exist, an assertion error will be raised.</li>
                                            <li>The `should_skip_path` function is called with the correct path to the `.git/hooks/pre-commit` file.</li>
                                            <li>If the `should_skip_path` function returns False for the path to the `.git/hooks/pre-commit`, a message indicating that the path should be skipped is printed to the console.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_path_starting_with_skip_dir</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies whether the `should_skip_path` function correctly identifies paths starting with a 'skip' directory name.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the function incorrectly returns False for paths that start with 'skip'.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert should_skip_path('venv') is True, # Test path 'venv' starts with 'skip' and should be skipped.</li>
                                            <li>assert should_skip_path('.venv') is True, # Test path '.venv' also starts with 'skip' and should be skipped.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_pycache</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies whether the 'src/__pycache__/module.cpython-312.pyc' path should be skipped.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where the test suite incorrectly includes or excludes certain paths based on incorrect assumptions about their contents.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert should_skip_path('src/__pycache__/module.cpython-312.pyc') is True</li>
                                            <li>assert 'src' in self.test_dir</li>
                                            <li>assert '__pycache__' in self.test_dir</li>
                                            <li>assert 'module.cpython-312' in self.test_file_list</li>
                                            <li>assert 'pycache' not in self.test_file_list</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_site_packages</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the 'site-packages' directory is skipped in the test.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where site-packages directories are incorrectly included in the test coverage.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return True for the given path.</li>
                                            <li>The function should check if the path is within the 'site-packages' directory.</li>
                                            <li>The function should raise an exception or perform some other error handling when encountering a site-package directory.</li>
                                            <li>The function should not include any files or directories from the site-packages directory in the test coverage.</li>
                                            <li>The function should handle paths that are not within the 'site-packages' directory correctly.</li>
                                            <li>The function should be able to skip site-packages directories with different names (e.g. '/usr/lib/python3.12/site-packages/my_package.py').</li>
                                            <li>The function should raise an error if it encounters a path that is not a valid Python package.</li>
                                            <li>The function should handle paths that are within the 'site-packages' directory correctly.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_venv</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies whether the 'venv' directory is skipped in the given path.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where the test fails due to incorrect handling of virtual environment directories.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert should_skip_path('venv/lib/python3.12/site.py') is True</li>
                                            <li>assert should_skip_path('.venv/lib/python3.12/site.py') is True</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-113)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_with_exclude_patterns</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that custom exclude patterns work to skip a path with an excluded file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the `should_skip_path` function incorrectly returns True for paths containing excluded files.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `should_skip_path` function should return False for the 'src/module.py' path because it contains an excluded file ('*secret*').</li>
                                            <li>The `should_skip_path` function should return True for the 'src/secret.py' path because it does not contain any excluded files.</li>
                                            <li>The `should_skip_path` function should correctly handle paths with multiple exclude patterns by returning False when no pattern matches.</li>
                                            <li>The `should_skip_path` function should raise an error if the `exclude_patterns` parameter is empty or None.</li>
                                            <li>The `should_skip_path` function should return True for a path that does not match any of the exclude patterns.</li>
                                            <li>The `should_skip_path` function should correctly handle paths with relative file names by returning False when no pattern matches.</li>
                                            <li>The `should_skip_path` function should raise an error if the `exclude_patterns` parameter is not a list or tuple.</li>
                                            <li>The `should_skip_path` function should return True for a path that contains a file name without any exclude patterns.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/fs.py</span>
                                        <span style="color: var(--text-secondary)">15 lines (ranges: 30, 33, 36, 39, 42, 100, 103, 111-112, 116-117, 119-121, 123)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_gemini_advanced.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">4 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_pruning</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that pruning of request times clears token usage records.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the rate limiter would incorrectly clear token usage records for requests made in the past.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The length of _request_times should be greater than 0 after pruning.</li>
                                            <li>The length of _token_usage should be greater than 0 after pruning.</li>
                                            <li>_request_times should not contain any timestamps from before the last request time recorded.</li>
                                            <li>_token_usage should not contain any timestamps from before the last token usage recorded.</li>
                                            <li>All tokens should still exist in the rate limiter's cache.</li>
                                            <li>The number of requests made within a minute should be less than or equal to 10 as expected by the _GeminiRateLimitConfig.</li>
                                            <li>The total token usage should be greater than or equal to 1000 as expected by the _GeminiRateLimiter.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 39-42, 81-85, 87-88)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_rpm_limit</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the rate limiter prevents requests from exceeding the specified limit in a real-time manner.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression when the rate limiter is not configured to handle requests above the specified limit.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `next_available_in` method should return a value greater than 0.</li>
                                            <li>The `next_available_in` method should return a value less than or equal to 60.0 seconds.</li>
                                            <li>The rate limiter should not be available for at least 1 second after recording the initial request.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95, 97, 100-102)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_tpm_limit</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the rate limiter prevents a regression when tokens are not used immediately.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression in the rate limiter's behavior when tokens are not used immediately, which could cause unexpected delays or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The next_available_in method should return 0 after 10 tokens have been recorded.</li>
                                            <li>The _token_usage list should contain only two elements: '20' and '10'.</li>
                                            <li>The limiter._token_usage list should not be updated until the first record is processed.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">33 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-94, 100-101, 103, 105, 107-108, 110-114)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_wait_for_slot</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `wait_for_slot` method of `_GeminiRateLimiter` waits for a slot before sleeping.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential race condition where multiple requests are made in quick succession, causing the limiter to sleep prematurely.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `wait_for_slot` method is called with the correct number of arguments (1)</li>
                                            <li>The `time.sleep` mock object is called exactly once</li>
                                            <li>The `assert` statement is executed correctly</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">31 lines (ranges: 39-42, 45-46, 48, 52-54, 58-59, 61-63, 73, 76-78, 81-82, 84, 87-88, 92-93, 95, 97, 100-102)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_gemini_coverage_v2.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">4 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_record_zero_tokens</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the rate limiter records zero tokens when no tokens are available.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the rate limiter does not record tokens for an extended period.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `_token_usage` list should be empty after calling `record_tokens(0)`.</li>
                                            <li>The length of `_token_usage` should match zero.</li>
                                            <li>No new token is added to the `_token_usage` list even if no tokens are available.</li>
                                            <li>_token_usage[0] should not exceed 100 (the default limit)</li>
                                            <li>The rate limiter does not throw an exception when `record_tokens(0)` is called with a non-zero value.</li>
                                            <li>The rate limiter correctly handles cases where the input to `record_tokens` is zero or negative.</li>
                                            <li>_token_usage[1] should be equal to 100 (the default limit) after calling `record_tokens(0)`.</li>
                                            <li>No error is thrown when `record_tokens(0)` is called with a non-zero value.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 39-42, 66-67)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_requests_per_day_exhaustion</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the rate limiter raises an error when exceeding the daily limit.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the rate limiter does not raise an error when exceeding the daily limit, potentially leading to unexpected behavior or errors in downstream systems.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `wait_for_slot` should be called with a valid slot value (10) and raise an exception `_GeminiRateLimitExceeded` with a matching message.</li>
                                            <li>The rate limiter's internal state should not have exceeded the daily limit after calling `record_request`.</li>
                                            <li>The error raised by the rate limiter should match `_GeminiRateLimitExceeded` exactly, including the exact error message and context.</li>
                                            <li>The test should fail when running with a valid slot value (10) to simulate an exhaustion scenario.</li>
                                            <li>The rate limiter's internal state should be reset to its initial state after calling `record_request` to prevent future exhaustion scenarios.</li>
                                            <li>The function `_GeminiRateLimitConfig` should create a valid rate limit configuration object with the correct parameters.</li>
                                            <li>The function `_GeminiRateLimiter` should correctly instantiate and configure the rate limiter with the provided limits.</li>
                                            <li>The test should pass when running without any errors or exceptions.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">24 lines (ranges: 32-34, 39-42, 45-46, 48-50, 58-60, 73, 76-78, 81-82, 84, 87-88)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_limiter_tpm_fallback_wait</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the TPM wait time fallback prevents a regression when filling up TPM with tokens.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the rate limiter does not properly handle filling up TPM with tokens, causing unexpected behavior in subsequent requests.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `wait` variable is greater than 0 after calling `limiter._seconds_until_tpm_available(now, 5)`.</li>
                                            <li>Tokens used plus request tokens are both above the limit before the rate limiter attempts to wait for TPM availability.</li>
                                            <li>Token usage is not empty before attempting to wait for TPM availability.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">24 lines (ranges: 39-42, 66, 68-70, 81-82, 84, 87-88, 100-101, 103, 105, 107-108, 110-114)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_gemini_coverage_v2.py::test_gemini_provider_rpm_cooldown</span>
                            <div class="test-meta">
                                <span>580ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that RPM rate limit cooldown handling is correctly implemented.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a bug where the RPM rate limit cooldown is not properly handled, leading to unexpected behavior or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'models/gemini-pro' model should be present in the provider's cooldowns.</li>
                                            <li>The cooldown for the 'models/gemini-pro' model should be greater than 1000.0 seconds (1 minute).</li>
                                            <li>The provider should not retry the request after a first rate limit exceeded error.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">23 lines (ranges: 52-53, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">117 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-215, 217-223, 225-226, 235, 237-238, 242-244, 246-247, 280-283, 286, 288-296, 298-301, 303-304, 306-307, 352, 354-356, 358-359, 387-388, 391-392)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_gemini_provider.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">5 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_rate_limit_retry</span>
                            <div class="test-meta">
                                <span>4ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the _annotate_internal method of GeminiProvider correctly handles a rate limit retry scenario.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in case the API rate limits are exceeded and the provider needs to retry the annotation process.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The mock_post call count should be equal to 2 (first time with 429, second time with 200).</li>
                                            <li>The scenario of the annotation should match 'Recovered Scenario'.</li>
                                            <li>No error is raised during the annotation process. The mock_parse.return_value indicates that no error occurred.</li>
                                            <li>The provider correctly retries the annotation after a rate limit exceeded.</li>
                                            <li>The correct model list is fetched from the API even though the first call fails with 429.</li>
                                            <li>The correct number of candidates are returned in response to successful annotations.</li>
                                            <li>No exception is raised during the parsing process. The mock_parse.return_value indicates that no error occurred.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">181 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-215, 217-222, 225-226, 235, 237-238, 242-244, 246-247, 280-283, 286-289, 292-296, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336-339, 341-347, 349, 352, 354-356, 358-361, 366-369, 380-383, 387-388, 391-393, 397-398, 402-405, 407-408, 411, 414-416, 418-420, 423, 425, 427-430, 434, 436-440, 443-446, 448-449, 451-453)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_success</span>
                            <div class="test-meta">
                                <span>5ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that _annotate_success verifies the correct model list and usage metadata when annotating success scenarios.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in cases where a successful annotation is reported without proper validation of the model list and usage metadata.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The mock response from _call_gemini contains the expected 'models' array with one item.</li>
                                            <li>The mock response from _parse_response has no error.</li>
                                            <li>The scenario assertion checks that the annotation's scenario matches the expected value.</li>
                                            <li>The annotation does not report an error.</li>
                                            <li>The usage metadata is correctly reported as 100 tokens.</li>
                                            <li>The model list is correctly fetched and validated.</li>
                                            <li>The actual call to _call_gemini returns the correct JSON structure.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">173 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-223, 225-226, 235, 280-283, 286-289, 292-296, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336, 341-349, 352, 354-356, 358-361, 366-369, 380-383, 387-388, 391-393, 397-398, 402-405, 407-408, 411, 414-416, 418-420, 423, 425, 427-430, 434, 436-440, 443-446, 448-449, 451-453)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_gemini_provider.py::TestGeminiProvider::test_availability</span>
                            <div class="test-meta">
                                <span>3ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify the availability of the Gemini API when no token is provided.</p>
                                    <p><strong>Why Needed:</strong> Prevent a potential bug where the provider attempts to access the API without a valid token, potentially causing unexpected behavior or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `provider._check_availability()` method returns False when no GEMINI_API_TOKEN environment variable is set.</li>
                                            <li>The `provider._check_availability()` method should return True when no GEMINI_API_TOKEN environment variable is set and the provider's configuration is correctly set to 'gemini'.</li>
                                            <li>When the `GEMINI_API_TOKEN` environment variable is not provided, the provider should attempt to use a default or fallback strategy.</li>
                                            <li>The provider should handle cases where the API token is missing or invalid when attempting to check availability.</li>
                                            <li>The provider's configuration should be able to detect and adjust for missing or invalid tokens in a way that maintains the expected behavior.</li>
                                            <li>In cases where multiple providers are used, the correct one should be selected based on the available environment variables.</li>
                                            <li>When using multiple providers with different configurations, the correct provider should be chosen based on the availability of specific environment variables.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 134, 136-139, 141-142, 272-273, 275)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpd_limit</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the rate limiter prevents excessive requests within a certain time frame.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the rate limiter allows too many requests in a short period, potentially causing performance issues or exceeding the limit.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The next_available_in method returns None when there are no available slots in the rate limiter.</li>
                                            <li>The limiter records a request before checking if it's within the allowed time frame.</li>
                                            <li>The limiter checks for available slots every time it records a new request.</li>
                                            <li>The limiter ensures that requests are not made too frequently within a short period.</li>
                                            <li>The limiter prevents exceeding the rate limit set in the configuration.</li>
                                            <li>The limiter maintains a record of all recorded requests to track usage and prevent abuse.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">18 lines (ranges: 39-42, 45-46, 48-50, 73, 76-78, 81-82, 84, 87-88)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpm_limit</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the rate limiter does not allow more than 2 requests per minute for a single user.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where multiple users could exceed the allowed number of requests per minute, potentially leading to unexpected behavior or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>limiter.next_available_in(100) == 0.0</li>
                                            <li>limiter.record_request()</li>
                                            <li>assert limiter.next_available_in(100) == 0.0</li>
                                            <li>limiter.record_request()</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">27 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-97, 100-102)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_hashing.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">13 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_hashing.py::TestComputeConfigHash::test_different_config</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that different configuration providers produce different hashes.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where two different configuration providers could produce the same hash, potentially leading to incorrect results or security vulnerabilities.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `compute_config_hash` should return a different hash for `config1` and `config2` when their provider is different.</li>
                                            <li>The function `compute_config_hash` should not return the same hash for `config1` and `config2` when their provider is the same.</li>
                                            <li>The function `compute_config_hash` should raise an error if both `config1` and `config2` have the same provider.</li>
                                            <li>The function `compute_config_hash` should be able to handle different providers without any issues.</li>
                                            <li>The function `compute_config_hash` should not be affected by the order of the configuration providers.</li>
                                            <li>The function `compute_config_hash` should raise an error if both `config1` and `config2` have the same provider when their hash is already known.</li>
                                            <li>The function `compute_config_hash` should return a different hash for `config1` with provider 'ollama' compared to `config2` with provider 'none'.</li>
                                            <li>The function `compute_config_hash` should be able to handle different providers without any issues, including when the same provider is used for both configurations.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/hashing.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 96-101, 103-104)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_hashing.py::TestComputeConfigHash::test_returns_short_hash</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the computed hash is of length 16.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential issue where the hash might be too long, potentially leading to incorrect comparisons or storage.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The length of the computed hash should be exactly 16 characters.</li>
                                            <li>The hash value should not exceed 15 bytes (128 bits).</li>
                                            <li>The hash value should start with a hexadecimal string starting with '0x'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/hashing.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 96-101, 103-104)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_hashing.py::TestComputeFileSha256::test_consistent_with_bytes</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the computed SHA-256 hash of a file matches its content hash.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the file hash is not consistent with the content hash due to differences in encoding or formatting.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The computed SHA-256 hash of the file should match its content hash.</li>
                                            <li>The content hash of the file should be equal to the computed SHA-256 hash of the same data.</li>
                                            <li>The file path and file contents should have a consistent byte order.</li>
                                            <li>The file contents should not contain any bytes that are not present in the original content.</li>
                                            <li>The file contents should not contain any bytes that are present but out of order.</li>
                                            <li>The computed SHA-256 hash of the file should be equal to the computed SHA-256 hash of the same data with all zeros appended at the end.</li>
                                            <li>The computed SHA-256 hash of the file should be equal to the computed SHA-256 hash of a file with identical content but different encoding or formatting.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/hashing.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 32, 44-48)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_hashing.py::TestComputeFileSha256::test_hashes_file</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify the correctness of computing a SHA-256 hash for a file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the hash computation is not accurate due to incorrect file contents or formatting.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The length of the computed hash should be exactly 64 bytes.</li>
                                            <li>The hash value should match the expected output from `compute_file_sha256()` function.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/hashing.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 44-48)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_hashing.py::TestComputeHmac::test_different_key</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/hashing.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 61)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_hashing.py::TestComputeHmac::test_with_key</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify the length of the HMAC signature.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where the HMAC signature is shorter than expected due to padding or other factors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The length of the HMAC signature should be exactly 64 bytes.</li>
                                            <li>The HMAC signature should not be shorter than 64 bytes.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/hashing.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 61)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_hashing.py::TestComputeSha256::test_consistent</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/hashing.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_hashing.py::TestComputeSha256::test_length</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify the length of the computed SHA-256 hash is 64 characters.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where the hash length may be less than 64 characters, potentially causing incorrect identification of the input data.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The length of the output hash should be exactly 64 hexadecimal characters.</li>
                                            <li>The hash value should not exceed 64 hexadecimal characters in length.</li>
                                            <li>The hash value should not be shorter than 64 hexadecimal characters in length.</li>
                                            <li>The hash value should contain all hexadecimal digits (0-9, A-F, a-f).</li>
                                            <li>No leading zeros are allowed in the output hash.</li>
                                            <li>No trailing zeros are allowed in the output hash.</li>
                                            <li>No duplicate characters are present in the output hash.</li>
                                            <li>All characters in the input data are properly encoded in hexadecimal.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/hashing.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_hashing.py::TestGetDependencySnapshot::test_includes_pytest</span>
                            <div class="test-meta">
                                <span>82ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `get_dependency_snapshot` function includes the 'pytest' package.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the dependency snapshot does not include the pytest package.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'pytest' package is included in the dependency snapshot.</li>
                                            <li>The 'pytest' package is present in the snapshot.</li>
                                            <li>The 'pytest' package is listed as an item in the snapshot.</li>
                                            <li>The 'pytest' package is part of the dependency information.</li>
                                            <li>The test includes pytest in its dependency information.</li>
                                            <li>Dependency snapshot includes pytest package.</li>
                                            <li>Snapshot includes pytest package as required.</li>
                                            <li>Test includes pytest in its dependency list.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/hashing.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 113-114, 116-121)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_hashing.py::TestGetDependencySnapshot::test_returns_dict</span>
                            <div class="test-meta">
                                <span>84ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The `get_dependency_snapshot()` function should return a dictionary containing the dependencies.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function returns an incorrect data type (e.g., list instead of dict).</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>snapshot is not None and isinstance(snapshot, dict)</li>
                                            <li>snapshot is a dictionary with keys matching the expected output</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/hashing.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 113-114, 116-121)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_loads_key</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test loads HMAC key from file.</p>
                                    <p><strong>Why Needed:</strong> Prevents a bug where the loaded key is not correctly deserialized due to incorrect encoding of the HMAC signature in the file.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'hmac.key' file contains the correct HMAC signature for the provided secret key.</li>
                                            <li>The loaded key matches the expected value from the 'hmac.key' file.</li>
                                            <li>The HMAC signature is correctly encoded in the 'hmac.key' file.</li>
                                            <li>The 'load_hmac_key' function correctly deserializes the HMAC signature from the 'hmac.key' file.</li>
                                            <li>The secret key provided to the 'Config' constructor matches the one stored in the 'hmac.key' file.</li>
                                            <li>The configuration object created with the loaded key has the correct HMAC signature.</li>
                                            <li>The HMAC signature is not present in the 'load_hmac_key' function's internal state.</li>
                                            <li>The 'load_hmac_key' function correctly handles cases where the HMAC signature is missing or corrupted.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/hashing.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 73, 76-77, 80-81)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_missing_key_file</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test verifies that the function returns None when a missing key file is provided.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly returns a non-None value for a missing key file.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `load_hmac_key` function should raise an exception or return None when the key file does not exist.</li>
                                            <li>The `Config` class should be able to detect and handle missing key files correctly.</li>
                                            <li>The test should verify that the expected error is raised with a meaningful message.</li>
                                            <li>The test should also verify that the `load_hmac_key` function returns None for a valid config object.</li>
                                            <li>The `key` variable should be set to None after calling `load_hmac_key(config).</li>
                                            <li>The `assert key is None` line should fail when `key` is not None.</li>
                                            <li>The test should also verify that the `Config` class correctly raises an exception for a missing key file.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/hashing.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 73, 76-78)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_hashing.py::TestLoadHmacKey::test_no_key_file</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `load_hmac_key` function returns `None` when no key file is specified.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an incorrect assumption about the configuration.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `Config()` object is created without specifying any key files.</li>
                                            <li>The `load_hmac_key(config)` function returns `None` when no key file is provided.</li>
                                            <li>The `assert` statement checks for `None` as the expected value, ensuring it's not raised due to an incorrect assumption.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/hashing.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 73-74)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_integration_gate.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">16 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_aggregation_defaults</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that aggregation defaults are set correctly.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where aggregation settings are not properly initialized.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>config.aggregate_dir should be None</li>
                                            <li>config.aggregate_policy should be 'latest'</li>
                                            <li>config.aggregate_include_history should be False</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 261)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_capture_failed_output_default_false</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the default capture failed output is set to False when no configuration is provided.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the default capture failed output was set to True without any configuration being provided.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>config.capture_failed_output is False</li>
                                            <li>assert config.capture_failed_output is False</li>
                                            <li>expected the value of config.capture_failed_output to be False</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 261)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_context_mode_default_minimal</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that the context mode is set to 'minimal' by default in the test configuration.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the context mode is not set to 'minimal' when running tests, potentially causing unexpected behavior or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `get_default_config()` returns an instance of `TestConfigDefaults` with `llm_context_mode` set to `'minimal'`.</li>
                                            <li>The value of `llm_context_mode` in the returned configuration is equal to 'minimal'.</li>
                                            <li>The context mode is not set to 'minimal' when running tests.</li>
                                            <li>Running tests without specifying a context mode would result in unexpected behavior or errors due to this setting.</li>
                                            <li>The test configuration does not include any overrides for `llm_context_mode`.</li>
                                            <li>Specifying the correct context mode ('minimal') during testing ensures proper execution of the test.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 261)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_llm_not_enabled_by_default</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that LLM is not enabled by default in the configuration.</p>
                                    <p><strong>Why Needed:</strong> Prevent a potential bug where LLM is enabled by default, potentially causing unexpected behavior or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `is_llm_enabled()` method returns False for the default configuration.</li>
                                            <li>The `get_default_config()` function returns a valid configuration object.</li>
                                            <li>The `config.is_llm_enabled()` assertion checks the correct value.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 123, 163, 252, 261)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_omit_tests_default_true</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `TestConfigDefaults` class correctly sets `omit_tests_from_coverage` to True when `default_omit_tests` is set to `True`.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where setting `default_omit_tests` to `True` does not automatically omit tests from coverage.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `TestConfigDefaults` class correctly sets `omit_tests_from_coverage` to True when `default_omit_tests` is set to `True`.</li>
                                            <li>The test omits all tests from coverage by default when `default_omit_tests` is `True`.</li>
                                            <li>When `default_omit_tests` is `True`, the test configuration does not include any tests in the coverage report.</li>
                                            <li>The `TestConfigDefaults` class correctly handles the case where `default_omit_tests` is set to `True` without explicitly omitting tests.</li>
                                            <li>The test covers all cases where `default_omit_tests` is `True` and ensures that tests are omitted from coverage.</li>
                                            <li>When `default_omit_tests` is `False`, the test configuration includes all tests in the coverage report.</li>
                                            <li>The `TestConfigDefaults` class correctly sets `omit_tests_from_coverage` to False when `default_omit_tests` is set to `False`.</li>
                                            <li>The test ensures that setting `default_omit_tests` to `False` does not affect the coverage report.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 261)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_provider_default_none</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Tests the default provider setting when it is set to None.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the provider is not set to 'none' in case of privacy requirements.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `config.provider` attribute is equal to 'none'.</li>
                                            <li>The `get_default_config()` function returns a configuration with a `provider` attribute equal to 'none'.</li>
                                            <li>The `assert` statement checks if the `config.provider` value matches 'none'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 261)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_integration_gate.py::TestConfigDefaults::test_secret_exclude_globs</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 261)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_deterministic_output</span>
                            <div class="test-meta">
                                <span>7ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the output of a full pipeline is deterministic and sorted by nodeid.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the output might not be deterministic or sorted correctly due to external factors like network latency or system load.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The list of nodeids in the report should be sorted in ascending order.</li>
                                            <li>Each nodeid should appear only once in the list.</li>
                                            <li>All nodeids should be present in the list.</li>
                                            <li>Nodeids without a test result (e.g., 'z_test.py::test_z') should not be included in the output.</li>
                                            <li>The presence of duplicate nodeids should be avoided.</li>
                                            <li>No empty lists or sets should be present in the output.</li>
                                            <li>All nodeids should have a corresponding test result.</li>
                                            <li>The order of nodeids should match the sorted list provided by the tests.</li>
                                            <li>Nodeid 'z_test.py::test_z' should appear first in the list.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">80 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">117 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_empty_test_suite</span>
                            <div class="test-meta">
                                <span>6ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that an empty test suite produces a valid report.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the test suite is empty, causing the report to be invalid.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The total count of tests in the report should be zero.</li>
                                            <li>The summary section of the report should have a 'total' key with a value of zero.</li>
                                            <li>The data dictionary in the report should contain a 'summary' section with a 'total' key set to zero.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">67 lines (ranges: 235-237, 239, 241, 370-386, 388-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516-518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">118 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_html_report_generation</span>
                            <div class="test-meta">
                                <span>35ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the full pipeline generates an HTML report.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the HTML report is not generated correctly.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The HTML report should be created at the specified path.</li>
                                            <li>The HTML report should contain the text content 'test_pass'.</li>
                                            <li>The HTML report should include the string '<html' in its contents.</li>
                                            <li>The HTML report should contain the string 'test_pass' in its contents.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 162)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">113 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_integration_gate.py::TestFullPipeline::test_json_report_generation</span>
                            <div class="test-meta">
                                <span>60ms</span>
                                <span title="Covered file count">üõ°Ô∏è 7</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the full pipeline generates a valid JSON report with the correct schema version, summary statistics, and number of tests.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in the integration gate where the report generation is not correctly formatted or does not contain the expected data.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'schema_version' key in the JSON report should be set to SCHEMA_VERSION.</li>
                                            <li>The 'summary' key in the JSON report should have a total of 3 tests, with 1 passed, 1 failed, and 1 skipped.</li>
                                            <li>The 'passed', 'failed', and 'skipped' keys in the summary should contain the correct numbers of tests.</li>
                                            <li>The 'schema_version' key should be present in the JSON report.</li>
                                            <li>The 'report_json' path should exist in the test directory.</li>
                                            <li>The 'report_html' path should also exist in the test directory.</li>
                                            <li>The data dictionary should contain the expected keys and values for the schema version, summary statistics, and number of tests.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/_git_info.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 2-3)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">80 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">133 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-322, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_report_root_has_required_fields</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the ReportRoot has required fields.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the report root is missing required fields, which could lead to incorrect reporting or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'schema_version' field should be present in the data.</li>
                                            <li>The 'run_meta' field should be present in the data.</li>
                                            <li>The 'summary' field should be present in the data.</li>
                                            <li>The 'tests' field should be present in the data.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">54 lines (ranges: 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_aggregation_fields</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test 'test_run_meta_has_aggregation_fields' verifies that RunMeta has aggregation fields.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the schema compatibility check fails due to missing aggregation fields.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>is_aggregated is present in data</li>
                                            <li>run_count is present in data</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">29 lines (ranges: 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_status_fields</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test 'RunMeta has run status fields' verifies that the RunMeta object contains status fields.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the RunMeta object is missing certain status fields, potentially causing incorrect interpretation of its metadata.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'exit_code' field is present in the data.</li>
                                            <li>The 'interrupted' field is present in the data.</li>
                                            <li>The 'collect_only' field is present in the data.</li>
                                            <li>The 'collected_count' field is present in the data.</li>
                                            <li>The 'selected_count' field is present in the data.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">29 lines (ranges: 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_schema_version_defined</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that the schema version is defined and matches a semver-like format.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the schema version is not correctly defined or does not match a valid semver-like format.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>SCHEMA_VERSION is defined and has a value.</li>
                                            <li>SCHEMA_VERSION contains at least one dot (.) character.</li>
                                            <li>SCHEMA_VERSION matches a semver-like format, such as '1.2.3' or 'x.x.x'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_integration_gate.py::TestSchemaCompatibility::test_test_case_has_required_fields</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `TestCaseResult` object has the required fields.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `TestCaseResult` object is missing required fields, causing inconsistent results.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>nodeid</li>
                                            <li>outcome</li>
                                            <li>duration</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">19 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_litellm_retry_coverage.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">4 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_all_retries_exhausted</span>
                            <div class="test-meta">
                                <span>2.00s</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the LiteLLMProvider class handles all retries exhausting and returns an annotation with an error when API calls are mocked to fail.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the LLMTokenRefreshRetry test case fails due to exhausted retries, causing the test to timeout or produce incorrect results.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `LiteLLMProvider` instance should be able to mock API calls to always fail with an exception.</li>
                                            <li>The `annotate` method of the provider should return a result object with an error attribute set to the mocked exception.</li>
                                            <li>The `error` attribute of the result object should not be None when the API call is mocked to fail.</li>
                                            <li>The test source function 'test_foo()' should not be executed when the annotation returns an error.</li>
                                            <li>The context files dictionary should remain empty when the annotation returns an error.</li>
                                            <li>The mock completion function raised an exception with a message 'API error' when called in the `mock_completion` patch.</li>
                                            <li>The `LiteLLMProvider` instance's `__call__` method should not be able to complete successfully when all retries are exhausted.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">37 lines (ranges: 37-38, 41, 60, 62, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 108, 110-111, 114, 129, 133, 135-136, 138-139, 164-168, 170-171, 175, 179-180, 183)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_non_401_error_no_force_refresh</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that non-401 errors don't force token refresh.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in case of non-401 error, where token refresh should not be forced.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `liteellm_provider` instance does not call the completion function with a status code other than 401.</li>
                                            <li>The `liteellm_provider` instance raises an exception instead of calling the completion function with a status code other than 401.</li>
                                            <li>The `liteellm_provider` instance returns None after mocking the API call to fail with 500 (not 401).</li>
                                            <li>The `liteellm_provider` instance does not raise any assertion errors when encountering a non-401 error.</li>
                                            <li>The `liteellm_provider` instance correctly handles the case where the completion function raises an exception instead of calling the completion function.</li>
                                            <li>The `liteellm_provider` instance correctly returns None after mocking the API call to fail with 500 (not 401).</li>
                                            <li>The `liteellm_provider` instance does not raise any assertion errors when encountering a non-401 error.</li>
                                            <li>The `liteellm_provider` instance correctly handles the case where the completion function raises an exception instead of calling the completion function.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">36 lines (ranges: 37-38, 41, 60, 62, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 108, 110-111, 114, 129, 133, 135, 138-139, 164-168, 170-171, 175, 179-180, 183)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_retry_succeeds_after_transient_error</span>
                            <div class="test-meta">
                                <span>6.00s</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that retry succeeds after transient error.</p>
                                    <p><strong>Why Needed:</strong> To ensure that the LLM token refresh retry mechanism works correctly in the presence of transient errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The test verifies that the retry process successfully completes after a transient error is encountered.</li>
                                            <li>The test ensures that the scenario is correctly set to 'test scenario' when the error occurs.</li>
                                            <li>The test verifies that the error message contains the expected content.</li>
                                            <li>The test checks for None values in the result object's error attribute.</li>
                                            <li>The test asserts that the scenario is not None.</li>
                                            <li>The test verifies that the context files are empty.</li>
                                            <li>The test checks if the mock completion function has been called exactly twice with a transient error.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">39 lines (ranges: 37-38, 41, 60, 62, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 108, 110-111, 114, 129, 133, 135-136, 164-168, 170-171, 175, 179-180, 183, 185-186, 188, 197)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_token_refresh_on_401</span>
                            <div class="test-meta">
                                <span>6.12s</span>
                                <span title="Covered file count">üõ°Ô∏è 7</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that 401 error triggers token refresh (lines 123-126).</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the token refresh fails on a 401 error.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `provider.annotate` should not be able to return None without calling the `test` method.</li>
                                            <li>The function `provider.annotate` should call the `test` method after token refresh.</li>
                                            <li>The function `provider.annotate` should not fail with an exception on a 401 error.</li>
                                            <li>The function `provider.annotate` should have at least two calls before returning None or failing with an exception.</li>
                                            <li>The function `provider.annotate` should return a valid response for the test source.</li>
                                            <li>The function `provider.annotate` should call the correct context files.</li>
                                            <li>The function `provider.annotate` should not fail due to an unhandled exception on the 401 error.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">46 lines (ranges: 37-38, 41-42, 44-48, 60-61, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 108, 110-111, 114, 129, 133, 135-136, 164-168, 170-171, 175, 179-181, 183, 185-186, 188, 197)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">28 lines (ranges: 59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_llm.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">9 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm.py::TestGetProvider::test_gemini_returns_provider</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `get_provider` function returns an instance of GeminiProvider when the 'provider' parameter is set to 'gemini'.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider is not correctly identified as 'GeminiProvider' even though it matches the expected model.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `get_provider` function returns an instance of `GeminiProvider` when the 'provider' parameter is set to 'gemini'.</li>
                                            <li>The `__class__.__name__` attribute of the returned provider instance is equal to 'GeminiProvider'.</li>
                                            <li>The provider instance has a valid model attribute that matches the expected model.</li>
                                            <li>The provider instance does not raise an exception when called with the correct configuration.</li>
                                            <li>The provider instance can be used as intended in the test code without any issues.</li>
                                            <li>The `get_provider` function is correctly implemented to handle different providers.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 52-53, 245, 247, 249, 252, 257, 262-263, 265)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm.py::TestGetProvider::test_litellm_returns_provider</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `get_provider` function returns an instance of LiteLLMProvider when the 'provider' parameter is set to 'litellm'.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider is not correctly identified as 'LiteLLMProvider'.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `get_provider` function should return an instance of LiteLLMProvider.</li>
                                            <li>The `provider` parameter should be set to 'litellm' when calling `get_provider`.</li>
                                            <li>The returned provider instance should have a class name of 'LiteLLMProvider'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">9 lines (ranges: 52-53, 245, 247, 249, 252, 257-258, 260)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 37-38, 41)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm.py::TestGetProvider::test_none_returns_noop</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `get_provider` function returns a `NoopProvider` when the `provider` is set to 'none'.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where setting the `provider` to 'none' would cause an error.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `get_provider` function should return an instance of `NoopProvider`.</li>
                                            <li>The `get_provider` function should not raise any exceptions when the `provider` is set to 'none'.</li>
                                            <li>The `get_provider` function should correctly handle the case where the `provider` is 'none' without throwing an error.</li>
                                            <li>The `get_provider` function should return a new instance of `NoopProvider` each time it is called with the same configuration.</li>
                                            <li>The `get_provider` function should not raise any exceptions when the `provider` is set to 'none'.</li>
                                            <li>The `get_provider` function should correctly handle the case where the `provider` is 'none' without raising an exception.</li>
                                            <li>The `get_provider` function should return a new instance of `NoopProvider` each time it is called with the same configuration.</li>
                                            <li>The `get_provider` function should not raise any exceptions when the `provider` is set to 'none'.</li>
                                            <li>The `get_provider` function should correctly handle the case where the `provider` is 'none' without raising an exception.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/noop.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm.py::TestGetProvider::test_ollama_returns_provider</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that OllamaProvider is returned when the 'provider' parameter is set to 'ollama'.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the correct provider type (OllamaProvider) is not detected.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert provider.__class__.__name__ == 'OllamaProvider'</li>
                                            <li>assert isinstance(provider, OllamaProvider)</li>
                                            <li>assert provider.model == 'llama3.2'</li>
                                            <li>assert provider.config.provider == 'ollama'</li>
                                            <li>assert provider.config.model == 'llama3.2'</li>
                                            <li>assert provider.config.provider == 'ollama'</li>
                                            <li>assert provider.__class__.__name__ in ['OllamaProvider', 'OllamaModelProvider']</li>
                                            <li>assert isinstance(provider, OllamaProvider)</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 245, 247, 249, 252-253, 255)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm.py::TestGetProvider::test_unknown_raises</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test case: Unknown provider raises ValueError when getting a provider.</p>
                                    <p><strong>Why Needed:</strong> This test prevents the unknown provider from being used without proper configuration.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `get_provider` should raise a `ValueError` with message 'unknown' when called with an unknown provider.</li>
                                            <li>The error message of the ValueError should contain the string 'unknown'.</li>
                                            <li>The function `get_provider` should not be able to handle unknown providers without raising an exception.</li>
                                            <li>The test should fail if the unknown provider is successfully retrieved from the provider registry.</li>
                                            <li>The function `get_provider` should raise a `ValueError` with message 'unknown' when called with an invalid or unsupported provider type.</li>
                                            <li>The error message of the ValueError should contain the string 'unknown'.</li>
                                            <li>The function `get_provider` should not be able to handle unknown providers without raising an exception.</li>
                                            <li>The test should fail if the unknown provider is successfully retrieved from the provider registry.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 245, 247, 249, 252, 257, 262, 267)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm.py::TestLlmProviderContract::test_noop_implements_interface</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Testing that NoopProvider implements the LlmProvider interface.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in case of a new method addition to LlmProvider without proper update to NoopProvider.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>provider should have annotate() method</li>
                                            <li>provider should have is_available() method</li>
                                            <li>provider should have get_model_name() method</li>
                                            <li>provider should have config attribute</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/noop.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm.py::TestNoopProvider::test_annotate_returns_empty</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the annotate method returns an empty LlmAnnotation object when no annotation is provided.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the NoopProvider does not return any annotation even when it's supposed to.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The annotation should be of type LlmAnnotation.</li>
                                            <li>The scenario attribute of the annotation should be an empty string.</li>
                                            <li>The why_needed attribute of the annotation should be an empty string.</li>
                                            <li>The key_assertions list should be empty.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/noop.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm.py::TestNoopProvider::test_get_model_name_empty</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `get_model_name` method of the `NoopProvider` class returns an empty string when given an empty configuration.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `get_model_name` method does not handle cases with empty configurations correctly, potentially leading to unexpected behavior or errors in downstream code.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert provider.get_model_name() == ""</li>
                                            <li>assert provider.config is None</li>
                                            <li>assert provider.model_name is None</li>
                                            <li>assert provider.model_type is None</li>
                                            <li>assert provider.name is None</li>
                                            <li>assert provider._config is None</li>
                                            <li>assert provider._model is None</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/noop.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 32, 66)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm.py::TestNoopProvider::test_is_available</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the NoopProvider instance is available.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential issue where the provider might not be available, potentially leading to unexpected behavior or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `is_available()` method of the `NoopProvider` instance should return True.</li>
                                            <li>The `is_available()` method of the `NoopProvider` instance should always be called before using it.</li>
                                            <li>The `is_available()` method should not raise any exceptions when called.</li>
                                            <li>The `is_available()` method should not block the execution of other methods on the same instance.</li>
                                            <li>The `is_available()` method should return True for all valid configurations.</li>
                                            <li>The `is_available()` method should return False for invalid or missing configuration parameters.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 107, 110-111)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/noop.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 32, 58)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_llm_annotator.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">6 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_emits_summary</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the annotation summary is printed when annotations run.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the annotation summary is not printed.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `get_provider` from `pytest_llm_report.llm.annotator` returns a `FakeProvider` instance.</li>
                                            <li>The `annotate_tests` function prints 'Annotated X test(s) via litellm' to stdout.</li>
                                            <li>The captured output contains the expected string 'Annotated 1 test(s) via litellm'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_reports_progress</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the progress report is generated correctly when annotating tests.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the progress report is not generated for all test cases.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The test case should start with a message indicating the number of tests being annotated.</li>
                                            <li>The test case should include the name of the test that was annotated.</li>
                                            <li>The progress messages should be appended to the list in the correct order.</li>
                                            <li>The progress messages should not contain any extra information (e.g. provider names).</li>
                                            <li>The progress messages should only contain relevant information about the annotation process (e.g. test ID, annotation status).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_respects_opt_out_and_limit</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that LLM annotations respect opt-out and limit settings.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression by ensuring LLM annotations do not skip opt-out tests or exceed the maximum number of tests.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'tests/test_a.py::test_a' node should be called when running LLM annotations with opt-out disabled.</li>
                                            <li>The first test case should have an annotation without LLM optimization.</li>
                                            <li>The second test case should not have an annotation.</li>
                                            <li>The third test case should not have an annotation.</li>
                                            <li>The number of tests called should not exceed the maximum allowed by the configuration.</li>
                                            <li>The 'tests/test_b.py::test_b' node should be called when running LLM annotations with opt-out disabled and a limit set to 1.</li>
                                            <li>The second test case should not have an annotation due to the limit being reached.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 45, 48-49, 56-57, 59, 61-62, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_respects_rate_limit</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the LLM annotator respects the requests-per-minute rate limit.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the annotator exceeds the allowed requests per minute.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>provider.calls should contain all the node IDs of the tests passed by the annotator.</li>
                                            <li>sleep_calls should be equal to [2.0] for each test, indicating that the annotator slept for exactly 2 minutes between calls to sleep.</li>
                                            <li>the time.sleep function was called only once with a value of 2.0</li>
                                            <li>provider.calls contains all node IDs of tests passed by the annotator.</li>
                                            <li>sleep_calls is equal to [2.0] for each test, indicating that the annotator slept for exactly 2 minutes between calls to sleep.</li>
                                            <li>the time.sleep function was called only once with a value of 2.0</li>
                                            <li>provider.calls contains all node IDs of tests passed by the annotator.</li>
                                            <li>the time.sleep function was called only once with a value of 2.0</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">68 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-173, 176, 178, 180-183, 185-190, 192, 198, 203)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_skips_unavailable_provider</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that annotation with unavailable providers skips the test.</p>
                                    <p><strong>Why Needed:</strong> To prevent regression when an unavailable provider is used for testing.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `is_available` method of the `UnavailableProvider` class returns False.</li>
                                            <li>The `get_provider` function from `pytest_llm_report.llm.annotator` sets the `provider` attribute to the `UnavailableProvider` instance.</li>
                                            <li>The `annotate_tests` function is called with an empty list of tests and the `UnavailableProvider` instance.</li>
                                            <li>The `is not available` message is captured in the `captured.out` variable.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 45, 48-52, 54)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_annotator.py::test_annotate_tests_uses_cache</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">30 lines (ranges: 39-41, 53, 55-56, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-81, 87-92, 97-98, 100, 102, 104, 115-122, 127, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-84)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_llm_contract.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">13 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_required_fields</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `test_required_fields` function checks for both 'scenario' and 'why_needed' fields.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the schema requires these two critical fields, which are essential for validating the functionality of the `TestAnnotationSchema`.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert 'scenario' in required</li>
                                            <li>assert 'why_needed' in required</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_from_dict</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that AnnotationSchema.from_dict() correctly parses a dictionary with required keys.</p>
                                    <p><strong>Why Needed:</strong> Prevents data tampering and ensures consistent schema structure.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>checks password</li>
                                            <li>checks username</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_empty</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_partial</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the AnnotationSchema correctly handles partial input without any additional validation.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the AnnotationSchema does not validate for partial inputs.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The schema is correctly initialized with the provided scenario.</li>
                                            <li>The expected why_needed value is empty, indicating no additional validation required.</li>
                                            <li>The assertion checks that the schema's scenario attribute matches the provided input.</li>
                                            <li>The assertion checks that the schema's why_needed attribute remains empty after initialization.</li>
                                            <li>Additional validation for partial inputs would require a separate annotation or validation step.</li>
                                            <li>This test ensures the AnnotationSchema is correctly handling partial inputs without any additional validation.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_has_required_fields</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the annotation schema has required fields.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the annotation schema is not properly defined with required fields, potentially leading to errors or inconsistencies.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert 'scenario' in ANNOTATION_JSON_SCHEMA['properties']</li>
                                            <li>assert 'why_needed' in ANNOTATION_JSON_SCHEMA['properties']</li>
                                            <li>assert 'key_assertions' in ANNOTATION_JSON_SCHEMA['properties']</li>
                                            <li>assert isinstance(ANNOTATION_JSON_SCHEMA, dict)</li>
                                            <li>assert len(ANNOTATION_JSON_SCHEMA) > 0</li>
                                            <li>assert all(key in ANNOTATION_JSON_SCHEMA for key in ['scenario', 'why_needed', 'key_assertions'])</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_contract.py::TestAnnotationSchema::test_schema_to_dict</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `AnnotationSchema` class correctly serializes to a dictionary.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the `AnnotationSchema` class handles scenarios and key assertions correctly.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assertion 1</li>
                                            <li>assertion 2</li>
                                            <li>assertion 3</li>
                                            <li>assertion 4</li>
                                            <li>assertion 5</li>
                                            <li>assertion 6</li>
                                            <li>assertion 7</li>
                                            <li>assertion 8</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 90-92, 94-96, 98)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_from_factory</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The factory function `get_provider` should return a NoopProvider instance when the provider is set to 'none'.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the factory function returns an incorrect provider type for the 'none' provider.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The returned provider should be of type `NoopProvider`.</li>
                                            <li>The provider instance should have no attributes or methods.</li>
                                            <li>The provider instance should not have any dependencies.</li>
                                            <li>The provider instance should not call any external functions.</li>
                                            <li>The provider instance should not have any state.</li>
                                            <li>The provider instance should not perform any actions.</li>
                                            <li>The provider instance should be a singleton.</li>
                                            <li>The factory function `get_provider` should return the correct NoopProvider instance for the 'none' provider.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 52-53, 245, 247, 249-250)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/noop.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_is_llm_provider</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The `NoopProvider` class is correctly instantiated as an instance of `LlmProvider`.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `NoopProvider` class might be incorrectly identified as an `LLMProvider` due to its lack of actual LLM functionality.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `provider` variable is assigned an instance of `LlmProvider` using `assert isinstance(provider, LlmProvider)`.</li>
                                            <li>The `provider` variable is assigned a new instance of `NoopProvider` created with the same configuration as `config` using `provider = NoopProvider(config)`.</li>
                                            <li>The `provider` variable's type is checked to be an instance of `LlmProvider` using `isinstance(provider, LlmProvider)`.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/noop.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_contract.py::TestNoopProvider::test_noop_returns_empty_annotation</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The NoopProvider should return an empty annotation when no node is found in the contract.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the NoopProvider returns an incorrect or incomplete annotation when no node is present in the contract.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The annotation returned by the NoopProvider is empty.</li>
                                            <li>The annotation returned by the NoopProvider does not contain any relevant information about the contract.</li>
                                            <li>The annotation returned by the NoopProvider does not match the expected output of the contract's annotate method.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/noop.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_annotate_returns_annotation</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the annotate method of the provider returns an LlmAnnotation-like object with the specified scenario, why needed, and key assertions.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the annotation result is missing certain critical checks.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'scenario' attribute of the annotated result should be set to the provided scenario.</li>
                                            <li>The 'why_needed' attribute of the annotated result should contain information about what bug or regression this test prevents.</li>
                                            <li>The 'key_assertions' attribute of the annotated result should include critical checks performed by the test.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/noop.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_handles_empty_code</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the provider handles an empty code by returning a valid result.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where an empty code might cause the contract to fail or return incorrect results.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The annotate method should not be called with an empty string as its first argument.</li>
                                            <li>The annotate method should call the provider's `annotate` method with a valid configuration and a non-empty test result.</li>
                                            <li>The annotate method should return a valid TestCaseResult object.</li>
                                            <li>The annotate method should not raise any exceptions when given an empty code.</li>
                                            <li>The annotate method should handle cases where the test has no outcome specified.</li>
                                            <li>The annotate method should update the provider's internal state correctly after calling it.</li>
                                            <li>The annotate method should not modify the test result in a way that would affect subsequent tests.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/noop.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_handles_none_context</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/noop.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 32, 50)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_contract.py::TestProviderContract::test_provider_has_annotate_method</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 7</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that all providers have an annotate method.</p>
                                    <p><strong>Why Needed:</strong> Prevents a bug where some providers do not have the annotate method.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The provider has an attribute named 'annotate'.</li>
                                            <li>The provider's annotate method is callable.</li>
                                            <li>All providers should have this annotation.</li>
                                            <li>If a provider does not have this annotation, it should be considered as a bug.</li>
                                            <li>The provider should raise an exception if the annotate method is missing.</li>
                                            <li>If a provider has the annotate method but its implementation is incorrect, it should still be considered as a bug.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">15 lines (ranges: 52-53, 245, 247, 249-250, 252-253, 255, 257-258, 260, 262-263, 265)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 134, 136-139, 141-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 37-38, 41)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/noop.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 32)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_llm_providers.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">35 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_handles_context_too_large</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the annotate function handles large contexts correctly.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where annotate might not work properly with very large contexts.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The annotate function should be able to handle context sizes up to a certain threshold without throwing an error or returning unexpected results.</li>
                                            <li>The annotate function should be able to handle context sizes above the threshold by wrapping them in a default annotation.</li>
                                            <li>The annotate function should not throw an exception when given very large contexts, but instead return a default annotation.</li>
                                            <li>The annotate function should return a default annotation for context sizes greater than the threshold, rather than throwing an error or returning unexpected results.</li>
                                            <li>The annotate function should be able to handle context sizes that are multiples of the threshold without wrapping them in a default annotation.</li>
                                            <li>The annotate function should not return any errors when given very large contexts, but instead provide a meaningful default annotation.</li>
                                            <li>The annotate function should be able to handle very large contexts by using a caching mechanism or other optimization techniques.</li>
                                            <li>The annotate function should be able to handle very large contexts without significant performance impact on subsequent tests.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">153 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-221, 237, 249-250, 280-283, 286-289, 292-296, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 352, 354-356, 358-361, 366-369, 380-383, 387-388, 391-393, 397-398, 402-405, 407-408, 411, 414-416, 418-420, 423-424, 434, 436-440, 443-446, 448-449, 451-453)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_dependency</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The LiteLLMProvider should report a missing dependency when the 'litellm' package is required but not installed.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider incorrectly reports a non-existent dependency, potentially leading to incorrect or misleading error messages.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>the annotation will contain an error message indicating that the 'litellm' package is missing and how to install it.</li>
                                            <li>the annotation will report the correct path to install 'litellm'.</li>
                                            <li>the annotation will not report a non-existent dependency, ensuring accurate error messages.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 134, 136-139, 141-142, 160-164)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_token</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `GeminiProvider` requires an API token and provides a meaningful error message when it's missing.</p>
                                    <p><strong>Why Needed:</strong> The current implementation does not prevent a potential bug where the `GeminiProvider` might be used with a non-existent API token, resulting in an unhandled exception or incorrect behavior.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `error` attribute of the annotation is set to 'GEMINI_API_TOKEN is not set'.</li>
                                            <li>The `annotation.error` attribute contains the expected error message.</li>
                                            <li>The `provider.annotate()` method correctly raises a `ValueError` when an API token is missing.</li>
                                            <li>The `test_case` function in the test case passes without raising any exceptions.</li>
                                            <li>The `CaseResult` object passed to the `annotate()` method indicates that the test passed successfully.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 134, 136-139, 141-142, 160-161, 167-169)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_records_tokens</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that tokens are recorded on the limiter correctly.</p>
                                    <p><strong>Why Needed:</strong> Prevents regressions where token usage is not recorded.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'json' key in captured contains the expected response data.</li>
                                            <li>The 'totalTokenCount' value in the 'usageMetadata' dictionary matches the expected count.</li>
                                            <li>At least one entry exists in the '_token_usage' list of the limiter.</li>
                                            <li>The first item in the '_token_usage' list has a 'value' equal to 123.</li>
                                            <li>The 'json' key in the captured dictionary does not contain any errors.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">183 lines (ranges: 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-223, 225-226, 235, 280-283, 286-289, 292-296, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336, 341-349, 352, 354-356, 358-361, 366-372, 374, 376-377, 380-383, 387-388, 391-393, 397-398, 402-405, 407-408, 411, 414-416, 418-420, 423, 425, 427-430, 434, 436-440, 443-446, 448-449, 451-453)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_retries_on_rate_limit</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the LLM provider annotates retries on rate limits.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the LLM provider does not retry after hitting a rate limit.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The method `annotate_retries_on_rate_limit` should be called with an additional argument to specify the rate limit.</li>
                                            <li>The method `annotate_retries_on_rate_limit` should return a new annotation instance with the updated rate limit.</li>
                                            <li>The method `annotate_retries_on_rate_limit` should update the LLM provider's retry count.</li>
                                            <li>The method `annotate_retries_on_rate_limit` should not raise an exception when rate limit is exceeded.</li>
                                            <li>The method `annotate_retries_on_rate_limit` should return a new annotation instance with the updated rate limit after retrying.</li>
                                            <li>The method `annotate_retries_on_rate_limit` should update the LLM provider's retry count after retrying.</li>
                                            <li>The method `annotate_retries_on_rate_limit` should not raise an exception when rate limit is exceeded after retrying.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">181 lines (ranges: 32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-226, 235, 237-238, 242-244, 246-247, 280-283, 286-289, 292, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336-339, 341-347, 349, 352, 354-356, 358-361, 366-372, 374-375, 380-383, 387-388, 391-393, 397-398, 402-405, 407-408, 411, 414-416, 418-420, 423, 425, 427-430, 434, 436-440, 443-446, 448-449, 451-453)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_rotates_models_on_daily_limit</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `annotate` method of the `GeminiProvider` class rotates models on the daily limit.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the model rotation is not applied correctly due to an incorrect implementation of the `annotate` method.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `annotate` method should rotate models on the daily limit by updating the `model_rotation` attribute.</li>
                                            <li>The `rotate_models_on_daily_limit` method should be called with the correct arguments (e.g., `daily_limit`) to update the model rotation.</li>
                                            <li>The `update_model_rotation` method should be called with the correct value (e.g., a new daily limit) to rotate models on the daily limit.</li>
                                            <li>The `rotate_models_on_daily_limit` method should not throw an exception if the daily limit is exceeded.</li>
                                            <li>The `annotate` method should update the model rotation attribute correctly after calling `rotate_models_on_daily_limit`.</li>
                                            <li>The `model_rotation` attribute should be updated with a new value after calling `rotate_models_on_daily_limit`.</li>
                                            <li>The test should pass without any errors or exceptions when running the test.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">177 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-208, 210, 213-214, 217-222, 225-226, 235, 280-283, 286-289, 292-296, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336, 341-347, 349, 352, 354-356, 358-361, 366-372, 374, 376, 378-383, 387-388, 391-393, 397-399, 402-405, 407-408, 411, 414-416, 418-420, 423, 425-426, 434, 436-440, 443-446, 448-449, 451-453)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_skips_on_daily_limit</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that annotating a model with `GeminiProvider` skips daily limits.</p>
                                    <p><strong>Why Needed:</strong> This test prevents the model from being annotated when it exceeds daily limits, ensuring data quality and compliance.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `GeminiProvider` is not called with an annotation key when the daily limit is exceeded.</li>
                                            <li>The `GeminiProvider` does not raise an exception when the daily limit is exceeded.</li>
                                            <li>The model's metadata does not contain a 'skipped' annotation when it exceeds daily limits.</li>
                                            <li>The model's annotations do not exceed the daily limit when it is annotated correctly.</li>
                                            <li>The `GeminiProvider` does not log any errors or warnings when it skips annotating the model due to daily limits.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">184 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-208, 210-211, 213-214, 217-222, 225-226, 235, 258-260, 280-283, 286-289, 292-296, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336, 341-347, 349, 352, 354-356, 358-361, 366-372, 374, 376, 378-383, 387-388, 391-393, 397-399, 402-405, 407-408, 411, 414-416, 418-420, 423, 425, 427-430, 434, 436-440, 443-446, 448-449, 451-453)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_annotate_success_with_mock_response</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the annotate method correctly annotates a successful response from LiteLLM.</p>
                                    <p><strong>Why Needed:</strong> Prevents regressions by ensuring the annotation is correct for successful responses.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>status ok</li>
                                            <li>redirect</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">177 lines (ranges: 39-42, 45-46, 48-49, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-226, 235, 280-283, 286-289, 292, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336, 341-347, 349, 352, 354-356, 358-361, 366-372, 374-383, 387-388, 391-393, 397-398, 402-405, 407-408, 411, 414-416, 418-420, 423, 425, 427-430, 434, 436-440, 443-446, 448-449, 451-453)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_exhausted_model_recovers_after_24h</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the exhausted model recovers after 24 hours.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the model does not recover from exhaustion.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The recovered model should have the same accuracy as before exhaustion.</li>
                                            <li>The recovered model should have the same number of parameters as before exhaustion.</li>
                                            <li>The recovered model's loss function should be close to its original value after 24 hours.</li>
                                            <li>The recovered model's training time should decrease by a factor of 2.5 after 24 hours.</li>
                                            <li>The recovered model's memory usage should decrease by a factor of 1.25 after 24 hours.</li>
                                            <li>The recovered model's GPU memory usage should decrease by a factor of 0.75 after 24 hours.</li>
                                            <li>The recovered model's CPU memory usage should decrease by a factor of 2.5 after 24 hours.</li>
                                            <li>The recovered model's memory usage percentage should be close to 100% after 24 hours.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">190 lines (ranges: 39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-188, 190-191, 193-194, 196, 200-208, 210-211, 213-214, 217-222, 225-226, 235, 258-260, 280-283, 286-289, 292-296, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336, 341-347, 349, 352, 354-356, 358-361, 366-372, 374, 376, 378-383, 387-388, 391-393, 397-399, 402-405, 407-408, 411, 414-416, 418-420, 423, 425, 427-430, 434, 436-440, 443-446, 448-449, 451-453)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_fetch_available_models_error</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The `fetch_available_models` method of the `GeminiProvider` class returns an error when there are no available models.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the `fetch_available_models` method returns an error when it should not.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert 'No available models found.' in str(getattr(self, 'provider', None).fetch_available_models())</li>
                                            <li>assert 'No available models found.' in str(getattr(self, 'provider', self._providers).fetch_available_models())</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 134, 136-139, 141-142, 286, 288-289, 292-296, 298-301, 303-304, 306-307, 352, 354-356, 358-361, 366-369, 380-383, 391, 393, 397-398, 402-408, 411, 414-416, 418-420, 423-424, 434, 436-438, 441-442)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestGeminiProvider::test_model_list_refreshes_after_interval</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The model list should refresh after a specified interval.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the model list does not update after an interval.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `refresh_interval` attribute of the provider is set to the expected value.</li>
                                            <li>The `model_list` attribute of the provider is updated with the latest data after the specified interval.</li>
                                            <li>The provider's state is consistent with the expected behavior after the interval has passed.</li>
                                            <li>No exceptions are raised when refreshing the model list.</li>
                                            <li>The refresh interval is correctly set to a positive value.</li>
                                            <li>The `refresh_interval` attribute is not overridden by any other provider.</li>
                                            <li>The `model_list` attribute is updated with the latest data after the specified interval, even if it's empty.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/gemini.py</span>
                                        <span style="color: var(--text-secondary)">169 lines (ranges: 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-226, 235, 280-283, 286-289, 292, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336, 341-347, 349, 352, 354-356, 358-361, 366-372, 374-375, 380-383, 387-388, 391-393, 397-399, 402-405, 407-408, 411, 414-416, 418-420, 423, 425, 427-430, 434, 436-440, 443-446, 448-449, 451-453)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_401_retry_with_token_refresh</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 7</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> test_401_retry_with_token_refresh verifies that the LiteLLM provider retries on 401 after refreshing token.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the provider fails to retry after token refresh, potentially causing unexpected behavior or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Verify that the provider calls `fake_completion` with the correct API key when it first encounters a 401 Unauthorized error.</li>
                                            <li>Verify that the provider calls `fake_run` with the correct token count and returns a successful status code after refreshing the token.</li>
                                            <li>Verify that the provider captures the refreshed token in the `captured_keys` list.</li>
                                            <li>Verify that the provider does not call `fake_completion` again when it encounters another 401 Unauthorized error.</li>
                                            <li>Verify that the provider correctly updates the `litellm_token_refresh_command` and `litellm_token_refresh_interval` attributes of the `Config` object.</li>
                                            <li>Verify that the provider creates a new `CaseResult` node with an outcome of 'passed' after passing the test case.</li>
                                            <li>Verify that the `annotation.error` attribute is set to None after the test passes.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">47 lines (ranges: 37-38, 41-42, 44-48, 60-61, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 108, 110-111, 114, 116, 118-121, 164-168, 170-171, 175, 179-181, 183, 185-186, 188, 197)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">28 lines (ranges: 59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156, 160-162)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_handles_completion_error</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the annotate method of LiteLLMProvider returns an error message when a completion error occurs.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the LLM provider does not surface completion errors correctly.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The annotation contains the string 'boom' which is the expected error message for a completion error.</li>
                                            <li>The annotation has an 'error' key with the value 'boom'.</li>
                                            <li>The annotation has an 'completion' key with the value 'boom'.</li>
                                            <li>The annotation does not contain any other error messages or information that could indicate a completion error occurred.</li>
                                            <li>The annotation is present in the test case where it is expected to occur.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">32 lines (ranges: 37-38, 41, 60, 62, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 108, 110, 114, 129, 131, 164-168, 170-171, 175, 179-180, 183)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_invalid_key_assertions</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that LiteLLMProvider rejects invalid key_assertions payloads.</p>
                                    <p><strong>Why Needed:</strong> To prevent bugs where the provider accepts non-list key_assertions payloads, which could lead to unexpected behavior or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>response_data must be a list</li>
                                            <li>response_data must contain at least one item with 'key_assertion' key</li>
                                            <li>response_data must not contain any items that are dictionaries themselves (i.e. not lists)</li>
                                            <li>response_data must not contain any items that are strings or other types of values</li>
                                            <li>response_data must be a valid JSON string</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">22 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207-209)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">32 lines (ranges: 37-38, 41, 60, 62, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 164-168, 170-171, 175, 179-180, 183, 185-186, 188, 190, 195)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_missing_dependency</span>
                            <div class="test-meta">
                                <span>3ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The LiteLLMProvider should report an error when a missing dependency is encountered.</p>
                                    <p><strong>Why Needed:</strong> To prevent the test from passing and potentially introducing a bug or regression by incorrectly reporting a missing dependency.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>annotation.error == 'litellm not installed. Install with: pip install litellm'</li>
                                            <li>provider.annotate(test, 'def test_case(): assert True')</li>
                                            <li>test.test_case() should raise an error when the LiteLLMProvider is used with a missing dependency</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 37-38, 41, 80-84)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_success_with_mock_response</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the annotate method of LiteLLMProvider correctly annotates a successful response with mock data.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regressions by ensuring that the annotation method returns an LlmAnnotation object with correct scenario, why_needed, and key_assertions values.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>annotation is an instance of LlmAnnotation</li>
                                            <li>annotation.scenario matches 'Checks login'</li>
                                            <li>annotation.why_needed matches 'Stops regressions'</li>
                                            <li>annotation.key_assertions matches ['status ok', 'redirect']</li>
                                            <li>confidence of annotation is 0.8 or higher</li>
                                            <li>captured.model matches 'gpt-4o'</li>
                                            <li>captured.messages contains 'system' role and 'def test_login()' in content</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">31 lines (ranges: 37-38, 41, 60, 62, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 164-168, 170-171, 175, 179-180, 183, 185-186, 188, 197)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_api_base_passthrough</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">32 lines (ranges: 37-38, 41, 60, 62, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 164-168, 170-171, 175-176, 179-180, 183, 185-186, 188, 197)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_api_key_passthrough</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the LiteLLM provider passes a static API key to the completion call.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in cases where the API key is not properly passed through to the completion function.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Verify that the captured API key matches the expected value (`static-key-placeholder`).</li>
                                            <li>Verify that the `litellm_api_key` attribute of the `Config` object matches the expected value (`static-key-placeholder`).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">32 lines (ranges: 37-38, 41, 60, 62, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 164-168, 170-171, 175, 179-181, 183, 185-186, 188, 197)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_is_available_with_module</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the LiteLLM provider detects installed modules correctly.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the provider does not detect installed modules due to incorrect module imports.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `is_available()` method should return True when the 'litellm' module is available in the system's modules.</li>
                                            <li>The `is_available()` method should raise an exception if the 'litellm' module is not available in the system's modules.</li>
                                            <li>The provider should be able to detect the presence of the 'litellm' module even if it is imported from a different package.</li>
                                            <li>The provider should handle cases where the 'litellm' module is not found in the system's modules, but still report that it is available.</li>
                                            <li>The provider should raise an exception when the 'litellm' module is not found in the system's modules and cannot be imported.</li>
                                            <li>The provider should correctly handle cases where the 'litellm' module is installed as a package rather than a standalone module.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 107, 110-111)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 37-38, 41, 205-206, 208)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestLiteLLMProvider::test_token_refresh_integration</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 7</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the LiteLLM provider's token refresh integration.</p>
                                    <p><strong>Why Needed:</strong> To prevent a bug where the provider does not refresh tokens in case of an authentication error, causing the API key to remain invalid.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'api_key' field in the test_case output should be set to 'dynamic-token-789'.</li>
                                            <li>The 'litellm_token_refresh_command' field in the config should be set to 'get-token'.</li>
                                            <li>The 'litellm_token_refresh_interval' field in the config should be set to 3600 (1 hour).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">38 lines (ranges: 37-38, 41-42, 44-48, 60-61, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 164-168, 170-171, 175, 179-181, 183, 185-186, 188, 197)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_fallbacks_on_context_length_error</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Tests the functionality of annotating fallback providers when context length error occurs.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in LLM providers when encountering a context length error.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Verify that the `annotate_fallbacks_on_context_length_error` method is called with the correct arguments.</li>
                                            <li>Check if the `context_length_error` exception is raised correctly.</li>
                                            <li>Verify that the fallback provider is annotated correctly with the provided arguments.</li>
                                            <li>Ensure that the annotation message contains the expected error context.</li>
                                            <li>Test that the fallback provider's behavior changes as expected when a context length error occurs.</li>
                                            <li>Verify that the LLM model's output remains unchanged despite the context length error.</li>
                                            <li>Check if any exceptions are raised during the execution of the annotated code.</li>
                                            <li>Ensure that the `annotate_fallbacks_on_context_length_error` method is called with the correct number of arguments.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">33 lines (ranges: 52-53, 72, 75-76, 78, 165, 167-173, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/ollama.py</span>
                                        <span style="color: var(--text-secondary)">15 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-60, 62, 64-65, 71)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_handles_call_error</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test OllamaProvider::test_annotate_handles_call_error verifies that the annotate method handles call errors and returns a meaningful error message.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the annotation fails to handle call errors, potentially leading to false positives or incorrect results.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The annotation should return an error message indicating that the call failed after 2 retries.</li>
                                            <li>The error message should include the last error that occurred during the call.</li>
                                            <li>The error message should be more informative than a generic 'Failed' message.</li>
                                            <li>The test case should pass even if the call is successful (i.e., no errors occur).</li>
                                            <li>The annotation should not return an error when the system prompt is different from the user prompt.</li>
                                            <li>The annotation should handle retries correctly, returning an error after 2 retries and a more detailed error message for subsequent retries.</li>
                                            <li>The test case should be able to reproduce the error consistently across different environments.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/ollama.py</span>
                                        <span style="color: var(--text-secondary)">17 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-59, 73, 76-77, 79-80, 82-83)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_missing_httpx</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The Ollama provider reports missing httpx dependency when annotating a test case.</p>
                                    <p><strong>Why Needed:</strong> This test prevents the Ollama provider from reporting an error for missing httpx, which could lead to incorrect or misleading results in downstream analysis.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert annotation.error == 'httpx not installed. Install with: pip install httpx'</li>
                                            <li>provider.annotate(test, 'def test_case(): assert True')</li>
                                            <li>test_case()</li>
                                            <li>assert test_case().__name__ == 'test_case'</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 52-53, 72, 75, 80)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/ollama.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 40-44)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_annotate_success_full_flow</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test annotating full flow of Ollama provider with mocked HTTP response.</p>
                                    <p><strong>Why Needed:</strong> Prevents authentication bugs by verifying the full annotation process.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Check if the status code is OK (200)</li>
                                            <li>Validate the token in the response</li>
                                            <li>Verify that the scenario matches the test case</li>
                                            <li>Ensure the why_needed message is displayed correctly</li>
                                            <li>Confirm that the key assertions are performed as expected</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/ollama.py</span>
                                        <span style="color: var(--text-secondary)">29 lines (ranges: 40-41, 47, 50, 52, 54-55, 57-60, 62, 71, 119, 121-128, 132-135, 137, 139-140)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_success</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Ollama provider makes correct API call to generate response.</p>
                                    <p><strong>Why Needed:</strong> The test prevents a regression where the Ollama provider fails to make an API call when the provided model and prompt are invalid.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `url` attribute of the captured dictionary is set to `http://localhost:11434/api/generate`.</li>
                                            <li>The `json` attribute of the captured dictionary contains a valid response with keys `model`, `prompt`, `system`, and `stream`.</li>
                                            <li>The `timeout` attribute of the captured dictionary is set to 60 seconds.</li>
                                            <li>The `url` attribute in the captured dictionary matches the expected URL for the Ollama provider's API call.</li>
                                            <li>The `json` attribute in the captured dictionary contains a valid response with the correct model and prompt values.</li>
                                            <li>The `timeout` attribute in the captured dictionary is set to 60 seconds, which is within the expected timeout range.</li>
                                            <li>The `url` attribute in the captured dictionary does not contain any invalid characters or special sequences.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/ollama.py</span>
                                        <span style="color: var(--text-secondary)">16 lines (ranges: 119, 121-128, 132-135, 137, 139-140)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_uses_default_model</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Ollama provider uses default model when not specified.</p>
                                    <p><strong>Why Needed:</strong> Prevents bug where OllamaProvider is used with an empty model, causing the default model to be used instead.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'model' key in the captured response should be equal to 'llama3.2'.</li>
                                            <li>The 'model' key in the captured response should not be empty.</li>
                                            <li>The 'response' key in the captured response should contain a JSON object with a single property named 'model'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/ollama.py</span>
                                        <span style="color: var(--text-secondary)">16 lines (ranges: 119, 121-128, 132-135, 137, 139-140)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_failure</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the Ollama provider returns False when the server is unavailable.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a bug where the provider incorrectly assumes the server is available even if it's not.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function _check_availability() of the OllamaProvider instance does not raise an exception or return a specific value when the server is unavailable.</li>
                                            <li>The function _check_availability() of the OllamaProvider instance returns False when the server is unavailable.</li>
                                            <li>The provider's configuration is updated to point to a fake HTTPX instance that raises a ConnectionError when trying to connect to the server.</li>
                                            <li>The provider's internal state is not updated to reflect the server being unavailable after calling _check_availability()</li>
                                            <li>The provider does not raise an exception or return a specific value when the server is unavailable, allowing it to continue functioning as expected</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/ollama.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 92-93, 95-96, 98-99)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_non_200</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The Ollama provider returns False for non-200 status codes.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider incorrectly reports availability when it should not.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert provider._check_availability() is False</li>
                                            <li>assert config.provider == 'ollama'</li>
                                            <li>assert FakeResponse().status_code != 200</li>
                                            <li>assert fake_get('https://example.com', **kwargs).status_code == 500</li>
                                            <li>assert not isinstance(provider._check_availability(), bool)</li>
                                            <li>assert provider._check_availability() is False in mock context</li>
                                            <li>assert config._mocked_httpx.get.return_value.status_code != 200</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/ollama.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 92-93, 95-97)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_success</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the Ollama provider checks availability via /api/tags endpoint successfully.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the provider fails to check availability when it's not available.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The '/api/tags' URL is present in the provided URL.</li>
                                            <li>The response status code is 200 (OK).</li>
                                            <li>The provider returns True for the _check_availability method.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/ollama.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 92-93, 95-97)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_is_local_returns_true</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The Ollama provider should always return `is_local=True` when the `provider` parameter is set to 'ollama'.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the provider returns `False` when it's supposed to be `True` due to an incorrect implementation.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `provider` attribute of the `OllamaProvider` instance should be set to `'ollama'`.</li>
                                            <li>The `is_local()` method of the `OllamaProvider` instance should return `True`.</li>
                                            <li>The `is_local()` method should not throw an exception or raise an error when called with a valid `provider` parameter.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 52-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/ollama.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 107)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_json</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `OllamaProvider` class's `_parse_response` method returns an error message when it encounters invalid JSON in the response.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the Ollama provider incorrectly reports valid JSON responses as failed parsing attempts.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `_parse_response` method of `OllamaProvider` should raise an exception with the message 'Failed to parse LLM response as JSON' when given invalid JSON.</li>
                                            <li>The error message returned by the `_parse_response` method should include the string 'Failed to parse LLM response as JSON'.</li>
                                            <li>The test should fail if the provided `annotation.error` is not equal to 'Failed to parse LLM response as JSON'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 52-53, 186-187, 190-192)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-52, 55)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_key_assertions</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The Ollama provider rejects invalid key_assertions payloads.</p>
                                    <p><strong>Why Needed:</strong> This test prevents the provider from silently failing when receiving an invalid 'key_assertions' payload in its responses.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>the 'key_assertions' field must be a list</li>
                                            <li>the 'key_assertions' field should contain valid assertions</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">16 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207-209)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The provided test verifies that the Ollama provider correctly parses a JSON response from a markdown code fence.</p>
                                    <p><strong>Why Needed:</strong> This test prevents potential bugs where the provider may incorrectly or incompletely parse the JSON response, leading to incorrect or missing information.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The JSON object is present and has the expected structure (i.e., it contains 'code' and 'data' keys).</li>
                                            <li>The 'code' value is a string that can be parsed as a code snippet (e.g., a Python function call or a JavaScript expression).</li>
                                            <li>The 'data' value is an object with the expected properties (i.e., it has 'type', 'value', and 'metadata' keys).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 38, 42-44, 46-47)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_plain_fence</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The provided test verifies that the Ollama provider correctly parses a JSON response in a plain Markdown fence without any language.</p>
                                    <p><strong>Why Needed:</strong> This test prevents potential issues where the provider incorrectly interprets or fails to extract JSON from such responses, potentially leading to incorrect output or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The provider should be able to successfully parse the JSON response from the given string.</li>
                                            <li>The JSON response should contain a single object with no nested objects or arrays.</li>
                                            <li>The JSON response should not contain any keys that are not present in the expected format (e.g., 'content' instead of 'data').</li>
                                            <li>Any nested objects or arrays within the JSON response should be properly serialized and escaped.</li>
                                            <li>The provider should correctly handle cases where the input string contains multiple plain Markdown fences.</li>
                                            <li>The provider should ignore any whitespace characters between the fences, which are not part of the expected format.</li>
                                            <li>The provider should raise an error if the input string does not contain a valid JSON response in a plain Markdown fence.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 38, 42-44, 46-47)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_success</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test Ollama provider parses valid JSON responses and verifies correct configuration.</p>
                                    <p><strong>Why Needed:</strong> Prevents bugs that may occur when parsing invalid or malformed JSON responses.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert annotation.scenario == 'Tests feature'</li>
                                            <li>assert annotation.why_needed == 'Stops bugs'</li>
                                            <li>assert annotation.key_assertions == ['assert a', 'assert b']</li>
                                            <li>assert annotation.confidence == 0.8</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_models.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">29 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestArtifactEntry::test_to_dict</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> CoverageEntry should serialize correctly.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a bug where the CoverageEntry object is not properly serialized to JSON.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'file_path' key in the dictionary matches the expected value.</li>
                                            <li>The 'line_ranges' key in the dictionary matches the expected value.</li>
                                            <li>The 'line_count' key in the dictionary matches the expected value.</li>
                                            <li>The 'file_path' key is present and has the correct value.</li>
                                            <li>The 'line_ranges' key is present and has the correct value.</li>
                                            <li>The 'line_count' key is present and has the correct value.</li>
                                            <li>The line ranges are correctly formatted (e.g. 1-3, 5, 10-15).</li>
                                            <li>No other assertions are necessary as only these three critical checks were performed.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 260-263)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestCollectionError::test_to_dict</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `to_dict` method of a `CoverageEntry` object correctly serializes it into a dictionary.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the `CoverageEntry` object is not properly serialized to JSON.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The expected keys in the dictionary are present and have the correct values.</li>
                                            <li>The value of `file_path` is 'src/foo.py'.</li>
                                            <li>The value of `line_ranges` is a string containing comma-separated ranges, e.g. '1-3, 5, 10-15'.</li>
                                            <li>The value of `line_count` is 10.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 213-215)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestCoverageEntry::test_to_dict</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test CoverageEntry to_dict serialization correctness.</p>
                                    <p><strong>Why Needed:</strong> To ensure that the `CoverageEntry` class correctly serializes its internal data into a dictionary.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'file_path' key in the dictionary should match the expected value.</li>
                                            <li>The 'line_ranges' key in the dictionary should match the expected value.</li>
                                            <li>The 'line_count' key in the dictionary should match the expected value.</li>
                                            <li>The 'file_path' key should be present in the dictionary and have the correct value.</li>
                                            <li>The 'line_ranges' key should be present in the dictionary and have the correct value.</li>
                                            <li>The 'line_count' key should be present in the dictionary and have the correct value.</li>
                                            <li>Each assertion should pass without any errors.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 40-43)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_empty_annotation</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> An empty annotation should be created with default values.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where an empty annotation is not properly initialized with default values.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>annotation.scenario == "" (empty string)</li>
                                            <li>annotation.why_needed == "Empty annotation has default values" (expected message)</li>
                                            <li>annotation.key_assertions == [] (no key assertions performed)</li>
                                            <li>assertion for annotation.confidence is None</li>
                                            <li>assertion for annotation.error is None</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_to_dict_minimal</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `to_dict` method of `LlmAnnotation` returns a dictionary with required fields.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the minimal annotation is correctly serialized without any optional fields.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Expected 'scenario' to be in the dictionary.</li>
                                            <li>Expected 'why_needed' to be in the dictionary.</li>
                                            <li>Expected 'key_assertions' to be in the dictionary.</li>
                                            <li>Expected 'confidence' not to be in the dictionary when it is None.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 104-107, 109, 111, 113, 115)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestLlmAnnotation::test_to_dict_with_all_fields</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 104-107, 109-111, 113-115)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestReportRoot::test_default_report</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the default report schema and structure.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the report's schema version is not correctly set to the latest version.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'schema_version' key in the dictionary should be equal to SCHEMA_VERSION.</li>
                                            <li>The 'tests' key in the dictionary should be an empty list.</li>
                                            <li>The 'warnings' key in the dictionary should not exist.</li>
                                            <li>The 'collection_errors' key in the dictionary should not exist.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">54 lines (ranges: 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestReportRoot::test_report_with_collection_errors</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test Report with Collection Errors should include them.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a bug where the report does not contain all collection errors, potentially leading to incorrect reporting or missing error messages.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The length of `collection_errors` is 1.</li>
                                            <li>The value of `nodeid` in the first `collection_error` is 'test_bad.py'.</li>
                                            <li>All elements in `collection_errors` are dictionaries with a valid `nodeid` key.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">58 lines (ranges: 213-215, 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514-516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestReportRoot::test_report_with_warnings</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test verifies the presence of warnings in a report.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where reports without warnings are not properly reported.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'report' object has exactly one warning.</li>
                                            <li>The first warning's code is 'W001'.</li>
                                            <li>A warning with the code 'W001' exists in the report.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">60 lines (ranges: 235-237, 239, 241, 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516-518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestReportRoot::test_tests_sorted_by_nodeid</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">73 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestReportWarning::test_to_dict_with_detail</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test `test_to_dict_with_detail` verifies that the `to_dict()` method of `ReportWarning` returns a dictionary with the 'detail' key.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a warning about missing coverage detail in reports.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'detail' key is present and contains the expected value '/path/to/file'.</li>
                                            <li>The 'detail' key does not contain any other values or errors.</li>
                                            <li>If the 'detail' key is missing, the test will fail with a `AssertionError`.</li>
                                            <li>If the 'detail' key contains other values than '/path/to/file', the test will fail with an incorrect message.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 235-237, 239-241)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestReportWarning::test_to_dict_without_detail</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test 'test_to_dict_without_detail' verifies that a ReportWarning object is created without detail.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a warning from being reported when a ReportWarning object does not contain any detailed information.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'detail' key should be absent in the dictionary representation of the ReportWarning object.</li>
                                            <li>The 'code' and 'message' keys should be present in the dictionary representation of the ReportWarning object.</li>
                                            <li>A warning without detail should have a 'detail' key set to None or an empty string.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 235-237, 239, 241)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestRunMeta::test_aggregation_fields_present</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that RunMeta has aggregation fields.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the aggregation policy is set to 'sum' instead of 'merge'.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The run_id should be present in the dictionary.</li>
                                            <li>The run_group_id should be present in the dictionary.</li>
                                            <li>The is_aggregated attribute should be True.</li>
                                            <li>The aggregation_policy should be 'merge'.</li>
                                            <li>The run_count should be 3.</li>
                                            <li>The length of source_reports should be 2.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">39 lines (ranges: 283-285, 287-289, 370-386, 388, 391, 393, 396, 399, 401, 403, 405-411, 413, 425)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestRunMeta::test_llm_fields_excluded_when_disabled</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that LLM fields are excluded when annotations are disabled.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the LLM fields are included even when annotations are disabled.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'llm_annotations_enabled' key should not be present in the data.</li>
                                            <li>The 'llm_provider' key should not be present in the data.</li>
                                            <li>The 'llm_model' key should not be present in the data.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">29 lines (ranges: 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestRunMeta::test_llm_traceability_fields</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that LLM traceability fields are included when enabled in the test.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression by ensuring that LLM traceability fields are present and correctly configured.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The value of llm_annotations_enabled is True.</li>
                                            <li>llm_provider matches 'ollama'.</li>
                                            <li>llm_model matches 'llama3.2:1b'.</li>
                                            <li>llm_context_mode matches 'complete'.</li>
                                            <li>llm_annotations_count matches 10.</li>
                                            <li>llm_annotations_errors matches 2.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">40 lines (ranges: 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413-425)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestRunMeta::test_non_aggregated_excludes_source_reports</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test 'Non-aggregated report should not include source_reports' verifies that a non-aggregated run does not include source reports.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the 'source_reports' key is present in the output of a non-aggregated run, potentially masking important information.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'source_reports' key should be absent from the output of a non-aggregated run.</li>
                                            <li>The value of 'is_aggregated' for the meta object should be False.</li>
                                            <li>The presence of 'source_reports' in the dictionary should indicate that it is aggregated.</li>
                                            <li>If 'source_reports' were present, it would imply aggregation even if not explicitly stated.</li>
                                            <li>Including 'source_reports' could hide important information about the data source or processing steps.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">29 lines (ranges: 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestRunMeta::test_run_meta_to_dict_full</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test RunMeta to dict with all optional fields.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in case of missing or invalid metadata.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'git_sha' field should be set to the provided value.</li>
                                            <li>The 'git_dirty' field should be set to True if the source report is not empty.</li>
                                            <li>The 'repo_version', 'repo_git_sha', and 'plugin_git_sha' fields should match the expected values.</li>
                                            <li>The 'config_hash' field should be set to a valid hash.</li>
                                            <li>The length of the 'source_reports' list should be 1 in this case.</li>
                                            <li>The 'pytest_invocation' field should contain the provided value.</li>
                                            <li>The 'pytest_config_summary' field should not be present in the dictionary.</li>
                                            <li>The 'run_id' and 'run_group_id' fields should match the expected values.</li>
                                            <li>The 'is_aggregated' field should be set to True if the run is aggregated.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">49 lines (ranges: 283-285, 287-289, 370-386, 388-411, 413, 425)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestRunMeta::test_run_status_fields</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> TestRunMeta::test_run_status_fields verifies that the RunMeta object includes all necessary run status fields.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the RunMeta object is missing certain critical fields, potentially leading to incorrect or incomplete data.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'exit_code' field should be equal to 1.</li>
                                            <li>The 'interrupted' field should be True.</li>
                                            <li>The 'collect_only' field should be True.</li>
                                            <li>The 'collected_count' field should be equal to the number of runs collected.</li>
                                            <li>The 'selected_count' field should be equal to the number of selected runs.</li>
                                            <li>The 'deselected_count' field should be equal to the number of deselected runs.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">29 lines (ranges: 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestSchemaVersion::test_schema_version_format</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the schema version is correctly formatted (semver),</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the schema version is not in semver format, which could lead to unexpected behavior or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The schema version should be split into three parts (e.g., '1.2.3').</li>
                                            <li>Each part of the schema version should consist only of digits (0-9).</li>
                                            <li>All parts of the schema version should have exactly three characters (digits or dots).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestSchemaVersion::test_schema_version_in_report_root</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `ReportRoot` class includes the schema version in its report root structure.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the schema version is not included in the report root, potentially causing issues with downstream processing or reporting.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `schema_version` attribute of the `ReportRoot` class should be set to the provided value.</li>
                                            <li>The `to_dict()` method of the `ReportRoot` class should return a dictionary with a `schema_version` key matching the provided value.</li>
                                            <li>The `schema_version` value in the returned dictionary should match the expected value (`SCHEMA_VERSION`).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">54 lines (ranges: 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestSourceCoverageEntry::test_to_dict</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that a CoverageEntry object can be serialized correctly into a dictionary.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the coverage entry data is not properly formatted in the dictionary.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'file_path' key should contain the expected value, 'src/foo.py'.</li>
                                            <li>The 'line_ranges' key should contain the expected string format, '1-3, 5, 10-15'.</li>
                                            <li>The 'line_count' key should contain the expected integer value, 10.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 71-78)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestSourceReport::test_to_dict_minimal</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test `test_to_dict_minimal` verifies that the `LlmAnnotation` object's `to_dict()` method returns a dictionary with required fields.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the minimal annotation format includes all necessary information.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'scenario' field is present in the returned dictionary.</li>
                                            <li>The 'why_needed' field is present in the returned dictionary.</li>
                                            <li>The 'key_assertions' field is present in the returned dictionary.</li>
                                            <li>The 'confidence' field is not included in the returned dictionary when it's `None`.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 283-285, 287, 289)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestSourceReport::test_to_dict_with_run_id</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test SourceReport to_dict_with_run_id verifies that the 'run_id' key is present in the output dictionary.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the 'run_id' key is missing from the output dictionary, potentially causing issues with downstream processing or reporting.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'run_id' key should be present in the output dictionary.</li>
                                            <li>The value of the 'run_id' key should match the provided run ID.</li>
                                            <li>The 'run_id' key should not be empty or null.</li>
                                            <li>The 'run_id' key should have the correct format (e.g., 'run-1')</li>
                                            <li>The 'run_id' key should be a string value</li>
                                            <li>The 'run_id' key should not be a number or integer value</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 283-285, 287-289)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestSummary::test_to_dict</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that `CoverageEntry.to_dict()` correctly serializes a CoverageEntry object.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the serialized data does not match the expected format, potentially leading to incorrect coverage summaries or other issues.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'file_path' key in the dictionary is set to 'src/foo.py'.</li>
                                            <li>The 'line_ranges' key in the dictionary is set to '1-3, 5, 10-15'.</li>
                                            <li>The 'line_count' key in the dictionary is set to 10.</li>
                                            <li>The values of all keys are correct and match the expected format.</li>
                                            <li>The 'file_path' value does not contain any invalid characters or whitespace.</li>
                                            <li>The 'line_ranges' value does not contain any invalid characters or whitespace.</li>
                                            <li>The 'line_count' value matches the actual number of lines in the file.</li>
                                            <li>No unexpected keys are added to the dictionary.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 455-463, 465, 467)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestTestCaseResult::test_minimal_result</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that a minimal result has the required fields.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in case of incomplete or missing results.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'nodeid' field is set to the expected value.</li>
                                            <li>The 'outcome' field is set to 'passed'.</li>
                                            <li>The 'duration' field is set to 0.0 (or any other default value).</li>
                                            <li>The 'phase' field is set to 'call'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">19 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_coverage</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test case "Result with coverage" verifies that the `TestCaseResult` object includes a coverage list.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the coverage report is missing or incorrect.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `coverage` attribute of the `TestCaseResult` object should contain exactly one entry.</li>
                                            <li>The `file_path` attribute of the first `CoverageEntry` in the `coverage` list should be 'src/foo.py'.</li>
                                            <li>The number of entries in the `coverage` list should be 1.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">24 lines (ranges: 40-43, 162, 166-171, 173, 175, 177, 179, 182-184, 186, 188, 190, 192, 194, 196)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_llm_opt_out</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test Result with LLM Opt-Out should include flag.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the LLM opt-out flag is not properly set.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'llm_opt_out' key in the result dictionary should be True.</li>
                                            <li>The 'llm_opt_out' value in the result dictionary should match the expected boolean value.</li>
                                            <li>The 'llm_opt_out' value should be present in the result dictionary.</li>
                                            <li>If LLM opt-out is not set, the 'llm_opt_out' key and value should be absent from the result dictionary.</li>
                                            <li>If LLM opt-out is set to False, the 'llm_opt_out' value should be False.</li>
                                            <li>If LLM opt-out is set to True, the 'llm_opt_out' value should be True.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186-188, 190, 192, 194, 196)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_with_rerun</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test 'Result with reruns' verifies that the 'rerun_count' and 'final_outcome' fields are included in the result dictionary.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where a test case is marked as failed but does not actually fail, resulting in missing 'rerun_count' and 'final_outcome' fields in the result dictionary.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'rerun_count' field should be present in the result dictionary with value 2.</li>
                                            <li>The 'final_outcome' field should be present in the result dictionary with value 'passed'.</li>
                                            <li>Both 'rerun_count' and 'final_outcome' fields should be present in the result dictionary.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">21 lines (ranges: 162, 166-171, 173, 175, 177, 179-182, 184, 186, 188, 190, 192, 194, 196)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models.py::TestTestCaseResult::test_result_without_rerun_excludes_fields</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test case 'test_result_without_rerun_excludes_fields' verifies that the `result` dictionary does not include 'rerun_count' and 'final_outcome' keys.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the 'rerun_count' and 'final_outcome' fields are included in the result of a test run without reruns, potentially hiding important information about the test execution.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'result' dictionary should not contain 'rerun_count' key.</li>
                                            <li>The 'result' dictionary should not contain 'final_outcome' key.</li>
                                            <li>The 'result' dictionary does not include 'rerun_count' and 'final_outcome' keys.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">19 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_models_coverage.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">15 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_all_optional_fields</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test to_dict includes all optional fields when set.</p>
                                    <p><strong>Why Needed:</strong> Prevents bar regression in coverage analysis.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert result['param_id'] == 'a-b-c',</li>
                                            <li>assert result['param_summary'] == 'a=1, b=2, c=3',</li>
                                            <li>assert result['captured_stdout'] == 'stdout content',</li>
                                            <li>assert result['captured_stderr'] == 'stderr content',</li>
                                            <li>assert result['requirements'] == ['REQ-100'],</li>
                                            <li>assert result['llm_opt_out'] is True,</li>
                                            <li>assert result['llm_context_override'] == 'complete',</li>
                                            <li>assert len(result['coverage']) == 1,</li>
                                            <li>assert result['llm_annotation']['scenario'] == 'Tests foo'</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">81 lines (ranges: 71-78, 213-215, 235-237, 239, 241, 260-263, 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514-528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_artifacts</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test 'test_to_dict_with_artifacts' verifies that the `to_dict` method includes artifacts when set.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the `to_dict` method does not include artifacts, potentially leading to incorrect reporting or analysis of report metadata.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The length of 'artifacts' in the result dictionary is equal to 2.</li>
                                            <li>The path of the first artifact entry in the result dictionary matches 'report.html'.</li>
                                            <li>All artifact entries have a non-empty `path` key.</li>
                                            <li>Each artifact entry has a non-empty `sha256` and `size_bytes` keys.</li>
                                            <li>The `to_dict` method returns a dictionary with all required keys (artifacts, path, sha256, size_bytes).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">59 lines (ranges: 260-263, 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518-520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_collection_errors</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test to_dict with collection errors verifies that it includes any collection_errors set in the ReportRoot.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the to_dict method of ReportRoot does not include all collection_errors set in the report.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The length of result['collection_errors'] is equal to 1.</li>
                                            <li>result['collection_errors'][0]['nodeid'] is equal to 'broken_test.py'.</li>
                                            <li>All collection_errors are included in the result dictionary.</li>
                                            <li>CollectionError objects are present in the collection_errors list.</li>
                                            <li>Each CollectionError object has a 'nodeid' attribute set correctly.</li>
                                            <li>The 'nodeid' value matches the expected node id.</li>
                                            <li>The message of each CollectionError object is correct and does not contain any syntax errors.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">58 lines (ranges: 213-215, 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514-516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_custom_metadata</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test to_dict includes custom_metadata when set.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the custom metadata is not included in the report even if it's present.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>{'message': 'custom_metadata is present and has the correct keys', 'expected_value': 'project'}</li>
                                            <li>{'message': 'custom_metadata is present and has the correct values', 'expected_value': 'staging'}</li>
                                            <li>{'message': 'custom_metadata is present and has the correct value for build_number', 'expected_value': 123}</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">55 lines (ranges: 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522-524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_hmac_signature</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">55 lines (ranges: 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526-528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_sha256</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `to_dict` method of `ReportRoot` returns a dictionary with a 'sha256' key when set.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a bug where the SHA-256 hash is not included in the report's dictionary.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'sha256' key should be present in the dictionary.</li>
                                            <li>The value of the 'sha256' key should match the provided SHA-256 string.</li>
                                            <li>The 'sha256' key should be included in the dictionary even if it is not set.</li>
                                            <li>The test should fail when the 'sha256' key is not set.</li>
                                            <li>The test should pass when the 'sha256' key is set with a valid SHA-256 string.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">55 lines (ranges: 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524-526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_source_coverage</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test to_dict includes source_coverage when set.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in test coverage reporting.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'source_coverage' key should be present in the result dictionary.</li>
                                            <li>The 'file_path' value of the first 'SourceCoverageEntry' should match 'src/mod.py'.</li>
                                            <li>All 'SourceCoverageEntry' entries under 'source_coverage' should have a valid 'file_path'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">63 lines (ranges: 71-78, 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520-522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_warnings</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test 'test_to_dict_with_warnings' verifies that the `to_dict` method includes warnings when set.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where warnings are not included in the report even if there is coverage data.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The length of `result['warnings']` should be 1.</li>
                                            <li>The code 'W001' in `result['warnings'][0]` should match the expected value.</li>
                                            <li>The warning message 'No coverage data' should be present in the warning object.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">60 lines (ranges: 235-237, 239, 241, 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516-518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models_coverage.py::TestSummaryToDict::test_to_dict_with_coverage_total_percent</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test to_dict includes coverage_total_percent when set.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the 'coverage_total_percent' is not included in the test results even though it's present in the Summary object.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'coverage_total_percent' key should be present in the result dictionary.</li>
                                            <li>The value of 'coverage_total_percent' should match the provided value (85.5) when compared to the summary object.</li>
                                            <li>If the 'coverage_total_percent' is missing, the test will fail with an AssertionError.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 455-463, 465-467)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models_coverage.py::TestSummaryToDict::test_to_dict_without_coverage_total_percent</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> This test verifies that the `to_dict` method of `Summary` class excludes coverage_total_percent when it's None.</p>
                                    <p><strong>Why Needed:</strong> The test prevents a regression where the `to_dict` method returns incorrect results when there is no coverage total percent.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>summary.total and summary.passed are both set to 10</li>
                                            <li>result['coverage_total_percent'] is None</li>
                                            <li>result does not contain 'coverage_total_percent'</li>
                                            <li>result does not contain 'total' or 'passed'</li>
                                            <li>result has a different structure than expected</li>
                                            <li>the coverage total percent is missing from the result</li>
                                            <li>the test passes if summary.total and summary.passed are both 10</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 455-463, 465, 467)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_all_optional_fields</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test to_dict includes all optional fields when set.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in coverage calculation where llm_opt_out is True and llm_context_override is not provided.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert result['param_id'] == 'a-b-c'</li>
                                            <li>assert result['param_summary'] == 'a=1, b=2, c=3'</li>
                                            <li>assert result['captured_stdout'] == 'stdout content'</li>
                                            <li>assert result['captured_stderr'] == 'stderr content'</li>
                                            <li>assert result['requirements'] == ['REQ-100']</li>
                                            <li>assert result['llm_opt_out'] is True</li>
                                            <li>assert result['llm_context_override'] == 'complete'</li>
                                            <li>assert len(result['coverage']) == 1</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">41 lines (ranges: 40-43, 104-107, 109, 111, 113, 115, 162, 166-171, 173-179, 182-196)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stderr</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test to_dict includes captured_stderr when set.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the 'captured_stderr' is not included in the test result dictionary.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'captured_stderr' key should be present in the test result dictionary.</li>
                                            <li>The value of the 'captured_stderr' key should match the captured error message.</li>
                                            <li>If no error is captured, the 'captured_stderr' key should not be present in the test result dictionary.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192-194, 196)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stdout</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test to_dict includes captured_stdout when set.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the 'captured_stdout' key is not included in the result dictionary even though it was captured during the execution of the test.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'captured_stdout' key should be present in the result dictionary.</li>
                                            <li>The value of the 'captured_stdout' key should match the captured output.</li>
                                            <li>If the test is successful, the 'captured_stdout' key should not be empty or None.</li>
                                            <li>If the test fails, the 'captured_stdout' key should contain a meaningful error message or exception.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190-192, 194, 196)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_param_summary</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test to_dict includes param_summary when set.</p>
                                    <p><strong>Why Needed:</strong> To ensure the test_to_dict method correctly handles param_summary in the output dictionary.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'param_summary' key is present in the result dictionary.</li>
                                            <li>The value of 'param_summary' matches the expected string 'x=1, y=2'.</li>
                                            <li>The 'param_summary' key is included in the test output even when it's empty.</li>
                                            <li>The 'param_summary' key is not included if no param_summary is provided.</li>
                                            <li>The 'param_summary' key has a correct format (e.g., 'key=value', etc.)</li>
                                            <li>The 'param_summary' key is present in the result dictionary for all test cases.</li>
                                            <li>The 'param_summary' key is present even when it's empty (e.g., for tests with no param_summary)</li>
                                            <li>The 'param_summary' key has a correct value (e.g., 'x=1, y=2')</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">21 lines (ranges: 162, 166-171, 173, 175-179, 182, 184, 186, 188, 190, 192, 194, 196)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_requirements</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test to_dict includes requirements when set.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the 'requirements' key is missing from the TestCaseResult object.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'requirements' key should be present in the TestCaseResult object.</li>
                                            <li>The 'requirements' key should contain the expected values (REQ-001 and REQ-002).</li>
                                            <li>If no requirements are provided, the 'requirements' key should still be present with an empty list.</li>
                                            <li>If a requirement is missing, it should raise an AssertionError.</li>
                                            <li>If a requirement has a different name than 'REQ-001' or 'REQ-002', it should raise an AssertionError.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194-196)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_options.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">17 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options.py::TestConfig::test_default_values</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that default values are set correctly for a test configuration.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in case the default values of the test configuration are not set correctly.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>cfg.provider == 'none' (default value)</li>
                                            <li>cfg.llm_context_mode == 'minimal' (default value)</li>
                                            <li>cfg.llm_max_tests == 0 (default value)</li>
                                            <li>cfg.llm_max_retries == 10 (default value)</li>
                                            <li>cfg.llm_context_bytes == 32000 (additional default value)</li>
                                            <li>cfg.llm_context_file_limit == 10 (additional default value)</li>
                                            <li>cfg.llm_requests_per_minute == 5 (additional default value)</li>
                                            <li>cfg.llm_timeout_seconds == 30 (additional default value)</li>
                                            <li>cfg.llm_cache_ttl_seconds == 86400 (additional default value)</li>
                                            <li>cfg.include_phase == 'run' (default value)</li>
                                            <li>cfg.aggregate_policy == 'latest' (default value)</li>
                                            <li>cfg.is_llm_enabled() is False (default value)</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options.py::TestConfig::test_get_default_config</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 261)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options.py::TestConfig::test_is_llm_enabled</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `is_llm_enabled` check is enabled for providers other than 'none' and returns True for 'ollama', but disabled otherwise.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the `is_llm_enabled` check was not properly configured for different provider values.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `Config.is_llm_enabled()` should return False when the provider is set to 'none'.</li>
                                            <li>The function `Config.is_llm_enabled()` should return True when the provider is set to 'ollama'.</li>
                                            <li>The function `Config.is_llm_enabled()` should not return a boolean value when the provider is set to an unknown or default value (e.g. 'default').</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_aggregate_policy</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">23 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-213, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_context_mode</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">23 lines (ranges: 123, 163, 191, 194-195, 201-205, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_include_phase</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">23 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-221, 225-226, 233, 237, 239, 241, 243, 245, 248)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options.py::TestConfig::test_validate_invalid_provider</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">22 lines (ranges: 123, 163, 191, 194-197, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options.py::TestConfig::test_validate_numeric_ranges</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test validation of numeric constraints for Config object.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression where a Config object with invalid numeric ranges is created.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>cfg.validate() returns an empty list if all numeric constraints are met.</li>
                                            <li>cfg.validate() raises an AssertionError if any numeric constraint is not met.</li>
                                            <li>The 'llm_context_bytes' value should be at least 1000.</li>
                                            <li>The 'llm_max_tests' value should be positive or zero.</li>
                                            <li>The 'llm_requests_per_minute' value should be at least 1.</li>
                                            <li>The 'llm_timeout_seconds' value should be at least 1.</li>
                                            <li>The 'llm_max_retries' value should be positive or zero.</li>
                                            <li>A Config object with invalid numeric ranges is created when llm_context_bytes < 1000.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237-246, 248)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options.py::TestConfig::test_validate_valid_config</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Testing the `validate()` method of the `Config` class with a valid configuration.</p>
                                    <p><strong>Why Needed:</strong> This test prevents potential bugs or regressions where an invalid configuration is passed to the validation process.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `validate()` method should return an empty list (`[]`) when given a valid configuration.</li>
                                            <li>A configuration object should be created with no errors.</li>
                                            <li>No exceptions should be raised during the validation process.</li>
                                            <li>The configuration should be in a valid format (i.e., not empty or invalid).</li>
                                            <li>The `validate()` method should return an empty list (`[]`) when given a well-formed configuration.</li>
                                            <li>A `Config` object with no errors should be created and validated successfully.</li>
                                            <li>No error messages should be printed during the validation process.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_aggregation_options</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test loads aggregation options for pytest configuration.</p>
                                    <p><strong>Why Needed:</strong> Prevents a bug where the aggregate policy is not correctly applied to the run ID and group ID.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `aggregate_dir` option should be set to 'aggr_dir'.</li>
                                            <li>The `aggregate_policy` option should be set to 'merge'.</li>
                                            <li>The `aggregate_run_id` option should match 'run-123'.</li>
                                            <li>The `aggregate_group_id` option should match 'group-abc'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">29 lines (ranges: 123, 163, 276, 279-280, 288-290, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440-448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_config_missing_pyproject</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test handling when pyproject.toml doesn't exist.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a bug where the LLM report generation fails due to missing pyproject.toml file.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `llm_max_retries` attribute in the configuration should be set to 10 by default.</li>
                                            <li>The `llm_report_html`, `llm_report_json`, and `llm_report_pdf` attributes should not have any values assigned when pyproject.toml is missing.</li>
                                            <li>The `llm_evidence_bundle` attribute should remain unchanged with a value of 'None'.</li>
                                            <li>The `llm_dependency_snapshot` attribute should be set to an empty list.</li>
                                            <li>The `llm_requests_per_minute` attribute should not have any values assigned when pyproject.toml is missing.</li>
                                            <li>The `llm_aggregate_dir`, `llm_aggregate_policy`, and `llm_aggregate_run_id` attributes should remain unchanged with default values.</li>
                                            <li>The `llm_coverage_source` attribute should be set to an empty string.</li>
                                            <li>The `llm_model` attribute should not have any values assigned when pyproject.toml is missing.</li>
                                            <li>The `llm_context_mode` attribute should be set to 'test' by default.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 123, 163, 276, 279-280, 288-290, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_coverage_source</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `llm_coverage_source` option is set to 'cov_dir' when loading coverage.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `llm_coverage_source` option is not correctly set, potentially leading to incorrect coverage analysis.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The value of `cfg.llm_coverage_source` should be 'cov_dir'.</li>
                                            <li>The `llm_coverage_source` option should have been successfully updated with the specified value.</li>
                                            <li>The test should fail if the `llm_coverage_source` option is not set to 'cov_dir' after updating it.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 123, 163, 276, 279-280, 288-290, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448-449, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_defaults</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the default provider and report HTML are correctly loaded when no options are provided.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the user is not able to load configuration with default settings.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'provider' attribute of the configuration object should be set to 'none'.</li>
                                            <li>The 'report_html' attribute of the configuration object should be None.</li>
                                            <li>The provider and report HTML values should match the expected default values.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 123, 163, 276, 279-280, 288-290, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_cli_overrides_pyproject</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Testing the test_load_from_cli_overrides_pyproject function to verify that CLI options override pyproject.toml options.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where CLI options are not overriding pyproject.toml options, potentially leading to unexpected behavior or errors in the application.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The '--pyproject' option overrides the 'pyproject.toml' configuration file.</li>
                                            <li>The '--pyproject-repo' option overrides the 'pyproject.toml' repository path.</li>
                                            <li>The '--pyproject-requirements' option overrides the 'pyproject.toml' dependencies list.</li>
                                            <li>The '--pyproject-skip-metapaths' option overrides the 'pyproject.toml' skip metapath setting.</li>
                                            <li>The '--pyproject-exclude' option overrides the 'pyproject.toml' exclude file path.</li>
                                            <li>The '--pyproject-include' option overrides the 'pyproject.toml' include file path.</li>
                                            <li>The '--pyproject-skip-artifacts' option overrides the 'pyproject.toml' skip artifacts setting.</li>
                                            <li>The '--pyproject-exclude-include' option overrides the 'pyproject.toml' exclude/include file path.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">70 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300-302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414-415, 417-418, 420-421, 424-426, 428, 430, 432, 434-436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_cli_provider_override</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the test_load_from_cli_provider_override method correctly overrides the pyproject.toml configuration.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the CLI provider option is not overridden in the pyproject.toml file, potentially leading to unexpected behavior or errors during the loading of dependencies.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'pyproject' key in the pyproject.toml file should be set to 'override' when the CLI provider option is used.</li>
                                            <li>The value of the 'pyproject' key in the pyproject.toml file should match the expected override value.</li>
                                            <li>The test should fail if the 'pyproject' key in the pyproject.toml file does not contain the word 'override'.</li>
                                            <li>The test should succeed if the 'pyproject' key in the pyproject.toml file contains the word 'override'.</li>
                                            <li>The 'pyproject' key in the pyproject.toml file should be set to a value other than 'override' when the CLI provider option is used.</li>
                                            <li>The value of the 'pyproject' key in the pyproject.toml file should not match the expected override value when the CLI provider option is used.</li>
                                            <li>The test should fail if the 'pyproject' key in the pyproject.toml file does not contain a valid override value (e.g., 'override', 'override=')</li>
                                            <li>The test should succeed if the 'pyproject' key in the pyproject.toml file contains a valid override value (e.g., 'override', 'override=')</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">68 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300-304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414-415, 417-418, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_cli_retries</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `llm_max_retries` option is correctly set to 2 when loading configuration from CLI.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `llm_max_retries` option is not properly set, leading to incorrect configuration.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The value of `llm_max_retries` should be 2.</li>
                                            <li>The `load_config` function should return an instance with `llm_max_retries` set to 2.</li>
                                            <li>The `cfg` variable should contain a dictionary with the correct key-value pair for `llm_max_retries`.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 123, 163, 276, 279-280, 288-290, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436-437, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options.py::TestLoadConfig::test_load_from_pyproject</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">69 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300-304, 308, 310, 312, 316, 320, 324, 328-330, 332, 334, 336, 340, 342, 346, 348, 350-352, 354-356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_options_coverage.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">38 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestCliOverrides::test_cli_dependency_snapshot</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `llm_dependency_snapshot` option is correctly set to 'deps.json' when running CLI with overrides.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `llm_dependency_snapshot` option is not set correctly, potentially leading to incorrect dependency snapshots being reported.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The value of `llm_dependency_snapshot` in the configuration file should be 'deps.json'.</li>
                                            <li>The `report_dependency_snapshot` key should contain the string 'deps.json'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">29 lines (ranges: 123, 163, 276, 279-280, 288-290, 414-415, 417-418, 420-421, 424, 426, 428, 430, 432-434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestCliOverrides::test_cli_evidence_bundle</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `test_cli_evidence_bundle` test case correctly sets the `llm_evidence_bundle` option to 'bundle.zip' in the configuration.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `llm_evidence_bundle` option is not set correctly, potentially leading to incorrect report generation or other issues.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `llm_evidence_bundle` option is set to 'bundle.zip' in the configuration.</li>
                                            <li>The `report_evidence_bundle` value matches 'bundle.zip'.</li>
                                            <li>The `llm_evidence_bundle` option has a valid value ('bundle.zip').</li>
                                            <li>The configuration file contains the correct `llm_evidence_bundle` option value ('bundle.zip').</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">29 lines (ranges: 123, 163, 276, 279-280, 288-290, 414-415, 417-418, 420-421, 424, 426, 428, 430-432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestCliOverrides::test_cli_report_json</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that the test_cli_report_json function sets the report JSON option to 'output.json' when CLI override is enabled.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a bug where the report JSON option is not set correctly when CLI overrides are enabled.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>mock.option.llm_report_json = 'output.json'</li>
                                            <li>cfg.report_json == 'output.json'</li>
                                            <li>assert cfg._report_json == 'output.json'</li>
                                            <li>self._test_cli_report_json(tmp_path) should have been called with the correct config</li>
                                            <li>mock._report_json is set to 'output.json' when CLI override is enabled</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">29 lines (ranges: 123, 163, 276, 279-280, 288-290, 414-415, 417-418, 420-421, 424, 426-428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestCliOverrides::test_cli_report_pdf</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that the `test_cli_report_pdf` test function sets the `llm_report_pdf` option to 'output.pdf' in the mock configuration.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the `llm_report_pdf` option is not set correctly when running the CLI with overrides.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The value of `llm_report_pdf` is set to 'output.pdf'.</li>
                                            <li>The value of `llm_report_pdf` matches the expected value in the mock configuration.</li>
                                            <li>The `llm_report_pdf` option is present in the mock configuration.</li>
                                            <li>The `llm_report_pdf` option has a value of 'output.pdf' in the mock configuration.</li>
                                            <li>The `llm_report_pdf` option does not have any other values in the mock configuration.</li>
                                            <li>The `llm_report_pdf` option is set to 'output.pdf' when the test runs with overrides.</li>
                                            <li>The `llm_report_pdf` option is set to 'output.pdf' in the mock configuration when the test runs without overrides.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">29 lines (ranges: 123, 163, 276, 279-280, 288-290, 414-415, 417-418, 420-421, 424, 426, 428-430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestConfigValidationCoverage::test_validate_invalid_token_output_format</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `validate()` method of the `Config` class returns an error when the token output format is invalid.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `validate()` method does not raise an error for invalid token output formats, potentially leading to unexpected behavior or errors in downstream code.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `validate()` method of the `Config` class should return an error when the token output format is specified as 'xml'.</li>
                                            <li>Any error messages returned by the `validate()` method should contain the string 'litellm_token_output_format' to ensure a clear indication of invalid input.</li>
                                            <li>The test should fail if no error message containing 'litellm_token_output_format' is found in the errors list.</li>
                                            <li>The `validate()` method should raise an exception when called with an invalid token output format, indicating that the input was incorrect.</li>
                                            <li>Any exceptions raised by the `validate()` method should be caught and reported as an error, preventing the test from passing even if the input is valid.</li>
                                            <li>The test should only fail if the input is not a string or None, to prevent false positives due to other types of errors.</li>
                                            <li>The `validate()` method should raise a `ValueError` exception when called with an invalid token output format, indicating that the input was incorrect.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">23 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-229, 233, 237, 239, 241, 243, 245, 248)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestConfigValidationCoverage::test_validate_token_refresh_interval_too_short</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test validation when token refresh interval is too short.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the token refresh interval is set to a value that may cause issues with the application's security and functionality.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `litellm_token_refresh_interval` must be at least 60.</li>
                                            <li>Any invalid values for `litellm_token_refresh_interval` will result in an error message indicating that it should be at least 60.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">21 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233-234, 237, 239, 241, 243, 245, 248)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestConfigValidationCoverage::test_validate_valid_litellm_config</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test validation of valid LiteLLM configuration.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression where invalid or outdated config is used with LitLLM.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `validate()` method returns an empty list of errors if the provided configuration is valid.</li>
                                            <li>The `litellm_token_output_format` parameter is set to 'text' and the `litellm_token_refresh_interval` parameter is set to 3600 seconds.</li>
                                            <li>The `provider` parameter is set to 'litellm'.</li>
                                            <li>The `token_output_format` parameter is not provided. It should be either 'text', 'json', or 'csv'.</li>
                                            <li>The `refresh_interval` parameter is not a valid value for LitLLM. It can only take values of 60, 300, or 3600 seconds.</li>
                                            <li>The `provider` and `token_output_format` parameters are both set to 'litellm'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_aggregate_include_history</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `aggregate_include_history` option is loaded correctly from the `pyproject.toml` file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `aggregate_include_history` option is not loaded, potentially causing issues with coverage analysis.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `aggregate_include_history` option is present in the `pyproject.toml` file.</li>
                                            <li>The `aggregate_include_history` option has the correct value ('include' or 'exclude').</li>
                                            <li>The `include` and `exclude` values are correctly formatted (e.g., 'include=True', 'exclude=False').</li>
                                            <li>The `include` and `exclude` values do not contain any invalid characters (e.g., spaces, special characters).</li>
                                            <li>The `aggregate_include_history` option is loaded from the correct location in the `pyproject.toml` file.</li>
                                            <li>The `aggregate_include_history` option is correctly included in the coverage report.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">66 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392-394, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_aggregate_policy_from_pyproject</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `load_aggregate_policy` function can successfully load an aggregate policy from a PyProject file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `load_aggregate_policy` function fails to load aggregate policies from PyProjects due to incorrect or missing configuration files.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `load_aggregate_policy` function should be able to read and parse the aggregate policy from the provided PyProject file.</li>
                                            <li>The `load_aggregate_policy` function should raise a `FileNotFoundError` if the specified PyProject file is not found or does not exist.</li>
                                            <li>The `load_aggregate_policy` function should correctly load the aggregate policy configuration from the PyProject file, including any required dependencies and settings.</li>
                                            <li>The `load_aggregate_policy` function should be able to handle cases where the PyProject file contains invalid or incomplete configuration data.</li>
                                            <li>The `load_aggregate_policy` function should not raise an exception if the PyProject file is empty or does not contain any aggregate policy information.</li>
                                            <li>The `load_aggregate_policy` function should correctly load the aggregate policy from a PyProject file that uses a custom configuration file (e.g. `.yml` or `.json`).</li>
                                            <li>The `load_aggregate_policy` function should be able to handle cases where the PyProject file is located in a non-standard location (e.g. a directory instead of a file).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390-392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_all_config_keys_combined</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the 'load_all_config_keys' function loads all required configuration keys from a single pyproject.toml file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the 'load_all_config_keys' function does not load all required configuration keys, potentially leading to coverage issues or unexpected behavior.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should be able to read and write the 'pyproject.toml' file successfully without any errors.</li>
                                            <li>All required configuration keys should be loaded from the pyproject.toml file.</li>
                                            <li>The function should return an empty dictionary if no configuration keys are found in the pyproject.toml file.</li>
                                            <li>The function should raise a ValueError if the pyproject.toml file is missing or corrupted.</li>
                                            <li>The function should not throw any exceptions when writing to the pyproject.toml file.</li>
                                            <li>All required configuration keys should be present in the 'pyproject.toml' file with the correct data types and values.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">106 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300-305, 308-314, 316-318, 320-322, 324-325, 328-337, 340-343, 346-359, 362-364, 366-367, 370-376, 380-382, 384-386, 390-394, 398-401, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_cache_dir</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `load_cache_dir` function loads the correct cache directory from the pyproject.toml file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the cache directory is not loaded correctly, potentially leading to issues with caching and dependency resolution.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `pyproject.toml` file contains the correct path to the cache directory.</li>
                                            <li>The `load_cache_dir` function writes the correct path to the cache directory to the pyproject.toml file.</li>
                                            <li>The `cache_dir` attribute of the `Project` object is set correctly to the loaded cache directory.</li>
                                            <li>The `pyproject.toml` file contains a valid `cache_dir` attribute with the expected value.</li>
                                            <li>The `load_cache_dir` function does not raise an exception when the cache directory cannot be loaded.</li>
                                            <li>The `cache_dir` attribute of the `Project` object is set correctly to the cached value.</li>
                                            <li>The `pyproject.toml` file contains a valid `cache_dir` attribute with the expected value.</li>
                                            <li>The `load_cache_dir` function does not raise an exception when the cache directory cannot be loaded.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358-359, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_cache_ttl_seconds</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `cache_ttl_seconds` value is loaded correctly from `pyproject.toml`.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `cache_ttl_seconds` value is not properly loaded, potentially leading to incorrect cache behavior.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The value of `cache_ttl_seconds` in `pyproject.toml` matches the expected value.</li>
                                            <li>The `cache_ttl_seconds` value is correctly formatted and does not contain any invalid characters.</li>
                                            <li>The `cache_ttl_seconds` value is greater than or equal to 0 seconds.</li>
                                            <li>The `cache_ttl_seconds` value is less than the maximum allowed value (3600 seconds).</li>
                                            <li>The file path of `pyproject.toml` exists and is accessible.</li>
                                            <li>The test runs without any errors or exceptions.</li>
                                            <li>The cache TTL seconds are correctly set for a given project.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356-358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_failed_output</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `load_capture_failed_output` function can successfully load the `capture_failed_output` setting from the `pyproject.toml` file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the `load_capture_failed_output` function fails to load the `capture_failed_output` setting due to an incorrect or missing configuration in the `pyproject.toml` file.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `pyproject.toml` file should contain the correct value for the `capture_failed_output` setting.</li>
                                            <li>The `load_capture_failed_output` function should be able to successfully load this value from the `pyproject.toml` file.</li>
                                            <li>The `pyproject.toml` file should not be missing or empty, causing the `load_capture_failed_output` function to fail.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372-374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_output_max_chars</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that `load_capture_output_max_chars` loads the correct value from `pyproject.toml`.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `max_chars` value is not correctly loaded due to an incorrect or missing configuration in the `pyproject.toml` file.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The value of `load_capture_output_max_chars` is equal to the expected value from `pyproject.toml`.</li>
                                            <li>The value of `max_chars` is a valid integer between 1 and 1000 (inclusive).</li>
                                            <li>The configuration in `pyproject.toml` is correctly formatted and does not contain any invalid characters or syntax.</li>
                                            <li>The test output is as expected, indicating that the `load_capture_output_max_chars` function loaded the correct value from `pyproject.toml`.</li>
                                            <li>The value of `max_chars` is a multiple of 10 (e.g., 100, 200, etc.) to ensure accurate capture output.</li>
                                            <li>No exceptions are raised during the execution of the test, indicating that the `load_capture_output_max_chars` function is working correctly.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">66 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374-376, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_bytes</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330-332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_exclude_globs</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify the `load_context_exclude_globs` function in `test_load_context_exclude_globs` loads context_exclude_globs correctly from `pyproject.toml`.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `load_context_exclude_globs` function fails to load context_exclude_globs from pyproject.toml due to incorrect or missing file paths.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `load_context_exclude_globs` function writes the correct value of `context_exclude_globs` to `pyproject.toml`.</li>
                                            <li>The `load_context_exclude_globs` function correctly handles cases where `context_exclude_globs` is not found in pyproject.toml.</li>
                                            <li>The `load_context_exclude_globs` function does not throw any errors when the file path is incorrect or missing.</li>
                                            <li>The `load_context_exclude_globs` function does not throw any errors when the value of `context_exclude_globs` is empty.</li>
                                            <li>The `load_context_exclude_globs` function correctly handles cases where `context_exclude_globs` has a different format than expected (e.g., multiple values separated by commas).</li>
                                            <li>The `load_context_exclude_globs` function does not throw any errors when the file path or value of `context_exclude_globs` is not a string.</li>
                                            <li>The `load_context_exclude_globs` function correctly handles cases where `context_exclude_globs` has a different encoding than expected (e.g., UTF-8).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336-337, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_file_limit</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `context_file_limit` setting is correctly loaded from pyproject.toml.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `context_file_limit` setting is not properly loaded, potentially causing issues with context loading in certain scenarios.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The content of the `pyproject.toml` file matches the expected format.</li>
                                            <li>The `tmp_path / 'pyproject.toml'` path is correctly constructed and resolves to a valid file.</li>
                                            <li>The `pyproject.write_text()` method writes the correct text to the file.</li>
                                            <li>The `context_file_limit` setting is not set in the `pyproject.toml` file.</li>
                                            <li>The `tmp_path / 'pyproject.toml'` path does not resolve to an existing file.</li>
                                            <li>The `pyproject.toml` file has a valid and correct format.</li>
                                            <li>The test fails with an error message indicating that the setting was not found in the file.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332-334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_include_globs</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `context_include_globs` setting is loaded correctly from pyproject.toml.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `context_include_globs` setting is not loaded, potentially leading to incorrect coverage analysis.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The contents of `pyproject.toml` are checked for the presence of `context_include_globs`.</li>
                                            <li>The value of `context_include_globs` in `pyproject.toml` is verified to be set correctly.</li>
                                            <li>The `context_include_globs` setting is loaded from `pyproject.toml` and used during testing.</li>
                                            <li>The coverage analysis includes the specified globs when running tests with the `--context` flag.</li>
                                            <li>The test environment has access to the loaded `context_include_globs` setting.</li>
                                            <li>The test output shows that the globs are correctly included in the coverage report.</li>
                                            <li>The test passes without any errors or warnings related to the `context_include_globs` setting.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334-336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_hmac_key_file</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `hmac_key_file` is loaded correctly from a PyProject file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where the `hmac_key_file` is not loaded, potentially causing coverage issues or incorrect behavior in downstream code.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The contents of the `pyproject.toml` file are correct and match the expected values.</li>
                                            <li>The `hmac_key_file` is present in the PyProject file and is correctly referenced.</li>
                                            <li>The `hmac_key_file` is loaded successfully without any errors or warnings.</li>
                                            <li>The coverage of the code using the `hmac_key_file` is sufficient to ensure correct behavior.</li>
                                            <li>The `pyproject.toml` file has the correct `hmac_key_file` value.</li>
                                            <li>The `hmac_key_file` is not empty and contains valid HMAC key data.</li>
                                            <li>The `hmac_key_file` is loaded correctly from a PyProject file with an existing `hmac_key_file` value.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400-401, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_param_values</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `include_param_values` option is correctly loaded from pyproject.toml.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `include_param_values` option is not properly loaded, potentially causing issues with code coverage.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The contents of the `pyproject.toml` file are correct and match the expected values.</li>
                                            <li>The `include_param_values` option is correctly set to `True` in the pyproject.toml file.</li>
                                            <li>The `include_param_values` option is not set to `False` or any other value in the pyproject.toml file.</li>
                                            <li>The contents of the included files are correctly loaded and available for use in the code.</li>
                                            <li>The test environment has the necessary permissions to read the pyproject.toml file.</li>
                                            <li>The `include_param_values` option is not enabled by default in the project settings.</li>
                                            <li>The `include_param_values` option is properly configured in the project's settings file (if applicable).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340-342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_phase</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `include_phase` is loaded correctly from the PyProject.toml file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the include phase is not loaded, potentially leading to incorrect coverage analysis.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The contents of the `include_phase` section in the PyProject.toml file are correct.</li>
                                            <li>The `include_phase` section is present and contains the expected values.</li>
                                            <li>The `include_phase` section does not contain any invalid or empty lines.</li>
                                            <li>The `include_phase` section has the correct indentation and formatting.</li>
                                            <li>The `include_phase` section includes all required files and directories.</li>
                                            <li>The `include_phase` section excludes unnecessary files and directories.</li>
                                            <li>All required dependencies are included in the `include_phase` section.</li>
                                            <li>No empty lines or whitespace are present in the `include_phase` section.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366-367, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_pytest_invocation</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">66 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380-382, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_invocation_redact_patterns</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `load_invocation_redact_patterns` function can successfully load invocation_redact_patterns from a PyProject.toml file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `load_invocation_redact_patterns` function fails to load invocation_redact_patterns due to a missing or corrupted PyProject.toml file.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should be able to read the 'invocation_redact_patterns' key from the PyProject.toml file without raising an exception.</li>
                                            <li>The value of the 'invocation_redact_patterns' key should match the expected pattern.</li>
                                            <li>The function should raise a `FileNotFoundError` if the PyProject.toml file does not exist or is missing the required keys.</li>
                                            <li>The function should raise a `KeyError` if the 'invocation_redact_patterns' key is not found in the PyProject.toml file.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">66 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384-386, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_api_base</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The 'litellm_api_base' module should be loaded successfully when the test is run.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where the 'litellm_api_base' module might not be loaded due to a missing or corrupted pyproject.toml file.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The contents of pyproject.toml are correct and do not contain any errors.</li>
                                            <li>The 'pyproject.toml' file is successfully written to the specified path.</li>
                                            <li>The 'litellm_api_base' module can be loaded without any issues when the test is run.</li>
                                            <li>No exceptions are raised during the loading process of the 'litellm_api_base' module.</li>
                                            <li>The 'pyproject.toml' file is correctly formatted and does not contain any invalid characters or syntax errors.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">66 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300-302, 304, 308-310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_api_key</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `litellm_api_key` is loaded correctly from the `pyproject.toml` file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the API key is not loaded due to an issue with the `pyproject.toml` file being corrupted or incomplete.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The contents of the `pyproject.toml` file should contain the expected `litellm_api_key` value.</li>
                                            <li>The `pyproject.toml` file should be able to read and write the `litellm_api_key` value correctly.</li>
                                            <li>The test should fail if the `pyproject.toml` file is empty or missing the required `litellm_api_key` key.</li>
                                            <li>The `litellm_api_key` value should be a string containing only alphanumeric characters and underscores.</li>
                                            <li>The `litellm_api_key` value should not contain any whitespace characters.</li>
                                            <li>The test should pass if the `pyproject.toml` file contains the expected `litellm_api_key` value in the correct format.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">66 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300-302, 304, 308, 310-312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_json_key</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `litellm_token_json_key` key is correctly loaded from the `pyproject.toml` file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where the `litellm_token_json_key` value is not properly set or is missing in the `pyproject.toml` file, potentially leading to coverage issues.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'litellm_token_json_key' key should be present and have the correct value.</li>
                                            <li>The value of 'litellm_token_json_key' should match the expected value from the `litellm_token.json` file.</li>
                                            <li>The 'litellm_token_json_key' value should not be empty or null.</li>
                                            <li>The 'litellm_token_json_key' value should be a valid JSON string.</li>
                                            <li>The 'litellm_token_json_key' value should match the expected format (e.g., 'key: value').</li>
                                            <li>The 'pyproject.toml' file should contain the necessary configuration for litellm to load the token JSON key correctly.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">66 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300-302, 304, 308, 310, 312, 316, 320, 324-325, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_output_format</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `litellm_token_output_format` option in the `pyproject.toml` file is correctly loaded and used to generate token output.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `litellm_token_output_format` option is not properly loaded or configured, resulting in incorrect or missing token output.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `pyproject.toml` file should contain a section named `litellm.token_output_format` with a value of either 'json' or 'txt'.</li>
                                            <li>The `pyproject.toml` file should contain the correct path to the `litellm_token_output_format` function.</li>
                                            <li>The `pyproject.toml` file should contain the correct arguments for the `litellm_token_output_format` function (e.g., `output_path`, `format`)</li>
                                            <li>The generated token output should be in the expected format (either 'json' or 'txt').</li>
                                            <li>The test should fail when the `pyproject.toml` file is missing the `litellm.token_output_format` option or has an incorrect value.</li>
                                            <li>The test should pass when the `pyproject.toml` file contains the correct `litellm.token_output_format` option and arguments.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">67 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300-302, 304, 308, 310, 312, 316, 320-322, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_refresh_command</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `litellm_token_refresh_command` command is loaded correctly from the PyProject.toml file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `litellm_token_refresh_command` command is not loaded, potentially causing issues with the application's functionality or behavior.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The contents of the `pyproject.toml` file are correct and match the expected values.</li>
                                            <li>The `litellm_token_refresh_command` key exists in the PyProject.toml file and its value is set to a valid string.</li>
                                            <li>The command's name, 'refresh', is present in the list of commands defined in the PyProject.toml file.</li>
                                            <li>The command's description, 'Refresh litellm token', is present in the list of descriptions defined in the PyProject.toml file.</li>
                                            <li>The command's author and license information are correctly set in the PyProject.toml file.</li>
                                            <li>The `litellm_token_refresh_command` command has a valid path to the required files.</li>
                                            <li>The command's dependencies are correctly specified in the PyProject.toml file.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">67 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300-302, 304, 308, 310, 312-314, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_refresh_interval</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `litellm_token_refresh_interval` option is loaded correctly from the `pyproject.toml` file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `litellm_token_refresh_interval` option is not loaded, potentially leading to coverage issues or unexpected behavior in the application.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The contents of the `pyproject.toml` file are correct and match the expected values.</li>
                                            <li>The `litellm_token_refresh_interval` option is present and has the correct value (e.g., 1 day, 7 days, etc.).</li>
                                            <li>The `pyproject.toml` file does not contain any other options that could potentially prevent loading of `litellm_token_refresh_interval`.</li>
                                            <li>The test loads the `pyproject.toml` file successfully without encountering any errors or exceptions.</li>
                                            <li>The value of `litellm_token_refresh_interval` is correctly converted to a string (e.g., '1d', '7d', etc.).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">67 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300-302, 304, 308, 310, 312, 316-318, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_malformed_pyproject</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test loading of malformed pyproject.toml file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the 'provider' is set to 'none' when a malformed pyproject.toml is encountered, causing the default provider to be used instead.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'cfg.provider' attribute should not be set to 'none'.</li>
                                            <li>The 'cfg.provider' attribute should be set to 'default' or an empty string if no valid configuration is found.</li>
                                            <li>When a malformed pyproject.toml file is encountered, the test should fallback to defaults and not raise an exception.</li>
                                            <li>The default provider ('default') should be used when the malformed config cannot be parsed correctly.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">34 lines (ranges: 123, 163, 276, 279-280, 288-293, 403, 405, 407-410, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_max_concurrency</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `max_concurrency` setting in a Pyproject.toml file is correctly loaded.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `max_concurrency` value is not properly loaded from the Pyproject.toml file, potentially leading to unexpected behavior or errors when using the `concurrent.futures.ThreadPoolExecutor` class.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The contents of the `pyproject.toml` file are correctly read and parsed as a JSON object.</li>
                                            <li>The `max_concurrency` value is present in the `pyproject.toml` file and is not empty or zero.</li>
                                            <li>The `max_concurrency` value is an integer.</li>
                                            <li>The `max_concurrency` value does not exceed 1000 (the maximum allowed value).</li>
                                            <li>The `concurrent.futures.ThreadPoolExecutor` class can correctly use the specified concurrency level.</li>
                                            <li>The test fails when the `max_concurrency` value exceeds 1000, indicating a bug in Pyproject.toml loading.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348-350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_max_tests</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `max_tests` setting in the `pyproject.toml` file is correctly loaded.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the maximum number of tests is not properly set, potentially leading to coverage issues.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The contents of the `pyproject.toml` file are correct and match the expected values.</li>
                                            <li>The `max_tests` setting in the `pyproject.toml` file is correctly loaded into the test environment.</li>
                                            <li>The number of tests loaded matches the maximum value specified in the `pyproject.toml` file.</li>
                                            <li>No coverage issues are reported when running the tests with the correct maximum number of tests.</li>
                                            <li>The test environment has the required configuration for loading the maximum number of tests.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346-348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_metadata_file</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398-400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_ollama_host</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `ollama_host` is loaded correctly from the `pyproject.toml` file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where the `ollama_host` is not loaded due to a missing or corrupted `pyproject.toml` file.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The contents of the `pyproject.toml` file are correct and match the expected values.</li>
                                            <li>The `pyproject.toml` file exists at the specified path.</li>
                                            <li>The `ollama_host` is present in the `pyproject.toml` file with the correct value.</li>
                                            <li>The `ollama_host` value matches the expected value from the `ollama.yaml` file.</li>
                                            <li>The `pyproject.toml` file is not corrupted or missing any necessary files.</li>
                                            <li>The `pyproject.toml` file is written to the specified path without any issues.</li>
                                            <li>The `ollama_host` value in the `pyproject.toml` file matches the expected value from the `ollama.yaml` file.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">66 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300-302, 304-305, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_omit_tests_from_coverage</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `load_omit_tests_from_coverage` function in `tests/test_options_coverage.py` can be loaded successfully from a PyProject.toml file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `load_omit_tests_from_coverage` function is not able to load correctly from a PyProject.toml file, potentially causing coverage issues.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `load_omit_tests_from_coverage` function should be able to read and write data from the PyProject.toml file successfully.</li>
                                            <li>The `load_omit_tests_from_coverage` function should not raise any exceptions when loading from a valid PyProject.toml file.</li>
                                            <li>The `load_omit_tests_from_coverage` function should correctly omit tests from coverage reports for the specified configuration.</li>
                                            <li>The `load_omit_tests_from_coverage` function should handle invalid or missing PyProject.toml files without raising an exception.</li>
                                            <li>The `load_omit_tests_from_coverage` function should not modify any existing coverage reports.</li>
                                            <li>The `load_omit_tests_from_coverage` function should preserve the original configuration settings in case of a failure to load.</li>
                                            <li>The `load_omit_tests_from_coverage` function should log any errors that occur during loading, if necessary.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">66 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362-364, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_param_value_max_chars</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `param_value_max_chars` value is loaded correctly from the PyProject.toml file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `param_value_max_chars` value is not loaded, potentially leading to coverage issues or incorrect results.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The content of the 'pyproject.toml' file matches the expected string.</li>
                                            <li>The 'max_chars' attribute of the 'param_value_max_chars' object in the PyProject.toml file matches the expected value.</li>
                                            <li>The 'value' attribute of the 'param_value_max_chars' object in the PyProject.toml file contains the correct maximum character count.</li>
                                            <li>The 'type' attribute of the 'param_value_max_chars' object in the PyProject.toml file is set to 'str'.</li>
                                            <li>The 'default' value of the 'max_chars' attribute of the 'param_value_max_chars' object in the PyProject.toml file is not set to a default value.</li>
                                            <li>The 'default' value of the 'max_chars' attribute of the 'param_value_max_chars' object in the PyProject.toml file is less than or equal to 100 characters.</li>
                                            <li>The 'max_chars' attribute of the 'param_value_max_chars' object in the PyProject.toml file contains a string with fewer than 100 characters.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342-343, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_report_collect_only</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370-372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_timeout_seconds</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `timeout_seconds` value in `pyproject.toml` is correctly loaded when the test runs for a specified number of seconds.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the timeout value is not updated correctly if the test runs for an extended period.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `timeout_seconds` value in `pyproject.toml` is written to disk and can be read back by the test.</li>
                                            <li>The `timeout_seconds` value is set to a non-zero value when the test starts and remains unchanged even after the test runs for an extended period.</li>
                                            <li>The `timeout_seconds` value is correctly updated when the test runs for a specified number of seconds (e.g., 5 seconds).</li>
                                            <li>The `timeout_seconds` value is not updated if the test runs for less than 5 seconds.</li>
                                            <li>The `timeout_seconds` value in `pyproject.toml` can be read back even after the test has finished running.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300, 302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352-354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_options_extended.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">13 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_aggregation_settings</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test Config with aggregation settings.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in aggregation settings configuration.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `aggregate_dir` attribute is set to `/reports`.</li>
                                            <li>The `aggregate_policy` attribute is set to 'merge'.</li>
                                            <li>The `aggregate_include_history` attribute is set to True.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_all_output_paths</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that all output paths are set correctly in the test configuration.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the report generation process fails to produce expected output files due to missing or incorrect input configurations.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `report_html` attribute is set to 'report.html'.</li>
                                            <li>The `report_json` attribute is set to 'report.json'.</li>
                                            <li>The `report_pdf` attribute is set to 'report.pdf'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_capture_settings</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that `capture_failed_output` is set to `True`.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where the captured output exceeds the maximum allowed length of 8000 characters.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>config.capture_failed_output is True</li>
                                            <li>config.capture_output_max_chars is 8000</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_compliance_settings</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `Config` object correctly sets its metadata file and HMAC key file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the configuration is not set with the expected files.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `metadata_file` attribute of the `Config` object should be set to 'metadata.json'.</li>
                                            <li>The `hmac_key_file` attribute of the `Config` object should be set to 'key.txt'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_coverage_settings</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Tests the configuration of coverage settings.</p>
                                    <p><strong>Why Needed:</strong> Prevents a regression where the test coverage settings are not correctly configured.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>config.omit_tests_from_coverage is set to False</li>
                                            <li>config.include_phase is set to "all"</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_custom_exclude_globs</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `llm_context_exclude_globs` attribute of the `Config` class is correctly populated with custom exclude globs.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a bug where the custom exclude globs are not properly propagated to the LLM context configuration.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `*.pyc` glob should be present in the `llm_context_exclude_globs` list.</li>
                                            <li>The `*.log` glob should be present in the `llm_context_exclude_globs` list.</li>
                                            <li>All custom exclude globs should be included in the `llm_context_exclude_globs` list.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_include_globs</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `include_globs` option includes only `.py` files.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `include_globs` option is not correctly applied, potentially leading to unexpected behavior or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `*.py` glob matches any file with a `.py` extension in the specified directory.</li>
                                            <li>The `*.pyi` glob matches any file with an `.pyi` extension in the specified directory.</li>
                                            <li>The `include_globs` option includes only files that match either of these patterns.</li>
                                            <li>If the `include_globs` option does not include all required files, it will fail to compile or run correctly.</li>
                                            <li>Including non-`.py` and `*.pyi` files in the `include_globs` list can cause issues with LLMs.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_invocation_settings</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 123)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_execution_settings</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the LLM execution settings are correctly configured.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the LLM execution settings are not properly set, potentially leading to performance issues or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The value of llm_max_tests is indeed 50.</li>
                                            <li>The value of llm_max_concurrency is indeed 8.</li>
                                            <li>The value of llm_requests_per_minute is indeed 12.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_param_settings</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Configures the LLM with param settings.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the LLM parameter values exceed the maximum allowed length.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>config.llm_include_param_values is True</li>
                                            <li>config.llm_param_value_max_chars == 200</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_llm_settings</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the configuration of LLM settings for the OLLAMA provider.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the model and context bytes are not correctly configured when using the OLLAMA provider.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `provider` attribute is set to 'ollama'.</li>
                                            <li>The `model` attribute is set to 'llama3.2'.</li>
                                            <li>The `llm_context_bytes` attribute is set to 64KB (64000).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_repo_root_path</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `repo_root` attribute of a `TestConfigAnnotations` instance is correctly set to the specified path.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `repo_root` attribute is not set correctly, potentially leading to incorrect configuration or unexpected behavior.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>config.repo_root is equal to Path('/project')</li>
                                            <li>config.repo_root is an instance of `path.Path`</li>
                                            <li>config.repo_root is a valid path</li>
                                            <li>config.repo_root is not empty</li>
                                            <li>config.repo_root does not start with a slash</li>
                                            <li>config.repo_root is not relative</li>
                                            <li>config.repo_root is not absolute</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_extended.py::TestConfigAnnotations::test_valid_phase_values</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the `test_valid_phase_values` function to ensure it validates include_phase values correctly.</p>
                                    <p><strong>Why Needed:</strong> This test prevents potential issues where invalid or missing include_phase values could cause validation failures.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>All 'run', 'setup', and 'teardown' phases should be included in the configuration without any errors.</li>
                                            <li>The 'all' phase should not have any errors if it is valid.</li>
                                            <li>Any invalid or missing include_phase value should be excluded from validation results.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_options_maximal.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">10 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_exclude_globs</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the default exclude globs returned by `Config().llm_context_exclude_globs` match the expected values.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the default exclude globs are not correctly set, potentially leading to incorrect analysis or errors in the model.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `*.pyc` should be included in the default exclude globs.</li>
                                            <li>The function `__pycache__/*` should also be included in the default exclude globs.</li>
                                            <li>The string `*secret*` should be included in the default exclude globs.</li>
                                            <li>The string `*password*` should be included in the default exclude globs.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_redact_patterns</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test default redact patterns with various configuration options.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the default redact patterns do not include sensitive information like passwords and tokens, potentially exposing user credentials or API keys.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `--password` pattern should be present in the default redact patterns.</li>
                                            <li>The `--token` pattern should be present in the default redact patterns.</li>
                                            <li>The `--api[_-]?key` pattern should be present in the default redact patterns.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_values</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the default values of the `get_default_config()` function are correct.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the default values might not be set correctly, potentially leading to incorrect behavior or errors in subsequent tests.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>config.provider == 'none'</li>
                                            <li>config.llm_context_mode == 'minimal'</li>
                                            <li>config.llm_context_bytes == 32000</li>
                                            <li>config.omit_tests_from_coverage is True</li>
                                            <li>config.include_phase == 'run'</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 261)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_maximal.py::TestConfigHelpersMaximal::test_is_llm_enabled</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that the `is_llm_enabled` method returns False for a provider without an LLM, and True for providers with LLMs.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in case a new provider is added without an LLM.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `is_llm_enabled` method should return `False` when the provider has no known LLM.</li>
                                            <li>The `is_llm_enabled` method should return `True` when the provider has an LLM (e.g., 'ollama', 'litellm', or 'gemini').</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_aggregate_policy</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the validation of an invalid aggregate policy.</p>
                                    <p><strong>Why Needed:</strong> To prevent a potential bug where an invalid aggregate policy is passed to the Config class, causing unexpected behavior or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The configuration object should have exactly one error message.</li>
                                            <li>The error message should contain 'Invalid aggregate_policy 'invalid''</li>
                                            <li>The error message should be present in the first error message of the list.</li>
                                            <li>The error message should not be empty.</li>
                                            <li>The error message should not be a string.</li>
                                            <li>The error message should not be a file path or any other type of object.</li>
                                            <li>The configuration object should have been validated successfully before this test.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">23 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-213, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_context_mode</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">23 lines (ranges: 123, 163, 191, 194-195, 201-205, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_include_phase</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the `validate` method of the `Config` class to ensure it correctly identifies and reports invalid include phases.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where an invalid include phase is silently ignored, potentially leading to incorrect configuration or errors in downstream code.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `validate` method returns exactly one error for the given invalid include phase.</li>
                                            <li>The error message contains the string 'Invalid include_phase 'invalid' which indicates the specific issue with the input value.</li>
                                            <li>The test asserts that at least one error is found in the list of errors returned by the `validate` method.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">23 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-221, 225-226, 233, 237, 239, 241, 243, 245, 248)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_provider</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">22 lines (ranges: 123, 163, 191, 194-197, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_numeric_bounds</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Should return errors for invalid numeric values.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the Config class does not validate its inputs correctly, potentially leading to unexpected behavior or errors when using the config.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>llm_context_bytes must be a non-negative integer.</li>
                                            <li>llm_max_tests must be an integer greater than 0.</li>
                                            <li>llm_requests_per_minute must be an integer greater than 0.</li>
                                            <li>llm_timeout_seconds must be a positive integer.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">24 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237-245, 248)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_valid_config</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Tests the `validate` method of the `Config` class with a valid configuration.</p>
                                    <p><strong>Why Needed:</strong> Prevents potential infinite recursion or other unexpected behavior in case of an invalid configuration.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `validate()` method should return an empty list for a well-formed, valid configuration.</li>
                                            <li>The `validate()` method should not throw any exceptions for a valid configuration.</li>
                                            <li>The method should correctly handle cases where the input is malformed or inconsistent.</li>
                                            <li>The method should not attempt to perform any actions on the input that are not valid for the given configuration.</li>
                                            <li>Any potential side effects of the `validate()` method should be properly cleaned up after use.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_plugin_integration.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">14 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_integration.py::TestPluginConfigLoading::test_config_defaults</span>
                            <div class="test-meta">
                                <span>3ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the config defaults to safe settings when no options are provided.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the config is set to unexpected or insecure values without explicit configuration.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `cfg` variable should be an instance of `Config`.</li>
                                            <li>The `cfg` variable should not have any explicitly registered options.</li>
                                            <li>The `cfg` variable should have safe defaults for all settings.</li>
                                            <li>The `cfg` variable should not contain any sensitive or insecure data.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">80 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300-305, 308, 310, 312, 316, 320, 324, 328-330, 332, 334, 336, 340, 342, 346-348, 350, 352-354, 356, 358, 362-364, 366-367, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414-415, 417-418, 420-421, 424-430, 432, 434, 436, 440, 442, 444-446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_integration.py::TestPluginConfigLoading::test_markers_exist_in_config</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `pytestconfig` object exists and is not None.</p>
                                    <p><strong>Why Needed:</strong> Prevent a potential bug where the plugin configuration is inaccessible due to a missing or corrupted configuration.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert pytestconfig is not None</li>
                                            <li>assert isinstance(pytestconfig, pytest.Config)</li>
                                            <li>pytest.config.dictConfig() should return a dictionary-like object</li>
                                            <li>pytest.config.getopt_list() should return an empty list if no options are provided</li>
                                            <li>pytest.config.addoption() should add the correct option to the config</li>
                                            <li>pytest.config.getopt_list() should correctly parse the added options</li>
                                            <li>pytest.config.dictConfig() should correctly convert the parsed options to a dictionary</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_both_json_and_html_outputs</span>
                            <div class="test-meta">
                                <span>87ms</span>
                                <span title="Covered file count">üõ°Ô∏è 8</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test generates both JSON and HTML reports for a plugin.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in case the plugin does not generate either report type.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'report.json' file exists at the specified path.</li>
                                            <li>The 'report.html' file exists at the specified path.</li>
                                            <li>The 'report.json' file is generated correctly by the plugin.</li>
                                            <li>The 'report.html' file is generated correctly by the plugin and matches the content of the 'report.json' file.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">75 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">46 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424-428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">152 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">117 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_collection_finish_counts_items</span>
                            <div class="test-meta">
                                <span>59ms</span>
                                <span title="Covered file count">üõ°Ô∏è 7</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">75 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">45 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424, 426-428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">152 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_creates_nested_directory</span>
                            <div class="test-meta">
                                <span>58ms</span>
                                <span title="Covered file count">üõ°Ô∏è 7</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that output directories are created if missing.</p>
                                    <p><strong>Why Needed:</strong> Prevents a bug where the test fails due to an empty directory being reported as created.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `report.json` file should be present in the 'nested' directory.</li>
                                            <li>The `report.json` file should not be empty.</li>
                                            <li>The `report.json` file should exist in the specified path.</li>
                                            <li>The test should fail if an empty directory is reported as created.</li>
                                            <li>The test should pass if a non-empty directory is reported as created.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">81 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 235-237, 239, 241, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516-518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">45 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424, 426-428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">152 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">111 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-477, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_fixture_error_captured</span>
                            <div class="test-meta">
                                <span>169ms</span>
                                <span title="Covered file count">üõ°Ô∏è 7</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that fixture errors are captured in report.</p>
                                    <p><strong>Why Needed:</strong> Prevents a regression where the test fails with an unhandled error.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'summary' key in the report contains the number of failed fixtures.</li>
                                            <li>The 'error' key in the 'summary' dictionary is set to 1 if there are any errors.</li>
                                            <li>The 'status' key in the 'summary' dictionary is set to 'error' if there are any errors.</li>
                                            <li>The 'name' key in the 'summary' dictionary contains a string indicating that an error occurred.</li>
                                            <li>The 'message' key in the 'summary' dictionary contains a message explaining why the test failed.</li>
                                            <li>The 'test_name' key in the 'summary' dictionary contains the name of the test that failed.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">50 lines (ranges: 78-79, 90, 93-94, 96, 99-103, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">76 lines (ranges: 162, 166-171, 173-175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">45 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424, 426-428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">152 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">110 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323, 325, 327-328, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_makereport_captures_all_outcomes</span>
                            <div class="test-meta">
                                <span>172ms</span>
                                <span title="Covered file count">üõ°Ô∏è 7</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test pytest_runtest_makereport captures all outcomes.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the report does not capture all outcomes due to an issue with the --llm-report-json flag.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'passed' outcome should be included in the report.</li>
                                            <li>The 'failed' outcome should be included in the report.</li>
                                            <li>The 'skipped' outcome should be included in the report.</li>
                                            <li>All outcomes (passed, failed, skipped) should be captured by pytest_runtest_makereport.</li>
                                            <li>The report path should contain a file named 'report.json'.</li>
                                            <li>The report path should not be empty. If it is, pytester.runpytest will raise an error.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">55 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 106-107, 109-112, 114-118, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 250-251, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">76 lines (ranges: 162, 166-171, 173-175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">45 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424, 426-428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">152 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">109 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-322, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_no_report_when_disabled</span>
                            <div class="test-meta">
                                <span>55ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">44 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">114 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225-226, 363-364, 367-368, 371-373, 384, 388, 407-408, 424-425)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_pdf_option_enables_plugin</span>
                            <div class="test-meta">
                                <span>598ms</span>
                                <span title="Covered file count">üõ°Ô∏è 8</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that --llm-pdf option enables the plugin.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in plugin key validation and ensures successful execution of plugin logic when only --llm-pdf is provided.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The test verifies that the `test_pass` function returns True without any errors.</li>
                                            <li>The test verifies that the JSON file generated by the plugin is present if we also ask for it (proving the plugin key validation passed).</li>
                                            <li>The test checks for a warning about Playwright if applicable, or ensures execution doesn't error out on 'disabled plugin'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 162)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">45 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424, 426, 428-430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">152 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">105 lines (ranges: 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 401, 410, 412, 414-416, 424-429, 432, 434-435, 448, 453, 455, 458-462, 470-471)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_session_start_records_time</span>
                            <div class="test-meta">
                                <span>61ms</span>
                                <span title="Covered file count">üõ°Ô∏è 7</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the pytest_sessionstart records start time is correctly reported in the report.json file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the start time of the session is not accurately recorded in the report.json file.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'start_time' key should be present in the run_meta dictionary with the correct value.</li>
                                            <li>The 'start_time' value should match the actual start time of the session.</li>
                                            <li>The 'start_time' value should be a valid datetime object.</li>
                                            <li>The report.json file should contain the 'start_time' key and its corresponding value.</li>
                                            <li>The 'start_time' value should not be None or empty.</li>
                                            <li>The 'start_time' value should be in the correct format (e.g., YYYY-MM-DD HH:MM:SS).</li>
                                            <li>The pytest_sessionstart records should start recording time as soon as the test is run.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">75 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">45 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424, 426-428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">152 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_llm_context_marker</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_llm_opt_out_marker</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">


                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_integration.py::TestPluginIntegration::test_requirement_marker</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the requirement marker does not cause any errors.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the requirement marker could be misinterpreted as an error.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `requirement_marker` function should not raise any exceptions when called with valid input.</li>
                                            <li>The `requirement_marker` function should not throw any errors when executed without arguments.</li>
                                            <li>The `requirement_marker` function should return a boolean value indicating whether the requirement is met or not.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_integration.py::TestReportGeneration::test_report_writer_integration</span>
                            <div class="test-meta">
                                <span>36ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the report writer integrates with pytest_llm_report models correctly.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the report writer may not generate reports for tests that fail or have errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Verify that the report writer writes a JSON file containing the correct summary statistics.</li>
                                            <li>Verify that the report writer includes all nodes in the report.</li>
                                            <li>Verify that the report writer includes the expected error message in case of an error.</li>
                                            <li>Check if the total number of tests is 2 (passed + failed) as expected.</li>
                                            <li>Check if only test_a.py and test_b.py are included in the HTML report as expected.</li>
                                            <li>Verify that the report writer can write a valid JSON file even when there are no tests to generate reports for.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">81 lines (ranges: 162, 166-171, 173-175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">131 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_plugin_maximal.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">26 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_disabled</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that collectreport skips when disabled and pytest_collectreport is mocked correctly.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where collectreport fails to run due to the plugin being disabled.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `pytest_collectreport` function is called with the `_enabled_key` as an argument and `False` as its second argument.</li>
                                            <li>The `get` method of `stash.get` is called on the mock report object with the `_enabled_key` key.</li>
                                            <li>The `session.config.stash.get` method is called with the `_enabled_key` key and `False` as its second argument.</li>
                                            <li>The `assert_called_with` method is called on the mock report object to verify it was called correctly.</li>
                                            <li>The `pytest_collectreport` function does not raise an exception when called with a disabled collectreport.</li>
                                            <li>The `pytest_collectreport` function does not modify the pytest session configuration.</li>
                                            <li>The `pytest_collectreport` function does not raise an exception when called without a mock report object.</li>
                                            <li>The `_enabled_key` is correctly set to `'collectreport'` on the mock report object.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 363-364, 367, 371-373, 384-385, 391-392)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_enabled</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that collectreport calls collector when enable is True.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the plugin does not collect reports even when enabled.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `pytest_collectreport` function should call `_collector_key` stash to get the collector instance.</li>
                                            <li>The `pytest_collectreport` function should call `_enabled_key` stash to get the enable flag.</li>
                                            <li>The `handle_collection_report` method of the collector instance should be called with a valid report object.</li>
                                            <li>The `session.config.stash.get` mock returns True for `_enabled_key` and False for `_collector_key` when enabled.</li>
                                            <li>The `handle_collection_report` method of the collector instance should not raise an exception when called with a valid report object.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 363-364, 367, 371-373, 384-385, 391, 395-397)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_no_session</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that `pytest_collectreport` does not throw an exception when a mock report object is created without a 'session' attribute.</p>
                                    <p><strong>Why Needed:</strong> Prevent regression in the `pytest_collectreport` plugin, which may cause it to skip collect reports when no session is available.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `pytest_collectreport(mock_report)` should not be called with an argument that has a 'session' attribute.</li>
                                            <li>The mock report object created without a 'session' attribute should not have a 'session' attribute.</li>
                                            <li>The mock report object created without a 'session' attribute should not raise any exceptions.</li>
                                            <li>The function `pytest_collectreport(mock_report)` should return without raising an exception when called with the mock report object.</li>
                                            <li>The function `pytest_collectreport(mock_report)` should call the original `collect` method on the mock report object if it exists.</li>
                                            <li>The function `pytest_collectreport(mock_report)` should not modify the mock report object in any way.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 363-364, 367, 371-373, 384, 388)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_session_none</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `pytest_collectreport` function does not throw an exception when a `None` session is passed.</p>
                                    <p><strong>Why Needed:</strong> Prevent regression in case of a `None` session, ensuring consistent behavior across different test environments.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `pytest_collectreport` function should not raise an error when given a `None` session.</li>
                                            <li>The `session` attribute of the mock report object should be set to `None` without any exceptions.</li>
                                            <li>No other errors or warnings should be raised in the test environment.</li>
                                            <li>The `pytest_collectreport` function should behave as expected when given a `None` session, without any side effects.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 363-364, 367, 371-373, 384, 388)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_llm_enabled_warning</span>
                            <div class="test-meta">
                                <span>3ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the 'pytest_llm_report' plugin raises a warning when LLM is enabled in the pyproject.toml file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the 'pytest_llm_report' plugin does not raise an error when LLM is enabled, potentially leading to silent failures or unexpected behavior.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'pytest_llm_report' plugin raises a warning in the pyproject.toml file when LLM is enabled.</li>
                                            <li>The warning message is related to the 'LLM' setting in the pyproject.toml file.</li>
                                            <li>The warning is not suppressed by any configuration options provided by the 'pytest_llm_report' plugin.</li>
                                            <li>The error is only raised for the specified test module and function.</li>
                                            <li>The warning does not affect the overall functionality of the test suite.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">86 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-293, 295-296, 300-304, 308, 310, 312, 316, 320, 324, 328-330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">30 lines (ranges: 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184-186, 188-189, 193-195, 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_validation_errors</span>
                            <div class="test-meta">
                                <span>3ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Testing the `pytest_configure` function to ensure it raises a `UsageError` when encountering invalid configuration files.</p>
                                    <p><strong>Why Needed:</strong> This test prevents potential regression in the plugin's behavior when encountering malformed configuration files.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `pytest_configure` function should raise a `UsageError` with an appropriate error message when given an invalid pyproject.toml file.</li>
                                            <li>The error message should be informative and clearly indicate that the provided configuration is invalid.</li>
                                            <li>The test should fail when attempting to run pytest with an invalid configuration file, indicating a bug in the plugin's validation logic.</li>
                                            <li>The `UsageError` should be raised with a specific exception name (e.g., `UsageError`) rather than just raising any exception.</li>
                                            <li>The error message should include details about the invalid configuration file, such as its contents or location.</li>
                                            <li>The test should only fail when attempting to run pytest with an invalid configuration file and not when running it with valid files.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">85 lines (ranges: 123, 163, 191, 194-197, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 276, 279-280, 288-293, 295-296, 300-302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-180, 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_worker_skip</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that configure skips on xdist workers.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the plugin might skip configuration on xdist workers due to an unhandled marker call.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>mock_config.addinivalue_line was not called before worker check</li>
                                            <li>addinivalue_line is still called for markers after worker check</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">17 lines (ranges: 150-152, 154-156, 158-160, 164-165, 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginConfigureFallback::test_pytest_configure_fallback_load</span>
                            <div class="test-meta">
                                <span>3ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">30 lines (ranges: 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184-186, 188-189, 193-195, 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_cli_overrides_pyproject</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that CLI options can override PyProject.toml settings.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where CLI options do not override PyProject.toml settings, leading to inconsistent configuration.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>pyproject.toml is created with the specified content.</li>
                                            <li>CLI options are correctly overridden by PyProject.toml settings.</li>
                                            <li>PyProject.toml settings are not overwritten by default.</li>
                                            <li>The test passes even when CLI options override PyProject.toml settings.</li>
                                            <li>The test fails when CLI options do not override PyProject.toml settings.</li>
                                            <li>The test ensures that CLI options take precedence over PyProject.toml settings.</li>
                                            <li>CLI options can be used to override specific settings in PyProject.toml.</li>
                                            <li>PyProject.toml is updated correctly after CLI option overrides.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">78 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300-302, 304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414-415, 417-418, 420-421, 424-436, 440-448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_from_pyproject</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the plugin can successfully load all options from the provided pyproject.toml file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where plugins may not be able to access or parse configuration files in certain scenarios.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>pyproject.toml should be created with the correct content.</li>
                                            <li>The plugin should be able to read and parse the pyproject.toml file correctly.</li>
                                            <li>All options specified in the pyproject.toml file should be accessible and usable by the plugin.</li>
                                            <li>Any configuration settings or values defined in the pyproject.toml file should be applied to the plugin's configuration.</li>
                                            <li>The plugin's configuration should not be corrupted or lost due to issues with the pyproject.toml file.</li>
                                            <li>Error messages from the plugin should indicate a successful load of the config from pyproject.toml.</li>
                                            <li>The plugin's behavior and functionality should remain unaffected by issues with the pyproject.toml file.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">68 lines (ranges: 123, 163, 276, 279-280, 288-293, 295-296, 300-304, 308, 310, 312, 316, 320, 324, 328-330, 332, 334, 336, 340, 342, 346, 348, 350-352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_disabled</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that terminal summary skips when plugin is disabled.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression where the plugin's terminal summary is not properly handled when it is disabled.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The stash.get method of the stash object was called with _enabled_key and False.</li>
                                            <li>The stash.get method of the stash object was called with _enabled_key and True.</li>
                                            <li>The stash.get method of the stash object was not called at all when _enabled_key is False.</li>
                                            <li>The stash.get method of the stash object was called once with _enabled_key as None.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">9 lines (ranges: 221, 225-226, 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_worker_skip</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that terminal summary skips on xdist worker when configured correctly.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the plugin does not skip terminal summaries on xdist workers due to incorrect configuration.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `pytest_terminal_summary` function returns None for mock_config.workerinput = {'workerid': 'gw0'}</li>
                                            <li>The `terminal_summary_worker_skip` method is called with a mock result of None</li>
                                            <li>The plugin does not skip terminal summaries on xdist workers due to incorrect configuration</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 221-222, 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginMaximal::testload_config</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">30 lines (ranges: 123, 163, 276, 279-280, 288-290, 414-415, 417-418, 420-421, 424-428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_disabled</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test makereport skips when disabled.</p>
                                    <p><strong>Why Needed:</strong> The test prevents a regression where the plugin does not report any errors when makereport is disabled.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>mock_item.config.stash.get() returns False</li>
                                            <li>mock_call.call() should have been called with a mock result</li>
                                            <li>mock_outcome.get_result().should.have.been.called_with(mocked_result)</li>
                                            <li>gen.send(mock_outcome).should.have.been.called_once_with</li>
                                            <li>mock_call.call().should.have.been.called_twice</li>
                                            <li>mock_item.config.stash.get() should have been called twice</li>
                                            <li>mock_outcome.get_result().should.have.been.called_twice</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 363-364, 367-368, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_enabled</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that makereport calls collector when enabled.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the plugin does not collect and report test results even if makereport is enabled.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `pytest_runtest_makereport` function should be able to find the `_collector_key` key in the mock item's configuration.</li>
                                            <li>The `pytest_runtest_makereport` function should be able to find the `_enabled_key` key in the mock item's configuration and return True if it is enabled.</li>
                                            <li>The `pytest_runtest_makereport` function should call the `handle_runtest_logreport` method on the mock collector with the provided report and item.</li>
                                            <li>The `handle_runtest_logreport` method of the mock collector should be called once with the provided report and item.</li>
                                            <li>The `get_result` method of the mock outcome should return a result that is not None when the test is run.</li>
                                            <li>The `send` method of the mock outcome should not raise an exception when the test is run successfully.</li>
                                            <li>The `stash_get` function of the mock item's configuration should be able to find the `_enabled_key` and `_collector_key` keys.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_disabled</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that collection_finish is skipped when disabled.</p>
                                    <p><strong>Why Needed:</strong> The test prevents a potential regression where collection_finish may not be executed correctly if it's disabled.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>mock_session.config.stash.get.assert_called_with(_enabled_key, False)</li>
                                            <li>pytest_collection_finish(mock_session) was called with the correct arguments</li>
                                            <li>collection_finish is skipped when stash.get returns False</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 363-364, 367, 371-373, 407-408)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_enabled</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that collection_finish is called when enabled.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the collector is not called when collection_finish is enabled.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The stash_get function returns True for _enabled_key and mock_collector.</li>
                                            <li>The stash_get function returns mock_collector for _collector_key.</li>
                                            <li>collection_finish is called once with mock_session.items.</li>
                                            <li>mock_collector.handle_collection_finish is called once with mock_session.items.</li>
                                            <li>mock_collector.handle_collection_finish is not called twice when collection_finish is enabled.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 363-364, 367, 371-373, 407, 411-413)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_disabled</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that pytest_sessionstart skips when disabled and checks enabled status.</p>
                                    <p><strong>Why Needed:</strong> To prevent a regression where pytest_sessionstart fails to check the plugin's enabled status when it is disabled.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>mocked session config stash.get was called with _enabled_key and False</li>
                                            <li>mocked session config stash.get was called with _enabled_key and True</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 363-364, 367, 371-373, 424-425)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_enabled</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that sessionstart initializes collector when enabled and creates a stash with the necessary keys.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where pytest_sessionstart is not initialized or does not create a stash with the required keys.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The _collector_key should be present in the mock_stash dictionary.</li>
                                            <li>The _start_time_key should also be present in the mock_stash dictionary.</li>
                                            <li>If sessionstart is called without enabling pytest_sessionstart, the collector should not be created.</li>
                                            <li>If sessionstart is called with an invalid stash, it should raise an error.</li>
                                            <li>The stash dictionary should have the correct keys: _enabled_key and _config_key.</li>
                                            <li>The stash dictionary should have a valid Config object as its value for _config_key.</li>
                                            <li>The stash dictionary should not have any other keys besides _enabled_key and _config_key.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 363-364, 367, 371-373, 424, 428, 431, 433-434)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption</span>
                            <div class="test-meta">
                                <span>3ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test pytest_addoption adds expected arguments to the command line.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a bug where pytest_addoption does not add the 'LLM-enhanced test reports' and 'LLM-coverage-source' options to the command line if they are not provided.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>parser.getgroup.assert_called_with('llm-report', 'LLM-enhanced test reports')</li>
                                            <li>calls.any('--llm-report in args[0]')</li>
                                            <li>calls.any('--llm-coverage-source in args[0]')</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">84 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption_no_ini</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 2</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test verifies that pytest_addoption no longer adds INI options.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where pytest_addoption would add INI options to the plugin.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `addini` method of the parser is not called.</li>
                                            <li>The `parser.addini` call is removed from the test.</li>
                                            <li>The `pytest_addoption.parser.addini` method is no longer available in this version of pytest_llm_report.plugin.</li>
                                            <li>The plugin does not add INI options to the terminal summary if `pytest_addoption.no_ini` is set.</li>
                                            <li>When `pytest_addoption.no_ini` is set, the plugin should not modify the terminal summary.</li>
                                            <li>The plugin's behavior changes when `pytest_addoption.no_ini` is set.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">84 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_coverage_calculation</span>
                            <div class="test-meta">
                                <span>3ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">53 lines (ranges: 221, 225, 229, 232, 251-252, 254, 256, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-288, 290, 292-295, 307-308, 313-314, 341-351, 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_llm_enabled</span>
                            <div class="test-meta">
                                <span>3ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test terminal summary with LLM enabled runs annotations.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in case the 'llm' provider is not properly configured or if the report HTML file is corrupted.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Verify that the correct configuration is passed to pytest_terminal_summary.</li>
                                            <li>Check if the mock stash matches the expected stash.</li>
                                            <li>Assert that the annotate_tests method is called once with the provided config.</li>
                                            <li>Verify that the correct model name is used for the LLM provider.</li>
                                            <li>Check if the get_provider method returns the correct provider instance.</li>
                                            <li>Verify that the mock writer class is created correctly and has a return value of mock_writer.</li>
                                            <li>Assert that the annotate method is called once with the provided config.</li>
                                            <li>Verify that the mock terminal reporter's stats dictionary contains the expected key-value pair.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">59 lines (ranges: 221, 225, 229, 232, 251-252, 254, 256, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307-308, 313-316, 319, 321, 324-326, 333-338, 341-351, 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_no_collector</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test terminal summary creates collector if missing.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the plugin does not create a collector even when it is supposed to be missing.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The stash `_enabled_key` is set to `True` before calling `pytest_terminal_summary()`</li>
                                            <li>The stash `_config_key` contains the configuration object `cfg` after calling `pytest_terminal_summary()`</li>
                                            <li>The coverage map returned by `mock_mapper.map_coverage()` does not contain any coverage data when called with a mock reporter and no collector present</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">45 lines (ranges: 221, 225, 229, 232, 251-252, 254, 256, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_with_aggregation</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">21 lines (ranges: 221, 225, 229, 232-233, 235-236, 239-240, 242, 244-248, 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_plugin_maximal.py::TestPluginTerminalSummaryErrors::test_terminal_summary_coverage_error</span>
                            <div class="test-meta">
                                <span>4ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test coverage calculation error when loading coverage map.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the coverage calculation fails due to an OSError during load.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `pytest_terminal_summary` function is called with a mock `CoverageMapper` instance.</li>
                                            <li>A warning is raised for failed coverage computation.</li>
                                            <li>The `load` method of the `Coverage` class raises an exception.</li>
                                            <li>The `coverage_map` attribute of the `CoverageMapper` instance is not set to None.</li>
                                            <li>The `report_writer` attribute of the `ReportWriter` instance is not set to None.</li>
                                            <li>The `MagicMock()` object passed to `pytest_terminal_summary` has a `workerinput` attribute that is deleted.</li>
                                            <li>The `Config` object passed to `pytest_terminal_summary` has a `stash` attribute with a non-None value.</li>
                                            <li>The `CoverageMapper` instance has a `coverage_map` attribute set to an empty dictionary.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">16 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 123, 163, 252)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">52 lines (ranges: 221, 225, 229, 232, 251-252, 254, 256, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-288, 298-301, 307-308, 313-314, 341-351, 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_prompts.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">6 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_balanced_context</span>
                            <div class="test-meta">
                                <span>7ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test assembling a balanced context for `test_assemble_balanced_context` test function</p>
                                    <p><strong>Why Needed:</strong> Prevents regression due to unbalanced contexts, where the assembler may not correctly assemble the dependencies.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'utils.py' file is present in the assembled context.</li>
                                            <li>The 'def util()' function is found in the 'utils.py' module of the assembled context.</li>
                                            <li>The coverage entry for 'utils.py' indicates that it was covered by the test.</li>
                                            <li>The assembler correctly assembles the dependencies, including 'test_a.py' and 'utils.py'.</li>
                                            <li>The 'def util()' function is present in the correct location within the 'utils.py' module.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">51 lines (ranges: 33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-155, 158-159, 163, 191-192, 194)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_complete_context</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that the ContextAssembler can assemble a complete context for a test file with a single test function.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression when the llm_context_mode is set to 'complete' and the test file has only one test function.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The source code of the test file should contain the specified test function.</li>
                                            <li>The ContextAssembler should be able to assemble a complete context for the test file.</li>
                                            <li>The assembled context should include the specified test function.</li>
                                            <li>The assembler should report an error if the test file has multiple test functions or other non-test code.</li>
                                            <li>The assembler should not report any errors when assembling a complete context with only one test function.</li>
                                            <li>The assembler should be able to assemble a complete context for a test file with a single test function even if it is in a different module than the test function.</li>
                                            <li>The assembler should be able to assemble a complete context for a test file with multiple test functions and other non-test code.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">34 lines (ranges: 33, 49, 52, 55, 58, 60, 63, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 132-133, 180)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_assemble_minimal_context</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Assembles a minimal context for testing `test_1` function in `test_a.py`.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the minimal context is not correctly assembled, potentially leading to incorrect test results or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'test_1' function should be found in the source code of `test_a.py`.</li>
                                            <li>The assembly result should contain only the `test_1` function definition.</li>
                                            <li>The context object should be an empty dictionary, indicating no dependencies are being assembled.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">30 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_balanced_context_limits</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that the ContextAssembler does not exceed the specified context limits when assembling a test file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents potential memory leaks or unexpected behavior due to excessive context size exceeding the configured limit.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The assembled context contains only 'f1.py' and does not contain any truncated content.</li>
                                            <li>The length of the assembled context is within the specified limit (40 bytes).</li>
                                            <li>No truncation message is present in the assembled context.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">34 lines (ranges: 33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-84, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-156, 158-159, 163, 191-192, 194)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_get_test_source_edge_cases</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the ContextAssembler's _get_test_source method with edge cases.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the assembler does not correctly handle non-existent files or nested test names with parameters.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function returns an empty string when given a non-existent file path.</li>
                                            <li>The function correctly extracts the test name and parameter from the source code of a nested test.</li>
                                            <li>The function includes all necessary keywords in its output.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 33, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts.py::TestContextAssembler::test_should_exclude</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the ContextAssembler should exclude certain files from being processed by LLM.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the ContextAssembler incorrectly excludes important files, leading to incorrect results or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert assembler._should_exclude('test.pyc') is True</li>
                                            <li>assert assembler._should_exclude('secret/key.txt') is True</li>
                                            <li>assert assembler._should_exclude('public/readme.md') is False</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 33, 191-194)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_prompts_coverage.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">12 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_assemble_minimal_mode</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test assemble minimal mode returns no context files.</p>
                                    <p><strong>Why Needed:</strong> Prevents a regression where assemble in minimal mode does not generate any context files.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>context_files is an empty list when config llm_context_mode = 'minimal'.</li>
                                            <li>test_source contains the function definition of test_foo.</li>
                                            <li>test_source does not contain any import statements or other code that could be used to create a context file.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">29 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_assemble_with_context_override</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">50 lines (ranges: 33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-155, 158-159, 163, 191-192, 194)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_excludes_patterns</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test balanced context excludes files matching exclude patterns.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the LLM context is not excluded from coverage when it should be.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `assembler._get_balanced_context` returns an empty dictionary when no matching files are found in the specified directory.</li>
                                            <li>The file path of the secret configuration file matches one of the exclude patterns.</li>
                                            <li>The number of lines in the secret configuration file is exactly 1.</li>
                                            <li>The line count of the secret configuration file is exactly 1.</li>
                                            <li>No matching files are found in the specified directory.</li>
                                            <li>The LLM context should be excluded from coverage when it contains a pattern that matches one of the exclude patterns.</li>
                                            <li>The output of `assembler._get_balanced_context` includes a message indicating that no matching files were found.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">16 lines (ranges: 33, 132, 135-138, 140-141, 144-145, 148-149, 163, 191-193)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_file_not_exists</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that a balanced context is skipped for a non-existent file.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the test passes when a file does not exist in the repository root.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `ContextAssembler` should return an empty context for a non-existent file.</li>
                                            <li>The file path of the non-existent file should match the expected coverage entry.</li>
                                            <li>The line ranges and line count of the coverage entry should be correct for the non-existent file.</li>
                                            <li>The test case result should indicate that the test passed with no errors or warnings.</li>
                                            <li>The context returned by the `ContextAssembler` should not contain any nodes or edges.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 33, 132, 135-138, 140-141, 144-146, 163)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_max_bytes_limit</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that balanced context respects max bytes limit.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential memory leak by ensuring the content of the context is truncated when it exceeds the maximum allowed bytes.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The length of the content in the source file `large_module.py` should be less than or equal to 120 bytes after truncation.</li>
                                            <li>A message indicating that the content was truncated should be present in the source file `large_module.py`.</li>
                                            <li>If the content is not truncated, it means the maximum allowed bytes limit has been exceeded and the content will be kept in memory.</li>
                                            <li>The content of the source file `large_module.py` should have a length less than 120 bytes after truncation.</li>
                                            <li>A message indicating that the content was truncated should be present in the source file `large_module.py` if it is truncated.</li>
                                            <li>If the content is not truncated, it means the maximum allowed bytes limit has been exceeded and the content will be kept in memory.</li>
                                            <li>The context of the test should have a length less than or equal to 120 bytes after truncation.</li>
                                            <li>A message indicating that the context was truncated should be present in the context of the test.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">22 lines (ranges: 33, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-156, 158-159, 163, 191-192, 194)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_no_coverage</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test balanced context with no coverage returns empty dict.</p>
                                    <p><strong>Why Needed:</strong> Prevents a regression where the ContextAssembler does not return any context for uncovered nodes in a test.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The ContextAssembler returns an empty dictionary when there are no uncovered nodes in the test.</li>
                                            <li>The ContextAssembler does not raise an exception or throw an error when there are no uncovered nodes in the test.</li>
                                            <li>The ContextAssembler correctly identifies and returns an empty dictionary for uncovered nodes.</li>
                                            <li>The ContextAssembler's output is consistent with the expected result for balanced contexts without coverage.</li>
                                            <li>The ContextAssembler's output is consistent with the expected result for unbalanced contexts with coverage.</li>
                                            <li>The ContextAssembler does not return any context when there are no uncovered nodes in the test, as per the test's requirements.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 33, 132-133)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_reaches_max_bytes_before_file</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that loop exits when max bytes is reached before processing file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the ContextAssembler exceeds the maximum allowed bytes in its context without encountering any errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The length of the balanced_context variable should be either 0 or 1, indicating that it has been truncated to accommodate only one file.</li>
                                            <li>If the number of files processed is greater than 1, the balanced_context variable should have a length equal to 1.</li>
                                            <li>If the ContextAssembler does not encounter any errors, the coverage report for both files should be empty.</li>
                                            <li>If the ContextAssembler encounters an error, the coverage report for the file that caused the error should be non-empty and include all lines in the file.</li>
                                            <li>The coverage report for the second file should also be non-empty and include all lines in the file.</li>
                                            <li>The ContextAssembler should not exceed the maximum allowed bytes in its context without encountering any errors.</li>
                                            <li>If the llm_context_bytes configuration is set to a value greater than 5, the ContextAssembler should still only process one file before hitting the limit.</li>
                                            <li>If the llm_context_file_limit configuration is set to a value less than or equal to 10, the ContextAssembler should not exceed the maximum allowed bytes in its context without encountering any errors.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">23 lines (ranges: 33, 132, 135-138, 140-142, 144-145, 148, 151-152, 154-156, 158-159, 163, 191-192, 194)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_complete_context_delegates_to_balanced</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `ContextAssembler` correctly delegates to a balanced configuration.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where incomplete contexts are used with unbalanced configurations.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `_get_complete_context` should return a context object containing the specified file.</li>
                                            <li>The context object should contain the same logic as the `balanced` configuration.</li>
                                            <li>The coverage entry for `module.py` should indicate that it was executed in complete contexts.</li>
                                            <li>The test case should be able to pass without any errors when using an unbalanced configuration.</li>
                                            <li>The context should not be empty or None when passed to a function that requires a complete context.</li>
                                            <li>The function `_get_complete_context` should handle cases where the specified file is not found.</li>
                                            <li>The coverage entry for `module.py` should indicate that it was executed in complete contexts, even if only one line was executed.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">22 lines (ranges: 33, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-155, 158-159, 163, 180, 191-192, 194)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_empty_nodeid</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test _get_test_source with empty nodeid returns empty string.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where an empty nodeid in the config causes the test to fail.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The input string is split into parts and any part after the split point is considered as empty.</li>
                                            <li>An empty string should be returned when no valid nodeids are found.</li>
                                            <li>The assembler's _get_test_source method should handle empty strings correctly.</li>
                                            <li>Without this test, an unexpected error might occur due to the potential bug.</li>
                                            <li>This test ensures that the assembler's behavior remains consistent with expected requirements.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">9 lines (ranges: 33, 78-79, 82-83, 86-89)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_extraction_stops_at_next_def</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 33, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 114, 116)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_file_not_exists</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that the _get_test_source method returns an empty string when a non-existent test file is provided.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the assembler does not handle cases with missing or invalid test files.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The result of calling _get_test_source with 'nonexistent.py::test_foo' as the file path and tmp_path should be an empty string.</li>
                                            <li>The method should raise a ValueError when called with a non-existent file.</li>
                                            <li>The assembler should handle cases where the test file is not found or does not exist.</li>
                                            <li>The assembler should provide a meaningful error message indicating the missing file.</li>
                                            <li>The assembler should not silently ignore the test file and return an empty string instead.</li>
                                            <li>The method should raise an exception when called with a non-existent file, rather than returning an empty string.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 33, 78-79, 82-84)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_with_class</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `_get_test_source` method correctly extracts a function from a test file with proper indentation.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the extracted function is not properly indented, leading to incorrect test source code.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should be indented according to Python's official style guide (PEP 8).</li>
                                            <li>The function name should match the expected class name.</li>
                                            <li>The function body should contain at least one line of code that matches the expected class method.</li>
                                            <li>The function should not have any trailing whitespace or unnecessary indentation.</li>
                                            <li>The function should be a valid Python function with no syntax errors.</li>
                                            <li>The extracted function should be able to be called without raising an exception.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 33, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 114, 116)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_ranges.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">13 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_consecutive_lines</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/ranges.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_duplicates</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the function correctly handles duplicate ranges.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly identifies non-duplicate ranges as duplicates.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The range '2-3' is correctly identified as a single range.</li>
                                            <li>The range '1-3' is correctly identified as a single range.</li>
                                            <li>The range '2-3' and '1-3' are not considered duplicates because they have different start values.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/ranges.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_empty_list</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Testing the `compress_ranges` function with an empty input list.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly returns a non-empty string for an empty input list.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return an empty string when given an empty list as input.</li>
                                            <li>The function should not raise any exceptions or errors when given an empty list as input.</li>
                                            <li>The function should correctly handle and return an empty string for an empty input list.</li>
                                            <li>The function should preserve the original order of elements in the input list.</li>
                                            <li>The function should ignore non-compressed ranges when returning a compressed result.</li>
                                            <li>The function should not produce incorrect results for lists with multiple consecutive empty ranges.</li>
                                            <li>The function should correctly handle and return an empty string for a single-element input list.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/ranges.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 29-30)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_mixed_ranges</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test compressing mixed ranges in the test_mixed_ranges function.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression when mixing ranges with single values.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The range '1-3' should be included in the output because it spans two numbers.</li>
                                            <li>The range '5, 10-12' should also be included because it spans multiple numbers.</li>
                                            <li>The range '15' should not be included as it only contains a single number.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/ranges.py</span>
                                        <span style="color: var(--text-secondary)">16 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_non_consecutive_lines</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that non-consecutive lines are correctly compressed into a single comma-separated string.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression when the input list contains non-consecutive line numbers.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return a comma-separated string of the input range values.</li>
                                            <li>The first and last values in the range should be included in the output.</li>
                                            <li>Any duplicate values within the range should be ignored.</li>
                                            <li>Non-consecutive values should not be separated by commas.</li>
                                            <li>The resulting string should have the same length as the original list.</li>
                                            <li>The function should handle empty input lists correctly.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/ranges.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_single_line</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that a single-line input does not use the range notation.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function incorrectly handles single-line inputs.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Input should be a list of numbers</li>
                                            <li>Output should be the same as the original number</li>
                                            <li>Range notation should not be used</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/ranges.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 29, 33, 35-37, 39, 50, 52, 65-66)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_two_consecutive</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> tests/test_ranges.py::TestCompressRanges::test_two_consecutive</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the function incorrectly handles consecutive lines without range notation.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The input list should be compressed to '1-2' using range notation.</li>
                                            <li>The output of `compress_ranges([1, 2])` should match '1-2'.</li>
                                            <li>The function should raise an error for non-consecutive lines without range notation.</li>
                                            <li>The function should handle consecutive lines correctly and return the correct compressed string.</li>
                                            <li>The input list should contain only integers.</li>
                                            <li>The output of `compress_ranges(['a', 'b'])` should match '1-2'.</li>
                                            <li>The function should not raise an error for non-consecutive lines like ['a', 2].</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/ranges.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 29, 33, 35-37, 39-40, 42, 50, 52, 65, 67)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_ranges.py::TestCompressRanges::test_unsorted_input</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test 'test_unsorted_input' verifies that the function handles unsorted input correctly.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function would incorrectly group ranges when input is not sorted.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return the correct compressed range for an unsorted input, e.g. '1-3, 5'.</li>
                                            <li>The function should handle cases where the input list contains duplicate values and still produce the expected output.</li>
                                            <li>The function should correctly group ranges even if they are not consecutive (e.g. [2, 4] is considered a single range).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/ranges.py</span>
                                        <span style="color: var(--text-secondary)">16 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_empty_string</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that an empty string produces an empty list when expanding ranges.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where an empty string is incorrectly expanded to include all elements, potentially leading to incorrect results or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert expand_ranges([]) == []</li>
                                            <li>assert expand_ranges(['a', 'b']) == ['a', 'b']</li>
                                            <li>assert expand_ranges(['a', 'b', 'c']) == ['a', 'b', 'c']</li>
                                            <li>assert expand_ranges(['x', 'y', 'z']) == ['x', 'y', 'z']</li>
                                            <li>assert expand_ranges(['a', 'b', 'c', 'd']) == ['a', 'b', 'c', 'd']</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/ranges.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 81-82)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_mixed</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> test_mixed verifies that the `expand_ranges` function correctly handles mixed ranges and singles.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in cases where a single value is part of a range (e.g., '1-3') or when multiple values are separated by ranges (e.g., '5, 10-12').</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function correctly splits the input string into individual numbers and range parts.</li>
                                            <li>It handles both single values within ranges and entire ranges separately.</li>
                                            <li>It preserves the original order of numbers in cases where a single value is part of a range.</li>
                                            <li>It correctly handles ranges with negative or zero start values (e.g., '10-12')</li>
                                            <li>It ignores empty strings as input (e.g., '1, 5, ', which would be split into ['1', '', '5'])</li>
                                            <li>It preserves the order of numbers when a range is specified after a single value (e.g., '1-3, 5, 10')</li>
                                            <li>It handles cases where multiple ranges are provided in a single string (e.g., '1-3, 5-7, 9-11')</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/ranges.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 81, 84-91, 93, 95)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_range</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The 'expand_ranges' function is tested with a valid range.</p>
                                    <p><strong>Why Needed:</strong> This test prevents the function from expanding ranges incorrectly when they are not in the format 'start-end'.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return a list of numbers between start and end.</li>
                                            <li>The start value should be greater than or equal to 0.</li>
                                            <li>The end value should be less than the start value plus one.</li>
                                            <li>The range should not include the start value if it is equal to the end value.</li>
                                            <li>The function should handle ranges with a single number correctly.</li>
                                            <li>The function should raise an error for invalid input (e.g. 'a-b' or '-a').</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/ranges.py</span>
                                        <span style="color: var(--text-secondary)">10 lines (ranges: 81, 84-91, 95)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_roundtrip</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `compress_ranges` and `expand_ranges` functions produce the same output for a given input.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in cases where the order of elements in the compressed or expanded ranges changes.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The original list is not modified when calling `compress_ranges()` and then `expand_ranges()`.</li>
                                            <li>The first element of the compressed range is equal to the first element of the original list.</li>
                                            <li>The second element of the compressed range is equal to the second element of the original list.</li>
                                            <li>The third element of the compressed range is equal to the fourth element of the original list.</li>
                                            <li>The last element of the expanded range is equal to the fifth element of the original list.</li>
                                            <li>The first element of the expanded range is equal to the first element of the original list.</li>
                                            <li>The second element of the expanded range is equal to the second element of the original list.</li>
                                            <li>The third element of the expanded range is equal to the fourth element of the original list.</li>
                                            <li>The last element of the expanded range is equal to the fifth element of the original list.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/ranges.py</span>
                                        <span style="color: var(--text-secondary)">27 lines (ranges: 29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67, 81, 84-91, 93, 95)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_ranges.py::TestExpandRanges::test_single_number</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The 'expand_ranges' function is expected to handle a single input ('5') and return a list containing only that number.</p>
                                    <p><strong>Why Needed:</strong> This test prevents the function from expanding multiple numbers into separate lists, which could lead to incorrect results or unexpected behavior.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert expand_ranges('5') == [5]</li>
                                            <li>assert expand_ranges('-5') == [-5]</li>
                                            <li>assert expand_ranges('10') == [10]</li>
                                            <li>assert expand_ranges('0') == [0]</li>
                                            <li>assert expand_ranges('1.2') == [1.2]</li>
                                            <li>assert expand_ranges('abc') == []</li>
                                            <li>assert expand_ranges('12345') == [12345]</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/ranges.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 81, 84-87, 93, 95)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_render.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">9 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_render.py::TestFormatDuration::test_milliseconds</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `format_duration` function correctly formats durations for millisecond intervals less than 1 second.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where durations less than 1 second are not formatted as milliseconds.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The duration is formatted as 'xms' (e.g. '500ms'),</li>
                                            <li>The duration is equal to the input value multiplied by 1000 (e.g. '1ms'),</li>
                                            <li>The duration is less than or equal to 1 second (1000ms),</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 65, 67)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_render.py::TestFormatDuration::test_seconds</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the function correctly formats durations in seconds for values greater than or equal to 1 second.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function does not handle durations of exactly 1 second correctly.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should return '1.23s' when given an argument of 1.23 seconds.</li>
                                            <li>The function should return '60.00s' when given an argument of 60 seconds.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 65-66)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_render.py::TestOutcomeToCssClass::test_all_outcomes</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> All outcomes should map to CSS classes.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in CSS class mapping for different outcome statuses.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>outcome-to-css-class mapping should be consistent across all outcome statuses.</li>
                                            <li>outcome-to-css-class mapping should handle special cases like 'xfailed' and 'xpassed'.</li>
                                            <li>outcome-to-css-class mapping should not map to invalid or unknown CSS classes.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 79-85, 87)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential regression where the 'outcome-unknown' class is not applied to unknown outcomes.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert outcome_to_css_class('unknown') == 'outcome-unknown'</li>
                                            <li>assert outcome_to_css_class('valid') != 'outcome-unknown'</li>
                                            <li>assert outcome_to_css_class('invalid') != 'outcome-unknown'</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 79-85, 87)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_basic_report</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Tests the rendering of a basic report with fallback HTML.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression that may occur when the report is rendered without fallback HTML.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The document type declaration '<!DOCTYPE html>' should be present in the rendered HTML.</li>
                                            <li>The title 'Test Report' should be included in the rendered HTML.</li>
                                            <li>The node IDs 'test::passed' and 'test::failed' should be found in the rendered HTML.</li>
                                            <li>The string 'PASSED' should appear in the rendered HTML for the passed nodes.</li>
                                            <li>The string 'FAILED' should appear in the rendered HTML for the failed nodes.</li>
                                            <li>The plugin version '0.1.0' should be present in the rendered HTML.</li>
                                            <li>The repository version '1.2.3' should be present in the rendered HTML.</li>
                                            <li>The text '<strong>Plugin:</strong> v0.1.0' should appear in the rendered HTML.</li>
                                            <li>The text '<strong>Repo:</strong> v1.2.3' should appear in the rendered HTML.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">52 lines (ranges: 65-67, 79-85, 87, 121-124, 126-127, 131-132, 141-143, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_coverage</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test renders coverage for fallback HTML.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential regression where the test fails to report coverage information when rendering fallback HTML.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'src/foo.py' file should be included in the rendered HTML.</li>
                                            <li>There should be exactly 5 lines in the rendered HTML.</li>
                                            <li>The line count of the rendered HTML should match the number of lines in the 'src/foo.py' file (5).</li>
                                            <li>The coverage information should include the file path 'src/foo.py'.</li>
                                            <li>The coverage information should indicate that there are no additional lines beyond what is expected (0-4).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">52 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-129, 131-132, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_llm_annotation</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the test includes LLM annotations in the rendered HTML report.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential security vulnerability where an attacker could manipulate the authentication flow to bypass authorization checks.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'Tests login flow' scenario is present in the rendered HTML report.</li>
                                            <li>The 'Prevents auth bypass' reason is mentioned in the report.</li>
                                            <li>The LlmAnnotation object contains the required information (scenario and why_needed).</li>
                                            <li>The key assertion 'Tests login flow' is included in the HTML content.</li>
                                            <li>The key assertion 'Prevents auth bypass' is present in the HTML content.</li>
                                            <li>The LlmAnnotation object has the correct structure (nodeid, outcome, scenario, and why_needed).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">54 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-134, 136-137, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_source_coverage</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">63 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-132, 141-142, 145-153, 158-164, 166-172, 177, 192, 196, 229-236, 239-245, 248-249)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_render.py::TestRenderFallbackHtml::test_renders_xpass_summary</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the rendered HTML includes both "XFailed" and "XPassed" summary entries.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the xfailed/xpassed summary is not displayed correctly.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The report root contains the "XFailed" and "XPassed" strings in its HTML output.</li>
                                            <li>The report root does not contain any "XPassed" string in its HTML output.</li>
                                            <li>The report root contains only "XFailed" string in its HTML output.</li>
                                            <li>The report root contains only "XPassed" string in its HTML output.</li>
                                            <li>The report root contains "XFailed" and "XPassed" strings in different positions in the HTML output.</li>
                                            <li>The report root does not contain "XPassed" string in a specific position (e.g. at the end of the page).</li>
                                            <li>The report root contains "XPassed" string but it is not displayed correctly (e.g. it is hidden behind other text).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">50 lines (ranges: 65, 67, 79-85, 87, 121-124, 126-127, 131-132, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_report_writer.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">19 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestComputeSha256::test_different_content</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 55)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestComputeSha256::test_empty_bytes</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test 'test_empty_bytes' verifies that an empty bytes input produces consistent hash and correct length.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where different inputs produce different hashes due to differences in byte order or encoding.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The two computed hashes should be equal (i.e., the same SHA256 hash).</li>
                                            <li>The length of the first computed hash should be 64 bytes (as expected for a SHA256 hash).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 55)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_run_meta</span>
                            <div class="test-meta">
                                <span>5ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the ReportWriter class to ensure it builds run metadata correctly.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regressions where the report writer does not include version information in the build run metadata.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The duration of the test should be 60 seconds.</li>
                                            <li>The pytest version should have a value.</li>
                                            <li>The plugin version should match the current __version__.</li>
                                            <li>The python version should match the current __python_version__.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">67 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_summary_all_outcomes</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the `build_summary` method counts all outcome types correctly.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the total count of outcomes is not accurate due to missing or incorrectly counted 'x' outcomes.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The sum of all outcome types should be equal to 6 (1 passed, 1 failed, 1 skipped, 1 xfailed, 1 xpassed, 1 error).</li>
                                            <li>The count of passed outcomes should be 1.</li>
                                            <li>The count of failed outcomes should be 1.</li>
                                            <li>The count of skipped outcomes should be 1.</li>
                                            <li>The count of xfailed outcomes should be 1.</li>
                                            <li>The count of xpassed outcomes should be 1.</li>
                                            <li>The count of error outcomes should be 1.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">19 lines (ranges: 156-158, 312, 314-315, 317-328, 330)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_build_summary_counts</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `build_summary` method correctly counts outcomes in a report.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the number of passed, failed, and skipped tests is not accurately reflected in the report's summary.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>asserts that the total count of all tests is equal to 4 (total number of tests)</li>
                                            <li>asserts that the number of passed tests is equal to 2 (number of tests with outcome 'passed')</li>
                                            <li>asserts that the number of failed tests is equal to 1 (number of tests with outcome 'failed')</li>
                                            <li>asserts that the number of skipped tests is equal to 1 (number of tests with outcome 'skipped')</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">13 lines (ranges: 156-158, 312, 314-315, 317-322, 330)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_create_writer</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The `ReportWriter` class initializes correctly with a provided configuration.</p>
                                    <p><strong>Why Needed:</strong> Without this test, the `ReportWriter` might not initialize with the expected configuration, potentially leading to unexpected behavior or errors.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>writer.config is set to the provided Config instance.</li>
                                            <li>writer.warnings is an empty list.</li>
                                            <li>writer.artifacts is an empty list.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 156-158)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_assembles_tests</span>
                            <div class="test-meta">
                                <span>5ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test ReportWriter::test_write_report_assembles_tests verifies that the report includes all tests and provides summary statistics.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the report writer correctly identifies and writes each test, even if no output paths are specified.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The length of the report.tests list is equal to 2 (the number of tests)</li>
                                            <li>The total value of the report.summary.total property is equal to 2 (the number of tests)</li>
                                            <li>Each test in the report.tests list has a nodeid that matches one of the test nodes in the input list</li>
                                            <li>Each test result has an outcome that matches either 'passed' or 'failed'</li>
                                            <li>The summary statistics are accurate and provide meaningful information about the tests</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">93 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_includes_coverage_percent</span>
                            <div class="test-meta">
                                <span>6ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `ReportWriter` class writes a report with an included coverage percentage.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the coverage percentage is not included in the report.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The total coverage percentage of the report should be equal to the provided `coverage_percent` value.</li>
                                            <li>The `summary` object of the report should contain a `coverage_total_percent` attribute with the same value as the provided `coverage_percent` value.</li>
                                            <li>The `report` object's `summary` attribute should have a `coverage_total_percent` property set to the provided `coverage_percent` value.</li>
                                            <li>The coverage percentage is included in the report summary.</li>
                                            <li>The total coverage percentage of the report matches the given coverage percentage.</li>
                                            <li>The `ReportWriter` class correctly includes the coverage percentage in its reports.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">93 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-199, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_includes_source_coverage</span>
                            <div class="test-meta">
                                <span>5ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test ReportWriter::test_write_report_includes_source_coverage verifies that the test writes a report with source coverage summary.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in case the source code changes and the report writer needs to include the source coverage summary.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The number of source coverage entries should be exactly 1.</li>
                                            <li>The file path of the first source coverage entry should match 'src/foo.py'.</li>
                                            <li>All source coverage entries should have a missing count between 0 and 7 inclusive.</li>
                                            <li>At least one source coverage entry should have a covered count greater than or equal to 8.</li>
                                            <li>The percentage of covered statements should be exactly 87.5%</li>
                                            <li>The covered ranges should match '1-4, 6-7'.</li>
                                            <li>All source coverage entries should have at least one missed range.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">92 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestReportWriter::test_write_report_merges_coverage</span>
                            <div class="test-meta">
                                <span>5ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test ReportWriter::test_write_report_merges_coverage verifies that the report writer merges coverage into tests.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the report writer does not merge coverage correctly, potentially leading to incorrect test results or missed coverage metrics.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The report should have at least one coverage entry for each test.</li>
                                            <li>Each coverage entry should be associated with the correct file path.</li>
                                            <li>The first coverage entry in the report should correspond to the first test.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">94 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186-189, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_atomic_write_fallback</span>
                            <div class="test-meta">
                                <span>7ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that the report writer falls back to direct write if atomic write fails.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the report writer fails to write reports even when it encounters an error during atomic writes.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The file "report.json" exists in the temporary directory.</li>
                                            <li>Any warnings with code 'W203' are present in the report.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">67 lines (ranges: 235-237, 239, 241, 370-386, 388-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516-518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">125 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506-507, 509-512, 515-516)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_creates_directory_if_missing</span>
                            <div class="test-meta">
                                <span>7ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test verifies that the `ReportWriter` creates an output directory if it doesn't exist.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where the report writer does not create the necessary output directory even when it is missing.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The output directory should be created with the correct path.</li>
                                            <li>The `report.json` file should be written to the correct location within the output directory.</li>
                                            <li>The `ReportWriter` instance should have been able to write the report successfully without raising an exception.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">86 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 235-237, 239, 241, 370-386, 388-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516-518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">123 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-477, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_ensure_dir_failure</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that a directory creation failure is captured as a warning with code W201.</p>
                                    <p><strong>Why Needed:</strong> To prevent the test from passing when the directory creation fails due to permission issues.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function should raise an OSError exception with code W201 for the given path.</li>
                                            <li>The function should catch any exceptions raised by mkdir and propagate them up to the caller as warnings with code W201.</li>
                                            <li>Any warnings raised during the directory creation process should be caught and reported correctly.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 156-158, 470-473, 480-484)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_git_info_failure</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the `get_git_info` function to handle Git command failures gracefully.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `get_git_info` function fails to retrieve Git information due to a missing or unresponsive Git installation, causing the report writer to fail.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `sha` variable is set to `None` after calling `get_git_info()`</li>
                                            <li>The `dirty` variable is set to `None` after calling `get_git_info()`</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">9 lines (ranges: 67-73, 85-86)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_creates_file</span>
                            <div class="test-meta">
                                <span>36ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test verifies that the report writer creates an HTML file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the report writer does not create an HTML file, potentially causing issues with reports containing such files.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'report.html' file should exist in the temporary directory.</li>
                                            <li>The 'report.html' file should contain expected content as specified by the tests.</li>
                                            <li>All test nodes ('test1', 'test2') should be found in the 'report.html' file.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 162)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">115 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_includes_xfail_summary</span>
                            <div class="test-meta">
                                <span>37ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the report writer includes xfail outcomes in the HTML summary.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where xfail outcomes are not included in the HTML summary.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Asserts that 'XFAILED' and 'XFailed' keywords are present in the HTML content.</li>
                                            <li>Asserts that 'XPASSED' and 'XPassed' keywords are present in the HTML content.</li>
                                            <li>Verifies that both xfail and passed outcomes are included in the summary.</li>
                                            <li>Checks for correct formatting of xfail and passed keywords.</li>
                                            <li>Ensures that xfailed and xpasstoken are not present in the HTML content.</li>
                                            <li>Verifies that all test nodes have their respective outcome tags (xfailed, xpassed, xfailed, or xpassed).</li>
                                            <li>Asserts that no other keywords are present in the HTML content.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 162)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">118 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-326, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_json_creates_file</span>
                            <div class="test-meta">
                                <span>6ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test verifies that a JSON file is created with the report.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the report writer does not create a JSON file.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'report.json' file should exist in the temporary directory.</li>
                                            <li>At least one artifact should be tracked for the report.</li>
                                            <li>The number of artifacts should be greater than zero.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">80 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">117 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_creates_file</span>
                            <div class="test-meta">
                                <span>39ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test verifies that a PDF file is created when Playwright is available.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the report writer does not create a PDF file even if Playwright is available.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `report.pdf` attribute of the test result should exist and contain a valid path to the created PDF file.</li>
                                            <li>Any artifacts created by the report writer should have paths that match the expected paths for the created PDF file.</li>
                                            <li>The `report.pdf` attribute of the test result should not be empty or None.</li>
                                            <li>The `report.pdf` attribute of the test result should contain a valid path to the created PDF file.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 162)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">125 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 401, 410, 412, 414-423, 434-435, 437-443, 448, 453, 455, 458-462, 470-471)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_missing_playwright_warns</span>
                            <div class="test-meta">
                                <span>6ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that a warning is raised when the Playwright module is missing for PDF output.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where the report writer does not warn users about missing Playwright for PDF output.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The file `report.pdf` should exist after running the test.</li>
                                            <li>Any warnings raised by the report writer should be of type WarningCode.W204_PDF_PLAYWRIGHT_MISSING.value.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">98 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 401-405, 408)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_report_writer_coverage.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">10 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_nonexistent_path</span>
                            <div class="test-meta">
                                <span>2ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test verifies that `get_git_info` returns None when the path does not exist.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where `get_git_info` incorrectly returns a Git SHA or status when the provided path is non-existent.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `get_git_info(tmp_path)` should return `None` for a nonexistent path.</li>
                                            <li>The function `get_git_info(tmp_path)` should not attempt to retrieve any information from the nonexistent path.</li>
                                            <li>The function `get_git_info(tmp_path)` should raise an exception or return an error when the provided path is non-existent.</li>
                                            <li>The function `get_git_info(tmp_path)` should not perform any Git operations on a non-existent path.</li>
                                            <li>The function `get_git_info(tmp_path)` should handle the case where the path does not exist without raising an exception.</li>
                                            <li>The function `get_git_info(tmp_path)` should return `None` for a nonexistent directory (not just file).</li>
                                            <li>The function `get_git_info(tmp_path)` should raise an exception when attempting to read from a non-existent Git repository.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">9 lines (ranges: 67-73, 85-86)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_valid_repo</span>
                            <div class="test-meta">
                                <span>5ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test getting git info from a valid repository should not crash and return the expected information.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function crashes if the input is invalid (e.g., not a git repo).</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function gets the commit hash or returns None if it's not in a valid git repository.</li>
                                            <li>The function does not crash when given an invalid input.</li>
                                            <li>The function returns the expected information for a valid git repository.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">16 lines (ranges: 67-74, 76-81, 83-84)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer_coverage.py::TestGetPluginGitInfo::test_plugin_git_info_fallback</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that a plugin's Git info fallback works as expected when the import fails.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the fallback to the git runtime fails due to an import failure in the _git_info module.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function get_plugin_git_info() should return None or a string (sha) even if the import of _git_info fails.</li>
                                            <li>The function get_plugin_git_info() should still work via the git runtime fallback when the import of _git_info fails.</li>
                                            <li>The function assert that the returned SHA is either None or an instance of str, indicating a successful fallback.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 127-128, 130)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer_coverage.py::TestGetPluginGitInfo::test_plugin_git_info_returns_values</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that plugin git info returns some values.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential crash by ensuring the test doesn't attempt to access an invalid Git object.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function get_plugin_git_info() should return either None or a string.</li>
                                            <li>The function get_plugin_git_info() should not attempt to access an invalid Git object (i.e., sha is None).</li>
                                            <li>The function get_plugin_git_info() should handle the case where runtime git info is available and return it correctly.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 127-128, 130)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer_coverage.py::TestReportWriterAtomicWrite::test_atomic_write_fallback</span>
                            <div class="test-meta">
                                <span>7ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test atomic write falls back to direct write on error.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the report writer fails to write a report even when an atomic write operation fails.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `report.json` file should exist after the test.</li>
                                            <li>The `report.json` file should not be empty after the test.</li>
                                            <li>The `report.json` file should have the correct content (e.g. JSON structure, data types).</li>
                                            <li>The `report.json` file should not contain any errors or warnings.</li>
                                            <li>The `report.json` file should have a valid timestamp.</li>
                                            <li>The `report.json` file should be written to the specified path.</li>
                                            <li>The `report.json` file should not be deleted or moved during the test.</li>
                                            <li>The `report.json` file should have the correct permissions (e.g. read-only, write permissions).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">80 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">117 lines (ranges: 55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer_coverage.py::TestReportWriterPDF::test_pdf_playwright_exception</span>
                            <div class="test-meta">
                                <span>115ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test PDF generation when playwright raises exception (lines 424-432) and expected a warning about PDF failure.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression where PDF generation fails due to playwright exception without raising an error.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>Mocked playwright context should have raised a RuntimeError on browser launch failure.</li>
                                            <li>Expected the writer to raise a Warning for PDF failure.</li>
                                            <li>The warning message should contain 'W201' indicating the code of the warning.</li>
                                            <li>The PDF generation process should not be successful due to the exception.</li>
                                            <li>The test should verify that the warning is present in the warnings list.</li>
                                            <li>The warning should have a specific code (in this case, 'W201').</li>
                                            <li>The warning message should contain the expected string ('Browser launch failed').</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 162)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65-67, 79-85, 87, 99, 101-102, 107)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">28 lines (ranges: 156-158, 401, 410, 412, 414-416, 424-429, 432, 434-435, 448, 453, 455, 458-462, 470-471)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer_coverage.py::TestReportWriterPDF::test_pdf_playwright_not_installed</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test PDF generation when playwright is not installed.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to missing playwright installation.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The report writer should have been able to generate a PDF file despite playwright not being installed.</li>
                                            <li>A warning message indicating that playwright is missing should be displayed.</li>
                                            <li>No PDF file should have been created in the temporary directory.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">9 lines (ranges: 156-158, 401-405, 408)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer_coverage.py::TestReportWriterPDF::test_resolve_html_source_creates_temp</span>
                            <div class="test-meta">
                                <span>30ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test _resolve_pdf_html_source creates temp file when no HTML source is provided.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the report writer does not create a temporary file for non-existent HTML sources.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The test verifies that the report writer creates a temporary file with the correct suffix (.html) even if no HTML path is configured.</li>
                                            <li>The test verifies that the created temp file exists and has the correct suffix.</li>
                                            <li>The test verifies that the created temp file does not contain any HTML content.</li>
                                            <li>The test checks for a clean-up of the original report file after creating the temporary file.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 162)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65-67, 79-85, 87, 99, 101-102, 107)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 156-158, 448, 453, 455, 458-462)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer_coverage.py::TestReportWriterPDF::test_resolve_html_source_missing_html_file</span>
                            <div class="test-meta">
                                <span>30ms</span>
                                <span title="Covered file count">üõ°Ô∏è 6</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test _resolve_pdf_html_source when configured HTML doesn't exist.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails due to an empty or missing HTML source file, causing the report writer to fall back to a temporary file instead of using the actual HTML source.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The path returned by _resolve_pdf_html_source should be an absolute path to the nonexistent HTML file.</li>
                                            <li>The path returned by _resolve_pdf_html_source should exist after the test is run.</li>
                                            <li>The report writer should use the actual HTML source if it exists, instead of falling back to a temporary file.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 162)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">26 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65-67, 79-85, 87, 99, 101-102, 107)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">13 lines (ranges: 156-158, 448-450, 453, 455, 458-462)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer_coverage.py::TestReportWriterPDF::test_resolve_html_source_uses_existing</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test _resolve_pdf_html_source uses existing HTML file.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the test fails if an existing HTML file is used as the source for the PDF report.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The path of the resolved HTML file matches the expected existing HTML file.</li>
                                            <li>The `is_temp` flag is set to False, indicating that the report was generated successfully without creating a temporary file.</li>
                                            <li>The test verifies that an existing HTML file can be used as the source for the PDF report without causing any issues.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 156-158, 448-451)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_report_writer_coverage_v2.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">2 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer_coverage_v2.py::test_report_writer_ensure_dir_creation</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 4</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Ensure directory creation is performed when report HTML file exists.</p>
                                    <p><strong>Why Needed:</strong> The test prevents a potential issue where the report writer does not create the required directory if it already exists.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>tmp_dir.exists()</li>
                                            <li>any(w.code == 'W202') for w in writer.warnings</li>
                                            <li>tmp_dir / 'r.html' exists</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 156-158, 470-477)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_report_writer_coverage_v2.py::test_report_writer_metadata_skips</span>
                            <div class="test-meta">
                                <span>10ms</span>
                                <span title="Covered file count">üõ°Ô∏è 5</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the report_writer_metadata_skips function correctly skips metadata when reports are disabled.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the report writer would include metadata even when reports are disabled.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'start_time' key should be present in the metadata.</li>
                                            <li>The 'llm_model' key should not be present in the metadata if the report is disabled.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">36 lines (ranges: 370-386, 388-399, 401, 403, 405, 407, 409, 413, 425)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 123, 163)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">67 lines (ranges: 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_schemas.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">2 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_schemas.py::TestAnnotationSchema::test_from_dict_full</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test from dictionary with all fields</p>
                                    <p><strong>Why Needed:</strong> Prevent regression in case of missing or malformed input data</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert 200</li>
                                            <li>assert token</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">5 lines (ranges: 77-81)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_schemas.py::TestAnnotationSchema::test_to_dict_full</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">8 lines (ranges: 90-92, 94-98)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_smoke_pytester.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">15 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_report_created</span>
                            <div class="test-meta">
                                <span>89ms</span>
                                <span title="Covered file count">üõ°Ô∏è 8</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The HTML report is generated correctly and contains the test function.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where the report may not contain the expected test function.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The report path exists at `report.html`</li>
                                            <li>The content of the report includes `<html>` and `test_simple`</li>
                                            <li>The report contains the test function `test_simple`</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 162)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">45 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424-426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">152 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">101 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_summary_counts_all_statuses</span>
                            <div class="test-meta">
                                <span>126ms</span>
                                <span title="Covered file count">üõ°Ô∏è 8</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> test_html_summary_counts_all_statuses verifies that the HTML summary counts include all statuses.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the HTML summary counts are missing for certain statuses (e.g., XFailed, XPassed).</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `assert_summary(labels: list[str], expected: int)` checks if the count of each status in the labels matches the expected value.</li>
                                            <li>If a match is found with an unexpected label, it raises an AssertionError.</li>
                                            <li>If no match is found for all statuses, it raises an AssertionError with a message indicating that there are missing summary labels.</li>
                                            <li>The function `assert_summary(labels: list[str], expected: int)` also checks if the count of each status in the labels matches the expected value when using fallback patterns.</li>
                                            <li>If a match is found with an unexpected label, it returns without raising an AssertionError.</li>
                                            <li>If no match is found for all statuses, it raises an AssertionError with a message indicating that there are missing summary labels.</li>
                                            <li>The function `assert_summary(labels: list[str], expected: int)` checks if the count of each status in the labels matches the expected value when using fallback patterns and joins the labels into a single string.</li>
                                            <li>If no match is found for all statuses, it raises an AssertionError with a message indicating that there are missing summary labels.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">65 lines (ranges: 78-79, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212-214, 216, 227-228, 230-236, 250-251, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 162)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">45 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424-426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">152 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">111 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-328, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_json_report_created</span>
                            <div class="test-meta">
                                <span>66ms</span>
                                <span title="Covered file count">üõ°Ô∏è 7</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The JSON report is created and its existence and content are verified.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the test fails to create a valid JSON report due to missing or malformed schema version information.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `schema_version` key in the report data should be present and contain the correct value (in this case, '1.0')</li>
                                            <li>The `summary` section of the report data should have exactly two items: 'total' with a count of 2 and 'passed' with a count of 1</li>
                                            <li>The `failed` item in the summary section should have a count of 1</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">51 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-118, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">76 lines (ranges: 162, 166-171, 173-175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">45 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424, 426-428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">152 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">107 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_annotations_in_report</span>
                            <div class="test-meta">
                                <span>60ms</span>
                                <span title="Covered file count">üõ°Ô∏è 13</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that LLM annotations are included in the report when a provider is enabled.</p>
                                    <p><strong>Why Needed:</strong> Prevents regressions by ensuring LLM annotations are present in the report.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The test passes if the 'LLM annotations' key exists in the report with the correct value.</li>
                                            <li>The 'LLM annotations' key should contain a dictionary with the required keys ('scenario', 'why_needed', and 'key_assertions').</li>
                                            <li>The 'LLM annotations' key should be present in the report's JSON output.</li>
                                            <li>The 'LLM annotations' key should have the correct value for the 'why_needed' key, which is 'Prevents regressions'.</li>
                                            <li>The 'LLM annotations' key should contain a list of strings with the required keys ('asserts True').</li>
                                            <li>The 'LLM annotations' key should be present in the report's JSON output and have the correct value for each assertion.</li>
                                            <li>The 'key_assertions' key should contain a dictionary with the expected values for each assertion.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">69 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">39 lines (ranges: 52-53, 72, 75, 80, 107, 110-111, 128, 136, 147, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218, 245, 247, 249, 252, 257-258, 260)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">34 lines (ranges: 37-38, 41, 60, 62, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 164-168, 170-171, 175, 179-180, 183, 185-186, 188, 197, 205-206, 208)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/schemas.py</span>
                                        <span style="color: var(--text-secondary)">7 lines (ranges: 38, 42-43, 50-53)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">96 lines (ranges: 104-107, 109-111, 113, 115, 162, 166-171, 173, 175, 177, 179, 182, 184-186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413-425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">86 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-293, 295-296, 300-304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424, 426-428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">172 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184-186, 188-189, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-316, 319, 321, 324-328, 331, 333-338, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">29 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">110 lines (ranges: 55, 67-73, 85-86, 98-99, 102, 105-108, 113, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_error_is_reported</span>
                            <div class="test-meta">
                                <span>88ms</span>
                                <span title="Covered file count">üõ°Ô∏è 13</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that an LLM error is reported in the generated HTML output.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where LLM errors are not surfaced in the generated HTML output.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `test_pass()` should raise an assertion error when executed.</li>
                                            <li>The function `test_llm_error_is_reported` should generate an HTML report with an error message.</li>
                                            <li>The test should fail if the LLM errors are not surfaced in the generated HTML output.</li>
                                            <li>The test should verify that the error is reported at the top of the page.</li>
                                            <li>The error message should be a RuntimeError exception.</li>
                                            <li>The error message should include the string 'boom'.</li>
                                            <li>The test should also verify that the error message includes the function name `test_pass()`.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/cache.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 39-41, 53, 55-56, 86, 88, 118-119, 121, 153)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">39 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/annotator.py</span>
                                        <span style="color: var(--text-secondary)">73 lines (ranges: 45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137-139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198-201, 203)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/base.py</span>
                                        <span style="color: var(--text-secondary)">21 lines (ranges: 52-53, 72, 75, 80, 107, 110-111, 128, 136, 147, 165, 167, 175, 245, 247, 249, 252, 257-258, 260)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/litellm_provider.py</span>
                                        <span style="color: var(--text-secondary)">36 lines (ranges: 37-38, 41, 60, 62, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 108, 110-111, 114, 129, 131, 164-168, 170-171, 175, 179-180, 183, 205-206, 208)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 162)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">86 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-293, 295-296, 300-304, 308, 310, 312, 316, 320, 324, 328, 330, 332, 334, 336, 340, 342, 346, 348, 350, 352, 354, 356, 358, 362, 366, 370, 372, 374, 380, 384, 390, 392, 398, 400, 414, 417, 420, 424-426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">172 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184-186, 188-189, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-316, 319, 321, 324-329, 333-338, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/prompts.py</span>
                                        <span style="color: var(--text-secondary)">29 lines (ranges: 33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">106 lines (ranges: 55, 67-73, 85-86, 98-99, 102, 105-108, 113, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_smoke_pytester.py::TestMarkers::test_llm_opt_out_marker</span>
                            <div class="test-meta">
                                <span>59ms</span>
                                <span title="Covered file count">üõ°Ô∏è 7</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the LLM opt-out marker functionality.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression in LLM opt-out marker recording and reporting.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The test verifies that the LLM opt-out marker is recorded correctly.</li>
                                            <li>The test ensures that the LLM opt-out marker is not recorded for non-LLM tests.</li>
                                            <li>The test checks if the LLM opt-out marker is set to True for all tests.</li>
                                            <li>The test verifies that the LLM opt-out marker is not set to False for non-LLM tests.</li>
                                            <li>The test ensures that the LLM opt-out marker is recorded correctly even when run with --llm-report-json flag</li>
                                            <li>The test checks if the LLM opt-out marker is correctly recorded in the report.json file</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181-182, 185-186, 198-199, 209-210, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">76 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186-188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">45 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424, 426-428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">152 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_smoke_pytester.py::TestMarkers::test_requirement_marker</span>
                            <div class="test-meta">
                                <span>57ms</span>
                                <span title="Covered file count">üõ°Ô∏è 7</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the requirement marker to ensure it is recorded correctly.</p>
                                    <p><strong>Why Needed:</strong> The test prevents a potential bug where the requirement marker is not properly recorded, potentially leading to incorrect test results or missed tests.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `pytest.mark.requirement` decorator should be applied to the test function with the required requirements.</li>
                                            <li>The `requirement` parameter of the `@pytest.mark.requirement` decorator should match the actual requirement name (REQ-001 and REQ-002 in this case).</li>
                                            <li>The `requirements` attribute of the test result data should contain the required requirements.</li>
                                            <li>The `reqs` list within the test result data should contain the required requirements as strings.</li>
                                            <li>The `REQ-001` and `REQ-002` strings should be present in the `reqs` list.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-200, 209-210, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">76 lines (ranges: 162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194-196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">45 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424, 426-428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">152 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_multiple_xfail_outcomes</span>
                            <div class="test-meta">
                                <span>66ms</span>
                                <span title="Covered file count">üõ°Ô∏è 7</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">47 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">76 lines (ranges: 162, 166-171, 173-175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">45 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424, 426-428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">152 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">108 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-324, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_skip_outcome</span>
                            <div class="test-meta">
                                <span>58ms</span>
                                <span title="Covered file count">üõ°Ô∏è 7</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that skipped tests are recorded and reported correctly.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the 'skip' marker is not properly recorded or reported in the report.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The number of skipped tests should be correctly counted in the report</li>
                                            <li>The skipped tests should be included in the report summary</li>
                                            <li>The skipped tests should have a correct status (e.g., 'skipped', 'not_skipped') in the report</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">43 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 106-107, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 250-251, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">76 lines (ranges: 162, 166-171, 173-175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">45 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424, 426-428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">152 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">107 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321-322, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_smoke_pytester.py::TestOutcomes::test_xfail_outcome</span>
                            <div class="test-meta">
                                <span>61ms</span>
                                <span title="Covered file count">üõ°Ô∏è 7</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that X failed tests are recorded in the report.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where X failed tests are not properly reported.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'summary' key in the report.json file should contain a count of X failed tests.</li>
                                            <li>The value of the 'xfailed' key in the report.json file should be equal to 1.</li>
                                            <li>The test function 'test_xfail()' should fail and produce an assertion error.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">47 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">76 lines (ranges: 162, 166-171, 173-175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">45 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424, 426-428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">152 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">108 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-324, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_smoke_pytester.py::TestParametrization::test_parametrized_tests</span>
                            <div class="test-meta">
                                <span>62ms</span>
                                <span title="Covered file count">üõ°Ô∏è 7</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that parameterized tests are recorded separately and their results are reported in a JSON file.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression by ensuring that the pytester is able to record and report on all parameterized tests correctly.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The total number of successful parameterized tests is 3 (1, 2, and 3).</li>
                                            <li>All parameterized tests are recorded in a JSON file named 'report.json'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">76 lines (ranges: 162, 166-171, 173, 175-177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">45 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424, 426-428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">152 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">105 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_help_contains_examples</span>
                            <div class="test-meta">
                                <span>51ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The CLI help text should include usage examples.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the help text is not displayed with examples.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `stdout.fnmatch_lines` method is used to match lines in the output.</li>
                                            <li>The `result.stdout` attribute is accessed to get the output of the `pytester.runpytest` function.</li>
                                            <li>The `--help` argument is passed to `pytester.runpytest` to run the test with help mode.</li>
                                            <li>The `stdout.fnmatch_lines` method is used to match lines that contain 'Example:*--llm-report*'.</li>
                                            <li>The `result.stdout` attribute is accessed to get the output of the `pytester.runpytest` function.</li>
                                            <li>The `--help` argument is passed to `pytester.runpytest` to run the test with help mode.</li>
                                            <li>The `stdout.fnmatch_lines` method is used to match lines that contain 'Example:*--llm-report*'.</li>
                                            <li>The `result.stdout` attribute is accessed to get the output of the `pytester.runpytest` function.</li>
                                            <li>The `--help` argument is passed to `pytester.runpytest` to run the test with help mode.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">44 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">104 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_markers_registered</span>
                            <div class="test-meta">
                                <span>45ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that LLM markers are registered and correctly matched in the output.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the LLM marker is not detected or matched correctly, potentially leading to incorrect results or warnings.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `--markers` option should match the expected list of markers (`*llm_opt_out*, *llm_context*, and *requirement*`).</li>
                                            <li>The output should contain at least one line that matches each marker (e.g., `*llm_opt_out*`, `*llm_context*`, or `*requirement*`).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">44 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">104 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_smoke_pytester.py::TestPluginRegistration::test_plugin_registered</span>
                            <div class="test-meta">
                                <span>51ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the plugin is registered correctly by checking if it can be run with --help flag.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the plugin might not register properly or may throw an error when trying to use it with pytest11.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `pytester.runpytest` method returns True if the plugin is registered correctly and False otherwise.</li>
                                            <li>The output of `result.stdout.fnmatch_lines` contains at least one line that starts with '--llm-report*'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">44 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424, 426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">104 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_smoke_pytester.py::TestSpecialCharacters::test_special_chars_in_nodeid</span>
                            <div class="test-meta">
                                <span>90ms</span>
                                <span title="Covered file count">üõ°Ô∏è 8</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that special characters in nodeid are handled correctly by Pytester.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential crash and ensures the HTML generated is valid.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The report file should exist after running pytester.runpytest with --llm-report.</li>
                                            <li>The report file should contain '<html>' in its contents.</li>
                                            <li>Special characters (like <, &, etc.) should not be treated as invalid nodeids by Pytester.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">40 lines (ranges: 78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/coverage_map.py</span>
                                        <span style="color: var(--text-secondary)">12 lines (ranges: 44-45, 58-60, 72-73, 83, 86, 88-90)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/errors.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 139-142)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/models.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 162)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/options.py</span>
                                        <span style="color: var(--text-secondary)">45 lines (ranges: 123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-280, 288-290, 414, 417, 420, 424-426, 428, 430, 432, 434, 436, 440, 442, 444, 446, 448, 452, 454)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">152 lines (ranges: 40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-125, 127-131, 133-137, 150-152, 154-156, 158-160, 164, 168-169, 171, 173, 176-177, 184, 193-195, 221, 225, 229, 232, 251-252, 259-260, 263-264, 266-267, 270-274, 276, 279-280, 282, 285-286, 307, 313-314, 341-351, 363-364, 367, 371-373, 384, 388, 407, 411-413, 424, 428, 431, 433-434)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/render.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/report_writer.py</span>
                                        <span style="color: var(--text-secondary)">101 lines (ranges: 55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_time.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">15 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_time.py::TestFormatDuration::test_boundary_one_minute</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Tests the 'format_duration' function to ensure it formats a duration of exactly one minute.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the function might incorrectly format a duration longer than one minute.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The result should be '1m 0.0s' (one minute and zero seconds).</li>
                                            <li>The result should not contain any leading zeros for minutes.</li>
                                            <li>The result should only include the unit 'm' for minutes, with no additional characters.</li>
                                            <li>The function should handle durations up to one minute correctly without producing incorrect results.</li>
                                            <li>Any non-numeric values in the input duration should be ignored and not used as input.</li>
                                            <li>The test should pass even if the input duration is exactly 60 seconds (one full minute).</li>
                                            <li>The function should return an error or raise a ValueError for invalid input durations.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/time.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_time.py::TestFormatDuration::test_microseconds_format</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that the `format_duration` function correctly formats sub-millisecond durations as microseconds.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the format string is not properly escaped for non-ASCII characters, potentially leading to incorrect formatting of microsecond durations.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The result should contain 'Œºs' in its string representation.</li>
                                            <li>The function should correctly convert 0.0005 seconds to '500Œºs'.</li>
                                            <li>The formatted string should match the expected output.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/time.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 39-40)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_time.py::TestFormatDuration::test_milliseconds_format</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that the `format_duration` function returns a string representation of sub-second durations as milliseconds.</p>
                                    <p><strong>Why Needed:</strong> Prevents incorrect formatting of millisecond-based durations, potentially leading to incorrect timing measurements in tests.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The result should contain 'ms' as a prefix.</li>
                                            <li>The result should be equal to the expected value '500.0ms'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/time.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 39, 41-42)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_time.py::TestFormatDuration::test_minutes_format</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that the function correctly formats durations over a minute.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression where the function might incorrectly format minutes as seconds instead of displaying them separately.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The result contains 'm' (minutes) in its string representation.</li>
                                            <li>The result equals '1m 30.5s' (correctly formatted duration with minutes and seconds).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/time.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_time.py::TestFormatDuration::test_multiple_minutes</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Tests the `format_duration` function with a scenario of formatting multiple minutes.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in the `format_duration` function when it is called with a duration that includes more than one minute.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The output should be '3m 5.0s' for a duration of 185.0 seconds.</li>
                                            <li>The function should correctly handle durations with multiple minutes.</li>
                                            <li>The function should not throw an error when given invalid input (e.g., negative time).</li>
                                            <li>The function should preserve the original order of decimal places in the input time.</li>
                                            <li>The function should handle cases where the input is a float or int.</li>
                                            <li>The function should support durations with different units (e.g., seconds, minutes, hours).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/time.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 39, 41, 43, 46-48)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_time.py::TestFormatDuration::test_one_second</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Tests the `format_duration` function with a duration of exactly 1 second.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in the case where the input duration is less than or equal to 0.01 seconds, which could cause incorrect formatting.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The output of the `format_duration(1.0)` function should be '1.00s'.</li>
                                            <li>The unit of the output should be 's' (seconds).</li>
                                            <li>The decimal part of the output should be '00'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/time.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 39, 41, 43-44)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_time.py::TestFormatDuration::test_seconds_format</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Tests the `format_duration` function to ensure it correctly formats seconds under a minute.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression when seconds are formatted under a minute, as it ensures the output is consistent with the expected format.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The result contains the string 's' (indicating seconds),</li>
                                            <li>The result equals the expected value '5.50s',</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/time.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 39, 41, 43-44)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_time.py::TestFormatDuration::test_small_milliseconds</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Tests the `format_duration` function with a duration of 1 millisecond.</p>
                                    <p><strong>Why Needed:</strong> This test prevents regression in handling very small durations, where the default format might not be accurate.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The output should be '1.0ms' for a duration of 1 millisecond.</li>
                                            <li>The function should handle durations less than 1 second correctly.</li>
                                            <li>The function should return the correct unit ('ms') for small durations.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/time.py</span>
                                        <span style="color: var(--text-secondary)">3 lines (ranges: 39, 41-42)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_time.py::TestFormatDuration::test_very_small_microseconds</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Tests the `format_duration` function with a very small duration (1 microsecond).</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where the function returns incorrect results for extremely short durations.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The output of `format_duration(0.000001)` should be '1Œºs'.</li>
                                            <li>The value returned by `format_duration(0.000001)` is equal to '1' followed by 'Œºs'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/time.py</span>
                                        <span style="color: var(--text-secondary)">2 lines (ranges: 39-40)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_datetime_with_utc</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Tests the `iso_format` function with a datetime object representing UTC time.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the function does not correctly format datetime objects with UTC timezone.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The output of `iso_format(dt)` should be in the ISO 8601 format `YYYY-MM-DDTHH:MM:SS+HH:MM:SSZ`.</li>
                                            <li>The time zone offset should be correctly represented as `+00:00`.</li>
                                            <li>The date and time components should be separated by a space, not a comma.</li>
                                            <li>The time component should have the correct format `HH:MM:SS`.</li>
                                            <li>The UTC timezone offset should be included in the output.</li>
                                            <li>The function should handle cases where the input datetime object is naive (i.e., does not have a timezone information).</li>
                                            <li>The function should correctly handle cases where the input datetime object has an invalid time zone.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/time.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_naive_datetime</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verifies that naive datetime is correctly formatted without timezone.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential issue where naive datetime formats are incorrectly assumed to be in the local timezone.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The result of `iso_format(dt)` should match '2024-06-20T14:00:00'.</li>
                                            <li>The time zone information is not present in the output.</li>
                                            <li>The date and time are correctly formatted without any timezone offset.</li>
                                            <li>The datetime object itself does not contain any timezone information.</li>
                                            <li>No timezone offset is applied to the datetime when formatting.</li>
                                            <li>The resulting string does not include a timezone indicator (e.g., UTC, GMT).</li>
                                            <li>The output does not contain any timezone-related metadata (e.g., timezone name, offset).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/time.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_time.py::TestIsoFormat::test_formats_with_microseconds</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Tests the `iso_format` function with a datetime object that includes microseconds.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where the `iso_format` function may not correctly handle dates with microseconds.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The result of calling `iso_format(dt)` should contain the string '123456'.</li>
                                            <li>The result of calling `iso_format(dt)` should be a valid ISO format string.</li>
                                            <li>The microseconds in the datetime object should be included in the output.</li>
                                            <li>The resulting string should not have any leading zeros.</li>
                                            <li>The resulting string should not contain any invalid characters.</li>
                                            <li>The resulting string should match the expected value when converted to a string using `isoformat()`.</li>
                                            <li>The resulting string should be able to be parsed correctly by other functions that expect an ISO format string.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/time.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 27)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_time.py::TestUtcNow::test_has_utc_timezone</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/time.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_time.py::TestUtcNow::test_is_current_time</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Verify that the `utc_now()` function returns a time within UTC's current time.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential issue where `utc_now()` might return an incorrect or outdated time due to network latency or system clock discrepancies.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The result of `utc_now()` should be within the range `[before, after)` (inclusive) when compared to `before` and `after` respectively.</li>
                                            <li>The value of `result` should not exceed `after` by more than a certain tolerance (e.g. 1 second).</li>
                                            <li>The value of `result` should not be less than `before` by more than a certain tolerance (e.g. 1 second).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/time.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_time.py::TestUtcNow::test_returns_datetime</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The function `utc_now()` returns a datetime object when called.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the function might return an incorrect or invalid datetime object.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>result is an instance of `datetime`</li>
                                            <li>result has a valid timezone.</li>
                                            <li>result is not None</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/util/time.py</span>
                                        <span style="color: var(--text-secondary)">1 lines (ranges: 15)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_token_refresh.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">12 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh.py::TestTokenRefresher::test_command_failure</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test TokenRefresher raises error on command failure.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a bug where the TokenRefresher does not raise an exception when the 'get-token' command fails, potentially leading to unexpected behavior or silent failures.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `get_token()` in the `TokenRefresher` class should raise a `TokenRefreshError` with an error message indicating that authentication failed.</li>
                                            <li>The error message returned by `get_token()` should include the string 'exit 1' to indicate that the command failed.</li>
                                            <li>The error message returned by `get_token()` should contain the string 'Authentication failed' to identify the specific reason for the failure.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 59-60, 63, 69, 83, 85-86, 90, 93-98, 101-104, 113, 115)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh.py::TestTokenRefresher::test_empty_output</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that TokenRefresher raises an error when given empty output.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the TokenRefresher class does not raise an error when it encounters no tokens to refresh.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>refresher.get_token() is called with no arguments.</li>
                                            <li>the stdout of refresher.get_token() is empty.</li>
                                            <li>the stderr of refresher.get_token() is empty.</li>
                                            <li>the returncode of refresher.get_token() is 0 (success), but the expected error message 'empty output' is not present in the output.</li>
                                            <li>the output format of refresher.get_token() is set to 'text', which does not contain any tokens, so there should be no output.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-109, 113, 115)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh.py::TestTokenRefresher::test_force_refresh</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that forcing a refresh bypasses the cache and updates the token with a new value.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a regression where the TokenRefresher does not update the token when forced to refresh.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function `get_token()` returns the expected token value for both initial and force refresh scenarios.</li>
                                            <li>The `call_count` variable is incremented correctly after each call to `get_token()` with `force=True`.</li>
                                            <li>The new token returned by `get_token()` has a unique identifier ('token-2' in this case).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh.py::TestTokenRefresher::test_get_token_json_custom_key</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `TokenRefresher` class correctly retrieves a custom JSON key for authentication.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the custom JSON key is not properly retrieved during token refresh.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The output of the `get-token` command should be 'custom-key-token' when the `json_key` parameter is set to 'access_token'.</li>
                                            <li>The `subprocess.run` function should return a CompletedProcess object with an stdout argument containing the expected JSON string.</li>
                                            <li>The `subprocess.run` function should return a CompletedProcess object with an stderr argument that is empty.</li>
                                            <li>The `TokenRefresher.get_token` method should call the `fake_run` function and return the result of the `subprocess.run` function.</li>
                                            <li>The `json.dumps` function should be called on the result of the `subprocess.run` function to generate the expected JSON string.</li>
                                            <li>The `token` variable should hold the value 'custom-key-token' after calling the `get_token` method.</li>
                                            <li>The test should pass without any errors or exceptions.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">29 lines (ranges: 59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132-135, 139, 143-144, 148)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh.py::TestTokenRefresher::test_get_token_json_format</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>LLM error:</strong> Failed to parse LLM response as JSON</p>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">29 lines (ranges: 59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132-135, 139, 143-144, 148)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh.py::TestTokenRefresher::test_get_token_text_format</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the `TokenRefresher` extracts the correct token from text output.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the token is not extracted correctly due to an incorrect or missing output format.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>token == 'my-secret-token'</li>
                                            <li>stdout contains 'INFO: Processing...' and 'my-secret-token'
</li>
                                            <li>stderr does not contain any error messages</li>
                                            <li>the `get_token()` method returns the correct token</li>
                                            <li>the token is extracted from the text output without any issues</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh.py::TestTokenRefresher::test_invalid_json</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that the TokenRefresher raises a TokenRefreshError when given invalid JSON.</p>
                                    <p><strong>Why Needed:</strong> This test prevents the TokenRefresher from incorrectly handling or crashing on non-JSON input data, ensuring it remains stable and functional.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The get_token() method of the TokenRefresher raises a TokenRefreshError with the message 'json' when given invalid JSON.</li>
                                            <li>The error message includes the string 'json', indicating that the issue is related to JSON format.</li>
                                            <li>When an invalid JSON is passed, the test asserts that the error message contains the word 'json'.</li>
                                            <li>The test verifies that the error message does not contain any other relevant information about the input data.</li>
                                            <li>The error message is case-insensitive, allowing for different input formats to be considered as valid or invalid JSON.</li>
                                            <li>The test checks that the error message includes the string 'json' in a manner consistent with standard error messages.</li>
                                            <li>When an invalid JSON is passed, the test verifies that the error message does not contain any other relevant information about the input data.</li>
                                            <li>The test ensures that the TokenRefresher correctly raises a TokenRefreshError when given invalid JSON.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">25 lines (ranges: 59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-134, 149-150)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh.py::TestTokenRefresher::test_invalidate</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test TokenRefresher.invalidate() clears cache and verifies that it correctly invalidates the cache.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the TokenRefresher does not invalidate the cache after calling get_token(), which could lead to stale token values being returned.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The function call count should increase by 1 when run() is called.</li>
                                            <li>The result of subprocess.run() should contain a 'token-' prefix in the stdout and stderr.</li>
                                            <li>The output format should be set to 'text' for get_token() calls.</li>
                                            <li>The invalidation of the cache should not affect token values returned after calling get_token().</li>
                                            <li>The function call count should decrease by 1 when refresher.invalidate() is called.</li>
                                            <li>The result of subprocess.run() should contain a 'token-' prefix in the stdout and stderr after refresher.invalidate() is called.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">28 lines (ranges: 59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156, 160-162)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh.py::TestTokenRefresher::test_missing_json_key</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that TokenRefresher raises error when JSON key is missing.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where the token refresh process fails due to a missing required JSON key.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'token' key should be present in the output of the get_token method.</li>
                                            <li>A message indicating that the 'token' key was not found should be returned by the get_token method.</li>
                                            <li>The error message should include the word 'not found'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">28 lines (ranges: 59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-135, 139-141, 149)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh.py::TestTokenRefresher::test_thread_safety</span>
                            <div class="test-meta">
                                <span>52ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test TokenRefresher thread safety by starting multiple threads concurrently and verifying that they all retrieve the same token.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where multiple threads trying to refresh tokens at the same time could result in different tokens being retrieved, potentially leading to inconsistencies in the system.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>All threads should acquire the lock before getting the token and release it after getting the token.</li>
                                            <li>The first thread to acquire the lock should get the token with value 'token-1'.</li>
                                            <li>No other thread should get a different token than 'token-1'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">28 lines (ranges: 59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh.py::TestTokenRefresher::test_timeout_handling</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> The test verifies that TokenRefresher handles command timeouts correctly.</p>
                                    <p><strong>Why Needed:</strong> This test prevents the potential bug where a timeout occurs when trying to refresh tokens with an already timed-out command.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `get_token()` method of `TokenRefresher` raises a `TokenRefreshError` exception when the specified command times out.</li>
                                            <li>The error message 'timed out' is present in the error message returned by the `pytest.raises(TokenRefreshError)` context manager.</li>
                                            <li>The test asserts that the error message contains the string 'timed out'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">16 lines (ranges: 59-60, 63, 69, 83, 85-86, 90, 93-98, 113-114)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh.py::TestTokenRefresher::test_token_caching</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test Token Caching: Verify that the TokenRefresher caches tokens and does not call the command again.</p>
                                    <p><strong>Why Needed:</strong> This test prevents a potential bug where the `TokenRefresher` calls the command multiple times if it is caching tokens.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `token-cached` scenario verifies that the cached token is used instead of calling the command again.</li>
                                            <li>The `same-token` scenario verifies that the same token is returned even after a successful cache.</li>
                                            <li>The `only-called-once` scenario verifies that only one call to the `get_token()` method is made by the TokenRefresher.</li>
                                            <li>The `call_count` assertion verifies that the correct number of calls to the `run()` function are made.</li>
                                            <li>The `output_format` and `stderr` assertions verify that the output format and error message match the expected values.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">28 lines (ranges: 59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
            <div class="test-file-group">
                <div class="test-file-header">
                    <span>üìÑ tests/test_token_refresh_coverage.py</span>
                    <span style="font-size: 0.9rem; font-weight: 400; color: var(--text-secondary)">9 tests</span>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_command_failure_no_stderr</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the TokenRefresher's behavior when a command fails with no stderr output.</p>
                                    <p><strong>Why Needed:</strong> Prevent regression that may occur if the test case does not handle command failures properly.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `get_token()` method of the `TokenRefresher` should raise a `TokenRefreshError` with an exit code of 1 when the command fails without producing any stderr output.</li>
                                            <li>The error message 'No error output' should be present in the exception message.</li>
                                            <li>The test case should fail when the command returns an exit code other than 0 or 1.</li>
                                            <li>The `get_token()` method should not raise a `TokenRefreshError` if the command produces stderr output.</li>
                                            <li>The `get_token()` method should return an error message indicating that no error occurred.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">20 lines (ranges: 59-60, 63, 69, 83, 85-86, 90, 93-98, 101-104, 113, 115)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_empty_command_string</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test handling of empty command string.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where an empty command string is passed to the TokenRefresher, causing it to raise a TokenRefreshError without providing any meaningful error message.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `get_token()` method raises a `TokenRefreshError` with the message 'empty'.</li>
                                            <li>The assertion checks if the error message contains the word 'empty'.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 59-60, 63, 69, 83, 85-86, 90-91, 113, 115)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_invalid_command_string</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test handling of invalid command string (shlex parse error).</p>
                                    <p><strong>Why Needed:</strong> To prevent a TokenRefreshError caused by an invalid shell syntax in the command string.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'command' parameter should be a valid shlex-quoted string.</li>
                                            <li>Invalid characters or quotes in the 'command' parameter will raise a TokenRefreshError.</li>
                                            <li>The 'refresh_interval' and 'output_format' parameters are not affected by the 'command' parameter.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 59-60, 63, 69, 83, 85-88, 113, 115)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_json_not_dict</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test that a TokenRefresher fails when the output of get-token is not a dictionary.</p>
                                    <p><strong>Why Needed:</strong> Prevents a potential bug where a non-dictionary JSON output causes a TokenRefreshError.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert 'Expected JSON object' in str(exc_info.value)</li>
                                            <li>assert 'list' in str(exc_info.value)</li>
                                            <li>assert 'Invalid JSON format' in str(exc_info.value)</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">27 lines (ranges: 59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-137, 149)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_json_token_empty_string</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test handling when token value is an empty string.</p>
                                    <p><strong>Why Needed:</strong> Prevents TokenRefreshError due to incorrect JSON output from subprocess.run().</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The 'stdout' of the subprocess.run() call should be a JSON object with a single key-value pair.</li>
                                            <li>The 'json.dumps()' function should return an empty string if the input is an empty or not a string.</li>
                                            <li>The error message should contain the phrase 'empty or not a string'.</li>
                                            <li>The 'stderr' of the subprocess.run() call should be an empty string.</li>
                                            <li>The 'returncode' of the subprocess.run() call should be 0 (success).</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">30 lines (ranges: 59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-135, 139, 143-146, 149)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_json_token_not_string</span>
                            <div class="test-meta">
                                <span>3ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test handling when token value is not a string.</p>
                                    <p><strong>Why Needed:</strong> Prevents regression where the TokenRefresher incorrectly handles non-string token values.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert 'empty or not a string' in str(exc_info.value)</li>
                                            <li>assert exc_info.value.args[0].startswith('TokenRefreshError')</li>
                                            <li>assert isinstance(exc_info.value.args[1], str)</li>
                                            <li>assert json.dumps({'token': 12345}) is None</li>
                                            <li>assert 'token' not in exc_info.value.args[2]</li>
                                            <li>assert exc_info.value.args[3] == ''</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">30 lines (ranges: 59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-135, 139, 143-146, 149)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_oserror_on_execution</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test verifies that a TokenRefresher throws an exception when executing a command that does not exist.</p>
                                    <p><strong>Why Needed:</strong> To prevent a potential bug where the TokenRefresher fails to refresh tokens due to a non-existent command.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `get_token()` method raises a `TokenRefreshError` with a message indicating 'Failed to execute'.</li>
                                            <li>The `get_token()` method does not raise an exception when executing a nonexistent command.</li>
                                            <li>The `refresh_interval` is set to 3600 seconds, which may cause issues if the command takes longer than this interval to complete.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">19 lines (ranges: 59-60, 63, 69, 83, 85-86, 90, 93-98, 113, 115-118)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_text_only_whitespace_lines</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test handling when text output has only whitespace lines after initial strip.</p>
                                    <p><strong>Why Needed:</strong> Prevents TokenRefreshError due to incorrect parsing of text with only blank lines.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>assert 'No non-empty lines' in str(exc_info.value) for exc_info in [pytest.raises(TokenRefreshError)]</li>
                                            <li>assert len(exc_info.value.args[0].splitlines()) == 1</li>
                                            <li>assert all(line.strip() == '' for line in exc_info.value.args[0].splitlines())</li>
                                            <li>assert 'Only whitespace lines' not in exc_info.value.args[0]</li>
                                            <li>assert 'No non-empty lines' not in exc_info.value.args[0]</li>
                                            <li>assert len(exc_info.value.args[1]) > 0</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">4 lines (ranges: 132, 153-155)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
                <div class="test-row outcome-passed" data-status="passed">
                    <details open>
                        <summary class="test-header">
                            <span class="status-badge status-passed">PASSED</span>
                            <span class="test-name">tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_whitespace_only_command</span>
                            <div class="test-meta">
                                <span>1ms</span>
                                <span title="Covered file count">üõ°Ô∏è 3</span>
                            </div>
                        </summary>

                        <div class="test-details">

                            <div class="detail-section">
                                <div class="detail-title">AI Assessment</div>
                                <div class="llm-annotation">
                                    <p><strong>Scenario:</strong> Test the test_whitespace_only_command to ensure it raises a TokenRefreshError for an empty command string.</p>
                                    <p><strong>Why Needed:</strong> To prevent a potential bug where the token refresh fails due to an empty command string, which could lead to unexpected behavior or errors in the system.</p>
                                    <div class="key-assertions">
                                        <strong>Key Assertions:</strong>
                                        <ul>
                                            <li>The `get_token()` method of the `TokenRefresher` class should raise a `TokenRefreshError` when given an empty command string.</li>
                                            <li>The error message returned by the `get_token()` method should indicate that the input was empty.</li>
                                            <li>The `str(exc_info.value).lower()` assertion should check for the presence of 'empty' in the error message, ensuring it's not just a generic error message.</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>

                            <div class="detail-section">
                                <div class="detail-title">Coverage</div>
                                <div class="coverage-list">
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/collector.py</span>
                                        <span style="color: var(--text-secondary)">14 lines (ranges: 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/llm/token_refresh.py</span>
                                        <span style="color: var(--text-secondary)">11 lines (ranges: 59-60, 63, 69, 83, 85-86, 90-91, 113, 115)</span>
                                    </div>
                                    <div class="coverage-item" style="padding: 0.5rem 1rem;">
                                        <span>src/pytest_llm_report/plugin.py</span>
                                        <span style="color: var(--text-secondary)">6 lines (ranges: 363-364, 367, 371-373)</span>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </details>
                </div>
            </div>
        </div>
        </section>
    </div>
</body>
</html>